{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "372ef2e3-5227-45e6-b342-4b37bb9ead7f",
   "metadata": {},
   "source": [
    "# Real-Time Foot Traffic Prediction using Spark Streaming + Kafka + Trained Random Forest Models\n",
    "\n",
    "This script connects to a live Kafka stream (`mta_turnstile_topic`), extracts and parses NYC turnstile station data, and applies pre-trained Random Forest regression models to **predict ENTRIES and EXITS** for each station-hour-day combination in real-time.\n",
    "\n",
    "Key components:\n",
    "- **Kafka Stream Ingestion** (using `spark-sql-kafka`)\n",
    "- **Timestamp Feature Extraction** (`hour`, `day_of_week`)\n",
    "- **Batch Inference** using saved `PipelineModel`s from prior training\n",
    "- **Streaming Output** printed using `joined_pred.show()` for debugging\n",
    "\n",
    "This pipeline enables predictive monitoring of NYC subway foot traffic as data flows in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a8808ab-2621-4ae8-ab40-85cec78cc59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream Prediction Script (fixed to match ENTRY_COUNT/EXIT_COUNT training pipeline)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql.functions import to_timestamp, hour, date_format, col, concat_ws, expr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e56bbe-a3be-4a51-8a42-3492fa9d273f",
   "metadata": {},
   "source": [
    "## Connect to Kafka Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49b9c79a-1fe7-40ab-8ef0-21bdd4148d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/anaconda3/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/gopalakrishnaabba/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/gopalakrishnaabba/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ee8048d6-150a-4a5f-8f5f-98adca0711ac;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 379ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ee8048d6-150a-4a5f-8f5f-98adca0711ac\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/7ms)\n",
      "25/05/05 11:19:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/05 11:19:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/05/05 11:19:20 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MTA_Stream_Predict\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89022b3-09ef-4a63-bfcf-b70528da0bee",
   "metadata": {},
   "source": [
    "- Connects to Kafka broker at `localhost:9092`.\n",
    "- Subscribes to the topic named `mta_turnstile_topic`.\n",
    "- Reads only new messages (latest offset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dd2f133-7ea8-4c5b-87e1-436203645b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read Kafka Stream\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"mta_turnstile_topic\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Step 2: Parse Kafka value\n",
    "turnstile_values = kafka_df.selectExpr(\"CAST(value AS STRING) as csv\")\n",
    "split_col = split(turnstile_values[\"csv\"], \",\")\n",
    "\n",
    "# Step 3: Extract relevant column (STATION only)\n",
    "\n",
    "\n",
    "\n",
    "station_df = turnstile_values.select(\n",
    "    split_col.getItem(3).alias(\"STATION\"),\n",
    "    split_col.getItem(6).alias(\"DATE\"),\n",
    "    split_col.getItem(7).alias(\"TIME\")\n",
    ").withColumn(\n",
    "    \"timestamp\", to_timestamp(concat_ws(\" \", col(\"DATE\"), col(\"TIME\")), \"MM/dd/yyyy HH:mm:ss\")\n",
    ").withColumn(\n",
    "    \"hour\", hour(col(\"timestamp\"))\n",
    ").withColumn(\n",
    "    \"day_of_week\", expr(\"extract(DAYOFWEEK FROM timestamp)\")\n",
    ").select(\"STATION\", \"hour\", \"day_of_week\").select([\"STATION\", \"hour\", \"day_of_week\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c41903-9a3f-4f42-86ab-b1a43c5f3853",
   "metadata": {},
   "source": [
    "- Converts Kafkaâ€™s byte stream to strings.\n",
    "\n",
    "- Splits the CSV string into individual fields.\n",
    "\n",
    "- Extracts relevant columns:\n",
    "\n",
    "    `STATION` (index 3)\n",
    "\n",
    "    `DATE` (index 6)\n",
    "\n",
    "    `TIME` (index 7)\n",
    "\n",
    "- Combines date and time into a proper timestamp.\n",
    "\n",
    "- Derives hour and `day_of_week` features for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031804aa-cc8f-43a9-bb41-352bad68a5a4",
   "metadata": {},
   "source": [
    "## Define Batch Prediction Function\n",
    "\n",
    "- Loads pre-trained Random Forest models for predicting ENTRIES and EXITS.\n",
    "- Applies them to each mini-batch of incoming data.\n",
    "- Joins both predictions into a unified DataFrame.\n",
    "- Prints results to the console.\n",
    "- Starts streaming query on the parsed station data.\n",
    "- Applies prediction function to each batch.\n",
    "- Keeps running until manually terminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3f021aa-dd5a-4f51-a3f3-360188476759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:19:23 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-cd96399d-fd05-4554-b499-c690c79bdd70. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/05 11:19:23 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/05/05 11:19:24 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----------+-----------------+---------------+\n",
      "|STATION|hour|day_of_week|predicted_ENTRIES|predicted_EXITS|\n",
      "+-------+----+-----------+-----------------+---------------+\n",
      "+-------+----+-----------+-----------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenJDK 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "OpenJDK 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=38240Kb max_used=38247Kb free=92831Kb\n",
      " bounds [0x00000001048e4000, 0x0000000106e74000, 0x000000010c8e4000]\n",
      " total_blobs=14740 nmethods=13618 adapters=1032\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+-----------+-----------------+----------------+\n",
      "|STATION      |hour|day_of_week|predicted_ENTRIES|predicted_EXITS |\n",
      "+-------------+----+-----------+-----------------+----------------+\n",
      "|TIME SQ-42 ST|11  |2          |8.327236983356865|8.85129257750241|\n",
      "+-------------+----+-----------+-----------------+----------------+\n",
      "\n",
      "+---------------+----+-----------+-----------------+-----------------+\n",
      "|STATION        |hour|day_of_week|predicted_ENTRIES|predicted_EXITS  |\n",
      "+---------------+----+-----------+-----------------+-----------------+\n",
      "|34 ST-HERALD SQ|11  |2          |8.763883182670865|8.962312789392662|\n",
      "+---------------+----+-----------+-----------------+-----------------+\n",
      "\n",
      "+-------+----+-----------+-----------------+----------------+\n",
      "|STATION|hour|day_of_week|predicted_ENTRIES|predicted_EXITS |\n",
      "+-------+----+-----------+-----------------+----------------+\n",
      "|23 ST  |11  |2          |8.439633063456519|8.69550202229375|\n",
      "+-------+----+-----------+-----------------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+-----------+-----------------+-----------------+\n",
      "|STATION        |hour|day_of_week|predicted_ENTRIES|predicted_EXITS  |\n",
      "+---------------+----+-----------+-----------------+-----------------+\n",
      "|WORLD TRADE CTR|11  |2          |8.327743794050125|8.711265964234162|\n",
      "+---------------+----+-----------+-----------------+-----------------+\n",
      "\n",
      "+-------+----+-----------+-----------------+----------------+\n",
      "|STATION|hour|day_of_week|predicted_ENTRIES|predicted_EXITS |\n",
      "+-------+----+-----------+-----------------+----------------+\n",
      "|23 ST  |11  |2          |8.439633063456519|8.69550202229375|\n",
      "+-------+----+-----------+-----------------+----------------+\n",
      "\n",
      "+---------------+----+-----------+-----------------+-----------------+\n",
      "|STATION        |hour|day_of_week|predicted_ENTRIES|predicted_EXITS  |\n",
      "+---------------+----+-----------+-----------------+-----------------+\n",
      "|34 ST-HERALD SQ|11  |2          |8.763883182670865|8.962312789392662|\n",
      "+---------------+----+-----------+-----------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 32\u001b[0m\n\u001b[1;32m     24\u001b[0m     joined_pred\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Step 5: Run streaming query with foreachBatch\u001b[39;00m\n\u001b[1;32m     28\u001b[0m station_df\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;241m.\u001b[39mforeachBatch(predict_batch) \\\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;241m.\u001b[39mstart() \\\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;241m.\u001b[39mawaitTermination()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/sql/streaming/query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Step 4: Define function to apply model in each batch\n",
    "\n",
    "def predict_batch(batch_df, batch_id):\n",
    "    # Load trained models\n",
    "    entries_model = PipelineModel.load(\"/Users/gopalakrishnaabba/mta_rf_entries_model\")\n",
    "    exits_model   = PipelineModel.load(\"/Users/gopalakrishnaabba/mta_rf_exits_model\")\n",
    "\n",
    "    # Ensure correct feature columns are passed to both models\n",
    "    input_cols = [\"STATION\", \"hour\", \"day_of_week\"]\n",
    "    \n",
    "    entries_pred = entries_model.transform(batch_df.select(*input_cols)) \\\n",
    "        .select(\"STATION\", \"hour\", \"day_of_week\", \"prediction\") \\\n",
    "        .withColumnRenamed(\"prediction\", \"predicted_ENTRIES\")\n",
    "    \n",
    "    exits_pred = exits_model.transform(batch_df.select(*input_cols)) \\\n",
    "        .select(\"STATION\", \"hour\", \"day_of_week\", \"prediction\") \\\n",
    "        .withColumnRenamed(\"prediction\", \"predicted_EXITS\")\n",
    "\n",
    "    # Join predictions\n",
    "    joined_pred = entries_pred.join(\n",
    "        exits_pred, on=[\"STATION\", \"hour\", \"day_of_week\"], how=\"inner\"\n",
    "    )\n",
    "\n",
    "    joined_pred.show(truncate=False)\n",
    "\n",
    "\n",
    "# Step 5: Run streaming query with foreachBatch\n",
    "station_df.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .foreachBatch(predict_batch) \\\n",
    "    .start() \\\n",
    "    .awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1923959-37a4-49ad-a01d-cb5c9d2bc60a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
