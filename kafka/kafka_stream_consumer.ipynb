{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "567276f5-07a6-4044-96f7-d6166645e2ba",
   "metadata": {},
   "source": [
    "# Real-Time Turnstile Data Ingestion Pipeline using PySpark, Kafka, and MongoDB\n",
    "\n",
    "This script establishes a real-time data pipeline that ingests simulated MTA subway turnstile events from Kafka, parses the raw CSV format using PySpark, and stores the structured data into:\n",
    "- A **MongoDB collection** for persistent storage,\n",
    "- A **console sink** for debugging, and\n",
    "- An optional **in-memory table** for live SQL queries.\n",
    "\n",
    "The pipeline ensures low-latency ETL and serves as the foundation for foot traffic prediction and analytics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b415fc2-c758-4900-a798-70887d073b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8226bf70-703a-4cb0-955d-f9ddc30457aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6335340b-f324-4513-a843-f8783c9a877b",
   "metadata": {},
   "source": [
    "## Initialize Spark Session with Kafka & MongoDB Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2962748-b689-4c5b-9891-2e47063604fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/anaconda3/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/gopalakrishnaabba/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/gopalakrishnaabba/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-26371bfb-5195-4871-9138-d7bbc1ecc233;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 430ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   15  |   0   |   0   |   0   ||   15  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-26371bfb-5195-4871-9138-d7bbc1ecc233\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 15 already retrieved (0kB/6ms)\n",
      "25/05/05 11:17:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MTA_Turnstile_Streaming\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,\" \\\n",
    "            \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b37fef8-f374-43b0-9594-48f65a022b9c",
   "metadata": {},
   "source": [
    "Creates a Spark session with Kafka and MongoDB connectors loaded, enabling structured streaming and sink capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9c8eba-3cd4-4a7a-a5f6-f4bba01e1590",
   "metadata": {},
   "source": [
    "## Define Schema for Turnstile Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70568db7-9e95-4bc3-be28-ac12f9558a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define schema of MTA turnstile data\n",
    "turnstile_schema = StructType([\n",
    "    StructField(\"C/A\", StringType(), True),\n",
    "    StructField(\"UNIT\", StringType(), True),\n",
    "    StructField(\"SCP\", StringType(), True),\n",
    "    StructField(\"STATION\", StringType(), True),\n",
    "    StructField(\"LINENAME\", StringType(), True),\n",
    "    StructField(\"DIVISION\", StringType(), True),\n",
    "    StructField(\"DATE\", StringType(), True),\n",
    "    StructField(\"TIME\", StringType(), True),\n",
    "    StructField(\"DESC\", StringType(), True),\n",
    "    StructField(\"ENTRIES\", IntegerType(), True),\n",
    "    StructField(\"EXITS\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0477b40c-acce-43af-a8e5-580a9c2eb77b",
   "metadata": {},
   "source": [
    "Specifies the structure of the incoming Kafka CSV data, allowing correct parsing into DataFrame columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34654b4a-92ba-421f-a7f3-a0cd7b1b84ea",
   "metadata": {},
   "source": [
    "## Read Stream from Kafka Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc399084-b09a-42fe-9899-c9bde1579e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:17:11 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "25/05/05 11:17:11 INFO SharedState: Warehouse path is 'file:/Users/gopalakrishnaabba/spark-warehouse'.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Read data from Kafka\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"mta_turnstile_topic\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a39cd4-2271-40af-875d-3210e49b9509",
   "metadata": {},
   "source": [
    "Connects to the Kafka topic `mta_turnstile_topic` and reads incoming data as a stream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c761c5-7b4f-4ce5-bbe6-adf5b73ab9aa",
   "metadata": {},
   "source": [
    "## Parse Kafka Message Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e3e56a5-92bc-4e24-b6de-2027a356fb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Parse the Kafka 'value' as CSV\n",
    "turnstile_values = kafka_df.selectExpr(\"CAST(value AS STRING) as csv\")\n",
    "split_col = split(turnstile_values['csv'], ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dbf546-b638-4392-9ee7-556c0c4f4292",
   "metadata": {},
   "source": [
    "Converts the Kafka byte stream to string and splits the CSV format into an array for column extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d54f600-3383-4892-9316-23d98b27a493",
   "metadata": {},
   "source": [
    "## Extract Columns from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7e9de02-ca8f-44e2-bf5e-fd7f87b42866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Extract structured columns\n",
    "turnstile_df = turnstile_values.select(\n",
    "    split_col.getItem(0).alias(\"C/A\"),\n",
    "    split_col.getItem(1).alias(\"UNIT\"),\n",
    "    split_col.getItem(2).alias(\"SCP\"),\n",
    "    split_col.getItem(3).alias(\"STATION\"),\n",
    "    split_col.getItem(4).alias(\"LINENAME\"),\n",
    "    split_col.getItem(5).alias(\"DIVISION\"),\n",
    "    split_col.getItem(6).alias(\"DATE\"),\n",
    "    split_col.getItem(7).alias(\"TIME\"),\n",
    "    split_col.getItem(8).alias(\"DESC\"),\n",
    "    split_col.getItem(9).cast(\"integer\").alias(\"ENTRIES\"),\n",
    "    split_col.getItem(10).cast(\"integer\").alias(\"EXITS\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831dec40-49fb-4153-bc23-ab9c0fa4df2e",
   "metadata": {},
   "source": [
    "Maps each field in the split array to a named DataFrame column, applying proper types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "447111be-2c52-4b37-9178-185aa98d3a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Use raw (non-aggregated) entries/exits instead of groupBy sum\n",
    "raw_df = turnstile_df.select(\n",
    "    \"C/A\", \"UNIT\", \"SCP\", \"STATION\", \"DATE\", \"TIME\", \"DESC\",\n",
    "    col(\"ENTRIES\").alias(\"ENTRY_COUNT\"),\n",
    "    col(\"EXITS\").alias(\"EXIT_COUNT\")\n",
    ")\n",
    "\n",
    "# Add a timestamp for ingestion time\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "raw_df = raw_df.withColumn(\"ingest_time\", current_timestamp())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56047163-3bc4-414c-b9b8-f863ad84719e",
   "metadata": {},
   "source": [
    "## Sink \n",
    "\n",
    "- Console Sink – Print Output to Terminal: Streams the parsed data to the console in real-time. Good for debugging or quick checks.\n",
    "\n",
    "- MongoDB Sink – Store Raw Data: Writes each micro-batch of raw turnstile data to a MongoDB collection named `raw_turnstile`.\n",
    "\n",
    "- Memory Sink – In-Memory Table: Registers the live data as an in-memory temporary table for Spark SQL queries during streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35861f6c-5a64-4cd3-9db5-1efc8c8630fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:17:13 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\n",
      "25/05/05 11:17:13 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/05 11:17:13 INFO ResolveWriteToStream: Checkpoint root file:///private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da resolved to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da.\n",
      "25/05/05 11:17:13 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/05/05 11:17:14 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/metadata using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/.metadata.ba051948-47b3-4357-ada6-179bc1048ac9.tmp\n",
      "25/05/05 11:17:14 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/.metadata.ba051948-47b3-4357-ada6-179bc1048ac9.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/metadata\n",
      "25/05/05 11:17:14 INFO MicroBatchExecution: Starting [id = 15579493-3c33-4783-b8e7-e927b8c92a74, runId = 6025a43e-17ac-47e3-b17e-2ebfc0d8e559]. Use file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da to store the query checkpoint.\n",
      "25/05/05 11:17:14 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@562bc25f] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@7d017bc8]\n",
      "25/05/05 11:17:14 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/05 11:17:14 INFO ResolveWriteToStream: Checkpoint root file:///private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c resolved to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c.\n",
      "25/05/05 11:17:14 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/05/05 11:17:14 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/metadata using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/.metadata.f27a39db-b429-4e3a-bb3c-ceaaac0774e1.tmp\n",
      "25/05/05 11:17:14 INFO OffsetSeqLog: BatchIds found from listing: \n",
      "25/05/05 11:17:14 INFO OffsetSeqLog: BatchIds found from listing: \n",
      "25/05/05 11:17:14 INFO MicroBatchExecution: Starting new streaming query.\n",
      "25/05/05 11:17:14 INFO MicroBatchExecution: Stream started from {}\n",
      "25/05/05 11:17:14 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/.metadata.f27a39db-b429-4e3a-bb3c-ceaaac0774e1.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/metadata\n",
      "25/05/05 11:17:14 INFO MicroBatchExecution: Starting [id = 158f6c2b-9de3-4554-a13e-4b3f3a8337e6, runId = 0a203767-4c11-40ea-b6d3-4932e8d107ad]. Use file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c to store the query checkpoint.\n",
      "25/05/05 11:17:14 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@562bc25f] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@7d017bc8]\n",
      "25/05/05 11:17:14 INFO OffsetSeqLog: BatchIds found from listing: \n",
      "25/05/05 11:17:14 INFO OffsetSeqLog: BatchIds found from listing: \n",
      "25/05/05 11:17:14 INFO MicroBatchExecution: Starting new streaming query.\n",
      "25/05/05 11:17:14 INFO MicroBatchExecution: Stream started from {}\n",
      "25/05/05 11:17:14 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/05 11:17:14 INFO ResolveWriteToStream: Checkpoint root file:///private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6 resolved to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6.\n",
      "25/05/05 11:17:14 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/05/05 11:17:14 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/metadata using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/.metadata.1c86949a-f287-46c6-b772-d6e60767fe62.tmp\n",
      "25/05/05 11:17:14 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/.metadata.1c86949a-f287-46c6-b772-d6e60767fe62.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/metadata\n",
      "25/05/05 11:17:14 INFO MicroBatchExecution: Starting live_turnstile [id = 5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa, runId = f332de59-d50a-450a-89c9-a1e60a2a0e5a]. Use file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6 to store the query checkpoint.\n",
      "25/05/05 11:17:14 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@562bc25f] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@7d017bc8]\n",
      "25/05/05 11:17:14 INFO OffsetSeqLog: BatchIds found from listing: \n",
      "25/05/05 11:17:14 INFO OffsetSeqLog: BatchIds found from listing: \n",
      "25/05/05 11:17:14 INFO MicroBatchExecution: Starting new streaming query.\n",
      "25/05/05 11:17:14 INFO MicroBatchExecution: Stream started from {}\n",
      "25/05/05 11:17:14 INFO AdminClientConfig: AdminClientConfig values: \n",
      "\tauto.include.jmx.reporter = true\n",
      "\tbootstrap.servers = [localhost:9092]\n",
      "\tclient.dns.lookup = use_all_dns_ips\n",
      "\tclient.id = \n",
      "\tconnections.max.idle.ms = 300000\n",
      "\tdefault.api.timeout.ms = 60000\n",
      "\tmetadata.max.age.ms = 300000\n",
      "\tmetric.reporters = []\n",
      "\tmetrics.num.samples = 2\n",
      "\tmetrics.recording.level = INFO\n",
      "\tmetrics.sample.window.ms = 30000\n",
      "\treceive.buffer.bytes = 65536\n",
      "\treconnect.backoff.max.ms = 1000\n",
      "\treconnect.backoff.ms = 50\n",
      "\trequest.timeout.ms = 30000\n",
      "\tretries = 2147483647\n",
      "\tretry.backoff.ms = 100\n",
      "\tsasl.client.callback.handler.class = null\n",
      "\tsasl.jaas.config = null\n",
      "\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n",
      "\tsasl.kerberos.min.time.before.relogin = 60000\n",
      "\tsasl.kerberos.service.name = null\n",
      "\tsasl.kerberos.ticket.renew.jitter = 0.05\n",
      "\tsasl.kerberos.ticket.renew.window.factor = 0.8\n",
      "\tsasl.login.callback.handler.class = null\n",
      "\tsasl.login.class = null\n",
      "\tsasl.login.connect.timeout.ms = null\n",
      "\tsasl.login.read.timeout.ms = null\n",
      "\tsasl.login.refresh.buffer.seconds = 300\n",
      "\tsasl.login.refresh.min.period.seconds = 60\n",
      "\tsasl.login.refresh.window.factor = 0.8\n",
      "\tsasl.login.refresh.window.jitter = 0.05\n",
      "\tsasl.login.retry.backoff.max.ms = 10000\n",
      "\tsasl.login.retry.backoff.ms = 100\n",
      "\tsasl.mechanism = GSSAPI\n",
      "\tsasl.oauthbearer.clock.skew.seconds = 30\n",
      "\tsasl.oauthbearer.expected.audience = null\n",
      "\tsasl.oauthbearer.expected.issuer = null\n",
      "\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n",
      "\tsasl.oauthbearer.jwks.endpoint.url = null\n",
      "\tsasl.oauthbearer.scope.claim.name = scope\n",
      "\tsasl.oauthbearer.sub.claim.name = sub\n",
      "\tsasl.oauthbearer.token.endpoint.url = null\n",
      "\tsecurity.protocol = PLAINTEXT\n",
      "\tsecurity.providers = null\n",
      "\tsend.buffer.bytes = 131072\n",
      "\tsocket.connection.setup.timeout.max.ms = 30000\n",
      "\tsocket.connection.setup.timeout.ms = 10000\n",
      "\tssl.cipher.suites = null\n",
      "\tssl.enabled.protocols = [TLSv1.2]\n",
      "\tssl.endpoint.identification.algorithm = https\n",
      "\tssl.engine.factory.class = null\n",
      "\tssl.key.password = null\n",
      "\tssl.keymanager.algorithm = SunX509\n",
      "\tssl.keystore.certificate.chain = null\n",
      "\tssl.keystore.key = null\n",
      "\tssl.keystore.location = null\n",
      "\tssl.keystore.password = null\n",
      "\tssl.keystore.type = JKS\n",
      "\tssl.protocol = TLSv1.2\n",
      "\tssl.provider = null\n",
      "\tssl.secure.random.implementation = null\n",
      "\tssl.trustmanager.algorithm = PKIX\n",
      "\tssl.truststore.certificates = null\n",
      "\tssl.truststore.location = null\n",
      "\tssl.truststore.password = null\n",
      "\tssl.truststore.type = JKS\n",
      "\n",
      "25/05/05 11:17:14 INFO AdminClientConfig: AdminClientConfig values: \n",
      "\tauto.include.jmx.reporter = true\n",
      "\tbootstrap.servers = [localhost:9092]\n",
      "\tclient.dns.lookup = use_all_dns_ips\n",
      "\tclient.id = \n",
      "\tconnections.max.idle.ms = 300000\n",
      "\tdefault.api.timeout.ms = 60000\n",
      "\tmetadata.max.age.ms = 300000\n",
      "\tmetric.reporters = []\n",
      "\tmetrics.num.samples = 2\n",
      "\tmetrics.recording.level = INFO\n",
      "\tmetrics.sample.window.ms = 30000\n",
      "\treceive.buffer.bytes = 65536\n",
      "\treconnect.backoff.max.ms = 1000\n",
      "\treconnect.backoff.ms = 50\n",
      "\trequest.timeout.ms = 30000\n",
      "\tretries = 2147483647\n",
      "\tretry.backoff.ms = 100\n",
      "\tsasl.client.callback.handler.class = null\n",
      "\tsasl.jaas.config = null\n",
      "\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n",
      "\tsasl.kerberos.min.time.before.relogin = 60000\n",
      "\tsasl.kerberos.service.name = null\n",
      "\tsasl.kerberos.ticket.renew.jitter = 0.05\n",
      "\tsasl.kerberos.ticket.renew.window.factor = 0.8\n",
      "\tsasl.login.callback.handler.class = null\n",
      "\tsasl.login.class = null\n",
      "\tsasl.login.connect.timeout.ms = null\n",
      "\tsasl.login.read.timeout.ms = null\n",
      "\tsasl.login.refresh.buffer.seconds = 300\n",
      "\tsasl.login.refresh.min.period.seconds = 60\n",
      "\tsasl.login.refresh.window.factor = 0.8\n",
      "\tsasl.login.refresh.window.jitter = 0.05\n",
      "\tsasl.login.retry.backoff.max.ms = 10000\n",
      "\tsasl.login.retry.backoff.ms = 100\n",
      "\tsasl.mechanism = GSSAPI\n",
      "\tsasl.oauthbearer.clock.skew.seconds = 30\n",
      "\tsasl.oauthbearer.expected.audience = null\n",
      "\tsasl.oauthbearer.expected.issuer = null\n",
      "\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n",
      "\tsasl.oauthbearer.jwks.endpoint.url = null\n",
      "\tsasl.oauthbearer.scope.claim.name = scope\n",
      "\tsasl.oauthbearer.sub.claim.name = sub\n",
      "\tsasl.oauthbearer.token.endpoint.url = null\n",
      "\tsecurity.protocol = PLAINTEXT\n",
      "\tsecurity.providers = null\n",
      "\tsend.buffer.bytes = 131072\n",
      "\tsocket.connection.setup.timeout.max.ms = 30000\n",
      "\tsocket.connection.setup.timeout.ms = 10000\n",
      "\tssl.cipher.suites = null\n",
      "\tssl.enabled.protocols = [TLSv1.2]\n",
      "\tssl.endpoint.identification.algorithm = https\n",
      "\tssl.engine.factory.class = null\n",
      "\tssl.key.password = null\n",
      "\tssl.keymanager.algorithm = SunX509\n",
      "\tssl.keystore.certificate.chain = null\n",
      "\tssl.keystore.key = null\n",
      "\tssl.keystore.location = null\n",
      "\tssl.keystore.password = null\n",
      "\tssl.keystore.type = JKS\n",
      "\tssl.protocol = TLSv1.2\n",
      "\tssl.provider = null\n",
      "\tssl.secure.random.implementation = null\n",
      "\tssl.trustmanager.algorithm = PKIX\n",
      "\tssl.truststore.certificates = null\n",
      "\tssl.truststore.location = null\n",
      "\tssl.truststore.password = null\n",
      "\tssl.truststore.type = JKS\n",
      "\n",
      "25/05/05 11:17:14 INFO AdminClientConfig: AdminClientConfig values: \n",
      "\tauto.include.jmx.reporter = true\n",
      "\tbootstrap.servers = [localhost:9092]\n",
      "\tclient.dns.lookup = use_all_dns_ips\n",
      "\tclient.id = \n",
      "\tconnections.max.idle.ms = 300000\n",
      "\tdefault.api.timeout.ms = 60000\n",
      "\tmetadata.max.age.ms = 300000\n",
      "\tmetric.reporters = []\n",
      "\tmetrics.num.samples = 2\n",
      "\tmetrics.recording.level = INFO\n",
      "\tmetrics.sample.window.ms = 30000\n",
      "\treceive.buffer.bytes = 65536\n",
      "\treconnect.backoff.max.ms = 1000\n",
      "\treconnect.backoff.ms = 50\n",
      "\trequest.timeout.ms = 30000\n",
      "\tretries = 2147483647\n",
      "\tretry.backoff.ms = 100\n",
      "\tsasl.client.callback.handler.class = null\n",
      "\tsasl.jaas.config = null\n",
      "\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n",
      "\tsasl.kerberos.min.time.before.relogin = 60000\n",
      "\tsasl.kerberos.service.name = null\n",
      "\tsasl.kerberos.ticket.renew.jitter = 0.05\n",
      "\tsasl.kerberos.ticket.renew.window.factor = 0.8\n",
      "\tsasl.login.callback.handler.class = null\n",
      "\tsasl.login.class = null\n",
      "\tsasl.login.connect.timeout.ms = null\n",
      "\tsasl.login.read.timeout.ms = null\n",
      "\tsasl.login.refresh.buffer.seconds = 300\n",
      "\tsasl.login.refresh.min.period.seconds = 60\n",
      "\tsasl.login.refresh.window.factor = 0.8\n",
      "\tsasl.login.refresh.window.jitter = 0.05\n",
      "\tsasl.login.retry.backoff.max.ms = 10000\n",
      "\tsasl.login.retry.backoff.ms = 100\n",
      "\tsasl.mechanism = GSSAPI\n",
      "\tsasl.oauthbearer.clock.skew.seconds = 30\n",
      "\tsasl.oauthbearer.expected.audience = null\n",
      "\tsasl.oauthbearer.expected.issuer = null\n",
      "\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n",
      "\tsasl.oauthbearer.jwks.endpoint.url = null\n",
      "\tsasl.oauthbearer.scope.claim.name = scope\n",
      "\tsasl.oauthbearer.sub.claim.name = sub\n",
      "\tsasl.oauthbearer.token.endpoint.url = null\n",
      "\tsecurity.protocol = PLAINTEXT\n",
      "\tsecurity.providers = null\n",
      "\tsend.buffer.bytes = 131072\n",
      "\tsocket.connection.setup.timeout.max.ms = 30000\n",
      "\tsocket.connection.setup.timeout.ms = 10000\n",
      "\tssl.cipher.suites = null\n",
      "\tssl.enabled.protocols = [TLSv1.2]\n",
      "\tssl.endpoint.identification.algorithm = https\n",
      "\tssl.engine.factory.class = null\n",
      "\tssl.key.password = null\n",
      "\tssl.keymanager.algorithm = SunX509\n",
      "\tssl.keystore.certificate.chain = null\n",
      "\tssl.keystore.key = null\n",
      "\tssl.keystore.location = null\n",
      "\tssl.keystore.password = null\n",
      "\tssl.keystore.type = JKS\n",
      "\tssl.protocol = TLSv1.2\n",
      "\tssl.provider = null\n",
      "\tssl.secure.random.implementation = null\n",
      "\tssl.trustmanager.algorithm = PKIX\n",
      "\tssl.truststore.certificates = null\n",
      "\tssl.truststore.location = null\n",
      "\tssl.truststore.password = null\n",
      "\tssl.truststore.type = JKS\n",
      "\n",
      "25/05/05 11:17:14 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/05/05 11:17:14 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/05/05 11:17:14 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/05/05 11:17:14 INFO AppInfoParser: Kafka version: 3.4.1\n",
      "25/05/05 11:17:14 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89\n",
      "25/05/05 11:17:14 INFO AppInfoParser: Kafka startTimeMs: 1746458234678\n",
      "25/05/05 11:17:14 INFO AppInfoParser: Kafka version: 3.4.1\n",
      "25/05/05 11:17:14 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89\n",
      "25/05/05 11:17:14 INFO AppInfoParser: Kafka startTimeMs: 1746458234678\n",
      "25/05/05 11:17:14 INFO AppInfoParser: Kafka version: 3.4.1\n",
      "25/05/05 11:17:14 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89\n",
      "25/05/05 11:17:14 INFO AppInfoParser: Kafka startTimeMs: 1746458234678\n",
      "25/05/05 11:17:15 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/sources/0/0 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/sources/0/.0.54e778bf-320d-4e17-8aef-d1b91b3c643a.tmp\n",
      "25/05/05 11:17:15 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/sources/0/0 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/sources/0/.0.76eea13e-3d98-47f8-bb6c-a8ae673746d7.tmp\n",
      "25/05/05 11:17:15 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/sources/0/0 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/sources/0/.0.db46483c-4da0-4997-b89e-fd2c3eb38b19.tmp\n",
      "25/05/05 11:17:15 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/sources/0/.0.db46483c-4da0-4997-b89e-fd2c3eb38b19.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/sources/0/0\n",
      "25/05/05 11:17:15 INFO KafkaMicroBatchStream: Initial offsets: {\"mta_turnstile_topic\":{\"0\":3232}}\n",
      "25/05/05 11:17:15 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/sources/0/.0.54e778bf-320d-4e17-8aef-d1b91b3c643a.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/sources/0/0\n",
      "25/05/05 11:17:15 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/sources/0/.0.76eea13e-3d98-47f8-bb6c-a8ae673746d7.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/sources/0/0\n",
      "25/05/05 11:17:15 INFO KafkaMicroBatchStream: Initial offsets: {\"mta_turnstile_topic\":{\"0\":3232}}\n",
      "25/05/05 11:17:15 INFO KafkaMicroBatchStream: Initial offsets: {\"mta_turnstile_topic\":{\"0\":3232}}\n",
      "25/05/05 11:17:15 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/0 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.0.d988b66c-c00f-4045-8016-57faa0484f39.tmp\n",
      "25/05/05 11:17:15 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/0 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.0.858c50c3-87fc-4804-9be3-ae665f5c7606.tmp\n",
      "25/05/05 11:17:15 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/0 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.0.a8a0f650-2950-41ac-83e1-b776f4ed91c8.tmp\n",
      "25/05/05 11:17:15 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.0.858c50c3-87fc-4804-9be3-ae665f5c7606.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/0\n",
      "25/05/05 11:17:15 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.0.a8a0f650-2950-41ac-83e1-b776f4ed91c8.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/0\n",
      "25/05/05 11:17:15 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.0.d988b66c-c00f-4045-8016-57faa0484f39.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/0\n",
      "25/05/05 11:17:15 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1746458235068,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:17:15 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1746458235068,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:17:15 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1746458235068,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:17:15 INFO IncrementalExecution: Current batch timestamp = 1746458235068\n",
      "25/05/05 11:17:15 INFO IncrementalExecution: Current batch timestamp = 1746458235068\n",
      "25/05/05 11:17:15 INFO IncrementalExecution: Current batch timestamp = 1746458235068\n",
      "25/05/05 11:17:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:15 INFO IncrementalExecution: Current batch timestamp = 1746458235068\n",
      "25/05/05 11:17:15 INFO IncrementalExecution: Current batch timestamp = 1746458235068\n",
      "25/05/05 11:17:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:15 INFO IncrementalExecution: Current batch timestamp = 1746458235068\n",
      "25/05/05 11:17:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:15 INFO IncrementalExecution: Current batch timestamp = 1746458235068\n",
      "25/05/05 11:17:15 INFO IncrementalExecution: Current batch timestamp = 1746458235068\n",
      "25/05/05 11:17:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:15 INFO CodeGenerator: Code generated in 256.226834 ms\n",
      "25/05/05 11:17:15 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:17:15 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@21b44d0e]. The input RDD has 1 partitions.\n",
      "25/05/05 11:17:15 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:15 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:15 INFO DAGScheduler: Got job 1 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:17:15 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:17:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:17:15 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:17:15 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[10] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:17:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.4 KiB, free 366.3 MiB)\n",
      "25/05/05 11:17:16 INFO CodeGenerator: Code generated in 25.041833 ms\n",
      "25/05/05 11:17:16 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:16 INFO DAGScheduler: Job 2 finished: start at NativeMethodAccessorImpl.java:0, took 0.001789 s\n",
      "25/05/05 11:17:16 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 248.0 B, free 366.3 MiB)\n",
      "25/05/05 11:17:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.5 KiB, free 366.3 MiB)\n",
      "25/05/05 11:17:16 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.3 MiB)\n",
      "25/05/05 11:17:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on gopalas-laptop.lan:59335 (size: 2.5 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:16 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:17:16 INFO SparkContext: Created broadcast 1 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:16 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:17:16 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/0 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.0.5b7a1ec6-2f3f-486f-ac7b-03f2cb5011e3.tmp\n",
      "25/05/05 11:17:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[10] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:17:16 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:17:16 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:17:16 INFO DAGScheduler: Final stage: ResultStage 1 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:17:16 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:17:16 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:17:16 INFO DAGScheduler: Submitting ResultStage 1 (ParallelCollectionRDD[11] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:17:16 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 3.4 KiB, free 366.3 MiB)\n",
      "25/05/05 11:17:16 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2035.0 B, free 366.3 MiB)\n",
      "25/05/05 11:17:16 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on gopalas-laptop.lan:59335 (size: 2035.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:17:16 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:17:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (ParallelCollectionRDD[11] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:17:16 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:17:16 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.0.5b7a1ec6-2f3f-486f-ac7b-03f2cb5011e3.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/0\n",
      "25/05/05 11:17:16 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 12128 bytes) \n",
      "25/05/05 11:17:16 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:17:14.220Z\",\n",
      "  \"batchId\" : 0,\n",
      "  \"numInputRows\" : 0,\n",
      "  \"inputRowsPerSecond\" : 0.0,\n",
      "  \"processedRowsPerSecond\" : 0.0,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 899,\n",
      "    \"commitOffsets\" : 48,\n",
      "    \"getBatch\" : 27,\n",
      "    \"latestOffset\" : 845,\n",
      "    \"queryPlanning\" : 181,\n",
      "    \"triggerExecution\" : 2056,\n",
      "    \"walCommit\" : 41\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : null,\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3232\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3232\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 0,\n",
      "    \"inputRowsPerSecond\" : 0.0,\n",
      "    \"processedRowsPerSecond\" : 0.0,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:17:16 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 12128 bytes) \n",
      "25/05/05 11:17:16 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "25/05/05 11:17:16 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "25/05/05 11:17:16 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:17:16 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)\n",
      "25/05/05 11:17:16 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:17:16 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)\n",
      "25/05/05 11:17:16 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1353 bytes result sent to driver\n",
      "25/05/05 11:17:16 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1252 bytes result sent to driver\n",
      "25/05/05 11:17:16 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 170 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:17:16 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:17:16 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 131 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:17:16 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:17:16 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 0.528 s\n",
      "25/05/05 11:17:16 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:17:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "25/05/05 11:17:16 INFO DAGScheduler: Job 1 finished: start at NativeMethodAccessorImpl.java:0, took 0.571748 s\n",
      "25/05/05 11:17:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@21b44d0e] is committing.\n",
      "25/05/05 11:17:16 INFO DAGScheduler: ResultStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 0.186 s\n",
      "25/05/05 11:17:16 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:17:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "25/05/05 11:17:16 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 0.575939 s\n",
      "25/05/05 11:17:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:17:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@21b44d0e] committed.\n",
      "25/05/05 11:17:16 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/0 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.0.33572d2f-bd58-4643-99fc-3173ee5ddf32.tmp\n",
      "25/05/05 11:17:16 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.0.33572d2f-bd58-4643-99fc-3173ee5ddf32.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/0\n",
      "25/05/05 11:17:16 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:17:14.314Z\",\n",
      "  \"batchId\" : 0,\n",
      "  \"numInputRows\" : 0,\n",
      "  \"inputRowsPerSecond\" : 0.0,\n",
      "  \"processedRowsPerSecond\" : 0.0,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 1129,\n",
      "    \"commitOffsets\" : 46,\n",
      "    \"getBatch\" : 27,\n",
      "    \"latestOffset\" : 751,\n",
      "    \"queryPlanning\" : 181,\n",
      "    \"triggerExecution\" : 2185,\n",
      "    \"walCommit\" : 41\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : null,\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3232\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3232\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 0,\n",
      "    \"inputRowsPerSecond\" : 0.0,\n",
      "    \"processedRowsPerSecond\" : 0.0,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 0\n",
      "  }\n",
      "}\n",
      "25/05/05 11:17:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:17:16 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/0 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.0.f4dc2fe0-d65b-4c85-a161-d18f68d3ebaa.tmp\n",
      "25/05/05 11:17:16 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.0.f4dc2fe0-d65b-4c85-a161-d18f68d3ebaa.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/0\n",
      "25/05/05 11:17:16 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:17:14.174Z\",\n",
      "  \"batchId\" : 0,\n",
      "  \"numInputRows\" : 0,\n",
      "  \"inputRowsPerSecond\" : 0.0,\n",
      "  \"processedRowsPerSecond\" : 0.0,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 1198,\n",
      "    \"commitOffsets\" : 51,\n",
      "    \"getBatch\" : 27,\n",
      "    \"latestOffset\" : 882,\n",
      "    \"queryPlanning\" : 181,\n",
      "    \"triggerExecution\" : 2398,\n",
      "    \"walCommit\" : 41\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : null,\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3232\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3232\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 0,\n",
      "    \"inputRowsPerSecond\" : 0.0,\n",
      "    \"processedRowsPerSecond\" : 0.0,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 0\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---+----+---+-------+----+----+----+-----------+----------+-----------+\n",
      "|C/A|UNIT|SCP|STATION|DATE|TIME|DESC|ENTRY_COUNT|EXIT_COUNT|ingest_time|\n",
      "+---+----+---+-------+----+----+----+-----------+----------+-----------+\n",
      "+---+----+---+-------+----+----+----+-----------+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:17:17 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/1 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.1.955418cd-7b0a-41fd-81a1-6e8b13599e89.tmp\n",
      "25/05/05 11:17:17 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/1 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.1.88a8a05e-3915-4581-ab7d-4f41accc7b6c.tmp\n",
      "25/05/05 11:17:17 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/1 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.1.21d9e9f1-ec33-420c-bd25-a9b35d7cf2f6.tmp\n",
      "25/05/05 11:17:17 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.1.88a8a05e-3915-4581-ab7d-4f41accc7b6c.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/1\n",
      "25/05/05 11:17:17 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1746458237089,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:17:17 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.1.955418cd-7b0a-41fd-81a1-6e8b13599e89.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/1\n",
      "25/05/05 11:17:17 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1746458237089,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:17:17 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.1.21d9e9f1-ec33-420c-bd25-a9b35d7cf2f6.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/1\n",
      "25/05/05 11:17:17 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1746458237089,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:17:17 INFO IncrementalExecution: Current batch timestamp = 1746458237089\n",
      "25/05/05 11:17:17 INFO IncrementalExecution: Current batch timestamp = 1746458237089\n",
      "25/05/05 11:17:17 INFO IncrementalExecution: Current batch timestamp = 1746458237089\n",
      "25/05/05 11:17:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:17 INFO IncrementalExecution: Current batch timestamp = 1746458237089\n",
      "25/05/05 11:17:17 INFO IncrementalExecution: Current batch timestamp = 1746458237089\n",
      "25/05/05 11:17:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:17 INFO IncrementalExecution: Current batch timestamp = 1746458237089\n",
      "25/05/05 11:17:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:17 INFO IncrementalExecution: Current batch timestamp = 1746458237089\n",
      "25/05/05 11:17:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:17 INFO IncrementalExecution: Current batch timestamp = 1746458237089\n",
      "25/05/05 11:17:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:17 INFO CodeGenerator: Code generated in 30.432417 ms\n",
      "25/05/05 11:17:17 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@13625719]. The input RDD has 1 partitions.\n",
      "25/05/05 11:17:17 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:17:17 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:17 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:17 INFO DAGScheduler: Got job 3 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:17:17 INFO DAGScheduler: Final stage: ResultStage 2 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:17:17 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:17:17 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:17:17 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[27] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:17:17 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.2 KiB, free 366.3 MiB)\n",
      "25/05/05 11:17:17 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.3 MiB)\n",
      "25/05/05 11:17:17 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:17 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:17:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[27] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:17:17 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:17:17 INFO DAGScheduler: Got job 4 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:17:17 INFO DAGScheduler: Final stage: ResultStage 3 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:17:17 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:17:17 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:17:17 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[25] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:17:17 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:17:17 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 16.2 KiB, free 366.3 MiB)\n",
      "25/05/05 11:17:17 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
      "25/05/05 11:17:17 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:17:17 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:17 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:17:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[25] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:17:17 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:17:17 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:17:17 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
      "25/05/05 11:17:17 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:17 INFO DAGScheduler: Got job 5 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:17:17 INFO DAGScheduler: Final stage: ResultStage 4 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:17:17 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:17:17 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:17:17 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[32] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:17:17 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 47.1 KiB, free 366.2 MiB)\n",
      "25/05/05 11:17:17 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.2 MiB)\n",
      "25/05/05 11:17:17 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:17 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:17:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[32] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:17:17 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:17:17 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:17:17 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
      "25/05/05 11:17:17 INFO BlockManagerInfo: Removed broadcast_2_piece0 on gopalas-laptop.lan:59335 in memory (size: 2035.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:17:17 INFO BlockManagerInfo: Removed broadcast_0_piece0 on gopalas-laptop.lan:59335 in memory (size: 2.5 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:17 INFO BlockManagerInfo: Removed broadcast_1_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:17:17 INFO CodeGenerator: Code generated in 37.339916 ms\n",
      "25/05/05 11:17:17 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3232 untilOffset=3233, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=1 taskId=2 partitionId=0\n",
      "25/05/05 11:17:17 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3232 untilOffset=3233, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=1 taskId=3 partitionId=0\n",
      "25/05/05 11:17:17 INFO CodeGenerator: Code generated in 18.909833 ms\n",
      "25/05/05 11:17:17 INFO CodeGenerator: Code generated in 15.408667 ms\n",
      "25/05/05 11:17:17 INFO CodeGenerator: Code generated in 19.085083 ms\n",
      "25/05/05 11:17:17 INFO ConsumerConfig: ConsumerConfig values: \n",
      "\tallow.auto.create.topics = true\n",
      "\tauto.commit.interval.ms = 5000\n",
      "\tauto.include.jmx.reporter = true\n",
      "\tauto.offset.reset = none\n",
      "\tbootstrap.servers = [localhost:9092]\n",
      "\tcheck.crcs = true\n",
      "\tclient.dns.lookup = use_all_dns_ips\n",
      "\tclient.id = consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1\n",
      "\tclient.rack = \n",
      "\tconnections.max.idle.ms = 540000\n",
      "\tdefault.api.timeout.ms = 60000\n",
      "\tenable.auto.commit = false\n",
      "\texclude.internal.topics = true\n",
      "\tfetch.max.bytes = 52428800\n",
      "\tfetch.max.wait.ms = 500\n",
      "\tfetch.min.bytes = 1\n",
      "\tgroup.id = spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor\n",
      "\tgroup.instance.id = null\n",
      "\theartbeat.interval.ms = 3000\n",
      "\tinterceptor.classes = []\n",
      "\tinternal.leave.group.on.close = true\n",
      "\tinternal.throw.on.fetch.stable.offset.unsupported = false\n",
      "\tisolation.level = read_uncommitted\n",
      "\tkey.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n",
      "\tmax.partition.fetch.bytes = 1048576\n",
      "\tmax.poll.interval.ms = 300000\n",
      "\tmax.poll.records = 500\n",
      "\tmetadata.max.age.ms = 300000\n",
      "\tmetric.reporters = []\n",
      "\tmetrics.num.samples = 2\n",
      "\tmetrics.recording.level = INFO\n",
      "\tmetrics.sample.window.ms = 30000\n",
      "\tpartition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]\n",
      "\treceive.buffer.bytes = 65536\n",
      "\treconnect.backoff.max.ms = 1000\n",
      "\treconnect.backoff.ms = 50\n",
      "\trequest.timeout.ms = 30000\n",
      "\tretry.backoff.ms = 100\n",
      "\tsasl.client.callback.handler.class = null\n",
      "\tsasl.jaas.config = null\n",
      "\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n",
      "\tsasl.kerberos.min.time.before.relogin = 60000\n",
      "\tsasl.kerberos.service.name = null\n",
      "\tsasl.kerberos.ticket.renew.jitter = 0.05\n",
      "\tsasl.kerberos.ticket.renew.window.factor = 0.8\n",
      "\tsasl.login.callback.handler.class = null\n",
      "\tsasl.login.class = null\n",
      "\tsasl.login.connect.timeout.ms = null\n",
      "\tsasl.login.read.timeout.ms = null\n",
      "\tsasl.login.refresh.buffer.seconds = 300\n",
      "\tsasl.login.refresh.min.period.seconds = 60\n",
      "\tsasl.login.refresh.window.factor = 0.8\n",
      "\tsasl.login.refresh.window.jitter = 0.05\n",
      "\tsasl.login.retry.backoff.max.ms = 10000\n",
      "\tsasl.login.retry.backoff.ms = 100\n",
      "\tsasl.mechanism = GSSAPI\n",
      "\tsasl.oauthbearer.clock.skew.seconds = 30\n",
      "\tsasl.oauthbearer.expected.audience = null\n",
      "\tsasl.oauthbearer.expected.issuer = null\n",
      "\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n",
      "\tsasl.oauthbearer.jwks.endpoint.url = null\n",
      "\tsasl.oauthbearer.scope.claim.name = scope\n",
      "\tsasl.oauthbearer.sub.claim.name = sub\n",
      "\tsasl.oauthbearer.token.endpoint.url = null\n",
      "\tsecurity.protocol = PLAINTEXT\n",
      "\tsecurity.providers = null\n",
      "\tsend.buffer.bytes = 131072\n",
      "\tsession.timeout.ms = 45000\n",
      "\tsocket.connection.setup.timeout.max.ms = 30000\n",
      "\tsocket.connection.setup.timeout.ms = 10000\n",
      "\tssl.cipher.suites = null\n",
      "\tssl.enabled.protocols = [TLSv1.2]\n",
      "\tssl.endpoint.identification.algorithm = https\n",
      "\tssl.engine.factory.class = null\n",
      "\tssl.key.password = null\n",
      "\tssl.keymanager.algorithm = SunX509\n",
      "\tssl.keystore.certificate.chain = null\n",
      "\tssl.keystore.key = null\n",
      "\tssl.keystore.location = null\n",
      "\tssl.keystore.password = null\n",
      "\tssl.keystore.type = JKS\n",
      "\tssl.protocol = TLSv1.2\n",
      "\tssl.provider = null\n",
      "\tssl.secure.random.implementation = null\n",
      "\tssl.trustmanager.algorithm = PKIX\n",
      "\tssl.truststore.certificates = null\n",
      "\tssl.truststore.location = null\n",
      "\tssl.truststore.password = null\n",
      "\tssl.truststore.type = JKS\n",
      "\tvalue.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n",
      "\n",
      "25/05/05 11:17:17 INFO ConsumerConfig: ConsumerConfig values: \n",
      "\tallow.auto.create.topics = true\n",
      "\tauto.commit.interval.ms = 5000\n",
      "\tauto.include.jmx.reporter = true\n",
      "\tauto.offset.reset = none\n",
      "\tbootstrap.servers = [localhost:9092]\n",
      "\tcheck.crcs = true\n",
      "\tclient.dns.lookup = use_all_dns_ips\n",
      "\tclient.id = consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2\n",
      "\tclient.rack = \n",
      "\tconnections.max.idle.ms = 540000\n",
      "\tdefault.api.timeout.ms = 60000\n",
      "\tenable.auto.commit = false\n",
      "\texclude.internal.topics = true\n",
      "\tfetch.max.bytes = 52428800\n",
      "\tfetch.max.wait.ms = 500\n",
      "\tfetch.min.bytes = 1\n",
      "\tgroup.id = spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor\n",
      "\tgroup.instance.id = null\n",
      "\theartbeat.interval.ms = 3000\n",
      "\tinterceptor.classes = []\n",
      "\tinternal.leave.group.on.close = true\n",
      "\tinternal.throw.on.fetch.stable.offset.unsupported = false\n",
      "\tisolation.level = read_uncommitted\n",
      "\tkey.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n",
      "\tmax.partition.fetch.bytes = 1048576\n",
      "\tmax.poll.interval.ms = 300000\n",
      "\tmax.poll.records = 500\n",
      "\tmetadata.max.age.ms = 300000\n",
      "\tmetric.reporters = []\n",
      "\tmetrics.num.samples = 2\n",
      "\tmetrics.recording.level = INFO\n",
      "\tmetrics.sample.window.ms = 30000\n",
      "\tpartition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]\n",
      "\treceive.buffer.bytes = 65536\n",
      "\treconnect.backoff.max.ms = 1000\n",
      "\treconnect.backoff.ms = 50\n",
      "\trequest.timeout.ms = 30000\n",
      "\tretry.backoff.ms = 100\n",
      "\tsasl.client.callback.handler.class = null\n",
      "\tsasl.jaas.config = null\n",
      "\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n",
      "\tsasl.kerberos.min.time.before.relogin = 60000\n",
      "\tsasl.kerberos.service.name = null\n",
      "\tsasl.kerberos.ticket.renew.jitter = 0.05\n",
      "\tsasl.kerberos.ticket.renew.window.factor = 0.8\n",
      "\tsasl.login.callback.handler.class = null\n",
      "\tsasl.login.class = null\n",
      "\tsasl.login.connect.timeout.ms = null\n",
      "\tsasl.login.read.timeout.ms = null\n",
      "\tsasl.login.refresh.buffer.seconds = 300\n",
      "\tsasl.login.refresh.min.period.seconds = 60\n",
      "\tsasl.login.refresh.window.factor = 0.8\n",
      "\tsasl.login.refresh.window.jitter = 0.05\n",
      "\tsasl.login.retry.backoff.max.ms = 10000\n",
      "\tsasl.login.retry.backoff.ms = 100\n",
      "\tsasl.mechanism = GSSAPI\n",
      "\tsasl.oauthbearer.clock.skew.seconds = 30\n",
      "\tsasl.oauthbearer.expected.audience = null\n",
      "\tsasl.oauthbearer.expected.issuer = null\n",
      "\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n",
      "\tsasl.oauthbearer.jwks.endpoint.url = null\n",
      "\tsasl.oauthbearer.scope.claim.name = scope\n",
      "\tsasl.oauthbearer.sub.claim.name = sub\n",
      "\tsasl.oauthbearer.token.endpoint.url = null\n",
      "\tsecurity.protocol = PLAINTEXT\n",
      "\tsecurity.providers = null\n",
      "\tsend.buffer.bytes = 131072\n",
      "\tsession.timeout.ms = 45000\n",
      "\tsocket.connection.setup.timeout.max.ms = 30000\n",
      "\tsocket.connection.setup.timeout.ms = 10000\n",
      "\tssl.cipher.suites = null\n",
      "\tssl.enabled.protocols = [TLSv1.2]\n",
      "\tssl.endpoint.identification.algorithm = https\n",
      "\tssl.engine.factory.class = null\n",
      "\tssl.key.password = null\n",
      "\tssl.keymanager.algorithm = SunX509\n",
      "\tssl.keystore.certificate.chain = null\n",
      "\tssl.keystore.key = null\n",
      "\tssl.keystore.location = null\n",
      "\tssl.keystore.password = null\n",
      "\tssl.keystore.type = JKS\n",
      "\tssl.protocol = TLSv1.2\n",
      "\tssl.provider = null\n",
      "\tssl.secure.random.implementation = null\n",
      "\tssl.trustmanager.algorithm = PKIX\n",
      "\tssl.truststore.certificates = null\n",
      "\tssl.truststore.location = null\n",
      "\tssl.truststore.password = null\n",
      "\tssl.truststore.type = JKS\n",
      "\tvalue.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n",
      "\n",
      "25/05/05 11:17:17 INFO AppInfoParser: Kafka version: 3.4.1\n",
      "25/05/05 11:17:17 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89\n",
      "25/05/05 11:17:17 INFO AppInfoParser: Kafka startTimeMs: 1746458237703\n",
      "25/05/05 11:17:17 INFO AppInfoParser: Kafka version: 3.4.1\n",
      "25/05/05 11:17:17 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89\n",
      "25/05/05 11:17:17 INFO AppInfoParser: Kafka startTimeMs: 1746458237703\n",
      "25/05/05 11:17:17 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Assigned to partition(s): mta_turnstile_topic-0\n",
      "25/05/05 11:17:17 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Assigned to partition(s): mta_turnstile_topic-0\n",
      "25/05/05 11:17:17 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3232 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:17 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3232 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:17 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting the last seen epoch of partition mta_turnstile_topic-0 to 0 since the associated topicId changed from null to bsUAHjekRpi-2oMDySYEGA\n",
      "25/05/05 11:17:17 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting the last seen epoch of partition mta_turnstile_topic-0 to 0 since the associated topicId changed from null to bsUAHjekRpi-2oMDySYEGA\n",
      "25/05/05 11:17:17 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Cluster ID: DtG2159VQSmJE0XkAt7YKg\n",
      "25/05/05 11:17:17 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Cluster ID: DtG2159VQSmJE0XkAt7YKg\n",
      "25/05/05 11:17:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3233, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3233, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:18 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:17:18 INFO DataWritingSparkTask: Committed partition 0 (task 2, attempt 0, stage 2.0)\n",
      "25/05/05 11:17:18 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 611396875 nanos, during time span of 704116250 nanos.\n",
      "25/05/05 11:17:18 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2231 bytes result sent to driver\n",
      "25/05/05 11:17:18 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1207 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:17:18 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:17:18 INFO DAGScheduler: ResultStage 2 (start at NativeMethodAccessorImpl.java:0) finished in 1.214 s\n",
      "25/05/05 11:17:18 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:17:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "25/05/05 11:17:18 INFO DAGScheduler: Job 3 finished: start at NativeMethodAccessorImpl.java:0, took 1.235974 s\n",
      "25/05/05 11:17:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:17:18 INFO CodeGenerator: Code generated in 9.271 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:17:18 INFO CodeGenerator: Code generated in 11.491667 ms\n",
      "25/05/05 11:17:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:17:18 INFO CodeGenerator: Code generated in 26.576041 ms\n",
      "25/05/05 11:17:18 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:17:18 INFO DataWritingSparkTask: Committed partition 0 (task 3, attempt 0, stage 3.0)\n",
      "25/05/05 11:17:18 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3232 untilOffset=3233, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=1 taskId=4 partitionId=0\n",
      "25/05/05 11:17:18 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 608194917 nanos, during time span of 1017974625 nanos.\n",
      "25/05/05 11:17:18 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/1 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.1.7bf80697-395e-46a5-8263-0eb14eb715b5.tmp\n",
      "25/05/05 11:17:18 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3645 bytes result sent to driver\n",
      "25/05/05 11:17:18 INFO ConsumerConfig: ConsumerConfig values: \n",
      "\tallow.auto.create.topics = true\n",
      "\tauto.commit.interval.ms = 5000\n",
      "\tauto.include.jmx.reporter = true\n",
      "\tauto.offset.reset = none\n",
      "\tbootstrap.servers = [localhost:9092]\n",
      "\tcheck.crcs = true\n",
      "\tclient.dns.lookup = use_all_dns_ips\n",
      "\tclient.id = consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3\n",
      "\tclient.rack = \n",
      "\tconnections.max.idle.ms = 540000\n",
      "\tdefault.api.timeout.ms = 60000\n",
      "\tenable.auto.commit = false\n",
      "\texclude.internal.topics = true\n",
      "\tfetch.max.bytes = 52428800\n",
      "\tfetch.max.wait.ms = 500\n",
      "\tfetch.min.bytes = 1\n",
      "\tgroup.id = spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor\n",
      "\tgroup.instance.id = null\n",
      "\theartbeat.interval.ms = 3000\n",
      "\tinterceptor.classes = []\n",
      "\tinternal.leave.group.on.close = true\n",
      "\tinternal.throw.on.fetch.stable.offset.unsupported = false\n",
      "\tisolation.level = read_uncommitted\n",
      "\tkey.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n",
      "\tmax.partition.fetch.bytes = 1048576\n",
      "\tmax.poll.interval.ms = 300000\n",
      "\tmax.poll.records = 500\n",
      "\tmetadata.max.age.ms = 300000\n",
      "\tmetric.reporters = []\n",
      "\tmetrics.num.samples = 2\n",
      "\tmetrics.recording.level = INFO\n",
      "\tmetrics.sample.window.ms = 30000\n",
      "\tpartition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]\n",
      "\treceive.buffer.bytes = 65536\n",
      "\treconnect.backoff.max.ms = 1000\n",
      "\treconnect.backoff.ms = 50\n",
      "\trequest.timeout.ms = 30000\n",
      "\tretry.backoff.ms = 100\n",
      "\tsasl.client.callback.handler.class = null\n",
      "\tsasl.jaas.config = null\n",
      "\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n",
      "\tsasl.kerberos.min.time.before.relogin = 60000\n",
      "\tsasl.kerberos.service.name = null\n",
      "\tsasl.kerberos.ticket.renew.jitter = 0.05\n",
      "\tsasl.kerberos.ticket.renew.window.factor = 0.8\n",
      "\tsasl.login.callback.handler.class = null\n",
      "\tsasl.login.class = null\n",
      "\tsasl.login.connect.timeout.ms = null\n",
      "\tsasl.login.read.timeout.ms = null\n",
      "\tsasl.login.refresh.buffer.seconds = 300\n",
      "\tsasl.login.refresh.min.period.seconds = 60\n",
      "\tsasl.login.refresh.window.factor = 0.8\n",
      "\tsasl.login.refresh.window.jitter = 0.05\n",
      "\tsasl.login.retry.backoff.max.ms = 10000\n",
      "\tsasl.login.retry.backoff.ms = 100\n",
      "\tsasl.mechanism = GSSAPI\n",
      "\tsasl.oauthbearer.clock.skew.seconds = 30\n",
      "\tsasl.oauthbearer.expected.audience = null\n",
      "\tsasl.oauthbearer.expected.issuer = null\n",
      "\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n",
      "\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n",
      "\tsasl.oauthbearer.jwks.endpoint.url = null\n",
      "\tsasl.oauthbearer.scope.claim.name = scope\n",
      "\tsasl.oauthbearer.sub.claim.name = sub\n",
      "\tsasl.oauthbearer.token.endpoint.url = null\n",
      "\tsecurity.protocol = PLAINTEXT\n",
      "\tsecurity.providers = null\n",
      "\tsend.buffer.bytes = 131072\n",
      "\tsession.timeout.ms = 45000\n",
      "\tsocket.connection.setup.timeout.max.ms = 30000\n",
      "\tsocket.connection.setup.timeout.ms = 10000\n",
      "\tssl.cipher.suites = null\n",
      "\tssl.enabled.protocols = [TLSv1.2]\n",
      "\tssl.endpoint.identification.algorithm = https\n",
      "\tssl.engine.factory.class = null\n",
      "\tssl.key.password = null\n",
      "\tssl.keymanager.algorithm = SunX509\n",
      "\tssl.keystore.certificate.chain = null\n",
      "\tssl.keystore.key = null\n",
      "\tssl.keystore.location = null\n",
      "\tssl.keystore.password = null\n",
      "\tssl.keystore.type = JKS\n",
      "\tssl.protocol = TLSv1.2\n",
      "\tssl.provider = null\n",
      "\tssl.secure.random.implementation = null\n",
      "\tssl.trustmanager.algorithm = PKIX\n",
      "\tssl.truststore.certificates = null\n",
      "\tssl.truststore.location = null\n",
      "\tssl.truststore.password = null\n",
      "\tssl.truststore.type = JKS\n",
      "\tvalue.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n",
      "\n",
      "25/05/05 11:17:18 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1512 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:17:18 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:17:18 INFO DAGScheduler: ResultStage 3 (start at NativeMethodAccessorImpl.java:0) finished in 1.517 s\n",
      "25/05/05 11:17:18 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:17:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "25/05/05 11:17:18 INFO DAGScheduler: Job 4 finished: start at NativeMethodAccessorImpl.java:0, took 1.547466 s\n",
      "25/05/05 11:17:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@13625719] is committing.\n",
      "25/05/05 11:17:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@13625719] committed.\n",
      "25/05/05 11:17:18 INFO AppInfoParser: Kafka version: 3.4.1\n",
      "25/05/05 11:17:18 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89\n",
      "25/05/05 11:17:18 INFO AppInfoParser: Kafka startTimeMs: 1746458238757\n",
      "25/05/05 11:17:18 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Assigned to partition(s): mta_turnstile_topic-0\n",
      "25/05/05 11:17:18 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3232 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:18 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting the last seen epoch of partition mta_turnstile_topic-0 to 0 since the associated topicId changed from null to bsUAHjekRpi-2oMDySYEGA\n",
      "25/05/05 11:17:18 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Cluster ID: DtG2159VQSmJE0XkAt7YKg\n",
      "25/05/05 11:17:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:18 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/1 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.1.9a074bc0-598c-4e2e-a867-56565688002d.tmp\n",
      "25/05/05 11:17:18 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.1.7bf80697-395e-46a5-8263-0eb14eb715b5.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/1\n",
      "25/05/05 11:17:18 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:17:17.086Z\",\n",
      "  \"batchId\" : 1,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 62.5,\n",
      "  \"processedRowsPerSecond\" : 0.5892751915144372,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 1576,\n",
      "    \"commitOffsets\" : 52,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 30,\n",
      "    \"triggerExecution\" : 1697,\n",
      "    \"walCommit\" : 35\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3232\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3233\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3233\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 62.5,\n",
      "    \"processedRowsPerSecond\" : 0.5892751915144372,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:17:18 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.1.9a074bc0-598c-4e2e-a867-56565688002d.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/1\n",
      "25/05/05 11:17:18 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:17:17.086Z\",\n",
      "  \"batchId\" : 1,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 62.5,\n",
      "  \"processedRowsPerSecond\" : 0.5844535359438925,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 1603,\n",
      "    \"commitOffsets\" : 40,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 29,\n",
      "    \"triggerExecution\" : 1711,\n",
      "    \"walCommit\" : 35\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3232\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3233\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3233\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 62.5,\n",
      "    \"processedRowsPerSecond\" : 0.5844535359438925,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A001|R079|01-00-00|WORLD TRADE CTR|05/05/2025|11:17:15|REGULAR|13         |7         |2025-05-05 11:17:17.089|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:17:19 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:19 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:19 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3233, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:19 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:17:19 INFO BlockManagerInfo: Removed broadcast_4_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:19 INFO BlockManagerInfo: Removed broadcast_3_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:19 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:17:19 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:17:19 INFO connection: Opened connection [connectionId{localValue:1, serverValue:4359}] to localhost:27017\n",
      "25/05/05 11:17:19 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=4342458}\n",
      "25/05/05 11:17:19 INFO connection: Opened connection [connectionId{localValue:2, serverValue:4360}] to localhost:27017\n",
      "25/05/05 11:17:19 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:17:19 INFO connection: Closed connection [connectionId{localValue:2, serverValue:4360}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:17:19 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 510914292 nanos, during time span of 673782584 nanos.\n",
      "25/05/05 11:17:19 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1688 bytes result sent to driver\n",
      "25/05/05 11:17:19 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 2077 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:17:19 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:17:19 INFO DAGScheduler: ResultStage 4 (start at NativeMethodAccessorImpl.java:0) finished in 2.143 s\n",
      "25/05/05 11:17:19 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:17:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "25/05/05 11:17:19 INFO DAGScheduler: Job 5 finished: start at NativeMethodAccessorImpl.java:0, took 2.150161 s\n",
      "25/05/05 11:17:19 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 248.0 B, free 366.2 MiB)\n",
      "25/05/05 11:17:19 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.2 MiB)\n",
      "25/05/05 11:17:19 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:17:19 INFO SparkContext: Created broadcast 6 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:19 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/1 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.1.ed7a939a-da6e-410d-9060-8f3546061f27.tmp\n",
      "25/05/05 11:17:19 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.1.ed7a939a-da6e-410d-9060-8f3546061f27.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/1\n",
      "25/05/05 11:17:19 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:17:17.084Z\",\n",
      "  \"batchId\" : 1,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 0.4122011541632316,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 2287,\n",
      "    \"commitOffsets\" : 71,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 5,\n",
      "    \"queryPlanning\" : 25,\n",
      "    \"triggerExecution\" : 2426,\n",
      "    \"walCommit\" : 35\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3232\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3233\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3233\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 0.4122011541632316,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:17:22 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/2 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.2.029d0607-f9ac-4596-912a-f3ce4ef549ba.tmp\n",
      "25/05/05 11:17:22 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/2 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.2.c7ed9d60-a2db-40da-97c1-edfab6c5480c.tmp\n",
      "25/05/05 11:17:22 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/2 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.2.474dc974-ec97-4bcd-9107-25f6fca229d5.tmp\n",
      "25/05/05 11:17:22 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.2.029d0607-f9ac-4596-912a-f3ce4ef549ba.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/2\n",
      "25/05/05 11:17:22 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1746458242684,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:17:22 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.2.474dc974-ec97-4bcd-9107-25f6fca229d5.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/2\n",
      "25/05/05 11:17:22 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1746458242698,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:17:22 INFO IncrementalExecution: Current batch timestamp = 1746458242684\n",
      "25/05/05 11:17:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:22 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.2.c7ed9d60-a2db-40da-97c1-edfab6c5480c.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/2\n",
      "25/05/05 11:17:22 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1746458242700,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:17:22 INFO IncrementalExecution: Current batch timestamp = 1746458242698\n",
      "25/05/05 11:17:22 INFO IncrementalExecution: Current batch timestamp = 1746458242684\n",
      "25/05/05 11:17:22 INFO IncrementalExecution: Current batch timestamp = 1746458242700\n",
      "25/05/05 11:17:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:22 INFO IncrementalExecution: Current batch timestamp = 1746458242698\n",
      "25/05/05 11:17:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:22 INFO IncrementalExecution: Current batch timestamp = 1746458242700\n",
      "25/05/05 11:17:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:22 INFO IncrementalExecution: Current batch timestamp = 1746458242698\n",
      "25/05/05 11:17:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:22 INFO IncrementalExecution: Current batch timestamp = 1746458242700\n",
      "25/05/05 11:17:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:22 INFO CodeGenerator: Code generated in 20.061958 ms\n",
      "25/05/05 11:17:22 INFO CodeGenerator: Code generated in 16.553667 ms\n",
      "25/05/05 11:17:22 INFO CodeGenerator: Code generated in 20.213458 ms\n",
      "25/05/05 11:17:22 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4e87a9f4]. The input RDD has 1 partitions.\n",
      "25/05/05 11:17:22 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:17:22 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:22 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:22 INFO DAGScheduler: Got job 6 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:17:22 INFO DAGScheduler: Final stage: ResultStage 5 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:17:22 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:17:22 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:17:22 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[42] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:17:22 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:17:22 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:17:22 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:22 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:17:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[42] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:17:22 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:17:22 INFO DAGScheduler: Got job 7 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:17:22 INFO DAGScheduler: Final stage: ResultStage 6 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:17:22 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:17:22 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:17:22 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[43] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:17:22 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:17:22 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)\n",
      "25/05/05 11:17:22 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:17:22 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:17:22 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:22 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:17:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[43] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:17:22 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:17:22 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:17:22 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)\n",
      "25/05/05 11:17:22 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:22 INFO DAGScheduler: Got job 8 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:17:22 INFO DAGScheduler: Final stage: ResultStage 7 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:17:22 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:17:22 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:17:22 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[48] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:17:22 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:17:22 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:17:22 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:22 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:17:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[48] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:17:22 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:17:22 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:17:22 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)\n",
      "25/05/05 11:17:22 INFO CodeGenerator: Code generated in 22.89775 ms\n",
      "25/05/05 11:17:22 INFO CodeGenerator: Code generated in 19.372375 ms\n",
      "25/05/05 11:17:22 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3233 untilOffset=3234, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=2 taskId=6 partitionId=0\n",
      "25/05/05 11:17:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3233 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:22 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3233 untilOffset=3234, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=2 taskId=5 partitionId=0\n",
      "25/05/05 11:17:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3233 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:22 INFO CodeGenerator: Code generated in 17.107458 ms\n",
      "25/05/05 11:17:22 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3233 untilOffset=3234, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=2 taskId=7 partitionId=0\n",
      "25/05/05 11:17:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3233 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3234, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:23 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:17:23 INFO DataWritingSparkTask: Committed partition 0 (task 6, attempt 0, stage 6.0)\n",
      "25/05/05 11:17:23 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 510230625 nanos, during time span of 510837209 nanos.\n",
      "25/05/05 11:17:23 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 2188 bytes result sent to driver\n",
      "25/05/05 11:17:23 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 546 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:17:23 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:17:23 INFO DAGScheduler: ResultStage 6 (start at NativeMethodAccessorImpl.java:0) finished in 0.551 s\n",
      "25/05/05 11:17:23 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:17:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
      "25/05/05 11:17:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:23 INFO DAGScheduler: Job 7 finished: start at NativeMethodAccessorImpl.java:0, took 0.564120 s\n",
      "25/05/05 11:17:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3234, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:17:23 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:17:23 INFO DataWritingSparkTask: Committed partition 0 (task 5, attempt 0, stage 5.0)\n",
      "25/05/05 11:17:23 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 507035209 nanos, during time span of 510079000 nanos.\n",
      "25/05/05 11:17:23 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 3558 bytes result sent to driver\n",
      "25/05/05 11:17:23 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 566 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:17:23 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:17:23 INFO DAGScheduler: ResultStage 5 (start at NativeMethodAccessorImpl.java:0) finished in 0.576 s\n",
      "25/05/05 11:17:23 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:17:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
      "25/05/05 11:17:23 INFO DAGScheduler: Job 6 finished: start at NativeMethodAccessorImpl.java:0, took 0.580978 s\n",
      "25/05/05 11:17:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4e87a9f4] is committing.\n",
      "25/05/05 11:17:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4e87a9f4] committed.\n",
      "25/05/05 11:17:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3234, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:23 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:17:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:17:23 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:17:23 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:17:23 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/2 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.2.c78910fa-1f39-4ee1-a4ab-857c467b6ea9.tmp\n",
      "25/05/05 11:17:23 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/2 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.2.ab12a4b5-fd34-4d62-8f4b-cc427843b912.tmp\n",
      "25/05/05 11:17:23 INFO connection: Opened connection [connectionId{localValue:3, serverValue:4361}] to localhost:27017\n",
      "25/05/05 11:17:23 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=742125}\n",
      "25/05/05 11:17:23 INFO connection: Opened connection [connectionId{localValue:4, serverValue:4362}] to localhost:27017\n",
      "25/05/05 11:17:23 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.2.ab12a4b5-fd34-4d62-8f4b-cc427843b912.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/2\n",
      "25/05/05 11:17:23 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:17:22.694Z\",\n",
      "  \"batchId\" : 2,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.2755102040816326,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 635,\n",
      "    \"commitOffsets\" : 74,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 6,\n",
      "    \"queryPlanning\" : 18,\n",
      "    \"triggerExecution\" : 784,\n",
      "    \"walCommit\" : 51\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3233\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3234\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3234\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.2755102040816326,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:17:23 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.2.c78910fa-1f39-4ee1-a4ab-857c467b6ea9.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/2\n",
      "25/05/05 11:17:23 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:17:22.694Z\",\n",
      "  \"batchId\" : 2,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.2690355329949239,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 643,\n",
      "    \"commitOffsets\" : 71,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 4,\n",
      "    \"queryPlanning\" : 25,\n",
      "    \"triggerExecution\" : 788,\n",
      "    \"walCommit\" : 45\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3233\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3234\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3234\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.2690355329949239,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:17:23 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:17:23 INFO connection: Closed connection [connectionId{localValue:4, serverValue:4362}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:17:23 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 510140709 nanos, during time span of 588741167 nanos.\n",
      "25/05/05 11:17:23 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1645 bytes result sent to driver\n",
      "25/05/05 11:17:23 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 627 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:17:23 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:17:23 INFO DAGScheduler: ResultStage 7 (start at NativeMethodAccessorImpl.java:0) finished in 0.640 s\n",
      "25/05/05 11:17:23 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:17:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
      "25/05/05 11:17:23 INFO DAGScheduler: Job 8 finished: start at NativeMethodAccessorImpl.java:0, took 0.645777 s\n",
      "25/05/05 11:17:23 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:17:23 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:17:23 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:17:23 INFO SparkContext: Created broadcast 10 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:23 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/2 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.2.8fb3337c-76ed-472b-b5c6-4ce400cafe51.tmp\n",
      "25/05/05 11:17:23 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.2.8fb3337c-76ed-472b-b5c6-4ce400cafe51.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/2\n",
      "25/05/05 11:17:23 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:17:22.682Z\",\n",
      "  \"batchId\" : 2,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 50.0,\n",
      "  \"processedRowsPerSecond\" : 1.1778563015312131,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 744,\n",
      "    \"commitOffsets\" : 33,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 21,\n",
      "    \"triggerExecution\" : 849,\n",
      "    \"walCommit\" : 44\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3233\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3234\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3234\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 50.0,\n",
      "    \"processedRowsPerSecond\" : 1.1778563015312131,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+----+----+--------+--------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION       |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+--------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A002|R051|02-00-00|34 ST-PENN STA|05/05/2025|11:17:21|REGULAR|6          |10        |2025-05-05 11:17:22.698|\n",
      "+----+----+--------+--------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:17:29 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/3 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.3.189de653-7152-4a68-9fd1-b91f67ad1107.tmp\n",
      "25/05/05 11:17:29 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/3 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.3.f736a532-27d7-4324-8c2c-14c46424c9a4.tmp\n",
      "25/05/05 11:17:29 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/3 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.3.f9e2c682-6f4c-4c93-8051-249cc6528489.tmp\n",
      "25/05/05 11:17:29 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.3.f736a532-27d7-4324-8c2c-14c46424c9a4.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/3\n",
      "25/05/05 11:17:29 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1746458249302,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:17:29 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.3.f9e2c682-6f4c-4c93-8051-249cc6528489.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/3\n",
      "25/05/05 11:17:29 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1746458249305,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:17:29 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.3.189de653-7152-4a68-9fd1-b91f67ad1107.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/3\n",
      "25/05/05 11:17:29 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1746458249302,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:17:29 INFO IncrementalExecution: Current batch timestamp = 1746458249302\n",
      "25/05/05 11:17:29 INFO IncrementalExecution: Current batch timestamp = 1746458249305\n",
      "25/05/05 11:17:29 INFO IncrementalExecution: Current batch timestamp = 1746458249302\n",
      "25/05/05 11:17:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:29 INFO IncrementalExecution: Current batch timestamp = 1746458249302\n",
      "25/05/05 11:17:29 INFO IncrementalExecution: Current batch timestamp = 1746458249305\n",
      "25/05/05 11:17:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:29 INFO IncrementalExecution: Current batch timestamp = 1746458249302\n",
      "25/05/05 11:17:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:29 INFO IncrementalExecution: Current batch timestamp = 1746458249302\n",
      "25/05/05 11:17:29 INFO IncrementalExecution: Current batch timestamp = 1746458249302\n",
      "25/05/05 11:17:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:29 INFO BlockManagerInfo: Removed broadcast_9_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:29 INFO BlockManagerInfo: Removed broadcast_7_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:29 INFO BlockManagerInfo: Removed broadcast_10_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:17:29 INFO CodeGenerator: Code generated in 36.803166 ms\n",
      "25/05/05 11:17:29 INFO BlockManagerInfo: Removed broadcast_6_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:17:29 INFO CodeGenerator: Code generated in 19.702125 ms\n",
      "25/05/05 11:17:29 INFO BlockManagerInfo: Removed broadcast_8_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:29 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 3, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@51366dac]. The input RDD has 1 partitions.\n",
      "25/05/05 11:17:29 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:29 INFO DAGScheduler: Got job 9 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:17:29 INFO DAGScheduler: Final stage: ResultStage 8 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:17:29 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:17:29 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:17:29 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[56] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:17:29 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 3, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:17:29 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:29 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:17:29 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:17:29 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:29 INFO BlockManagerInfo: Removed broadcast_5_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:29 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:17:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[56] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:17:29 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:17:29 INFO DAGScheduler: Got job 10 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:17:29 INFO DAGScheduler: Final stage: ResultStage 9 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:17:29 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:17:29 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:17:29 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)\n",
      "25/05/05 11:17:29 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:17:29 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[59] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:17:29 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 15.2 KiB, free 366.3 MiB)\n",
      "25/05/05 11:17:29 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.3 MiB)\n",
      "25/05/05 11:17:29 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:29 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:17:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[59] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:17:29 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:17:29 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:17:29 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)\n",
      "25/05/05 11:17:29 INFO CodeGenerator: Code generated in 20.796792 ms\n",
      "25/05/05 11:17:29 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3234 untilOffset=3235, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=3 taskId=9 partitionId=0\n",
      "25/05/05 11:17:29 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3234 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:29 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3234 untilOffset=3235, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=3 taskId=8 partitionId=0\n",
      "25/05/05 11:17:29 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:29 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:29 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3234 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:29 INFO DAGScheduler: Got job 11 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:17:29 INFO DAGScheduler: Final stage: ResultStage 10 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:17:29 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:17:29 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:17:29 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:29 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[64] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:17:29 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 47.1 KiB, free 366.2 MiB)\n",
      "25/05/05 11:17:29 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.2 MiB)\n",
      "25/05/05 11:17:29 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:29 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:17:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[64] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:17:29 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:17:29 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:17:29 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)\n",
      "25/05/05 11:17:29 INFO CodeGenerator: Code generated in 30.300625 ms\n",
      "25/05/05 11:17:29 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3234 untilOffset=3235, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=3 taskId=10 partitionId=0\n",
      "25/05/05 11:17:29 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3234 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:29 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:29 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:29 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:29 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3235, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:29 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:17:29 INFO DataWritingSparkTask: Committed partition 0 (task 9, attempt 0, stage 9.0)\n",
      "25/05/05 11:17:29 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 507637166 nanos, during time span of 508218416 nanos.\n",
      "25/05/05 11:17:29 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:29 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 2180 bytes result sent to driver\n",
      "25/05/05 11:17:29 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:29 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3235, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:29 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 534 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:17:29 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:17:29 INFO DAGScheduler: ResultStage 9 (start at NativeMethodAccessorImpl.java:0) finished in 0.538 s\n",
      "25/05/05 11:17:29 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:17:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
      "25/05/05 11:17:29 INFO DAGScheduler: Job 10 finished: start at NativeMethodAccessorImpl.java:0, took 0.548357 s\n",
      "25/05/05 11:17:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:17:29 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:17:29 INFO DataWritingSparkTask: Committed partition 0 (task 8, attempt 0, stage 8.0)\n",
      "25/05/05 11:17:29 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 503127500 nanos, during time span of 505838125 nanos.\n",
      "25/05/05 11:17:29 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 3550 bytes result sent to driver\n",
      "25/05/05 11:17:29 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 548 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:17:29 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:17:29 INFO DAGScheduler: ResultStage 8 (start at NativeMethodAccessorImpl.java:0) finished in 0.553 s\n",
      "25/05/05 11:17:29 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:17:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
      "25/05/05 11:17:29 INFO DAGScheduler: Job 9 finished: start at NativeMethodAccessorImpl.java:0, took 0.557607 s\n",
      "25/05/05 11:17:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@51366dac] is committing.\n",
      "25/05/05 11:17:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@51366dac] committed.\n",
      "25/05/05 11:17:29 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/3 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.3.f8b47e75-9358-47be-92ea-ed0441bf3154.tmp\n",
      "25/05/05 11:17:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:17:30 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/3 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.3.d9feab5f-396f-4c08-ab44-91b534545eaa.tmp\n",
      "25/05/05 11:17:30 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.3.f8b47e75-9358-47be-92ea-ed0441bf3154.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/3\n",
      "25/05/05 11:17:30 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:17:29.300Z\",\n",
      "  \"batchId\" : 3,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "  \"processedRowsPerSecond\" : 1.3986013986013988,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 629,\n",
      "    \"commitOffsets\" : 34,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 12,\n",
      "    \"triggerExecution\" : 715,\n",
      "    \"walCommit\" : 38\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3234\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3235\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3235\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "    \"processedRowsPerSecond\" : 1.3986013986013988,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:17:30 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.3.d9feab5f-396f-4c08-ab44-91b534545eaa.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/3\n",
      "25/05/05 11:17:30 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:17:29.300Z\",\n",
      "  \"batchId\" : 3,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.3717421124828533,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 645,\n",
      "    \"commitOffsets\" : 32,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 14,\n",
      "    \"triggerExecution\" : 729,\n",
      "    \"walCommit\" : 36\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3234\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3235\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3235\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.3717421124828533,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:17:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3235, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:30 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:17:30 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:17:30 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:17:30 INFO connection: Opened connection [connectionId{localValue:5, serverValue:4363}] to localhost:27017\n",
      "25/05/05 11:17:30 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=407916}\n",
      "25/05/05 11:17:30 INFO connection: Opened connection [connectionId{localValue:6, serverValue:4364}] to localhost:27017\n",
      "25/05/05 11:17:30 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:17:30 INFO connection: Closed connection [connectionId{localValue:6, serverValue:4364}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:17:30 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 510621000 nanos, during time span of 517041750 nanos.\n",
      "25/05/05 11:17:30 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 1645 bytes result sent to driver\n",
      "25/05/05 11:17:30 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 565 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:17:30 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:17:30 INFO DAGScheduler: ResultStage 10 (start at NativeMethodAccessorImpl.java:0) finished in 0.575 s\n",
      "25/05/05 11:17:30 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:17:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
      "25/05/05 11:17:30 INFO DAGScheduler: Job 11 finished: start at NativeMethodAccessorImpl.java:0, took 0.578047 s\n",
      "25/05/05 11:17:30 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 248.0 B, free 366.2 MiB)\n",
      "25/05/05 11:17:30 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.2 MiB)\n",
      "25/05/05 11:17:30 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:17:30 INFO SparkContext: Created broadcast 14 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:30 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/3 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.3.fdd99e2d-ab30-4118-9c3f-bf001cbc35cb.tmp\n",
      "25/05/05 11:17:30 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.3.fdd99e2d-ab30-4118-9c3f-bf001cbc35cb.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/3\n",
      "25/05/05 11:17:30 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:17:29.302Z\",\n",
      "  \"batchId\" : 3,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.2804097311139564,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 698,\n",
      "    \"commitOffsets\" : 31,\n",
      "    \"getBatch\" : 1,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 12,\n",
      "    \"triggerExecution\" : 781,\n",
      "    \"walCommit\" : 34\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3234\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3235\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3235\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.2804097311139564,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+----+----+--------+-------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION|DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+-------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A004|R033|00-00-00|125 ST |05/05/2025|11:17:28|REGULAR|3          |12        |2025-05-05 11:17:29.302|\n",
      "+----+----+--------+-------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:17:33 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/4 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.4.ed7d705d-ccbf-4eee-ad0c-7e7160ea353f.tmp\n",
      "25/05/05 11:17:33 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/4 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.4.9bb43103-13a8-4fd7-9757-11769c50ea66.tmp\n",
      "25/05/05 11:17:33 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/4 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.4.7d7467b2-7448-44f6-982d-75c55edb143d.tmp\n",
      "OpenJDK 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "OpenJDK 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n",
      "25/05/05 11:17:33 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.4.ed7d705d-ccbf-4eee-ad0c-7e7160ea353f.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/4\n",
      "25/05/05 11:17:33 INFO MicroBatchExecution: Committed offsets for batch 4. Metadata OffsetSeqMetadata(0,1746458253851,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:17:33 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.4.7d7467b2-7448-44f6-982d-75c55edb143d.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/4\n",
      "25/05/05 11:17:33 INFO MicroBatchExecution: Committed offsets for batch 4. Metadata OffsetSeqMetadata(0,1746458253854,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:17:33 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.4.9bb43103-13a8-4fd7-9757-11769c50ea66.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/4\n",
      "25/05/05 11:17:33 INFO MicroBatchExecution: Committed offsets for batch 4. Metadata OffsetSeqMetadata(0,1746458253851,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:17:33 INFO IncrementalExecution: Current batch timestamp = 1746458253854\n",
      "25/05/05 11:17:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:33 INFO IncrementalExecution: Current batch timestamp = 1746458253851\n",
      "25/05/05 11:17:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:33 INFO IncrementalExecution: Current batch timestamp = 1746458253851\n",
      "25/05/05 11:17:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:33 INFO IncrementalExecution: Current batch timestamp = 1746458253854\n",
      "25/05/05 11:17:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:33 INFO IncrementalExecution: Current batch timestamp = 1746458253851\n",
      "25/05/05 11:17:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:33 INFO IncrementalExecution: Current batch timestamp = 1746458253851\n",
      "25/05/05 11:17:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:33 INFO IncrementalExecution: Current batch timestamp = 1746458253854\n",
      "25/05/05 11:17:33 INFO IncrementalExecution: Current batch timestamp = 1746458253851\n",
      "25/05/05 11:17:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:33 INFO CodeGenerator: Code generated in 12.824208 ms\n",
      "25/05/05 11:17:33 INFO CodeGenerator: Code generated in 9.727 ms\n",
      "25/05/05 11:17:33 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 4, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@59d0a7e3]. The input RDD has 1 partitions.\n",
      "25/05/05 11:17:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:33 INFO DAGScheduler: Got job 12 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:17:33 INFO DAGScheduler: Final stage: ResultStage 11 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:17:33 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:17:33 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:17:33 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[72] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:17:33 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 4, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:17:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:33 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:17:33 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:17:33 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:33 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:17:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[72] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:17:33 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:17:33 INFO DAGScheduler: Got job 13 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:17:33 INFO DAGScheduler: Final stage: ResultStage 12 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:17:33 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:17:33 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:17:33 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[75] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:17:33 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:17:33 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:17:33 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:17:33 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)\n",
      "25/05/05 11:17:33 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:33 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:17:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[75] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:17:33 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:17:33 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:17:33 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)\n",
      "25/05/05 11:17:33 INFO CodeGenerator: Code generated in 17.698583 ms\n",
      "25/05/05 11:17:33 INFO CodeGenerator: Code generated in 18.347792 ms\n",
      "25/05/05 11:17:33 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3235 untilOffset=3236, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=4 taskId=11 partitionId=0\n",
      "25/05/05 11:17:33 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3235 untilOffset=3236, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=4 taskId=12 partitionId=0\n",
      "25/05/05 11:17:34 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3235 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:34 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3235 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:34 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:34 INFO DAGScheduler: Got job 14 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:17:34 INFO DAGScheduler: Final stage: ResultStage 13 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:17:34 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:17:34 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:17:34 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[80] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:17:34 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:17:34 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.1 MiB)\n",
      "25/05/05 11:17:34 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:17:34 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:17:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[80] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:17:34 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:17:34 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:17:34 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)\n",
      "25/05/05 11:17:34 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3235 untilOffset=3236, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=4 taskId=13 partitionId=0\n",
      "25/05/05 11:17:34 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3235 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=35468Kb max_used=35769Kb free=95603Kb\n",
      " bounds [0x0000000102190000, 0x00000001044b0000, 0x000000010a190000]\n",
      " total_blobs=13008 nmethods=11950 adapters=970\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:17:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3236, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3236, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:34 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:17:34 INFO DataWritingSparkTask: Committed partition 0 (task 12, attempt 0, stage 12.0)\n",
      "25/05/05 11:17:34 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 505627583 nanos, during time span of 506180541 nanos.\n",
      "25/05/05 11:17:34 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 2188 bytes result sent to driver\n",
      "25/05/05 11:17:34 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 540 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:17:34 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:17:34 INFO DAGScheduler: ResultStage 12 (start at NativeMethodAccessorImpl.java:0) finished in 0.546 s\n",
      "25/05/05 11:17:34 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:17:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished\n",
      "25/05/05 11:17:34 INFO DAGScheduler: Job 13 finished: start at NativeMethodAccessorImpl.java:0, took 0.550979 s\n",
      "25/05/05 11:17:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:17:34 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:17:34 INFO DataWritingSparkTask: Committed partition 0 (task 11, attempt 0, stage 11.0)\n",
      "25/05/05 11:17:34 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 502863750 nanos, during time span of 505901708 nanos.\n",
      "25/05/05 11:17:34 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 3559 bytes result sent to driver\n",
      "25/05/05 11:17:34 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 551 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:17:34 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:17:34 INFO DAGScheduler: ResultStage 11 (start at NativeMethodAccessorImpl.java:0) finished in 0.556 s\n",
      "25/05/05 11:17:34 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:17:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished\n",
      "25/05/05 11:17:34 INFO DAGScheduler: Job 12 finished: start at NativeMethodAccessorImpl.java:0, took 0.559874 s\n",
      "25/05/05 11:17:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@59d0a7e3] is committing.\n",
      "25/05/05 11:17:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@59d0a7e3] committed.\n",
      "25/05/05 11:17:34 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/4 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.4.2331be46-07ca-4f76-b11f-f59c71476d31.tmp\n",
      "25/05/05 11:17:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:17:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3236, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:34 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:17:34 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:17:34 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:17:34 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/4 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.4.79c910ce-10dd-4721-b54f-59d38ed5f0a7.tmp\n",
      "25/05/05 11:17:34 INFO connection: Opened connection [connectionId{localValue:7, serverValue:4365}] to localhost:27017\n",
      "25/05/05 11:17:34 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=516458}\n",
      "25/05/05 11:17:34 INFO connection: Opened connection [connectionId{localValue:8, serverValue:4366}] to localhost:27017\n",
      "25/05/05 11:17:34 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:17:34 INFO connection: Closed connection [connectionId{localValue:8, serverValue:4366}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:17:34 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 509785167 nanos, during time span of 518385708 nanos.\n",
      "25/05/05 11:17:34 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 1645 bytes result sent to driver\n",
      "25/05/05 11:17:34 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 531 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:17:34 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:17:34 INFO DAGScheduler: ResultStage 13 (start at NativeMethodAccessorImpl.java:0) finished in 0.542 s\n",
      "25/05/05 11:17:34 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:17:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished\n",
      "25/05/05 11:17:34 INFO DAGScheduler: Job 14 finished: start at NativeMethodAccessorImpl.java:0, took 0.543799 s\n",
      "25/05/05 11:17:34 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:17:34 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:17:34 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:17:34 INFO SparkContext: Created broadcast 18 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:34 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.4.2331be46-07ca-4f76-b11f-f59c71476d31.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/4\n",
      "25/05/05 11:17:34 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:17:33.853Z\",\n",
      "  \"batchId\" : 4,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.4224751066856332,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 598,\n",
      "    \"commitOffsets\" : 42,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 15,\n",
      "    \"triggerExecution\" : 703,\n",
      "    \"walCommit\" : 46\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3235\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3236\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3236\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.4224751066856332,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:17:34 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/4 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.4.b26011aa-8937-4112-9177-ae8bfae2d457.tmp\n",
      "25/05/05 11:17:34 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.4.79c910ce-10dd-4721-b54f-59d38ed5f0a7.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/4\n",
      "25/05/05 11:17:34 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:17:33.850Z\",\n",
      "  \"batchId\" : 4,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "  \"processedRowsPerSecond\" : 1.392757660167131,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 611,\n",
      "    \"commitOffsets\" : 40,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 11,\n",
      "    \"triggerExecution\" : 718,\n",
      "    \"walCommit\" : 54\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3235\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3236\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3236\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "    \"processedRowsPerSecond\" : 1.392757660167131,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:17:34 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.4.b26011aa-8937-4112-9177-ae8bfae2d457.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/4\n",
      "25/05/05 11:17:34 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:17:33.850Z\",\n",
      "  \"batchId\" : 4,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.364256480218281,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 632,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 19,\n",
      "    \"triggerExecution\" : 733,\n",
      "    \"walCommit\" : 48\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3235\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3236\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3236\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.364256480218281,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R023|R045|00-00-00|34 ST-HERALD SQ|05/05/2025|11:17:32|REGULAR|8          |13        |2025-05-05 11:17:33.851|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:17:39 INFO BlockManagerInfo: Removed broadcast_18_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:17:39 INFO BlockManagerInfo: Removed broadcast_15_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:17:39 INFO BlockManagerInfo: Removed broadcast_13_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:39 INFO BlockManagerInfo: Removed broadcast_11_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:39 INFO BlockManagerInfo: Removed broadcast_14_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:17:39 INFO BlockManagerInfo: Removed broadcast_17_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:39 INFO BlockManagerInfo: Removed broadcast_12_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:39 INFO BlockManagerInfo: Removed broadcast_16_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:40 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/5 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.5.bb29305b-38b9-4c9a-b1d8-18a73c98972b.tmp\n",
      "25/05/05 11:17:40 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/5 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.5.eb670be8-11b4-42d8-aef2-b173e7ca39db.tmp\n",
      "25/05/05 11:17:40 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/5 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.5.8d81c17a-58b0-4c55-acf4-5a822a1009e5.tmp\n",
      "25/05/05 11:17:40 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.5.bb29305b-38b9-4c9a-b1d8-18a73c98972b.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/5\n",
      "25/05/05 11:17:40 INFO MicroBatchExecution: Committed offsets for batch 5. Metadata OffsetSeqMetadata(0,1746458260417,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:17:40 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.5.8d81c17a-58b0-4c55-acf4-5a822a1009e5.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/5\n",
      "25/05/05 11:17:40 INFO MicroBatchExecution: Committed offsets for batch 5. Metadata OffsetSeqMetadata(0,1746458260417,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:17:40 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.5.eb670be8-11b4-42d8-aef2-b173e7ca39db.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/5\n",
      "25/05/05 11:17:40 INFO MicroBatchExecution: Committed offsets for batch 5. Metadata OffsetSeqMetadata(0,1746458260417,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:17:40 INFO IncrementalExecution: Current batch timestamp = 1746458260417\n",
      "25/05/05 11:17:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:40 INFO IncrementalExecution: Current batch timestamp = 1746458260417\n",
      "25/05/05 11:17:40 INFO IncrementalExecution: Current batch timestamp = 1746458260417\n",
      "25/05/05 11:17:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:40 INFO IncrementalExecution: Current batch timestamp = 1746458260417\n",
      "25/05/05 11:17:40 INFO IncrementalExecution: Current batch timestamp = 1746458260417\n",
      "25/05/05 11:17:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:40 INFO IncrementalExecution: Current batch timestamp = 1746458260417\n",
      "25/05/05 11:17:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:40 INFO IncrementalExecution: Current batch timestamp = 1746458260417\n",
      "25/05/05 11:17:40 INFO IncrementalExecution: Current batch timestamp = 1746458260417\n",
      "25/05/05 11:17:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:40 INFO CodeGenerator: Code generated in 14.003375 ms\n",
      "25/05/05 11:17:40 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 5, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:17:40 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:40 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 5, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@567f028d]. The input RDD has 1 partitions.\n",
      "25/05/05 11:17:40 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:40 INFO DAGScheduler: Got job 15 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:17:40 INFO DAGScheduler: Final stage: ResultStage 14 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:17:40 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:17:40 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:17:40 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[86] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:17:40 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 15.2 KiB, free 366.3 MiB)\n",
      "25/05/05 11:17:40 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.3 MiB)\n",
      "25/05/05 11:17:40 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:40 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:17:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[86] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:17:40 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:17:40 INFO DAGScheduler: Got job 16 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:17:40 INFO DAGScheduler: Final stage: ResultStage 15 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:17:40 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:17:40 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:17:40 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[88] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:17:40 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:17:40 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)\n",
      "25/05/05 11:17:40 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 16.2 KiB, free 366.3 MiB)\n",
      "25/05/05 11:17:40 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.3 MiB)\n",
      "25/05/05 11:17:40 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:40 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:17:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[88] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:17:40 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:17:40 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:17:40 INFO Executor: Running task 0.0 in stage 15.0 (TID 15)\n",
      "25/05/05 11:17:40 INFO CodeGenerator: Code generated in 20.559708 ms\n",
      "25/05/05 11:17:40 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3236 untilOffset=3237, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=5 taskId=14 partitionId=0\n",
      "25/05/05 11:17:40 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3236 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:40 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3236 untilOffset=3237, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=5 taskId=15 partitionId=0\n",
      "25/05/05 11:17:40 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3236 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:40 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:40 INFO DAGScheduler: Got job 17 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:17:40 INFO DAGScheduler: Final stage: ResultStage 16 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:17:40 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:17:40 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:17:40 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[96] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:17:40 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 47.1 KiB, free 366.2 MiB)\n",
      "25/05/05 11:17:40 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.2 MiB)\n",
      "25/05/05 11:17:40 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:40 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:17:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[96] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:17:40 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:17:40 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:17:40 INFO Executor: Running task 0.0 in stage 16.0 (TID 16)\n",
      "25/05/05 11:17:40 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3236 untilOffset=3237, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=5 taskId=16 partitionId=0\n",
      "25/05/05 11:17:40 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3236 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:41 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:41 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:41 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3237, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:41 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:17:41 INFO DataWritingSparkTask: Committed partition 0 (task 14, attempt 0, stage 14.0)\n",
      "25/05/05 11:17:41 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 504994625 nanos, during time span of 505528042 nanos.\n",
      "25/05/05 11:17:41 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:41 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:41 INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 2188 bytes result sent to driver\n",
      "25/05/05 11:17:41 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3237, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:41 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 537 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:17:41 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:17:41 INFO DAGScheduler: ResultStage 14 (start at NativeMethodAccessorImpl.java:0) finished in 0.541 s\n",
      "25/05/05 11:17:41 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:17:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished\n",
      "25/05/05 11:17:41 INFO DAGScheduler: Job 15 finished: start at NativeMethodAccessorImpl.java:0, took 0.544874 s\n",
      "25/05/05 11:17:41 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:17:41 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:17:41 INFO DataWritingSparkTask: Committed partition 0 (task 15, attempt 0, stage 15.0)\n",
      "25/05/05 11:17:41 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 502931417 nanos, during time span of 507155125 nanos.\n",
      "25/05/05 11:17:41 INFO Executor: Finished task 0.0 in stage 15.0 (TID 15). 3559 bytes result sent to driver\n",
      "25/05/05 11:17:41 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 536 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:17:41 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:17:41 INFO DAGScheduler: ResultStage 15 (start at NativeMethodAccessorImpl.java:0) finished in 0.543 s\n",
      "25/05/05 11:17:41 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:17:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished\n",
      "25/05/05 11:17:41 INFO DAGScheduler: Job 16 finished: start at NativeMethodAccessorImpl.java:0, took 0.550384 s\n",
      "25/05/05 11:17:41 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@567f028d] is committing.\n",
      "25/05/05 11:17:41 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@567f028d] committed.\n",
      "25/05/05 11:17:41 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/5 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.5.d5da2d7e-2b07-460a-a9fc-e8adb8b74fb7.tmp\n",
      "25/05/05 11:17:41 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:17:41 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:41 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:41 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3237, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:41 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:17:41 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/5 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.5.98a31d33-659f-41a5-80e3-77377d581b69.tmp\n",
      "25/05/05 11:17:41 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:17:41 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:17:41 INFO connection: Opened connection [connectionId{localValue:9, serverValue:4367}] to localhost:27017\n",
      "25/05/05 11:17:41 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1504167}\n",
      "25/05/05 11:17:41 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.5.d5da2d7e-2b07-460a-a9fc-e8adb8b74fb7.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/5\n",
      "25/05/05 11:17:41 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:17:40.413Z\",\n",
      "  \"batchId\" : 5,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.4367816091954024,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 582,\n",
      "    \"commitOffsets\" : 36,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 4,\n",
      "    \"queryPlanning\" : 14,\n",
      "    \"triggerExecution\" : 696,\n",
      "    \"walCommit\" : 60\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3236\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3237\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3237\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.4367816091954024,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:17:41 INFO connection: Opened connection [connectionId{localValue:10, serverValue:4368}] to localhost:27017\n",
      "25/05/05 11:17:41 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:17:41 INFO connection: Closed connection [connectionId{localValue:10, serverValue:4368}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:17:41 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 507306209 nanos, during time span of 528465167 nanos.\n",
      "25/05/05 11:17:41 INFO Executor: Finished task 0.0 in stage 16.0 (TID 16). 1645 bytes result sent to driver\n",
      "25/05/05 11:17:41 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 541 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:17:41 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:17:41 INFO DAGScheduler: ResultStage 16 (start at NativeMethodAccessorImpl.java:0) finished in 0.550 s\n",
      "25/05/05 11:17:41 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:17:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished\n",
      "25/05/05 11:17:41 INFO DAGScheduler: Job 17 finished: start at NativeMethodAccessorImpl.java:0, took 0.551552 s\n",
      "25/05/05 11:17:41 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 248.0 B, free 366.2 MiB)\n",
      "25/05/05 11:17:41 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.2 MiB)\n",
      "25/05/05 11:17:41 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:17:41 INFO SparkContext: Created broadcast 22 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:41 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.5.98a31d33-659f-41a5-80e3-77377d581b69.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/5\n",
      "25/05/05 11:17:41 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:17:40.413Z\",\n",
      "  \"batchId\" : 5,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 52.631578947368425,\n",
      "  \"processedRowsPerSecond\" : 1.4124293785310735,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 596,\n",
      "    \"commitOffsets\" : 35,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 4,\n",
      "    \"queryPlanning\" : 13,\n",
      "    \"triggerExecution\" : 708,\n",
      "    \"walCommit\" : 60\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3236\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3237\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3237\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 52.631578947368425,\n",
      "    \"processedRowsPerSecond\" : 1.4124293785310735,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:17:41 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/5 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.5.026aafeb-0913-472c-bf0c-7f9bb3eb31d1.tmp\n",
      "25/05/05 11:17:41 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.5.026aafeb-0913-472c-bf0c-7f9bb3eb31d1.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/5\n",
      "25/05/05 11:17:41 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:17:40.411Z\",\n",
      "  \"batchId\" : 5,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.3531799729364005,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 629,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 6,\n",
      "    \"queryPlanning\" : 12,\n",
      "    \"triggerExecution\" : 739,\n",
      "    \"walCommit\" : 61\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3236\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3237\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3237\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.3531799729364005,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R023|R045|00-00-00|34 ST-HERALD SQ|05/05/2025|11:17:39|REGULAR|7          |9         |2025-05-05 11:17:40.417|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:17:44 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/6 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.6.1ea76363-da5f-427b-bfea-918991eeaac7.tmp\n",
      "25/05/05 11:17:44 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/6 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.6.44371182-aa96-4f19-b8ae-c8ffde88807a.tmp\n",
      "25/05/05 11:17:44 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/6 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.6.93de4e4a-b955-4a22-846e-e1d0e010b3c7.tmp\n",
      "25/05/05 11:17:45 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.6.1ea76363-da5f-427b-bfea-918991eeaac7.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/6\n",
      "25/05/05 11:17:45 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.6.93de4e4a-b955-4a22-846e-e1d0e010b3c7.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/6\n",
      "25/05/05 11:17:45 INFO MicroBatchExecution: Committed offsets for batch 6. Metadata OffsetSeqMetadata(0,1746458264963,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:17:45 INFO MicroBatchExecution: Committed offsets for batch 6. Metadata OffsetSeqMetadata(0,1746458264963,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:17:45 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.6.44371182-aa96-4f19-b8ae-c8ffde88807a.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/6\n",
      "25/05/05 11:17:45 INFO MicroBatchExecution: Committed offsets for batch 6. Metadata OffsetSeqMetadata(0,1746458264970,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:17:45 INFO IncrementalExecution: Current batch timestamp = 1746458264963\n",
      "25/05/05 11:17:45 INFO IncrementalExecution: Current batch timestamp = 1746458264970\n",
      "25/05/05 11:17:45 INFO IncrementalExecution: Current batch timestamp = 1746458264963\n",
      "25/05/05 11:17:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:45 INFO IncrementalExecution: Current batch timestamp = 1746458264970\n",
      "25/05/05 11:17:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:45 INFO IncrementalExecution: Current batch timestamp = 1746458264963\n",
      "25/05/05 11:17:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:45 INFO IncrementalExecution: Current batch timestamp = 1746458264963\n",
      "25/05/05 11:17:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:45 INFO IncrementalExecution: Current batch timestamp = 1746458264970\n",
      "25/05/05 11:17:45 INFO IncrementalExecution: Current batch timestamp = 1746458264963\n",
      "25/05/05 11:17:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:45 INFO CodeGenerator: Code generated in 10.9225 ms\n",
      "25/05/05 11:17:45 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 6, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:17:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:45 INFO DAGScheduler: Got job 18 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:17:45 INFO DAGScheduler: Final stage: ResultStage 17 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:17:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:17:45 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:17:45 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[102] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:17:45 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:17:45 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:17:45 INFO CodeGenerator: Code generated in 10.36 ms\n",
      "25/05/05 11:17:45 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:45 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:17:45 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 6, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@7de2da3b]. The input RDD has 1 partitions.\n",
      "25/05/05 11:17:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[102] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:17:45 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:17:45 INFO DAGScheduler: Got job 19 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:17:45 INFO DAGScheduler: Final stage: ResultStage 18 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:17:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:17:45 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:17:45 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[107] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:17:45 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:17:45 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:17:45 INFO Executor: Running task 0.0 in stage 17.0 (TID 17)\n",
      "25/05/05 11:17:45 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:17:45 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:45 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:17:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[107] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:17:45 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:17:45 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 18) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:17:45 INFO Executor: Running task 0.0 in stage 18.0 (TID 18)\n",
      "25/05/05 11:17:45 INFO CodeGenerator: Code generated in 9.356958 ms\n",
      "25/05/05 11:17:45 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3237 untilOffset=3238, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=6 taskId=17 partitionId=0\n",
      "25/05/05 11:17:45 INFO CodeGenerator: Code generated in 10.638041 ms\n",
      "25/05/05 11:17:45 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3237 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:45 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3237 untilOffset=3238, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=6 taskId=18 partitionId=0\n",
      "25/05/05 11:17:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:45 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3237 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:45 INFO DAGScheduler: Got job 20 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:17:45 INFO DAGScheduler: Final stage: ResultStage 19 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:17:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:17:45 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:17:45 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[112] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:17:45 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:17:45 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.1 MiB)\n",
      "25/05/05 11:17:45 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:17:45 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:17:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[112] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:17:45 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:17:45 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:17:45 INFO Executor: Running task 0.0 in stage 19.0 (TID 19)\n",
      "25/05/05 11:17:45 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3237 untilOffset=3238, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=6 taskId=19 partitionId=0\n",
      "25/05/05 11:17:45 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3237 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3238, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3238, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:45 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:17:45 INFO DataWritingSparkTask: Committed partition 0 (task 17, attempt 0, stage 17.0)\n",
      "25/05/05 11:17:45 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 507239667 nanos, during time span of 507890125 nanos.\n",
      "25/05/05 11:17:45 INFO Executor: Finished task 0.0 in stage 17.0 (TID 17). 2188 bytes result sent to driver\n",
      "25/05/05 11:17:45 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 532 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:17:45 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:17:45 INFO DAGScheduler: ResultStage 17 (start at NativeMethodAccessorImpl.java:0) finished in 0.538 s\n",
      "25/05/05 11:17:45 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:17:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished\n",
      "25/05/05 11:17:45 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:17:45 INFO DataWritingSparkTask: Committed partition 0 (task 18, attempt 0, stage 18.0)\n",
      "25/05/05 11:17:45 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 502814000 nanos, during time span of 505333875 nanos.\n",
      "25/05/05 11:17:45 INFO DAGScheduler: Job 18 finished: start at NativeMethodAccessorImpl.java:0, took 0.540358 s\n",
      "25/05/05 11:17:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:17:45 INFO Executor: Finished task 0.0 in stage 18.0 (TID 18). 3559 bytes result sent to driver\n",
      "25/05/05 11:17:45 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 18) in 530 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:17:45 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:17:45 INFO DAGScheduler: ResultStage 18 (start at NativeMethodAccessorImpl.java:0) finished in 0.535 s\n",
      "25/05/05 11:17:45 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:17:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished\n",
      "25/05/05 11:17:45 INFO DAGScheduler: Job 19 finished: start at NativeMethodAccessorImpl.java:0, took 0.537883 s\n",
      "25/05/05 11:17:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@7de2da3b] is committing.\n",
      "25/05/05 11:17:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@7de2da3b] committed.\n",
      "25/05/05 11:17:45 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/6 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.6.c83b863b-98e6-40a1-97a0-56d089e0c30a.tmp\n",
      "25/05/05 11:17:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:17:45 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/6 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.6.b34b36ad-a630-4711-af0c-6735b4b16d00.tmp\n",
      "25/05/05 11:17:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3238, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:45 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:17:45 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:17:45 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:17:45 INFO connection: Opened connection [connectionId{localValue:11, serverValue:4369}] to localhost:27017\n",
      "25/05/05 11:17:45 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=520000}\n",
      "25/05/05 11:17:45 INFO connection: Opened connection [connectionId{localValue:12, serverValue:4370}] to localhost:27017\n",
      "25/05/05 11:17:45 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.6.c83b863b-98e6-40a1-97a0-56d089e0c30a.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/6\n",
      "25/05/05 11:17:45 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:17:44.961Z\",\n",
      "  \"batchId\" : 6,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 62.5,\n",
      "  \"processedRowsPerSecond\" : 1.4749262536873156,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 578,\n",
      "    \"commitOffsets\" : 35,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 9,\n",
      "    \"queryPlanning\" : 10,\n",
      "    \"triggerExecution\" : 678,\n",
      "    \"walCommit\" : 46\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3237\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3238\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3238\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 62.5,\n",
      "    \"processedRowsPerSecond\" : 1.4749262536873156,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:17:45 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:17:45 INFO connection: Closed connection [connectionId{localValue:12, serverValue:4370}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:17:45 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 504669917 nanos, during time span of 513105667 nanos.\n",
      "25/05/05 11:17:45 INFO Executor: Finished task 0.0 in stage 19.0 (TID 19). 1645 bytes result sent to driver\n",
      "25/05/05 11:17:45 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 525 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:17:45 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:17:45 INFO DAGScheduler: ResultStage 19 (start at NativeMethodAccessorImpl.java:0) finished in 0.536 s\n",
      "25/05/05 11:17:45 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:17:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished\n",
      "25/05/05 11:17:45 INFO DAGScheduler: Job 20 finished: start at NativeMethodAccessorImpl.java:0, took 0.538125 s\n",
      "25/05/05 11:17:45 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:17:45 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:17:45 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:17:45 INFO SparkContext: Created broadcast 26 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:45 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/6 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.6.a232028f-2c75-4c0e-87f8-aefacb00a973.tmp\n",
      "25/05/05 11:17:45 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.6.b34b36ad-a630-4711-af0c-6735b4b16d00.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/6\n",
      "25/05/05 11:17:45 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:17:44.961Z\",\n",
      "  \"batchId\" : 6,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 62.5,\n",
      "  \"processedRowsPerSecond\" : 1.4409221902017293,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 595,\n",
      "    \"commitOffsets\" : 34,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 11,\n",
      "    \"triggerExecution\" : 694,\n",
      "    \"walCommit\" : 51\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3237\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3238\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3238\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 62.5,\n",
      "    \"processedRowsPerSecond\" : 1.4409221902017293,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:17:45 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.6.a232028f-2c75-4c0e-87f8-aefacb00a973.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/6\n",
      "25/05/05 11:17:45 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:17:44.959Z\",\n",
      "  \"batchId\" : 6,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.3986013986013988,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 617,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 1,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 13,\n",
      "    \"triggerExecution\" : 715,\n",
      "    \"walCommit\" : 51\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3237\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3238\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3238\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.3986013986013988,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A001|R079|01-00-00|WORLD TRADE CTR|05/05/2025|11:17:43|REGULAR|7          |14        |2025-05-05 11:17:44.963|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:17:49 INFO BlockManagerInfo: Removed broadcast_20_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:17:49 INFO BlockManagerInfo: Removed broadcast_19_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:49 INFO BlockManagerInfo: Removed broadcast_21_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:49 INFO BlockManagerInfo: Removed broadcast_24_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:49 INFO BlockManagerInfo: Removed broadcast_26_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:17:49 INFO BlockManagerInfo: Removed broadcast_23_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:49 INFO BlockManagerInfo: Removed broadcast_25_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:49 INFO BlockManagerInfo: Removed broadcast_22_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:17:50 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/7 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.7.fa243768-a0c1-44a9-97f3-ac7a0c61ea25.tmp\n",
      "25/05/05 11:17:50 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/7 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.7.0aa4673c-4fc0-476d-956b-d11bb7231b9b.tmp\n",
      "25/05/05 11:17:50 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/7 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.7.68288186-eae5-4873-bf18-bfc317c21b67.tmp\n",
      "25/05/05 11:17:50 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.7.fa243768-a0c1-44a9-97f3-ac7a0c61ea25.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/7\n",
      "25/05/05 11:17:50 INFO MicroBatchExecution: Committed offsets for batch 7. Metadata OffsetSeqMetadata(0,1746458270552,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:17:50 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.7.0aa4673c-4fc0-476d-956b-d11bb7231b9b.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/7\n",
      "25/05/05 11:17:50 INFO MicroBatchExecution: Committed offsets for batch 7. Metadata OffsetSeqMetadata(0,1746458270553,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:17:50 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.7.68288186-eae5-4873-bf18-bfc317c21b67.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/7\n",
      "25/05/05 11:17:50 INFO MicroBatchExecution: Committed offsets for batch 7. Metadata OffsetSeqMetadata(0,1746458270552,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:17:50 INFO IncrementalExecution: Current batch timestamp = 1746458270553\n",
      "25/05/05 11:17:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:50 INFO IncrementalExecution: Current batch timestamp = 1746458270552\n",
      "25/05/05 11:17:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:50 INFO IncrementalExecution: Current batch timestamp = 1746458270552\n",
      "25/05/05 11:17:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:50 INFO IncrementalExecution: Current batch timestamp = 1746458270553\n",
      "25/05/05 11:17:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:50 INFO IncrementalExecution: Current batch timestamp = 1746458270552\n",
      "25/05/05 11:17:50 INFO IncrementalExecution: Current batch timestamp = 1746458270552\n",
      "25/05/05 11:17:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:50 INFO IncrementalExecution: Current batch timestamp = 1746458270552\n",
      "25/05/05 11:17:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:50 INFO IncrementalExecution: Current batch timestamp = 1746458270552\n",
      "25/05/05 11:17:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:50 INFO CodeGenerator: Code generated in 12.054417 ms\n",
      "25/05/05 11:17:50 INFO CodeGenerator: Code generated in 14.377584 ms\n",
      "25/05/05 11:17:50 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 7, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:17:50 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:50 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 7, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@26a3417e]. The input RDD has 1 partitions.\n",
      "25/05/05 11:17:50 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:50 INFO DAGScheduler: Got job 21 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:17:50 INFO DAGScheduler: Final stage: ResultStage 20 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:17:50 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:17:50 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:17:50 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[122] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:17:50 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 15.2 KiB, free 366.3 MiB)\n",
      "25/05/05 11:17:50 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.3 MiB)\n",
      "25/05/05 11:17:50 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:50 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:17:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[122] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:17:50 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:17:50 INFO DAGScheduler: Got job 22 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:17:50 INFO DAGScheduler: Final stage: ResultStage 21 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:17:50 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:17:50 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:17:50 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 20) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:17:50 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[123] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:17:50 INFO Executor: Running task 0.0 in stage 20.0 (TID 20)\n",
      "25/05/05 11:17:50 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 16.2 KiB, free 366.3 MiB)\n",
      "25/05/05 11:17:50 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.3 MiB)\n",
      "25/05/05 11:17:50 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:50 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:17:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[123] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:17:50 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:17:50 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 21) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:17:50 INFO Executor: Running task 0.0 in stage 21.0 (TID 21)\n",
      "25/05/05 11:17:50 INFO CodeGenerator: Code generated in 19.722792 ms\n",
      "25/05/05 11:17:50 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3238 untilOffset=3239, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=7 taskId=20 partitionId=0\n",
      "25/05/05 11:17:50 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3238 untilOffset=3239, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=7 taskId=21 partitionId=0\n",
      "25/05/05 11:17:50 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3238 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:50 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:50 INFO DAGScheduler: Got job 23 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:17:50 INFO DAGScheduler: Final stage: ResultStage 22 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:17:50 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:17:50 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:17:50 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[128] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:17:50 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3238 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:50 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 47.1 KiB, free 366.2 MiB)\n",
      "25/05/05 11:17:50 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.2 MiB)\n",
      "25/05/05 11:17:50 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:50 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:17:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[128] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:17:50 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:17:50 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 22) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:17:50 INFO Executor: Running task 0.0 in stage 22.0 (TID 22)\n",
      "25/05/05 11:17:50 INFO CodeGenerator: Code generated in 9.95325 ms\n",
      "25/05/05 11:17:50 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3238 untilOffset=3239, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=7 taskId=22 partitionId=0\n",
      "25/05/05 11:17:50 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3238 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3239, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:51 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:17:51 INFO DataWritingSparkTask: Committed partition 0 (task 20, attempt 0, stage 20.0)\n",
      "25/05/05 11:17:51 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 505532250 nanos, during time span of 506089792 nanos.\n",
      "25/05/05 11:17:51 INFO Executor: Finished task 0.0 in stage 20.0 (TID 20). 2188 bytes result sent to driver\n",
      "25/05/05 11:17:51 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 20) in 540 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:17:51 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:17:51 INFO DAGScheduler: ResultStage 20 (start at NativeMethodAccessorImpl.java:0) finished in 0.547 s\n",
      "25/05/05 11:17:51 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:17:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished\n",
      "25/05/05 11:17:51 INFO DAGScheduler: Job 21 finished: start at NativeMethodAccessorImpl.java:0, took 0.550009 s\n",
      "25/05/05 11:17:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 7, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:17:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3239, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:51 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:17:51 INFO DataWritingSparkTask: Committed partition 0 (task 21, attempt 0, stage 21.0)\n",
      "25/05/05 11:17:51 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 506614041 nanos, during time span of 509284750 nanos.\n",
      "25/05/05 11:17:51 INFO Executor: Finished task 0.0 in stage 21.0 (TID 21). 3559 bytes result sent to driver\n",
      "25/05/05 11:17:51 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 21) in 541 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:17:51 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:17:51 INFO DAGScheduler: ResultStage 21 (start at NativeMethodAccessorImpl.java:0) finished in 0.546 s\n",
      "25/05/05 11:17:51 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:17:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished\n",
      "25/05/05 11:17:51 INFO DAGScheduler: Job 22 finished: start at NativeMethodAccessorImpl.java:0, took 0.555727 s\n",
      "25/05/05 11:17:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 7, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@26a3417e] is committing.\n",
      "25/05/05 11:17:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 7, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@26a3417e] committed.\n",
      "25/05/05 11:17:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 7, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:17:51 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/7 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.7.83c0d679-bb20-4ff3-bd12-2cb9e4c1b4da.tmp\n",
      "25/05/05 11:17:51 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/7 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.7.70de9c2f-c5ac-484a-8256-57f654f66975.tmp\n",
      "25/05/05 11:17:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3239, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:51 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:17:51 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:17:51 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:17:51 INFO connection: Opened connection [connectionId{localValue:13, serverValue:4371}] to localhost:27017\n",
      "25/05/05 11:17:51 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=529041}\n",
      "25/05/05 11:17:51 INFO connection: Opened connection [connectionId{localValue:14, serverValue:4372}] to localhost:27017\n",
      "25/05/05 11:17:51 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:17:51 INFO connection: Closed connection [connectionId{localValue:14, serverValue:4372}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:17:51 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 502302166 nanos, during time span of 515435792 nanos.\n",
      "25/05/05 11:17:51 INFO Executor: Finished task 0.0 in stage 22.0 (TID 22). 1645 bytes result sent to driver\n",
      "25/05/05 11:17:51 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 22) in 539 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:17:51 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:17:51 INFO DAGScheduler: ResultStage 22 (start at NativeMethodAccessorImpl.java:0) finished in 0.549 s\n",
      "25/05/05 11:17:51 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:17:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished\n",
      "25/05/05 11:17:51 INFO DAGScheduler: Job 23 finished: start at NativeMethodAccessorImpl.java:0, took 0.551419 s\n",
      "25/05/05 11:17:51 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 248.0 B, free 366.2 MiB)\n",
      "25/05/05 11:17:51 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.2 MiB)\n",
      "25/05/05 11:17:51 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:17:51 INFO SparkContext: Created broadcast 30 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:51 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.7.83c0d679-bb20-4ff3-bd12-2cb9e4c1b4da.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/7\n",
      "25/05/05 11:17:51 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:17:50.551Z\",\n",
      "  \"batchId\" : 7,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.4084507042253522,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 592,\n",
      "    \"commitOffsets\" : 43,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 11,\n",
      "    \"triggerExecution\" : 710,\n",
      "    \"walCommit\" : 63\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3238\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3239\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3239\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.4084507042253522,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:17:51 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/7 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.7.2d37914a-4ace-49c6-b1a3-4ba0d0ceaff3.tmp\n",
      "25/05/05 11:17:51 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.7.70de9c2f-c5ac-484a-8256-57f654f66975.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/7\n",
      "25/05/05 11:17:51 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:17:50.551Z\",\n",
      "  \"batchId\" : 7,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.3966480446927374,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 601,\n",
      "    \"commitOffsets\" : 39,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 16,\n",
      "    \"triggerExecution\" : 716,\n",
      "    \"walCommit\" : 58\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3238\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3239\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3239\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.3966480446927374,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:17:51 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.7.2d37914a-4ace-49c6-b1a3-4ba0d0ceaff3.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/7\n",
      "25/05/05 11:17:51 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:17:50.551Z\",\n",
      "  \"batchId\" : 7,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.3550135501355014,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 636,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 9,\n",
      "    \"triggerExecution\" : 738,\n",
      "    \"walCommit\" : 59\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3238\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3239\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3239\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.3550135501355014,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A001|R079|01-00-00|WORLD TRADE CTR|05/05/2025|11:17:49|REGULAR|5          |11        |2025-05-05 11:17:50.552|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:17:56 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/8 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.8.8cfde3d0-edf7-4c17-b9f4-a21f146df6b6.tmp\n",
      "25/05/05 11:17:56 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/8 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.8.92e0d907-4ea0-421b-803e-b59f83669349.tmp\n",
      "25/05/05 11:17:56 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/8 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.8.2e49694f-3628-4c59-b028-389b6b04435b.tmp\n",
      "25/05/05 11:17:56 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.8.8cfde3d0-edf7-4c17-b9f4-a21f146df6b6.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/8\n",
      "25/05/05 11:17:56 INFO MicroBatchExecution: Committed offsets for batch 8. Metadata OffsetSeqMetadata(0,1746458276152,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:17:56 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.8.92e0d907-4ea0-421b-803e-b59f83669349.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/8\n",
      "25/05/05 11:17:56 INFO MicroBatchExecution: Committed offsets for batch 8. Metadata OffsetSeqMetadata(0,1746458276151,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:17:56 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.8.2e49694f-3628-4c59-b028-389b6b04435b.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/8\n",
      "25/05/05 11:17:56 INFO MicroBatchExecution: Committed offsets for batch 8. Metadata OffsetSeqMetadata(0,1746458276160,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:17:56 INFO IncrementalExecution: Current batch timestamp = 1746458276151\n",
      "25/05/05 11:17:56 INFO IncrementalExecution: Current batch timestamp = 1746458276152\n",
      "25/05/05 11:17:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:56 INFO IncrementalExecution: Current batch timestamp = 1746458276160\n",
      "25/05/05 11:17:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:56 INFO IncrementalExecution: Current batch timestamp = 1746458276152\n",
      "25/05/05 11:17:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:56 INFO IncrementalExecution: Current batch timestamp = 1746458276160\n",
      "25/05/05 11:17:56 INFO IncrementalExecution: Current batch timestamp = 1746458276151\n",
      "25/05/05 11:17:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:56 INFO IncrementalExecution: Current batch timestamp = 1746458276152\n",
      "25/05/05 11:17:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:56 INFO IncrementalExecution: Current batch timestamp = 1746458276151\n",
      "25/05/05 11:17:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:17:56 INFO CodeGenerator: Code generated in 7.623209 ms\n",
      "25/05/05 11:17:56 INFO CodeGenerator: Code generated in 7.974875 ms\n",
      "25/05/05 11:17:56 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 8, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@5a43a1fa]. The input RDD has 1 partitions.\n",
      "25/05/05 11:17:56 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:56 INFO DAGScheduler: Got job 24 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:17:56 INFO DAGScheduler: Final stage: ResultStage 23 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:17:56 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:17:56 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:17:56 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[136] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:17:56 INFO CodeGenerator: Code generated in 11.477916 ms\n",
      "25/05/05 11:17:56 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:17:56 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 8, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:17:56 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:17:56 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:56 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:56 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:17:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[136] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:17:56 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:17:56 INFO DAGScheduler: Got job 25 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:17:56 INFO DAGScheduler: Final stage: ResultStage 24 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:17:56 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:17:56 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:17:56 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[139] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:17:56 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:17:56 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 23) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:17:56 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:17:56 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:56 INFO Executor: Running task 0.0 in stage 23.0 (TID 23)\n",
      "25/05/05 11:17:56 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:17:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[139] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:17:56 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:17:56 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 24) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:17:56 INFO Executor: Running task 0.0 in stage 24.0 (TID 24)\n",
      "25/05/05 11:17:56 INFO CodeGenerator: Code generated in 7.958083 ms\n",
      "25/05/05 11:17:56 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:56 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3239 untilOffset=3240, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=8 taskId=23 partitionId=0\n",
      "25/05/05 11:17:56 INFO DAGScheduler: Got job 26 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:17:56 INFO DAGScheduler: Final stage: ResultStage 25 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:17:56 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:17:56 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:17:56 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[144] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:17:56 INFO CodeGenerator: Code generated in 10.028084 ms\n",
      "25/05/05 11:17:56 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3239 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:56 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3239 untilOffset=3240, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=8 taskId=24 partitionId=0\n",
      "25/05/05 11:17:56 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:17:56 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:17:56 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:17:56 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:17:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[144] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:17:56 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:17:56 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3239 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:56 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 25) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:17:56 INFO Executor: Running task 0.0 in stage 25.0 (TID 25)\n",
      "25/05/05 11:17:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:56 INFO CodeGenerator: Code generated in 9.403 ms\n",
      "25/05/05 11:17:56 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3239 untilOffset=3240, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=8 taskId=25 partitionId=0\n",
      "25/05/05 11:17:56 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3239 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3240, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3240, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:56 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:17:56 INFO DataWritingSparkTask: Committed partition 0 (task 24, attempt 0, stage 24.0)\n",
      "25/05/05 11:17:56 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 504583833 nanos, during time span of 505133333 nanos.\n",
      "25/05/05 11:17:56 INFO Executor: Finished task 0.0 in stage 24.0 (TID 24). 2180 bytes result sent to driver\n",
      "25/05/05 11:17:56 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 24) in 526 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:17:56 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:17:56 INFO DAGScheduler: ResultStage 24 (start at NativeMethodAccessorImpl.java:0) finished in 0.530 s\n",
      "25/05/05 11:17:56 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:17:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished\n",
      "25/05/05 11:17:56 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:17:56 INFO DataWritingSparkTask: Committed partition 0 (task 23, attempt 0, stage 23.0)\n",
      "25/05/05 11:17:56 INFO DAGScheduler: Job 25 finished: start at NativeMethodAccessorImpl.java:0, took 0.532964 s\n",
      "25/05/05 11:17:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:17:56 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 510759625 nanos, during time span of 513483917 nanos.\n",
      "25/05/05 11:17:56 INFO Executor: Finished task 0.0 in stage 23.0 (TID 23). 3549 bytes result sent to driver\n",
      "25/05/05 11:17:56 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 23) in 533 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:17:56 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:17:56 INFO DAGScheduler: ResultStage 23 (start at NativeMethodAccessorImpl.java:0) finished in 0.540 s\n",
      "25/05/05 11:17:56 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:17:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished\n",
      "25/05/05 11:17:56 INFO DAGScheduler: Job 24 finished: start at NativeMethodAccessorImpl.java:0, took 0.542074 s\n",
      "25/05/05 11:17:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@5a43a1fa] is committing.\n",
      "25/05/05 11:17:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@5a43a1fa] committed.\n",
      "25/05/05 11:17:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:17:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:17:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3240, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:17:56 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:17:56 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:17:56 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:17:56 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/8 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.8.6e84e493-7e21-4cc6-8b48-6263c832e338.tmp\n",
      "25/05/05 11:17:56 INFO connection: Opened connection [connectionId{localValue:15, serverValue:4373}] to localhost:27017\n",
      "25/05/05 11:17:56 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=297208}\n",
      "25/05/05 11:17:56 INFO connection: Opened connection [connectionId{localValue:16, serverValue:4374}] to localhost:27017\n",
      "25/05/05 11:17:56 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:17:56 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/8 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.8.f763d338-c8ef-444c-b12f-07389e10befa.tmp\n",
      "25/05/05 11:17:56 INFO connection: Closed connection [connectionId{localValue:16, serverValue:4374}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:17:56 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 506531833 nanos, during time span of 512879750 nanos.\n",
      "25/05/05 11:17:56 INFO Executor: Finished task 0.0 in stage 25.0 (TID 25). 1645 bytes result sent to driver\n",
      "25/05/05 11:17:56 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 25) in 533 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:17:56 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:17:56 INFO DAGScheduler: ResultStage 25 (start at NativeMethodAccessorImpl.java:0) finished in 0.539 s\n",
      "25/05/05 11:17:56 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:17:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished\n",
      "25/05/05 11:17:56 INFO DAGScheduler: Job 26 finished: start at NativeMethodAccessorImpl.java:0, took 0.543093 s\n",
      "25/05/05 11:17:56 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:17:56 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:17:56 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:17:56 INFO SparkContext: Created broadcast 34 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:17:56 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/8 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.8.8d420c40-8e4d-42f3-bceb-27c15df3518b.tmp\n",
      "25/05/05 11:17:56 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.8.6e84e493-7e21-4cc6-8b48-6263c832e338.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/8\n",
      "25/05/05 11:17:56 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:17:56.149Z\",\n",
      "  \"batchId\" : 8,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.4598540145985401,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 577,\n",
      "    \"commitOffsets\" : 42,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 9,\n",
      "    \"triggerExecution\" : 685,\n",
      "    \"walCommit\" : 53\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3239\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3240\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3240\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.4598540145985401,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:17:56 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.8.f763d338-c8ef-444c-b12f-07389e10befa.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/8\n",
      "25/05/05 11:17:56 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:17:56.149Z\",\n",
      "  \"batchId\" : 8,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.4492753623188408,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 589,\n",
      "    \"commitOffsets\" : 35,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 9,\n",
      "    \"triggerExecution\" : 690,\n",
      "    \"walCommit\" : 55\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3239\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3240\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3240\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.4492753623188408,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:17:56 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.8.8d420c40-8e4d-42f3-bceb-27c15df3518b.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/8\n",
      "25/05/05 11:17:56 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:17:56.151Z\",\n",
      "  \"batchId\" : 8,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.4388489208633095,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 601,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 9,\n",
      "    \"queryPlanning\" : 10,\n",
      "    \"triggerExecution\" : 695,\n",
      "    \"walCommit\" : 46\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3239\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3240\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3240\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.4388489208633095,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+----+----+--------+-------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION|DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+-------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R029|R044|00-00-00|23 ST  |05/05/2025|11:17:54|REGULAR|10         |8         |2025-05-05 11:17:56.151|\n",
      "+----+----+--------+-------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:17:57 INFO BlockManagerInfo: Removed broadcast_32_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:17:57 INFO BlockManagerInfo: Removed broadcast_28_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:57 INFO BlockManagerInfo: Removed broadcast_33_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:57 INFO BlockManagerInfo: Removed broadcast_27_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:57 INFO BlockManagerInfo: Removed broadcast_29_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:57 INFO BlockManagerInfo: Removed broadcast_34_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:17:57 INFO BlockManagerInfo: Removed broadcast_31_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:17:57 INFO BlockManagerInfo: Removed broadcast_30_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:18:01 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/9 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.9.03642930-ff4d-473d-a56c-162aebaaa8e2.tmp\n",
      "25/05/05 11:18:01 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/9 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.9.778c374f-0152-4801-95a1-8a55b842daa5.tmp\n",
      "25/05/05 11:18:01 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/9 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.9.9abf5e4d-a1b9-4518-b4b3-b7f8bc87a449.tmp\n",
      "25/05/05 11:18:01 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.9.9abf5e4d-a1b9-4518-b4b3-b7f8bc87a449.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/9\n",
      "25/05/05 11:18:01 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.9.03642930-ff4d-473d-a56c-162aebaaa8e2.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/9\n",
      "25/05/05 11:18:01 INFO MicroBatchExecution: Committed offsets for batch 9. Metadata OffsetSeqMetadata(0,1746458281735,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:18:01 INFO MicroBatchExecution: Committed offsets for batch 9. Metadata OffsetSeqMetadata(0,1746458281735,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:18:01 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.9.778c374f-0152-4801-95a1-8a55b842daa5.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/9\n",
      "25/05/05 11:18:01 INFO MicroBatchExecution: Committed offsets for batch 9. Metadata OffsetSeqMetadata(0,1746458281737,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:18:01 INFO IncrementalExecution: Current batch timestamp = 1746458281735\n",
      "25/05/05 11:18:01 INFO IncrementalExecution: Current batch timestamp = 1746458281735\n",
      "25/05/05 11:18:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:01 INFO IncrementalExecution: Current batch timestamp = 1746458281737\n",
      "25/05/05 11:18:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:01 INFO IncrementalExecution: Current batch timestamp = 1746458281735\n",
      "25/05/05 11:18:01 INFO IncrementalExecution: Current batch timestamp = 1746458281735\n",
      "25/05/05 11:18:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:01 INFO IncrementalExecution: Current batch timestamp = 1746458281737\n",
      "25/05/05 11:18:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:01 INFO IncrementalExecution: Current batch timestamp = 1746458281737\n",
      "25/05/05 11:18:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:01 INFO IncrementalExecution: Current batch timestamp = 1746458281735\n",
      "25/05/05 11:18:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:01 INFO CodeGenerator: Code generated in 18.797208 ms\n",
      "25/05/05 11:18:01 INFO CodeGenerator: Code generated in 10.640583 ms\n",
      "25/05/05 11:18:01 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 9, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@19faa3c4]. The input RDD has 1 partitions.\n",
      "25/05/05 11:18:01 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:01 INFO DAGScheduler: Got job 27 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:18:01 INFO DAGScheduler: Final stage: ResultStage 26 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:18:01 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:18:01 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:18:01 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[152] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:18:01 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 9, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:18:01 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 16.2 KiB, free 366.3 MiB)\n",
      "25/05/05 11:18:01 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:01 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.3 MiB)\n",
      "25/05/05 11:18:01 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:01 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:18:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[152] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:18:01 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:18:01 INFO DAGScheduler: Got job 28 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:18:01 INFO DAGScheduler: Final stage: ResultStage 27 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:18:01 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:18:01 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:18:01 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[155] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:18:01 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 26) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:18:01 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 15.2 KiB, free 366.3 MiB)\n",
      "25/05/05 11:18:01 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.3 MiB)\n",
      "25/05/05 11:18:01 INFO Executor: Running task 0.0 in stage 26.0 (TID 26)\n",
      "25/05/05 11:18:01 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:01 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:18:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[155] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:18:01 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:18:01 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 27) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:18:01 INFO Executor: Running task 0.0 in stage 27.0 (TID 27)\n",
      "25/05/05 11:18:01 INFO CodeGenerator: Code generated in 8.678916 ms\n",
      "25/05/05 11:18:01 INFO CodeGenerator: Code generated in 6.135458 ms\n",
      "25/05/05 11:18:01 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3240 untilOffset=3241, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=9 taskId=27 partitionId=0\n",
      "25/05/05 11:18:01 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3240 untilOffset=3241, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=9 taskId=26 partitionId=0\n",
      "25/05/05 11:18:01 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3240 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:01 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3240 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:01 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:01 INFO DAGScheduler: Got job 29 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:18:01 INFO DAGScheduler: Final stage: ResultStage 28 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:18:01 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:18:01 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:18:01 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[160] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:18:01 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 47.1 KiB, free 366.2 MiB)\n",
      "25/05/05 11:18:01 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.2 MiB)\n",
      "25/05/05 11:18:01 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:01 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:18:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[160] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:18:01 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:18:01 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 28) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:18:01 INFO Executor: Running task 0.0 in stage 28.0 (TID 28)\n",
      "25/05/05 11:18:01 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3240 untilOffset=3241, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=9 taskId=28 partitionId=0\n",
      "25/05/05 11:18:01 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3240 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3241, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3241, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:02 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:18:02 INFO DataWritingSparkTask: Committed partition 0 (task 27, attempt 0, stage 27.0)\n",
      "25/05/05 11:18:02 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 506215167 nanos, during time span of 506723250 nanos.\n",
      "25/05/05 11:18:02 INFO Executor: Finished task 0.0 in stage 27.0 (TID 27). 2188 bytes result sent to driver\n",
      "25/05/05 11:18:02 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 27) in 522 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:18:02 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:18:02 INFO DAGScheduler: ResultStage 27 (start at NativeMethodAccessorImpl.java:0) finished in 0.526 s\n",
      "25/05/05 11:18:02 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:18:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished\n",
      "25/05/05 11:18:02 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:18:02 INFO DataWritingSparkTask: Committed partition 0 (task 26, attempt 0, stage 26.0)\n",
      "25/05/05 11:18:02 INFO DAGScheduler: Job 28 finished: start at NativeMethodAccessorImpl.java:0, took 0.530130 s\n",
      "25/05/05 11:18:02 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 504126083 nanos, during time span of 506643208 nanos.\n",
      "25/05/05 11:18:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 9, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:18:02 INFO Executor: Finished task 0.0 in stage 26.0 (TID 26). 3559 bytes result sent to driver\n",
      "25/05/05 11:18:02 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 26) in 528 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:18:02 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:18:02 INFO DAGScheduler: ResultStage 26 (start at NativeMethodAccessorImpl.java:0) finished in 0.533 s\n",
      "25/05/05 11:18:02 INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:18:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished\n",
      "25/05/05 11:18:02 INFO DAGScheduler: Job 27 finished: start at NativeMethodAccessorImpl.java:0, took 0.535539 s\n",
      "25/05/05 11:18:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 9, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@19faa3c4] is committing.\n",
      "25/05/05 11:18:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 9, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@19faa3c4] committed.\n",
      "25/05/05 11:18:02 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/9 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.9.84d534d3-db15-4ca8-8407-b1d16228c3f1.tmp\n",
      "25/05/05 11:18:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 9, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:18:02 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/9 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.9.51f1c708-ba85-4c0c-beac-07073d1dea59.tmp\n",
      "25/05/05 11:18:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3241, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:02 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:18:02 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:18:02 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:18:02 INFO connection: Opened connection [connectionId{localValue:17, serverValue:4383}] to localhost:27017\n",
      "25/05/05 11:18:02 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=281375}\n",
      "25/05/05 11:18:02 INFO connection: Opened connection [connectionId{localValue:18, serverValue:4384}] to localhost:27017\n",
      "25/05/05 11:18:02 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:18:02 INFO connection: Closed connection [connectionId{localValue:18, serverValue:4384}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:18:02 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 506793250 nanos, during time span of 514437667 nanos.\n",
      "25/05/05 11:18:02 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.9.84d534d3-db15-4ca8-8407-b1d16228c3f1.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/9\n",
      "25/05/05 11:18:02 INFO Executor: Finished task 0.0 in stage 28.0 (TID 28). 1645 bytes result sent to driver\n",
      "25/05/05 11:18:02 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 28) in 524 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:18:02 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:18:02 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:18:01.734Z\",\n",
      "  \"batchId\" : 9,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.4992503748125936,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 566,\n",
      "    \"commitOffsets\" : 34,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 9,\n",
      "    \"triggerExecution\" : 667,\n",
      "    \"walCommit\" : 54\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3240\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3241\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3241\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.4992503748125936,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:18:02 INFO DAGScheduler: ResultStage 28 (start at NativeMethodAccessorImpl.java:0) finished in 0.530 s\n",
      "25/05/05 11:18:02 INFO DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:18:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished\n",
      "25/05/05 11:18:02 INFO DAGScheduler: Job 29 finished: start at NativeMethodAccessorImpl.java:0, took 0.532504 s\n",
      "25/05/05 11:18:02 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 248.0 B, free 366.2 MiB)\n",
      "25/05/05 11:18:02 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.2 MiB)\n",
      "25/05/05 11:18:02 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:18:02 INFO SparkContext: Created broadcast 38 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:02 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.9.51f1c708-ba85-4c0c-beac-07073d1dea59.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/9\n",
      "25/05/05 11:18:02 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/9 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.9.e70d0bc7-2f7c-456d-b709-183b0bfdeb97.tmp\n",
      "25/05/05 11:18:02 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:18:01.733Z\",\n",
      "  \"batchId\" : 9,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.4727540500736376,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 581,\n",
      "    \"commitOffsets\" : 33,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 8,\n",
      "    \"triggerExecution\" : 679,\n",
      "    \"walCommit\" : 55\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3240\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3241\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3241\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.4727540500736376,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:18:02 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.9.e70d0bc7-2f7c-456d-b709-183b0bfdeb97.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/9\n",
      "25/05/05 11:18:02 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:18:01.732Z\",\n",
      "  \"batchId\" : 9,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.3986013986013988,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 608,\n",
      "    \"commitOffsets\" : 40,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 8,\n",
      "    \"triggerExecution\" : 715,\n",
      "    \"walCommit\" : 55\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3240\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3241\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3241\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.3986013986013988,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R023|R045|00-00-00|34 ST-HERALD SQ|05/05/2025|11:18:00|REGULAR|13         |14        |2025-05-05 11:18:01.735|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:18:08 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/10 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.10.e0ef5551-8866-4c93-8c71-661a7f7c9c0d.tmp\n",
      "25/05/05 11:18:08 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/10 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.10.d3ceacdb-3bc0-45b9-a861-f739a3b779ef.tmp\n",
      "25/05/05 11:18:08 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/10 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.10.3edc1688-914f-4d0d-9672-bb3e6da9b563.tmp\n",
      "25/05/05 11:18:08 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.10.3edc1688-914f-4d0d-9672-bb3e6da9b563.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/10\n",
      "25/05/05 11:18:08 INFO MicroBatchExecution: Committed offsets for batch 10. Metadata OffsetSeqMetadata(0,1746458288316,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:18:08 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.10.e0ef5551-8866-4c93-8c71-661a7f7c9c0d.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/10\n",
      "25/05/05 11:18:08 INFO MicroBatchExecution: Committed offsets for batch 10. Metadata OffsetSeqMetadata(0,1746458288316,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:18:08 INFO IncrementalExecution: Current batch timestamp = 1746458288316\n",
      "25/05/05 11:18:08 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.10.d3ceacdb-3bc0-45b9-a861-f739a3b779ef.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/10\n",
      "25/05/05 11:18:08 INFO MicroBatchExecution: Committed offsets for batch 10. Metadata OffsetSeqMetadata(0,1746458288318,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:18:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:08 INFO IncrementalExecution: Current batch timestamp = 1746458288316\n",
      "25/05/05 11:18:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:08 INFO IncrementalExecution: Current batch timestamp = 1746458288318\n",
      "25/05/05 11:18:08 INFO IncrementalExecution: Current batch timestamp = 1746458288316\n",
      "25/05/05 11:18:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:08 INFO IncrementalExecution: Current batch timestamp = 1746458288316\n",
      "25/05/05 11:18:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:08 INFO IncrementalExecution: Current batch timestamp = 1746458288318\n",
      "25/05/05 11:18:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:08 INFO IncrementalExecution: Current batch timestamp = 1746458288316\n",
      "25/05/05 11:18:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:08 INFO IncrementalExecution: Current batch timestamp = 1746458288318\n",
      "25/05/05 11:18:08 INFO CodeGenerator: Code generated in 7.03025 ms\n",
      "25/05/05 11:18:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:08 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 10, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@32e502d0]. The input RDD has 1 partitions.\n",
      "25/05/05 11:18:08 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:08 INFO DAGScheduler: Got job 30 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:18:08 INFO DAGScheduler: Final stage: ResultStage 29 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:18:08 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:18:08 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:18:08 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[168] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:18:08 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:18:08 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:18:08 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:08 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:18:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[168] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:18:08 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:18:08 INFO CodeGenerator: Code generated in 22.701583 ms\n",
      "25/05/05 11:18:08 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 29) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:18:08 INFO Executor: Running task 0.0 in stage 29.0 (TID 29)\n",
      "25/05/05 11:18:08 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 10, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:18:08 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:08 INFO DAGScheduler: Got job 31 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:18:08 INFO DAGScheduler: Final stage: ResultStage 30 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:18:08 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:18:08 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:18:08 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[171] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:18:08 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:18:08 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:18:08 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:08 INFO CodeGenerator: Code generated in 7.004625 ms\n",
      "25/05/05 11:18:08 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:18:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[171] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:18:08 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:18:08 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3241 untilOffset=3242, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=10 taskId=29 partitionId=0\n",
      "25/05/05 11:18:08 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3241 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:08 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 30) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:18:08 INFO Executor: Running task 0.0 in stage 30.0 (TID 30)\n",
      "25/05/05 11:18:08 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:08 INFO DAGScheduler: Got job 32 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:18:08 INFO DAGScheduler: Final stage: ResultStage 31 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:18:08 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:18:08 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:18:08 INFO DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[176] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:18:08 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:18:08 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.1 MiB)\n",
      "25/05/05 11:18:08 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:18:08 INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:18:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[176] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:18:08 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:18:08 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 31) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:18:08 INFO Executor: Running task 0.0 in stage 31.0 (TID 31)\n",
      "25/05/05 11:18:08 INFO CodeGenerator: Code generated in 11.287334 ms\n",
      "25/05/05 11:18:08 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3241 untilOffset=3242, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=10 taskId=30 partitionId=0\n",
      "25/05/05 11:18:08 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3241 untilOffset=3242, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=10 taskId=31 partitionId=0\n",
      "25/05/05 11:18:08 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3241 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:08 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3241 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:08 INFO BlockManagerInfo: Removed broadcast_38_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:18:08 INFO BlockManagerInfo: Removed broadcast_36_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:18:08 INFO BlockManagerInfo: Removed broadcast_35_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:08 INFO BlockManagerInfo: Removed broadcast_37_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3242, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:08 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:18:08 INFO DataWritingSparkTask: Committed partition 0 (task 29, attempt 0, stage 29.0)\n",
      "25/05/05 11:18:08 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 508623625 nanos, during time span of 511348792 nanos.\n",
      "25/05/05 11:18:08 INFO Executor: Finished task 0.0 in stage 29.0 (TID 29). 3596 bytes result sent to driver\n",
      "25/05/05 11:18:08 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 29) in 540 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:18:08 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:18:08 INFO DAGScheduler: ResultStage 29 (start at NativeMethodAccessorImpl.java:0) finished in 0.565 s\n",
      "25/05/05 11:18:08 INFO DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:18:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished\n",
      "25/05/05 11:18:08 INFO DAGScheduler: Job 30 finished: start at NativeMethodAccessorImpl.java:0, took 0.567720 s\n",
      "25/05/05 11:18:08 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 10, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@32e502d0] is committing.\n",
      "25/05/05 11:18:08 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 10, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@32e502d0] committed.\n",
      "25/05/05 11:18:08 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/10 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.10.2f25863d-113e-4b1f-8415-960fe465f58f.tmp\n",
      "25/05/05 11:18:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3242, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3242, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:09 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:18:09 INFO DataWritingSparkTask: Committed partition 0 (task 30, attempt 0, stage 30.0)\n",
      "25/05/05 11:18:09 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:18:09 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 503538750 nanos, during time span of 505238833 nanos.\n",
      "25/05/05 11:18:09 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:18:09 INFO Executor: Finished task 0.0 in stage 30.0 (TID 30). 2231 bytes result sent to driver\n",
      "25/05/05 11:18:09 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:18:09 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 30) in 540 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:18:09 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:18:09 INFO DAGScheduler: ResultStage 30 (start at NativeMethodAccessorImpl.java:0) finished in 0.561 s\n",
      "25/05/05 11:18:09 INFO DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:18:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished\n",
      "25/05/05 11:18:09 INFO DAGScheduler: Job 31 finished: start at NativeMethodAccessorImpl.java:0, took 0.565665 s\n",
      "25/05/05 11:18:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 10, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:18:09 INFO connection: Opened connection [connectionId{localValue:19, serverValue:4387}] to localhost:27017\n",
      "25/05/05 11:18:09 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=371833}\n",
      "25/05/05 11:18:09 INFO connection: Opened connection [connectionId{localValue:20, serverValue:4388}] to localhost:27017\n",
      "25/05/05 11:18:09 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:18:09 INFO connection: Closed connection [connectionId{localValue:20, serverValue:4388}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:18:09 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 503339875 nanos, during time span of 510633625 nanos.\n",
      "25/05/05 11:18:09 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.10.2f25863d-113e-4b1f-8415-960fe465f58f.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/10\n",
      "25/05/05 11:18:09 INFO Executor: Finished task 0.0 in stage 31.0 (TID 31). 1688 bytes result sent to driver\n",
      "25/05/05 11:18:09 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:18:08.315Z\",\n",
      "  \"batchId\" : 10,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.4245014245014247,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 594,\n",
      "    \"commitOffsets\" : 32,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 11,\n",
      "    \"triggerExecution\" : 702,\n",
      "    \"walCommit\" : 64\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3241\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3242\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3242\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.4245014245014247,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:18:09 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 31) in 536 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:18:09 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:18:09 INFO DAGScheduler: ResultStage 31 (start at NativeMethodAccessorImpl.java:0) finished in 0.544 s\n",
      "25/05/05 11:18:09 INFO DAGScheduler: Job 32 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:18:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 31: Stage finished\n",
      "25/05/05 11:18:09 INFO DAGScheduler: Job 32 finished: start at NativeMethodAccessorImpl.java:0, took 0.545357 s\n",
      "25/05/05 11:18:09 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 248.0 B, free 366.2 MiB)\n",
      "25/05/05 11:18:09 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.2 MiB)\n",
      "25/05/05 11:18:09 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:18:09 INFO SparkContext: Created broadcast 42 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 10, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:18:09 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/10 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.10.13c1bd35-4870-464b-b95a-d998a4159b72.tmp\n",
      "25/05/05 11:18:09 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/10 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.10.b05dc5e8-157a-4cf3-9cb7-ef013cef6d16.tmp\n",
      "25/05/05 11:18:09 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.10.13c1bd35-4870-464b-b95a-d998a4159b72.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/10\n",
      "25/05/05 11:18:09 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:18:08.314Z\",\n",
      "  \"batchId\" : 10,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.3550135501355014,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 631,\n",
      "    \"commitOffsets\" : 31,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 9,\n",
      "    \"triggerExecution\" : 738,\n",
      "    \"walCommit\" : 64\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3241\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3242\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3242\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.3550135501355014,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:18:09 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.10.b05dc5e8-157a-4cf3-9cb7-ef013cef6d16.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/10\n",
      "25/05/05 11:18:09 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:18:08.315Z\",\n",
      "  \"batchId\" : 10,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "  \"processedRowsPerSecond\" : 1.3458950201884252,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 632,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 11,\n",
      "    \"triggerExecution\" : 743,\n",
      "    \"walCommit\" : 68\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3241\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3242\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3242\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "    \"processedRowsPerSecond\" : 1.3458950201884252,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 10\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION  |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A002|R170|00-00-00|FULTON ST|05/05/2025|11:18:07|REGULAR|14         |6         |2025-05-05 11:18:08.318|\n",
      "+----+----+--------+---------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:18:14 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/11 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.11.4d871ff6-1085-4335-a8c6-bb5ee408a5b9.tmp\n",
      "25/05/05 11:18:14 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/11 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.11.bebb7ceb-db4c-4b29-8f60-c352e3ebbb01.tmp\n",
      "25/05/05 11:18:14 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/11 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.11.ba72a698-2ae9-4168-8d15-7878b3a0c370.tmp\n",
      "25/05/05 11:18:14 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.11.bebb7ceb-db4c-4b29-8f60-c352e3ebbb01.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/11\n",
      "25/05/05 11:18:14 INFO MicroBatchExecution: Committed offsets for batch 11. Metadata OffsetSeqMetadata(0,1746458294915,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:18:14 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.11.ba72a698-2ae9-4168-8d15-7878b3a0c370.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/11\n",
      "25/05/05 11:18:14 INFO MicroBatchExecution: Committed offsets for batch 11. Metadata OffsetSeqMetadata(0,1746458294914,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:18:14 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.11.4d871ff6-1085-4335-a8c6-bb5ee408a5b9.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/11\n",
      "25/05/05 11:18:14 INFO MicroBatchExecution: Committed offsets for batch 11. Metadata OffsetSeqMetadata(0,1746458294914,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:18:14 INFO IncrementalExecution: Current batch timestamp = 1746458294914\n",
      "25/05/05 11:18:14 INFO IncrementalExecution: Current batch timestamp = 1746458294915\n",
      "25/05/05 11:18:14 INFO IncrementalExecution: Current batch timestamp = 1746458294914\n",
      "25/05/05 11:18:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:14 INFO IncrementalExecution: Current batch timestamp = 1746458294914\n",
      "25/05/05 11:18:14 INFO IncrementalExecution: Current batch timestamp = 1746458294914\n",
      "25/05/05 11:18:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:14 INFO IncrementalExecution: Current batch timestamp = 1746458294915\n",
      "25/05/05 11:18:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:14 INFO IncrementalExecution: Current batch timestamp = 1746458294914\n",
      "25/05/05 11:18:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:14 INFO IncrementalExecution: Current batch timestamp = 1746458294915\n",
      "25/05/05 11:18:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:15 INFO CodeGenerator: Code generated in 9.583042 ms\n",
      "25/05/05 11:18:15 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 11, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:18:15 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:15 INFO CodeGenerator: Code generated in 7.643208 ms\n",
      "25/05/05 11:18:15 INFO DAGScheduler: Got job 33 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:18:15 INFO DAGScheduler: Final stage: ResultStage 32 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:18:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:18:15 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:18:15 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[181] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:18:15 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 11, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@63e2b65d]. The input RDD has 1 partitions.\n",
      "25/05/05 11:18:15 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:15 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:18:15 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:18:15 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:15 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:18:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[181] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:18:15 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:18:15 INFO DAGScheduler: Got job 34 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:18:15 INFO DAGScheduler: Final stage: ResultStage 33 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:18:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:18:15 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:18:15 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[187] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:18:15 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:18:15 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:18:15 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:15 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 32) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:18:15 INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:18:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[187] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:18:15 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:18:15 INFO Executor: Running task 0.0 in stage 32.0 (TID 32)\n",
      "25/05/05 11:18:15 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 33) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:18:15 INFO Executor: Running task 0.0 in stage 33.0 (TID 33)\n",
      "25/05/05 11:18:15 INFO CodeGenerator: Code generated in 21.306959 ms\n",
      "25/05/05 11:18:15 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3242 untilOffset=3243, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=11 taskId=32 partitionId=0\n",
      "25/05/05 11:18:15 INFO CodeGenerator: Code generated in 23.653625 ms\n",
      "25/05/05 11:18:15 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3242 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:15 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:15 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3242 untilOffset=3243, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=11 taskId=33 partitionId=0\n",
      "25/05/05 11:18:15 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3242 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:15 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:15 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:15 INFO DAGScheduler: Got job 35 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:18:15 INFO DAGScheduler: Final stage: ResultStage 34 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:18:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:18:15 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:18:15 INFO DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[192] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:18:15 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:18:15 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.1 MiB)\n",
      "25/05/05 11:18:15 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:18:15 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:18:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[192] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:18:15 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:18:15 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 34) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:18:15 INFO Executor: Running task 0.0 in stage 34.0 (TID 34)\n",
      "25/05/05 11:18:15 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3242 untilOffset=3243, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=11 taskId=34 partitionId=0\n",
      "25/05/05 11:18:15 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3242 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:15 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:15 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:15 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:15 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3243, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:15 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:18:15 INFO DataWritingSparkTask: Committed partition 0 (task 32, attempt 0, stage 32.0)\n",
      "25/05/05 11:18:15 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 507148208 nanos, during time span of 507668625 nanos.\n",
      "25/05/05 11:18:15 INFO Executor: Finished task 0.0 in stage 32.0 (TID 32). 2188 bytes result sent to driver\n",
      "25/05/05 11:18:15 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:15 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:15 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 32) in 540 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:18:15 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:18:15 INFO DAGScheduler: ResultStage 32 (start at NativeMethodAccessorImpl.java:0) finished in 0.548 s\n",
      "25/05/05 11:18:15 INFO DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:18:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 32: Stage finished\n",
      "25/05/05 11:18:15 INFO DAGScheduler: Job 33 finished: start at NativeMethodAccessorImpl.java:0, took 0.552931 s\n",
      "25/05/05 11:18:15 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 11, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:18:15 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3243, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:15 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:18:15 INFO DataWritingSparkTask: Committed partition 0 (task 33, attempt 0, stage 33.0)\n",
      "25/05/05 11:18:15 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 502999834 nanos, during time span of 505449458 nanos.\n",
      "25/05/05 11:18:15 INFO Executor: Finished task 0.0 in stage 33.0 (TID 33). 3557 bytes result sent to driver\n",
      "25/05/05 11:18:15 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 33) in 542 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:18:15 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:18:15 INFO DAGScheduler: ResultStage 33 (start at NativeMethodAccessorImpl.java:0) finished in 0.547 s\n",
      "25/05/05 11:18:15 INFO DAGScheduler: Job 34 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:18:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished\n",
      "25/05/05 11:18:15 INFO DAGScheduler: Job 34 finished: start at NativeMethodAccessorImpl.java:0, took 0.552759 s\n",
      "25/05/05 11:18:15 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 11, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@63e2b65d] is committing.\n",
      "25/05/05 11:18:15 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 11, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@63e2b65d] committed.\n",
      "25/05/05 11:18:15 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 11, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:18:15 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/11 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.11.3e99ddc5-5c43-4476-a148-192d3ca3e9e4.tmp\n",
      "25/05/05 11:18:15 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/11 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.11.adfb1820-95a3-41b8-b519-5c70ac9cb3ff.tmp\n",
      "25/05/05 11:18:15 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:15 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:15 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3243, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:15 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:18:15 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:18:15 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:18:15 INFO connection: Opened connection [connectionId{localValue:21, serverValue:4389}] to localhost:27017\n",
      "25/05/05 11:18:15 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=476041}\n",
      "25/05/05 11:18:15 INFO connection: Opened connection [connectionId{localValue:22, serverValue:4390}] to localhost:27017\n",
      "25/05/05 11:18:15 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:18:15 INFO connection: Closed connection [connectionId{localValue:22, serverValue:4390}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:18:15 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 505450459 nanos, during time span of 513201292 nanos.\n",
      "25/05/05 11:18:15 INFO Executor: Finished task 0.0 in stage 34.0 (TID 34). 1645 bytes result sent to driver\n",
      "25/05/05 11:18:15 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 34) in 525 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:18:15 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:18:15 INFO DAGScheduler: ResultStage 34 (start at NativeMethodAccessorImpl.java:0) finished in 0.533 s\n",
      "25/05/05 11:18:15 INFO DAGScheduler: Job 35 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:18:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 34: Stage finished\n",
      "25/05/05 11:18:15 INFO DAGScheduler: Job 35 finished: start at NativeMethodAccessorImpl.java:0, took 0.534791 s\n",
      "25/05/05 11:18:15 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:18:15 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:18:15 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:18:15 INFO SparkContext: Created broadcast 46 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:15 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.11.3e99ddc5-5c43-4476-a148-192d3ca3e9e4.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/11\n",
      "25/05/05 11:18:15 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:18:14.912Z\",\n",
      "  \"batchId\" : 11,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.4388489208633095,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 584,\n",
      "    \"commitOffsets\" : 40,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 14,\n",
      "    \"triggerExecution\" : 695,\n",
      "    \"walCommit\" : 53\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3242\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3243\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3243\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.4388489208633095,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:18:15 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/11 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.11.2cf9210f-1c68-4016-842d-6aaf321bb7cd.tmp\n",
      "25/05/05 11:18:15 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.11.adfb1820-95a3-41b8-b519-5c70ac9cb3ff.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/11\n",
      "25/05/05 11:18:15 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:18:14.910Z\",\n",
      "  \"batchId\" : 11,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.4245014245014247,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 591,\n",
      "    \"commitOffsets\" : 38,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 4,\n",
      "    \"queryPlanning\" : 13,\n",
      "    \"triggerExecution\" : 702,\n",
      "    \"walCommit\" : 54\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3242\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3243\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3243\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.4245014245014247,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:18:15 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.11.2cf9210f-1c68-4016-842d-6aaf321bb7cd.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/11\n",
      "25/05/05 11:18:15 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:18:14.912Z\",\n",
      "  \"batchId\" : 11,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.3812154696132597,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 621,\n",
      "    \"commitOffsets\" : 32,\n",
      "    \"getBatch\" : 1,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 9,\n",
      "    \"triggerExecution\" : 724,\n",
      "    \"walCommit\" : 57\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3242\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3243\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3243\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.3812154696132597,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 11\n",
      "-------------------------------------------\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION      |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R016|R032|00-00-01|TIME SQ-42 ST|05/05/2025|11:18:13|REGULAR|7          |9         |2025-05-05 11:18:14.914|\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:18:21 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/12 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.12.8ed1b4c1-0e1b-461f-b3b6-6cb491fc843f.tmp\n",
      "25/05/05 11:18:21 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/12 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.12.ad45aea1-8358-499b-ae06-348daaa1bcf2.tmp\n",
      "25/05/05 11:18:21 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/12 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.12.0b8f7ead-6862-42fb-ad0b-68ec92775f1e.tmp\n",
      "25/05/05 11:18:21 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.12.8ed1b4c1-0e1b-461f-b3b6-6cb491fc843f.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/12\n",
      "25/05/05 11:18:21 INFO MicroBatchExecution: Committed offsets for batch 12. Metadata OffsetSeqMetadata(0,1746458301440,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:18:21 INFO IncrementalExecution: Current batch timestamp = 1746458301440\n",
      "25/05/05 11:18:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:21 INFO IncrementalExecution: Current batch timestamp = 1746458301440\n",
      "25/05/05 11:18:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:21 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.12.0b8f7ead-6862-42fb-ad0b-68ec92775f1e.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/12\n",
      "25/05/05 11:18:21 INFO MicroBatchExecution: Committed offsets for batch 12. Metadata OffsetSeqMetadata(0,1746458301451,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:18:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:21 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.12.ad45aea1-8358-499b-ae06-348daaa1bcf2.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/12\n",
      "25/05/05 11:18:21 INFO MicroBatchExecution: Committed offsets for batch 12. Metadata OffsetSeqMetadata(0,1746458301451,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:18:21 INFO IncrementalExecution: Current batch timestamp = 1746458301451\n",
      "25/05/05 11:18:21 INFO IncrementalExecution: Current batch timestamp = 1746458301451\n",
      "25/05/05 11:18:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:21 INFO CodeGenerator: Code generated in 28.851917 ms\n",
      "25/05/05 11:18:21 INFO IncrementalExecution: Current batch timestamp = 1746458301451\n",
      "25/05/05 11:18:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:21 INFO IncrementalExecution: Current batch timestamp = 1746458301451\n",
      "25/05/05 11:18:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:21 INFO IncrementalExecution: Current batch timestamp = 1746458301451\n",
      "25/05/05 11:18:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:21 INFO IncrementalExecution: Current batch timestamp = 1746458301451\n",
      "25/05/05 11:18:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:21 INFO BlockManagerInfo: Removed broadcast_44_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:18:21 INFO CodeGenerator: Code generated in 12.13975 ms\n",
      "25/05/05 11:18:21 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 12, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4518c5e2]. The input RDD has 1 partitions.\n",
      "25/05/05 11:18:21 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:21 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 12, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:18:21 INFO BlockManagerInfo: Removed broadcast_42_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:18:21 INFO DAGScheduler: Got job 36 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:18:21 INFO DAGScheduler: Final stage: ResultStage 35 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:18:21 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:18:21 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:21 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:18:21 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[201] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:18:21 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:18:21 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.1 MiB)\n",
      "25/05/05 11:18:21 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:18:21 INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:18:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[201] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:18:21 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:18:21 INFO DAGScheduler: Got job 37 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:18:21 INFO DAGScheduler: Final stage: ResultStage 36 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:18:21 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:18:21 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:18:21 INFO DAGScheduler: Submitting ResultStage 36 (MapPartitionsRDD[203] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:18:21 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 35) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:18:21 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:18:21 INFO Executor: Running task 0.0 in stage 35.0 (TID 35)\n",
      "25/05/05 11:18:21 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:18:21 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:18:21 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:18:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 36 (MapPartitionsRDD[203] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:18:21 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:18:21 INFO BlockManagerInfo: Removed broadcast_46_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:18:21 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:21 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 36) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:18:21 INFO DAGScheduler: Got job 38 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:18:21 INFO DAGScheduler: Final stage: ResultStage 37 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:18:21 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:18:21 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:18:21 INFO DAGScheduler: Submitting ResultStage 37 (MapPartitionsRDD[208] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:18:21 INFO Executor: Running task 0.0 in stage 36.0 (TID 36)\n",
      "25/05/05 11:18:21 INFO BlockManagerInfo: Removed broadcast_45_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:21 INFO CodeGenerator: Code generated in 11.097666 ms\n",
      "25/05/05 11:18:21 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:18:21 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.1 MiB)\n",
      "25/05/05 11:18:21 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:18:21 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3243 untilOffset=3244, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=12 taskId=36 partitionId=0\n",
      "25/05/05 11:18:21 INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:18:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (MapPartitionsRDD[208] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:18:21 INFO TaskSchedulerImpl: Adding task set 37.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:18:21 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 37) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:18:21 INFO Executor: Running task 0.0 in stage 37.0 (TID 37)\n",
      "25/05/05 11:18:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3243 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:21 INFO BlockManagerInfo: Removed broadcast_40_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:18:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:21 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3243 untilOffset=3244, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=12 taskId=35 partitionId=0\n",
      "25/05/05 11:18:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3243 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:21 INFO BlockManagerInfo: Removed broadcast_43_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:18:21 INFO CodeGenerator: Code generated in 10.470166 ms\n",
      "25/05/05 11:18:21 INFO BlockManagerInfo: Removed broadcast_41_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:21 INFO BlockManagerInfo: Removed broadcast_39_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:21 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3243 untilOffset=3244, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=12 taskId=37 partitionId=0\n",
      "25/05/05 11:18:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3243 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3244, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:22 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:18:22 INFO DataWritingSparkTask: Committed partition 0 (task 36, attempt 0, stage 36.0)\n",
      "25/05/05 11:18:22 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 504029500 nanos, during time span of 504474833 nanos.\n",
      "25/05/05 11:18:22 INFO Executor: Finished task 0.0 in stage 36.0 (TID 36). 2188 bytes result sent to driver\n",
      "25/05/05 11:18:22 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 36) in 521 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:18:22 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:18:22 INFO DAGScheduler: ResultStage 36 (start at NativeMethodAccessorImpl.java:0) finished in 0.526 s\n",
      "25/05/05 11:18:22 INFO DAGScheduler: Job 37 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:18:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 36: Stage finished\n",
      "25/05/05 11:18:22 INFO DAGScheduler: Job 37 finished: start at NativeMethodAccessorImpl.java:0, took 0.532456 s\n",
      "25/05/05 11:18:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 12, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:18:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3244, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:22 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:18:22 INFO DataWritingSparkTask: Committed partition 0 (task 35, attempt 0, stage 35.0)\n",
      "25/05/05 11:18:22 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 503727541 nanos, during time span of 505867500 nanos.\n",
      "25/05/05 11:18:22 INFO Executor: Finished task 0.0 in stage 35.0 (TID 35). 3559 bytes result sent to driver\n",
      "25/05/05 11:18:22 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 35) in 533 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:18:22 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:18:22 INFO DAGScheduler: ResultStage 35 (start at NativeMethodAccessorImpl.java:0) finished in 0.539 s\n",
      "25/05/05 11:18:22 INFO DAGScheduler: Job 36 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:18:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished\n",
      "25/05/05 11:18:22 INFO DAGScheduler: Job 36 finished: start at NativeMethodAccessorImpl.java:0, took 0.540816 s\n",
      "25/05/05 11:18:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 12, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4518c5e2] is committing.\n",
      "25/05/05 11:18:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 12, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4518c5e2] committed.\n",
      "25/05/05 11:18:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 12, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:18:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3244, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:22 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:18:22 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:18:22 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/12 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.12.ec43182c-4998-493e-809f-d8bb44b75b89.tmp\n",
      "25/05/05 11:18:22 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:18:22 INFO connection: Opened connection [connectionId{localValue:23, serverValue:4391}] to localhost:27017\n",
      "25/05/05 11:18:22 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=3359917}\n",
      "25/05/05 11:18:22 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/12 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.12.6ccdf506-ccfc-4de1-ae1d-d00938030a7c.tmp\n",
      "25/05/05 11:18:22 INFO connection: Opened connection [connectionId{localValue:24, serverValue:4392}] to localhost:27017\n",
      "25/05/05 11:18:22 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:18:22 INFO connection: Closed connection [connectionId{localValue:24, serverValue:4392}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:18:22 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 508950084 nanos, during time span of 526128625 nanos.\n",
      "25/05/05 11:18:22 INFO Executor: Finished task 0.0 in stage 37.0 (TID 37). 1645 bytes result sent to driver\n",
      "25/05/05 11:18:22 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 37) in 549 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:18:22 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:18:22 INFO DAGScheduler: ResultStage 37 (start at NativeMethodAccessorImpl.java:0) finished in 0.562 s\n",
      "25/05/05 11:18:22 INFO DAGScheduler: Job 38 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:18:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 37: Stage finished\n",
      "25/05/05 11:18:22 INFO DAGScheduler: Job 38 finished: start at NativeMethodAccessorImpl.java:0, took 0.563591 s\n",
      "25/05/05 11:18:22 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 248.0 B, free 366.2 MiB)\n",
      "25/05/05 11:18:22 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.2 MiB)\n",
      "25/05/05 11:18:22 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:18:22 INFO SparkContext: Created broadcast 50 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:22 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/12 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.12.993f4325-f776-4cf3-adf2-e78e626dfca3.tmp\n",
      "25/05/05 11:18:22 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.12.ec43182c-4998-493e-809f-d8bb44b75b89.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/12\n",
      "25/05/05 11:18:22 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.12.993f4325-f776-4cf3-adf2-e78e626dfca3.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/12\n",
      "25/05/05 11:18:22 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:18:21.439Z\",\n",
      "  \"batchId\" : 12,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 62.5,\n",
      "  \"processedRowsPerSecond\" : 1.287001287001287,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 669,\n",
      "    \"commitOffsets\" : 40,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 17,\n",
      "    \"triggerExecution\" : 777,\n",
      "    \"walCommit\" : 49\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3243\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3244\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3244\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 62.5,\n",
      "    \"processedRowsPerSecond\" : 1.287001287001287,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 12\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A001|R079|01-00-00|WORLD TRADE CTR|05/05/2025|11:18:20|REGULAR|3          |7         |2025-05-05 11:18:21.451|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:18:22 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:18:21.446Z\",\n",
      "  \"batchId\" : 12,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.3404825737265416,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 594,\n",
      "    \"commitOffsets\" : 53,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 5,\n",
      "    \"queryPlanning\" : 30,\n",
      "    \"triggerExecution\" : 746,\n",
      "    \"walCommit\" : 64\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3243\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3244\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3244\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.3404825737265416,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:18:22 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.12.6ccdf506-ccfc-4de1-ae1d-d00938030a7c.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/12\n",
      "25/05/05 11:18:22 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:18:21.446Z\",\n",
      "  \"batchId\" : 12,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 0.9832841691248771,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 602,\n",
      "    \"commitOffsets\" : 316,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 5,\n",
      "    \"queryPlanning\" : 33,\n",
      "    \"triggerExecution\" : 1017,\n",
      "    \"walCommit\" : 61\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3243\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3244\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3244\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 0.9832841691248771,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:18:28 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/13 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.13.83e91ec7-7784-4da4-888d-489ee28311f6.tmp\n",
      "25/05/05 11:18:28 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/13 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.13.d12dab24-1ace-4e30-ade8-78120f6311b3.tmp\n",
      "25/05/05 11:18:28 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/13 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.13.19dbc7d7-a501-4f76-9808-e58a89433579.tmp\n",
      "25/05/05 11:18:28 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.13.d12dab24-1ace-4e30-ade8-78120f6311b3.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/13\n",
      "25/05/05 11:18:28 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.13.83e91ec7-7784-4da4-888d-489ee28311f6.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/13\n",
      "25/05/05 11:18:28 INFO MicroBatchExecution: Committed offsets for batch 13. Metadata OffsetSeqMetadata(0,1746458308015,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:18:28 INFO MicroBatchExecution: Committed offsets for batch 13. Metadata OffsetSeqMetadata(0,1746458308015,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:18:28 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.13.19dbc7d7-a501-4f76-9808-e58a89433579.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/13\n",
      "25/05/05 11:18:28 INFO MicroBatchExecution: Committed offsets for batch 13. Metadata OffsetSeqMetadata(0,1746458308017,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:18:28 INFO IncrementalExecution: Current batch timestamp = 1746458308017\n",
      "25/05/05 11:18:28 INFO IncrementalExecution: Current batch timestamp = 1746458308015\n",
      "25/05/05 11:18:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:28 INFO IncrementalExecution: Current batch timestamp = 1746458308015\n",
      "25/05/05 11:18:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:28 INFO IncrementalExecution: Current batch timestamp = 1746458308015\n",
      "25/05/05 11:18:28 INFO IncrementalExecution: Current batch timestamp = 1746458308017\n",
      "25/05/05 11:18:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:28 INFO IncrementalExecution: Current batch timestamp = 1746458308015\n",
      "25/05/05 11:18:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:28 INFO IncrementalExecution: Current batch timestamp = 1746458308017\n",
      "25/05/05 11:18:28 INFO IncrementalExecution: Current batch timestamp = 1746458308015\n",
      "25/05/05 11:18:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:28 INFO CodeGenerator: Code generated in 7.291625 ms\n",
      "25/05/05 11:18:28 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 13, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@e99c432]. The input RDD has 1 partitions.\n",
      "25/05/05 11:18:28 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:28 INFO DAGScheduler: Got job 39 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:18:28 INFO DAGScheduler: Final stage: ResultStage 38 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:18:28 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:18:28 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:18:28 INFO DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[216] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:18:28 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:18:28 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:18:28 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:28 INFO SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:18:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 38 (MapPartitionsRDD[216] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:18:28 INFO CodeGenerator: Code generated in 5.0945 ms\n",
      "25/05/05 11:18:28 INFO TaskSchedulerImpl: Adding task set 38.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:18:28 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 38) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:18:28 INFO Executor: Running task 0.0 in stage 38.0 (TID 38)\n",
      "25/05/05 11:18:28 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 13, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:18:28 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:28 INFO DAGScheduler: Got job 40 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:18:28 INFO DAGScheduler: Final stage: ResultStage 39 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:18:28 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:18:28 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:18:28 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[219] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:18:28 INFO MemoryStore: Block broadcast_52 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:18:28 INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:18:28 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:28 INFO SparkContext: Created broadcast 52 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:18:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[219] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:18:28 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:18:28 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 39) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:18:28 INFO Executor: Running task 0.0 in stage 39.0 (TID 39)\n",
      "25/05/05 11:18:28 INFO CodeGenerator: Code generated in 6.623917 ms\n",
      "25/05/05 11:18:28 INFO CodeGenerator: Code generated in 4.960833 ms\n",
      "25/05/05 11:18:28 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3244 untilOffset=3245, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=13 taskId=38 partitionId=0\n",
      "25/05/05 11:18:28 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3244 untilOffset=3245, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=13 taskId=39 partitionId=0\n",
      "25/05/05 11:18:28 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3244 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:28 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3244 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:28 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:28 INFO DAGScheduler: Got job 41 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:18:28 INFO DAGScheduler: Final stage: ResultStage 40 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:18:28 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:18:28 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:18:28 INFO DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[224] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:18:28 INFO MemoryStore: Block broadcast_53 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:18:28 INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.1 MiB)\n",
      "25/05/05 11:18:28 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:18:28 INFO SparkContext: Created broadcast 53 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:18:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[224] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:18:28 INFO TaskSchedulerImpl: Adding task set 40.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:18:28 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 40) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:18:28 INFO Executor: Running task 0.0 in stage 40.0 (TID 40)\n",
      "25/05/05 11:18:28 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3244 untilOffset=3245, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=13 taskId=40 partitionId=0\n",
      "25/05/05 11:18:28 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3244 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3245, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3245, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:28 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:18:28 INFO DataWritingSparkTask: Committed partition 0 (task 39, attempt 0, stage 39.0)\n",
      "25/05/05 11:18:28 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 504229958 nanos, during time span of 504762708 nanos.\n",
      "25/05/05 11:18:28 INFO Executor: Finished task 0.0 in stage 39.0 (TID 39). 2188 bytes result sent to driver\n",
      "25/05/05 11:18:28 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 39) in 516 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:18:28 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:18:28 INFO DAGScheduler: ResultStage 39 (start at NativeMethodAccessorImpl.java:0) finished in 0.518 s\n",
      "25/05/05 11:18:28 INFO DAGScheduler: Job 40 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:18:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 39: Stage finished\n",
      "25/05/05 11:18:28 INFO DAGScheduler: Job 40 finished: start at NativeMethodAccessorImpl.java:0, took 0.518897 s\n",
      "25/05/05 11:18:28 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:18:28 INFO DataWritingSparkTask: Committed partition 0 (task 38, attempt 0, stage 38.0)\n",
      "25/05/05 11:18:28 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 504558458 nanos, during time span of 506762667 nanos.\n",
      "25/05/05 11:18:28 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 13, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:18:28 INFO Executor: Finished task 0.0 in stage 38.0 (TID 38). 3559 bytes result sent to driver\n",
      "25/05/05 11:18:28 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 38) in 521 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:18:28 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:18:28 INFO DAGScheduler: ResultStage 38 (start at NativeMethodAccessorImpl.java:0) finished in 0.525 s\n",
      "25/05/05 11:18:28 INFO DAGScheduler: Job 39 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:18:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 38: Stage finished\n",
      "25/05/05 11:18:28 INFO DAGScheduler: Job 39 finished: start at NativeMethodAccessorImpl.java:0, took 0.526213 s\n",
      "25/05/05 11:18:28 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 13, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@e99c432] is committing.\n",
      "25/05/05 11:18:28 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 13, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@e99c432] committed.\n",
      "25/05/05 11:18:28 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/13 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.13.f08729d5-ab48-482f-bf56-e7f9507d0820.tmp\n",
      "25/05/05 11:18:28 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 13, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:18:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3245, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:28 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:18:28 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:18:28 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:18:28 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/13 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.13.4c65209f-139f-4149-b6b9-af37505e1bea.tmp\n",
      "25/05/05 11:18:28 INFO connection: Opened connection [connectionId{localValue:25, serverValue:4393}] to localhost:27017\n",
      "25/05/05 11:18:28 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=439166}\n",
      "25/05/05 11:18:28 INFO connection: Opened connection [connectionId{localValue:26, serverValue:4394}] to localhost:27017\n",
      "25/05/05 11:18:28 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:18:28 INFO connection: Closed connection [connectionId{localValue:26, serverValue:4394}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:18:28 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 506325000 nanos, during time span of 518617834 nanos.\n",
      "25/05/05 11:18:28 INFO Executor: Finished task 0.0 in stage 40.0 (TID 40). 1645 bytes result sent to driver\n",
      "25/05/05 11:18:28 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 40) in 526 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:18:28 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:18:28 INFO DAGScheduler: ResultStage 40 (start at NativeMethodAccessorImpl.java:0) finished in 0.532 s\n",
      "25/05/05 11:18:28 INFO DAGScheduler: Job 41 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:18:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 40: Stage finished\n",
      "25/05/05 11:18:28 INFO DAGScheduler: Job 41 finished: start at NativeMethodAccessorImpl.java:0, took 0.533853 s\n",
      "25/05/05 11:18:28 INFO MemoryStore: Block broadcast_54 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:18:28 INFO MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:18:28 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:18:28 INFO SparkContext: Created broadcast 54 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:28 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.13.f08729d5-ab48-482f-bf56-e7f9507d0820.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/13\n",
      "25/05/05 11:18:28 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:18:28.009Z\",\n",
      "  \"batchId\" : 13,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "  \"processedRowsPerSecond\" : 1.5220700152207,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 547,\n",
      "    \"commitOffsets\" : 36,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 6,\n",
      "    \"queryPlanning\" : 9,\n",
      "    \"triggerExecution\" : 657,\n",
      "    \"walCommit\" : 59\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3244\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3245\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3245\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "    \"processedRowsPerSecond\" : 1.5220700152207,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:18:28 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/13 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.13.ae808ad7-2bbd-4db5-9f82-fb7de4f4fd4d.tmp\n",
      "25/05/05 11:18:28 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.13.4c65209f-139f-4149-b6b9-af37505e1bea.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/13\n",
      "25/05/05 11:18:28 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:18:28.014Z\",\n",
      "  \"batchId\" : 13,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 62.5,\n",
      "  \"processedRowsPerSecond\" : 1.5151515151515151,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 557,\n",
      "    \"commitOffsets\" : 34,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 8,\n",
      "    \"triggerExecution\" : 660,\n",
      "    \"walCommit\" : 58\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3244\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3245\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3245\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 62.5,\n",
      "    \"processedRowsPerSecond\" : 1.5151515151515151,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:18:28 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.13.ae808ad7-2bbd-4db5-9f82-fb7de4f4fd4d.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/13\n",
      "25/05/05 11:18:28 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:18:28.014Z\",\n",
      "  \"batchId\" : 13,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 62.5,\n",
      "  \"processedRowsPerSecond\" : 1.4641288433382136,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 579,\n",
      "    \"commitOffsets\" : 34,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 9,\n",
      "    \"triggerExecution\" : 683,\n",
      "    \"walCommit\" : 59\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3244\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3245\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3245\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 62.5,\n",
      "    \"processedRowsPerSecond\" : 1.4641288433382136,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 13\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|B013|R081|02-00-00|161/YANKEE STAD|05/05/2025|11:18:26|REGULAR|6          |3         |2025-05-05 11:18:28.017|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:18:33 INFO BlockManagerInfo: Removed broadcast_53_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:33 INFO BlockManagerInfo: Removed broadcast_48_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:33 INFO BlockManagerInfo: Removed broadcast_49_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:33 INFO BlockManagerInfo: Removed broadcast_52_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:33 INFO BlockManagerInfo: Removed broadcast_47_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:33 INFO BlockManagerInfo: Removed broadcast_51_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:33 INFO BlockManagerInfo: Removed broadcast_50_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:18:33 INFO BlockManagerInfo: Removed broadcast_54_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:18:33 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/14 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.14.848a0503-f9d6-410d-9782-63bf73db1eb4.tmp\n",
      "25/05/05 11:18:33 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/14 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.14.420962a1-5502-4fce-9eca-25e7bf2679f3.tmp\n",
      "25/05/05 11:18:33 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/14 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.14.d6b4241b-27f2-4f81-a18f-1f854a3c8f24.tmp\n",
      "25/05/05 11:18:33 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.14.848a0503-f9d6-410d-9782-63bf73db1eb4.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/14\n",
      "25/05/05 11:18:33 INFO MicroBatchExecution: Committed offsets for batch 14. Metadata OffsetSeqMetadata(0,1746458313703,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:18:33 INFO IncrementalExecution: Current batch timestamp = 1746458313703\n",
      "25/05/05 11:18:33 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.14.420962a1-5502-4fce-9eca-25e7bf2679f3.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/14\n",
      "25/05/05 11:18:33 INFO MicroBatchExecution: Committed offsets for batch 14. Metadata OffsetSeqMetadata(0,1746458313703,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:18:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:33 INFO IncrementalExecution: Current batch timestamp = 1746458313703\n",
      "25/05/05 11:18:33 INFO IncrementalExecution: Current batch timestamp = 1746458313703\n",
      "25/05/05 11:18:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:33 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.14.d6b4241b-27f2-4f81-a18f-1f854a3c8f24.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/14\n",
      "25/05/05 11:18:33 INFO MicroBatchExecution: Committed offsets for batch 14. Metadata OffsetSeqMetadata(0,1746458313711,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:18:33 INFO IncrementalExecution: Current batch timestamp = 1746458313703\n",
      "25/05/05 11:18:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:33 INFO IncrementalExecution: Current batch timestamp = 1746458313703\n",
      "25/05/05 11:18:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:33 INFO IncrementalExecution: Current batch timestamp = 1746458313711\n",
      "25/05/05 11:18:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:33 INFO IncrementalExecution: Current batch timestamp = 1746458313711\n",
      "25/05/05 11:18:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:33 INFO IncrementalExecution: Current batch timestamp = 1746458313703\n",
      "25/05/05 11:18:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:33 INFO CodeGenerator: Code generated in 6.057583 ms\n",
      "25/05/05 11:18:33 INFO CodeGenerator: Code generated in 6.391208 ms\n",
      "25/05/05 11:18:33 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 14, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@52457069]. The input RDD has 1 partitions.\n",
      "25/05/05 11:18:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:33 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 14, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:18:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:33 INFO DAGScheduler: Got job 42 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:18:33 INFO DAGScheduler: Final stage: ResultStage 41 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:18:33 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:18:33 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:18:33 INFO DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[230] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:18:33 INFO MemoryStore: Block broadcast_55 stored as values in memory (estimated size 16.2 KiB, free 366.3 MiB)\n",
      "25/05/05 11:18:33 INFO MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.3 MiB)\n",
      "25/05/05 11:18:33 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:33 INFO SparkContext: Created broadcast 55 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:18:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 41 (MapPartitionsRDD[230] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:18:33 INFO TaskSchedulerImpl: Adding task set 41.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:18:33 INFO DAGScheduler: Got job 43 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:18:33 INFO DAGScheduler: Final stage: ResultStage 42 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:18:33 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:18:33 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:18:33 INFO DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[235] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:18:33 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 41) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:18:33 INFO Executor: Running task 0.0 in stage 41.0 (TID 41)\n",
      "25/05/05 11:18:33 INFO MemoryStore: Block broadcast_56 stored as values in memory (estimated size 15.2 KiB, free 366.3 MiB)\n",
      "25/05/05 11:18:33 INFO MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.3 MiB)\n",
      "25/05/05 11:18:33 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:33 INFO SparkContext: Created broadcast 56 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:18:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[235] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:18:33 INFO TaskSchedulerImpl: Adding task set 42.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:18:33 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 42) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:18:33 INFO Executor: Running task 0.0 in stage 42.0 (TID 42)\n",
      "25/05/05 11:18:33 INFO CodeGenerator: Code generated in 6.677584 ms\n",
      "25/05/05 11:18:33 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3245 untilOffset=3246, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=14 taskId=42 partitionId=0\n",
      "25/05/05 11:18:33 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3245 untilOffset=3246, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=14 taskId=41 partitionId=0\n",
      "25/05/05 11:18:33 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3245 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:33 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3245 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:33 INFO DAGScheduler: Got job 44 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:18:33 INFO DAGScheduler: Final stage: ResultStage 43 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:18:33 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:18:33 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:18:33 INFO DAGScheduler: Submitting ResultStage 43 (MapPartitionsRDD[240] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:18:33 INFO MemoryStore: Block broadcast_57 stored as values in memory (estimated size 47.1 KiB, free 366.2 MiB)\n",
      "25/05/05 11:18:33 INFO MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.2 MiB)\n",
      "25/05/05 11:18:33 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:33 INFO SparkContext: Created broadcast 57 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:18:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 43 (MapPartitionsRDD[240] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:18:33 INFO TaskSchedulerImpl: Adding task set 43.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:18:33 INFO TaskSetManager: Starting task 0.0 in stage 43.0 (TID 43) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:18:33 INFO Executor: Running task 0.0 in stage 43.0 (TID 43)\n",
      "25/05/05 11:18:33 INFO CodeGenerator: Code generated in 7.589167 ms\n",
      "25/05/05 11:18:33 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3245 untilOffset=3246, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=14 taskId=43 partitionId=0\n",
      "25/05/05 11:18:33 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3245 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3246, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:34 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:18:34 INFO DataWritingSparkTask: Committed partition 0 (task 42, attempt 0, stage 42.0)\n",
      "25/05/05 11:18:34 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 503548166 nanos, during time span of 503969292 nanos.\n",
      "25/05/05 11:18:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3246, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:34 INFO Executor: Finished task 0.0 in stage 42.0 (TID 42). 2188 bytes result sent to driver\n",
      "25/05/05 11:18:34 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 42) in 516 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:18:34 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:18:34 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:18:34 INFO DataWritingSparkTask: Committed partition 0 (task 41, attempt 0, stage 41.0)\n",
      "25/05/05 11:18:34 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 502845292 nanos, during time span of 504855917 nanos.\n",
      "25/05/05 11:18:34 INFO DAGScheduler: ResultStage 42 (start at NativeMethodAccessorImpl.java:0) finished in 0.519 s\n",
      "25/05/05 11:18:34 INFO DAGScheduler: Job 43 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:18:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 42: Stage finished\n",
      "25/05/05 11:18:34 INFO DAGScheduler: Job 43 finished: start at NativeMethodAccessorImpl.java:0, took 0.525114 s\n",
      "25/05/05 11:18:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 14, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:18:34 INFO Executor: Finished task 0.0 in stage 41.0 (TID 41). 3554 bytes result sent to driver\n",
      "25/05/05 11:18:34 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 41) in 524 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:18:34 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:18:34 INFO DAGScheduler: ResultStage 41 (start at NativeMethodAccessorImpl.java:0) finished in 0.528 s\n",
      "25/05/05 11:18:34 INFO DAGScheduler: Job 42 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:18:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 41: Stage finished\n",
      "25/05/05 11:18:34 INFO DAGScheduler: Job 42 finished: start at NativeMethodAccessorImpl.java:0, took 0.530984 s\n",
      "25/05/05 11:18:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 14, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@52457069] is committing.\n",
      "25/05/05 11:18:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 14, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@52457069] committed.\n",
      "25/05/05 11:18:34 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/14 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.14.38558034-7745-4708-8a92-1d8cd2ab4c02.tmp\n",
      "25/05/05 11:18:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 14, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:18:34 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/14 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.14.f38be5c5-1e20-4650-a948-944328278587.tmp\n",
      "25/05/05 11:18:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:34 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.14.38558034-7745-4708-8a92-1d8cd2ab4c02.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/14\n",
      "25/05/05 11:18:34 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:18:33.701Z\",\n",
      "  \"batchId\" : 14,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "  \"processedRowsPerSecond\" : 1.4925373134328357,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 584,\n",
      "    \"commitOffsets\" : 33,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 7,\n",
      "    \"triggerExecution\" : 670,\n",
      "    \"walCommit\" : 43\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3245\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3246\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3246\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "    \"processedRowsPerSecond\" : 1.4925373134328357,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:18:34 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.14.f38be5c5-1e20-4650-a948-944328278587.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/14\n",
      "25/05/05 11:18:34 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:18:33.701Z\",\n",
      "  \"batchId\" : 14,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.4727540500736376,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 587,\n",
      "    \"commitOffsets\" : 32,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 9,\n",
      "    \"triggerExecution\" : 679,\n",
      "    \"walCommit\" : 48\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3245\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3246\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3246\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.4727540500736376,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:18:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3246, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:34 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:18:34 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:18:34 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:18:34 INFO connection: Opened connection [connectionId{localValue:27, serverValue:4395}] to localhost:27017\n",
      "25/05/05 11:18:34 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=6382292}\n",
      "25/05/05 11:18:34 INFO connection: Opened connection [connectionId{localValue:28, serverValue:4396}] to localhost:27017\n",
      "25/05/05 11:18:34 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:18:34 INFO connection: Closed connection [connectionId{localValue:28, serverValue:4396}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:18:34 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 523684000 nanos, during time span of 569733542 nanos.\n",
      "25/05/05 11:18:34 INFO Executor: Finished task 0.0 in stage 43.0 (TID 43). 1645 bytes result sent to driver\n",
      "25/05/05 11:18:34 INFO TaskSetManager: Finished task 0.0 in stage 43.0 (TID 43) in 586 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:18:34 INFO TaskSchedulerImpl: Removed TaskSet 43.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:18:34 INFO DAGScheduler: ResultStage 43 (start at NativeMethodAccessorImpl.java:0) finished in 0.592 s\n",
      "25/05/05 11:18:34 INFO DAGScheduler: Job 44 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:18:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 43: Stage finished\n",
      "25/05/05 11:18:34 INFO DAGScheduler: Job 44 finished: start at NativeMethodAccessorImpl.java:0, took 0.594235 s\n",
      "25/05/05 11:18:34 INFO MemoryStore: Block broadcast_58 stored as values in memory (estimated size 248.0 B, free 366.2 MiB)\n",
      "25/05/05 11:18:34 INFO MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.2 MiB)\n",
      "25/05/05 11:18:34 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:18:34 INFO SparkContext: Created broadcast 58 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:34 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/14 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.14.2b28a600-82b5-4fbe-af1a-44c48f9a8b53.tmp\n",
      "25/05/05 11:18:34 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.14.2b28a600-82b5-4fbe-af1a-44c48f9a8b53.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/14\n",
      "25/05/05 11:18:34 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:18:33.706Z\",\n",
      "  \"batchId\" : 14,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.310615989515072,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 665,\n",
      "    \"commitOffsets\" : 35,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 5,\n",
      "    \"queryPlanning\" : 6,\n",
      "    \"triggerExecution\" : 763,\n",
      "    \"walCommit\" : 51\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3245\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3246\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3246\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.310615989515072,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 14\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R022|R033|01-00-00|42 ST-PORT AUTH|05/05/2025|11:18:32|REGULAR|6          |6         |2025-05-05 11:18:33.703|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:18:39 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/15 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.15.30988340-cb38-4351-8871-bab8d8ab2349.tmp\n",
      "25/05/05 11:18:39 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/15 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.15.40673a64-8502-42f6-a6d3-bc02016e53f1.tmp\n",
      "25/05/05 11:18:39 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/15 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.15.28666900-cfc1-4557-8f3b-2cf157636b59.tmp\n",
      "25/05/05 11:18:39 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.15.30988340-cb38-4351-8871-bab8d8ab2349.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/15\n",
      "25/05/05 11:18:39 INFO MicroBatchExecution: Committed offsets for batch 15. Metadata OffsetSeqMetadata(0,1746458319376,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:18:39 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.15.40673a64-8502-42f6-a6d3-bc02016e53f1.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/15\n",
      "25/05/05 11:18:39 INFO MicroBatchExecution: Committed offsets for batch 15. Metadata OffsetSeqMetadata(0,1746458319376,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:18:39 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.15.28666900-cfc1-4557-8f3b-2cf157636b59.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/15\n",
      "25/05/05 11:18:39 INFO MicroBatchExecution: Committed offsets for batch 15. Metadata OffsetSeqMetadata(0,1746458319387,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:18:39 INFO IncrementalExecution: Current batch timestamp = 1746458319376\n",
      "25/05/05 11:18:39 INFO IncrementalExecution: Current batch timestamp = 1746458319387\n",
      "25/05/05 11:18:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:39 INFO IncrementalExecution: Current batch timestamp = 1746458319376\n",
      "25/05/05 11:18:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:39 INFO IncrementalExecution: Current batch timestamp = 1746458319387\n",
      "25/05/05 11:18:39 INFO IncrementalExecution: Current batch timestamp = 1746458319376\n",
      "25/05/05 11:18:39 INFO IncrementalExecution: Current batch timestamp = 1746458319376\n",
      "25/05/05 11:18:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:39 INFO IncrementalExecution: Current batch timestamp = 1746458319376\n",
      "25/05/05 11:18:39 INFO IncrementalExecution: Current batch timestamp = 1746458319376\n",
      "25/05/05 11:18:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:39 INFO CodeGenerator: Code generated in 6.643458 ms\n",
      "25/05/05 11:18:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:39 INFO CodeGenerator: Code generated in 7.00475 ms\n",
      "25/05/05 11:18:39 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 15, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:18:39 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 15, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@7c7df687]. The input RDD has 1 partitions.\n",
      "25/05/05 11:18:39 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:39 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:39 INFO DAGScheduler: Got job 45 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:18:39 INFO DAGScheduler: Final stage: ResultStage 44 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:18:39 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:18:39 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:18:39 INFO DAGScheduler: Submitting ResultStage 44 (MapPartitionsRDD[251] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:18:39 INFO MemoryStore: Block broadcast_59 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:18:39 INFO MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:18:39 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:39 INFO SparkContext: Created broadcast 59 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:18:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 44 (MapPartitionsRDD[251] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:18:39 INFO TaskSchedulerImpl: Adding task set 44.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:18:39 INFO DAGScheduler: Got job 46 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:18:39 INFO DAGScheduler: Final stage: ResultStage 45 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:18:39 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:18:39 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:18:39 INFO TaskSetManager: Starting task 0.0 in stage 44.0 (TID 44) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:18:39 INFO DAGScheduler: Submitting ResultStage 45 (MapPartitionsRDD[250] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:18:39 INFO Executor: Running task 0.0 in stage 44.0 (TID 44)\n",
      "25/05/05 11:18:39 INFO MemoryStore: Block broadcast_60 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:18:39 INFO MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:18:39 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:39 INFO SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:18:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 45 (MapPartitionsRDD[250] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:18:39 INFO TaskSchedulerImpl: Adding task set 45.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:18:39 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 45) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:18:39 INFO Executor: Running task 0.0 in stage 45.0 (TID 45)\n",
      "25/05/05 11:18:39 INFO CodeGenerator: Code generated in 7.191709 ms\n",
      "25/05/05 11:18:39 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3246 untilOffset=3247, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=15 taskId=45 partitionId=0\n",
      "25/05/05 11:18:39 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:39 INFO DAGScheduler: Got job 47 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:18:39 INFO DAGScheduler: Final stage: ResultStage 46 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:18:39 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:18:39 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:18:39 INFO DAGScheduler: Submitting ResultStage 46 (MapPartitionsRDD[256] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:18:39 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3246 untilOffset=3247, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=15 taskId=44 partitionId=0\n",
      "25/05/05 11:18:39 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3246 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:39 INFO MemoryStore: Block broadcast_61 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:18:39 INFO MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.1 MiB)\n",
      "25/05/05 11:18:39 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:18:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:39 INFO SparkContext: Created broadcast 61 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:18:39 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3246 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 46 (MapPartitionsRDD[256] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:18:39 INFO TaskSchedulerImpl: Adding task set 46.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:18:39 INFO TaskSetManager: Starting task 0.0 in stage 46.0 (TID 46) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:18:39 INFO Executor: Running task 0.0 in stage 46.0 (TID 46)\n",
      "25/05/05 11:18:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:39 INFO CodeGenerator: Code generated in 4.821291 ms\n",
      "25/05/05 11:18:39 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3246 untilOffset=3247, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=15 taskId=46 partitionId=0\n",
      "25/05/05 11:18:39 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3246 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3247, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3247, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:40 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:18:40 INFO DataWritingSparkTask: Committed partition 0 (task 45, attempt 0, stage 45.0)\n",
      "25/05/05 11:18:40 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 505933833 nanos, during time span of 506529291 nanos.\n",
      "25/05/05 11:18:40 INFO Executor: Finished task 0.0 in stage 45.0 (TID 45). 2188 bytes result sent to driver\n",
      "25/05/05 11:18:40 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 45) in 517 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:18:40 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:18:40 INFO DAGScheduler: ResultStage 45 (start at NativeMethodAccessorImpl.java:0) finished in 0.521 s\n",
      "25/05/05 11:18:40 INFO DAGScheduler: Job 46 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:18:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 45: Stage finished\n",
      "25/05/05 11:18:40 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:18:40 INFO DataWritingSparkTask: Committed partition 0 (task 44, attempt 0, stage 44.0)\n",
      "25/05/05 11:18:40 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 503195209 nanos, during time span of 505684083 nanos.\n",
      "25/05/05 11:18:40 INFO DAGScheduler: Job 46 finished: start at NativeMethodAccessorImpl.java:0, took 0.532015 s\n",
      "25/05/05 11:18:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 15, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:18:40 INFO Executor: Finished task 0.0 in stage 44.0 (TID 44). 3548 bytes result sent to driver\n",
      "25/05/05 11:18:40 INFO TaskSetManager: Finished task 0.0 in stage 44.0 (TID 44) in 523 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:18:40 INFO TaskSchedulerImpl: Removed TaskSet 44.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:18:40 INFO DAGScheduler: ResultStage 44 (start at NativeMethodAccessorImpl.java:0) finished in 0.526 s\n",
      "25/05/05 11:18:40 INFO DAGScheduler: Job 45 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:18:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 44: Stage finished\n",
      "25/05/05 11:18:40 INFO DAGScheduler: Job 45 finished: start at NativeMethodAccessorImpl.java:0, took 0.536880 s\n",
      "25/05/05 11:18:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 15, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@7c7df687] is committing.\n",
      "25/05/05 11:18:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 15, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@7c7df687] committed.\n",
      "25/05/05 11:18:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 15, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:18:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:40 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/15 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.15.91e4fb15-c091-45fa-96ad-747e55f9b368.tmp\n",
      "25/05/05 11:18:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3247, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:40 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:18:40 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:18:40 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:18:40 INFO connection: Opened connection [connectionId{localValue:29, serverValue:4399}] to localhost:27017\n",
      "25/05/05 11:18:40 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=296917}\n",
      "25/05/05 11:18:40 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/15 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.15.9bbbaa95-ec6c-464c-917e-22f669b2faf6.tmp\n",
      "25/05/05 11:18:40 INFO connection: Opened connection [connectionId{localValue:30, serverValue:4400}] to localhost:27017\n",
      "25/05/05 11:18:40 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:18:40 INFO connection: Closed connection [connectionId{localValue:30, serverValue:4400}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:18:40 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 510252375 nanos, during time span of 518475792 nanos.\n",
      "25/05/05 11:18:40 INFO Executor: Finished task 0.0 in stage 46.0 (TID 46). 1645 bytes result sent to driver\n",
      "25/05/05 11:18:40 INFO TaskSetManager: Finished task 0.0 in stage 46.0 (TID 46) in 533 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:18:40 INFO TaskSchedulerImpl: Removed TaskSet 46.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:18:40 INFO DAGScheduler: ResultStage 46 (start at NativeMethodAccessorImpl.java:0) finished in 0.539 s\n",
      "25/05/05 11:18:40 INFO DAGScheduler: Job 47 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:18:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 46: Stage finished\n",
      "25/05/05 11:18:40 INFO DAGScheduler: Job 47 finished: start at NativeMethodAccessorImpl.java:0, took 0.541146 s\n",
      "25/05/05 11:18:40 INFO MemoryStore: Block broadcast_62 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:18:40 INFO MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:18:40 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:18:40 INFO SparkContext: Created broadcast 62 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:40 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/15 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.15.658839f8-312a-4182-81c5-98bc37cc28c2.tmp\n",
      "25/05/05 11:18:40 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.15.91e4fb15-c091-45fa-96ad-747e55f9b368.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/15\n",
      "25/05/05 11:18:40 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:18:39.375Z\",\n",
      "  \"batchId\" : 15,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.4492753623188408,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 569,\n",
      "    \"commitOffsets\" : 59,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 8,\n",
      "    \"triggerExecution\" : 690,\n",
      "    \"walCommit\" : 52\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3246\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3247\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3247\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.4492753623188408,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:18:40 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.15.9bbbaa95-ec6c-464c-917e-22f669b2faf6.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/15\n",
      "25/05/05 11:18:40 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:18:39.375Z\",\n",
      "  \"batchId\" : 15,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.4245014245014247,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 579,\n",
      "    \"commitOffsets\" : 61,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 8,\n",
      "    \"triggerExecution\" : 702,\n",
      "    \"walCommit\" : 51\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3246\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3247\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3247\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.4245014245014247,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:18:40 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.15.658839f8-312a-4182-81c5-98bc37cc28c2.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/15\n",
      "25/05/05 11:18:40 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:18:39.382Z\",\n",
      "  \"batchId\" : 15,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.4204545454545456,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 603,\n",
      "    \"commitOffsets\" : 46,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 5,\n",
      "    \"queryPlanning\" : 6,\n",
      "    \"triggerExecution\" : 704,\n",
      "    \"walCommit\" : 43\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3246\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3247\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3247\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.4204545454545456,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 15\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION  |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A002|R170|00-00-00|FULTON ST|05/05/2025|11:18:38|REGULAR|6          |6         |2025-05-05 11:18:39.376|\n",
      "+----+----+--------+---------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:18:42 INFO BlockManagerInfo: Removed broadcast_58_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:18:42 INFO BlockManagerInfo: Removed broadcast_56_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:18:42 INFO BlockManagerInfo: Removed broadcast_59_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:42 INFO BlockManagerInfo: Removed broadcast_61_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:42 INFO BlockManagerInfo: Removed broadcast_62_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:18:42 INFO BlockManagerInfo: Removed broadcast_57_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:42 INFO BlockManagerInfo: Removed broadcast_55_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:42 INFO BlockManagerInfo: Removed broadcast_60_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:45 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/16 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.16.f081b0bb-1d77-405e-aefb-037f14fff851.tmp\n",
      "25/05/05 11:18:45 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/16 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.16.b939af13-c902-42f6-a04a-9ca711ef8d1e.tmp\n",
      "25/05/05 11:18:45 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/16 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.16.cf57e1d1-5971-40c6-82a6-b90650451371.tmp\n",
      "25/05/05 11:18:46 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.16.f081b0bb-1d77-405e-aefb-037f14fff851.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/16\n",
      "25/05/05 11:18:46 INFO MicroBatchExecution: Committed offsets for batch 16. Metadata OffsetSeqMetadata(0,1746458325975,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:18:46 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.16.b939af13-c902-42f6-a04a-9ca711ef8d1e.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/16\n",
      "25/05/05 11:18:46 INFO MicroBatchExecution: Committed offsets for batch 16. Metadata OffsetSeqMetadata(0,1746458325978,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:18:46 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.16.cf57e1d1-5971-40c6-82a6-b90650451371.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/16\n",
      "25/05/05 11:18:46 INFO MicroBatchExecution: Committed offsets for batch 16. Metadata OffsetSeqMetadata(0,1746458325978,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:18:46 INFO IncrementalExecution: Current batch timestamp = 1746458325978\n",
      "25/05/05 11:18:46 INFO IncrementalExecution: Current batch timestamp = 1746458325975\n",
      "25/05/05 11:18:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:46 INFO IncrementalExecution: Current batch timestamp = 1746458325978\n",
      "25/05/05 11:18:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:46 INFO IncrementalExecution: Current batch timestamp = 1746458325978\n",
      "25/05/05 11:18:46 INFO IncrementalExecution: Current batch timestamp = 1746458325975\n",
      "25/05/05 11:18:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:46 INFO IncrementalExecution: Current batch timestamp = 1746458325978\n",
      "25/05/05 11:18:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:46 INFO IncrementalExecution: Current batch timestamp = 1746458325978\n",
      "25/05/05 11:18:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:46 INFO IncrementalExecution: Current batch timestamp = 1746458325978\n",
      "25/05/05 11:18:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:46 INFO CodeGenerator: Code generated in 13.195625 ms\n",
      "25/05/05 11:18:46 INFO CodeGenerator: Code generated in 9.0275 ms\n",
      "25/05/05 11:18:46 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 16, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@2acf53d3]. The input RDD has 1 partitions.\n",
      "25/05/05 11:18:46 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:46 INFO DAGScheduler: Got job 48 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:18:46 INFO DAGScheduler: Final stage: ResultStage 47 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:18:46 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:18:46 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:18:46 INFO DAGScheduler: Submitting ResultStage 47 (MapPartitionsRDD[265] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:18:46 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 16, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:18:46 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:46 INFO MemoryStore: Block broadcast_63 stored as values in memory (estimated size 16.2 KiB, free 366.3 MiB)\n",
      "25/05/05 11:18:46 INFO MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.3 MiB)\n",
      "25/05/05 11:18:46 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:46 INFO SparkContext: Created broadcast 63 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:18:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 47 (MapPartitionsRDD[265] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:18:46 INFO TaskSchedulerImpl: Adding task set 47.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:18:46 INFO DAGScheduler: Got job 49 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:18:46 INFO DAGScheduler: Final stage: ResultStage 48 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:18:46 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:18:46 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:18:46 INFO DAGScheduler: Submitting ResultStage 48 (MapPartitionsRDD[267] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:18:46 INFO TaskSetManager: Starting task 0.0 in stage 47.0 (TID 47) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:18:46 INFO Executor: Running task 0.0 in stage 47.0 (TID 47)\n",
      "25/05/05 11:18:46 INFO MemoryStore: Block broadcast_64 stored as values in memory (estimated size 15.2 KiB, free 366.3 MiB)\n",
      "25/05/05 11:18:46 INFO MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.3 MiB)\n",
      "25/05/05 11:18:46 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:46 INFO SparkContext: Created broadcast 64 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:18:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 48 (MapPartitionsRDD[267] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:18:46 INFO TaskSchedulerImpl: Adding task set 48.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:18:46 INFO TaskSetManager: Starting task 0.0 in stage 48.0 (TID 48) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:18:46 INFO Executor: Running task 0.0 in stage 48.0 (TID 48)\n",
      "25/05/05 11:18:46 INFO CodeGenerator: Code generated in 10.342542 ms\n",
      "25/05/05 11:18:46 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3247 untilOffset=3248, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=16 taskId=48 partitionId=0\n",
      "25/05/05 11:18:46 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:46 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3247 untilOffset=3248, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=16 taskId=47 partitionId=0\n",
      "25/05/05 11:18:46 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3247 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:46 INFO DAGScheduler: Got job 50 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:18:46 INFO DAGScheduler: Final stage: ResultStage 49 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:18:46 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:18:46 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:18:46 INFO DAGScheduler: Submitting ResultStage 49 (MapPartitionsRDD[272] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:18:46 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3247 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:46 INFO MemoryStore: Block broadcast_65 stored as values in memory (estimated size 47.1 KiB, free 366.2 MiB)\n",
      "25/05/05 11:18:46 INFO MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.2 MiB)\n",
      "25/05/05 11:18:46 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:46 INFO SparkContext: Created broadcast 65 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:18:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 49 (MapPartitionsRDD[272] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:18:46 INFO TaskSchedulerImpl: Adding task set 49.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:18:46 INFO TaskSetManager: Starting task 0.0 in stage 49.0 (TID 49) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:18:46 INFO Executor: Running task 0.0 in stage 49.0 (TID 49)\n",
      "25/05/05 11:18:46 INFO CodeGenerator: Code generated in 14.759459 ms\n",
      "25/05/05 11:18:46 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3247 untilOffset=3248, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=16 taskId=49 partitionId=0\n",
      "25/05/05 11:18:46 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3247 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3248, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3248, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:46 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:18:46 INFO DataWritingSparkTask: Committed partition 0 (task 48, attempt 0, stage 48.0)\n",
      "25/05/05 11:18:46 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 512681042 nanos, during time span of 513341459 nanos.\n",
      "25/05/05 11:18:46 INFO Executor: Finished task 0.0 in stage 48.0 (TID 48). 2188 bytes result sent to driver\n",
      "25/05/05 11:18:46 INFO TaskSetManager: Finished task 0.0 in stage 48.0 (TID 48) in 531 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:18:46 INFO TaskSchedulerImpl: Removed TaskSet 48.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:18:46 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:18:46 INFO DataWritingSparkTask: Committed partition 0 (task 47, attempt 0, stage 47.0)\n",
      "25/05/05 11:18:46 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 509261459 nanos, during time span of 512754458 nanos.\n",
      "25/05/05 11:18:46 INFO DAGScheduler: ResultStage 48 (start at NativeMethodAccessorImpl.java:0) finished in 0.535 s\n",
      "25/05/05 11:18:46 INFO DAGScheduler: Job 49 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:18:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 48: Stage finished\n",
      "25/05/05 11:18:46 INFO DAGScheduler: Job 49 finished: start at NativeMethodAccessorImpl.java:0, took 0.543128 s\n",
      "25/05/05 11:18:46 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 16, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:18:46 INFO Executor: Finished task 0.0 in stage 47.0 (TID 47). 3557 bytes result sent to driver\n",
      "25/05/05 11:18:46 INFO TaskSetManager: Finished task 0.0 in stage 47.0 (TID 47) in 538 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:18:46 INFO TaskSchedulerImpl: Removed TaskSet 47.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:18:46 INFO DAGScheduler: ResultStage 47 (start at NativeMethodAccessorImpl.java:0) finished in 0.548 s\n",
      "25/05/05 11:18:46 INFO DAGScheduler: Job 48 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:18:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 47: Stage finished\n",
      "25/05/05 11:18:46 INFO DAGScheduler: Job 48 finished: start at NativeMethodAccessorImpl.java:0, took 0.549778 s\n",
      "25/05/05 11:18:46 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 16, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@2acf53d3] is committing.\n",
      "25/05/05 11:18:46 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 16, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@2acf53d3] committed.\n",
      "25/05/05 11:18:46 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 16, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:18:46 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/16 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.16.777eefcf-fa2c-43d6-b0a1-b4d618da8faf.tmp\n",
      "25/05/05 11:18:46 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/16 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.16.bc247406-71f3-4f3d-b817-165369a8d3f8.tmp\n",
      "25/05/05 11:18:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3248, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:46 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:18:46 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:18:46 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:18:46 INFO connection: Opened connection [connectionId{localValue:31, serverValue:4401}] to localhost:27017\n",
      "25/05/05 11:18:46 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.16.777eefcf-fa2c-43d6-b0a1-b4d618da8faf.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/16\n",
      "25/05/05 11:18:46 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:18:45.975Z\",\n",
      "  \"batchId\" : 16,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.4285714285714286,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 590,\n",
      "    \"commitOffsets\" : 43,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 10,\n",
      "    \"triggerExecution\" : 700,\n",
      "    \"walCommit\" : 54\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3247\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3248\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3248\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.4285714285714286,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:18:46 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=4629917}\n",
      "25/05/05 11:18:46 INFO connection: Opened connection [connectionId{localValue:32, serverValue:4402}] to localhost:27017\n",
      "25/05/05 11:18:46 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:18:46 INFO connection: Closed connection [connectionId{localValue:32, serverValue:4402}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:18:46 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 506818416 nanos, during time span of 523918333 nanos.\n",
      "25/05/05 11:18:46 INFO Executor: Finished task 0.0 in stage 49.0 (TID 49). 1645 bytes result sent to driver\n",
      "25/05/05 11:18:46 INFO TaskSetManager: Finished task 0.0 in stage 49.0 (TID 49) in 552 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:18:46 INFO TaskSchedulerImpl: Removed TaskSet 49.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:18:46 INFO DAGScheduler: ResultStage 49 (start at NativeMethodAccessorImpl.java:0) finished in 0.575 s\n",
      "25/05/05 11:18:46 INFO DAGScheduler: Job 50 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:18:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 49: Stage finished\n",
      "25/05/05 11:18:46 INFO DAGScheduler: Job 50 finished: start at NativeMethodAccessorImpl.java:0, took 0.577499 s\n",
      "25/05/05 11:18:46 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.16.bc247406-71f3-4f3d-b817-165369a8d3f8.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/16\n",
      "25/05/05 11:18:46 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:18:45.975Z\",\n",
      "  \"batchId\" : 16,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.400560224089636,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 591,\n",
      "    \"commitOffsets\" : 50,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 10,\n",
      "    \"triggerExecution\" : 714,\n",
      "    \"walCommit\" : 59\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3247\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3248\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3248\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.400560224089636,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:18:46 INFO MemoryStore: Block broadcast_66 stored as values in memory (estimated size 248.0 B, free 366.2 MiB)\n",
      "25/05/05 11:18:46 INFO MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.2 MiB)\n",
      "25/05/05 11:18:46 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:18:46 INFO SparkContext: Created broadcast 66 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:46 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/16 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.16.cfc7a3f2-1257-43ad-9095-bcaa390d8cc1.tmp\n",
      "25/05/05 11:18:46 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.16.cfc7a3f2-1257-43ad-9095-bcaa390d8cc1.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/16\n",
      "25/05/05 11:18:46 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:18:45.974Z\",\n",
      "  \"batchId\" : 16,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.3280212483399734,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 649,\n",
      "    \"commitOffsets\" : 35,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 10,\n",
      "    \"triggerExecution\" : 753,\n",
      "    \"walCommit\" : 57\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3247\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3248\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3248\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.3280212483399734,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 16\n",
      "-------------------------------------------\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION      |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R016|R032|00-00-01|TIME SQ-42 ST|05/05/2025|11:18:44|REGULAR|3          |8         |2025-05-05 11:18:45.978|\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:18:51 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/17 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.17.557cb7ec-453e-4ee9-b839-1fe783f5fad5.tmp\n",
      "25/05/05 11:18:51 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/17 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.17.1fd40642-179e-4de4-b834-5c1fa7597441.tmp\n",
      "25/05/05 11:18:51 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/17 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.17.c7e6573d-0f43-4c12-b607-8e7a98abc681.tmp\n",
      "25/05/05 11:18:51 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.17.557cb7ec-453e-4ee9-b839-1fe783f5fad5.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/17\n",
      "25/05/05 11:18:51 INFO MicroBatchExecution: Committed offsets for batch 17. Metadata OffsetSeqMetadata(0,1746458331564,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:18:51 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.17.1fd40642-179e-4de4-b834-5c1fa7597441.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/17\n",
      "25/05/05 11:18:51 INFO MicroBatchExecution: Committed offsets for batch 17. Metadata OffsetSeqMetadata(0,1746458331565,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:18:51 INFO IncrementalExecution: Current batch timestamp = 1746458331564\n",
      "25/05/05 11:18:51 INFO IncrementalExecution: Current batch timestamp = 1746458331565\n",
      "25/05/05 11:18:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:51 INFO IncrementalExecution: Current batch timestamp = 1746458331564\n",
      "25/05/05 11:18:51 INFO IncrementalExecution: Current batch timestamp = 1746458331565\n",
      "25/05/05 11:18:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:51 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.17.c7e6573d-0f43-4c12-b607-8e7a98abc681.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/17\n",
      "25/05/05 11:18:51 INFO MicroBatchExecution: Committed offsets for batch 17. Metadata OffsetSeqMetadata(0,1746458331577,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:18:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:51 INFO IncrementalExecution: Current batch timestamp = 1746458331564\n",
      "25/05/05 11:18:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:51 INFO IncrementalExecution: Current batch timestamp = 1746458331577\n",
      "25/05/05 11:18:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:51 INFO IncrementalExecution: Current batch timestamp = 1746458331577\n",
      "25/05/05 11:18:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:51 INFO IncrementalExecution: Current batch timestamp = 1746458331577\n",
      "25/05/05 11:18:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:51 INFO CodeGenerator: Code generated in 8.12675 ms\n",
      "25/05/05 11:18:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:51 INFO CodeGenerator: Code generated in 5.97475 ms\n",
      "25/05/05 11:18:51 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 17, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:18:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:51 INFO DAGScheduler: Got job 51 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:18:51 INFO DAGScheduler: Final stage: ResultStage 50 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:18:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:18:51 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:18:51 INFO DAGScheduler: Submitting ResultStage 50 (MapPartitionsRDD[280] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:18:51 INFO MemoryStore: Block broadcast_67 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:18:51 INFO MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:18:51 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:51 INFO SparkContext: Created broadcast 67 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:18:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 50 (MapPartitionsRDD[280] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:18:51 INFO TaskSchedulerImpl: Adding task set 50.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:18:51 INFO TaskSetManager: Starting task 0.0 in stage 50.0 (TID 50) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:18:51 INFO Executor: Running task 0.0 in stage 50.0 (TID 50)\n",
      "25/05/05 11:18:51 INFO CodeGenerator: Code generated in 4.959042 ms\n",
      "25/05/05 11:18:51 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 17, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@64b2d296]. The input RDD has 1 partitions.\n",
      "25/05/05 11:18:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:51 INFO DAGScheduler: Got job 52 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:18:51 INFO DAGScheduler: Final stage: ResultStage 51 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:18:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:18:51 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:18:51 INFO DAGScheduler: Submitting ResultStage 51 (MapPartitionsRDD[283] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:18:51 INFO MemoryStore: Block broadcast_68 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:18:51 INFO MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:18:51 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:51 INFO SparkContext: Created broadcast 68 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:18:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 51 (MapPartitionsRDD[283] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:18:51 INFO CodeGenerator: Code generated in 7.184458 ms\n",
      "25/05/05 11:18:51 INFO TaskSchedulerImpl: Adding task set 51.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:18:51 INFO TaskSetManager: Starting task 0.0 in stage 51.0 (TID 51) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:18:51 INFO Executor: Running task 0.0 in stage 51.0 (TID 51)\n",
      "25/05/05 11:18:51 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3248 untilOffset=3249, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=17 taskId=50 partitionId=0\n",
      "25/05/05 11:18:51 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3248 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:51 INFO CodeGenerator: Code generated in 5.520375 ms\n",
      "25/05/05 11:18:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:51 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3248 untilOffset=3249, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=17 taskId=51 partitionId=0\n",
      "25/05/05 11:18:51 INFO DAGScheduler: Got job 53 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:18:51 INFO DAGScheduler: Final stage: ResultStage 52 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:18:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:18:51 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:18:51 INFO DAGScheduler: Submitting ResultStage 52 (MapPartitionsRDD[288] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:18:51 INFO MemoryStore: Block broadcast_69 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:18:51 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3248 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:51 INFO MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.1 MiB)\n",
      "25/05/05 11:18:51 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:18:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:51 INFO SparkContext: Created broadcast 69 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:18:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 52 (MapPartitionsRDD[288] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:18:51 INFO TaskSchedulerImpl: Adding task set 52.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:18:51 INFO TaskSetManager: Starting task 0.0 in stage 52.0 (TID 52) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:18:51 INFO Executor: Running task 0.0 in stage 52.0 (TID 52)\n",
      "25/05/05 11:18:51 INFO BlockManagerInfo: Removed broadcast_64_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:18:51 INFO BlockManagerInfo: Removed broadcast_65_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:51 INFO BlockManagerInfo: Removed broadcast_66_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:18:51 INFO BlockManagerInfo: Removed broadcast_63_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:51 INFO CodeGenerator: Code generated in 30.993375 ms\n",
      "25/05/05 11:18:51 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3248 untilOffset=3249, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=17 taskId=52 partitionId=0\n",
      "25/05/05 11:18:51 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3248 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:52 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:52 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:52 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3249, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:52 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:18:52 INFO DataWritingSparkTask: Committed partition 0 (task 50, attempt 0, stage 50.0)\n",
      "25/05/05 11:18:52 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 506855292 nanos, during time span of 507402916 nanos.\n",
      "25/05/05 11:18:52 INFO Executor: Finished task 0.0 in stage 50.0 (TID 50). 2231 bytes result sent to driver\n",
      "25/05/05 11:18:52 INFO TaskSetManager: Finished task 0.0 in stage 50.0 (TID 50) in 522 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:18:52 INFO TaskSchedulerImpl: Removed TaskSet 50.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:18:52 INFO DAGScheduler: ResultStage 50 (start at NativeMethodAccessorImpl.java:0) finished in 0.525 s\n",
      "25/05/05 11:18:52 INFO DAGScheduler: Job 51 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:18:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 50: Stage finished\n",
      "25/05/05 11:18:52 INFO DAGScheduler: Job 51 finished: start at NativeMethodAccessorImpl.java:0, took 0.525708 s\n",
      "25/05/05 11:18:52 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 17, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:18:52 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 17, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:18:52 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:52 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:52 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3249, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:52 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/17 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.17.701f6a80-9d20-41f6-afb2-fe303cf51f81.tmp\n",
      "25/05/05 11:18:52 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:18:52 INFO DataWritingSparkTask: Committed partition 0 (task 51, attempt 0, stage 51.0)\n",
      "25/05/05 11:18:52 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 506300041 nanos, during time span of 508584416 nanos.\n",
      "25/05/05 11:18:52 INFO Executor: Finished task 0.0 in stage 51.0 (TID 51). 3596 bytes result sent to driver\n",
      "25/05/05 11:18:52 INFO TaskSetManager: Finished task 0.0 in stage 51.0 (TID 51) in 531 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:18:52 INFO TaskSchedulerImpl: Removed TaskSet 51.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:18:52 INFO DAGScheduler: ResultStage 51 (start at NativeMethodAccessorImpl.java:0) finished in 0.536 s\n",
      "25/05/05 11:18:52 INFO DAGScheduler: Job 52 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:18:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 51: Stage finished\n",
      "25/05/05 11:18:52 INFO DAGScheduler: Job 52 finished: start at NativeMethodAccessorImpl.java:0, took 0.537047 s\n",
      "25/05/05 11:18:52 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 17, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@64b2d296] is committing.\n",
      "25/05/05 11:18:52 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 17, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@64b2d296] committed.\n",
      "25/05/05 11:18:52 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/17 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.17.8fb1e004-480c-48ad-92e4-2e45d3b1aa0d.tmp\n",
      "25/05/05 11:18:52 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.17.701f6a80-9d20-41f6-afb2-fe303cf51f81.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/17\n",
      "25/05/05 11:18:52 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:18:51.563Z\",\n",
      "  \"batchId\" : 17,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.5600624024960998,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 568,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 7,\n",
      "    \"triggerExecution\" : 641,\n",
      "    \"walCommit\" : 34\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3248\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3249\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3249\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.5600624024960998,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:18:52 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.17.8fb1e004-480c-48ad-92e4-2e45d3b1aa0d.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/17\n",
      "25/05/05 11:18:52 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:18:51.574Z\",\n",
      "  \"batchId\" : 17,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.567398119122257,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 558,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 15,\n",
      "    \"triggerExecution\" : 638,\n",
      "    \"walCommit\" : 33\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3248\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3249\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3249\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.567398119122257,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:18:52 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:52 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:52 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3249, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:52 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:18:52 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:18:52 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:18:52 INFO connection: Opened connection [connectionId{localValue:33, serverValue:4403}] to localhost:27017\n",
      "25/05/05 11:18:52 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=514083}\n",
      "25/05/05 11:18:52 INFO connection: Opened connection [connectionId{localValue:34, serverValue:4404}] to localhost:27017\n",
      "25/05/05 11:18:52 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:18:52 INFO connection: Closed connection [connectionId{localValue:34, serverValue:4404}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:18:52 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 506600083 nanos, during time span of 518862708 nanos.\n",
      "25/05/05 11:18:52 INFO Executor: Finished task 0.0 in stage 52.0 (TID 52). 1688 bytes result sent to driver\n",
      "25/05/05 11:18:52 INFO TaskSetManager: Finished task 0.0 in stage 52.0 (TID 52) in 560 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:18:52 INFO TaskSchedulerImpl: Removed TaskSet 52.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:18:52 INFO DAGScheduler: ResultStage 52 (start at NativeMethodAccessorImpl.java:0) finished in 0.572 s\n",
      "25/05/05 11:18:52 INFO DAGScheduler: Job 53 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:18:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 52: Stage finished\n",
      "25/05/05 11:18:52 INFO DAGScheduler: Job 53 finished: start at NativeMethodAccessorImpl.java:0, took 0.573464 s\n",
      "25/05/05 11:18:52 INFO MemoryStore: Block broadcast_70 stored as values in memory (estimated size 248.0 B, free 366.2 MiB)\n",
      "25/05/05 11:18:52 INFO MemoryStore: Block broadcast_70_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.2 MiB)\n",
      "25/05/05 11:18:52 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:18:52 INFO SparkContext: Created broadcast 70 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:52 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/17 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.17.1bce32a9-c084-4aa4-a19c-71253d0cec2b.tmp\n",
      "25/05/05 11:18:52 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.17.1bce32a9-c084-4aa4-a19c-71253d0cec2b.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/17\n",
      "25/05/05 11:18:52 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:18:51.564Z\",\n",
      "  \"batchId\" : 17,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.4245014245014247,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 631,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 702,\n",
      "    \"walCommit\" : 35\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3248\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3249\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3249\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.4245014245014247,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 17\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION  |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A002|R170|00-00-00|FULTON ST|05/05/2025|11:18:50|REGULAR|8          |14        |2025-05-05 11:18:51.564|\n",
      "+----+----+--------+---------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:18:57 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/18 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.18.f1feb5d7-5610-46cc-877d-164beade927e.tmp\n",
      "25/05/05 11:18:57 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/18 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.18.ff81113b-3363-4f26-90da-5aff027bf12a.tmp\n",
      "25/05/05 11:18:57 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/18 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.18.56354649-7b21-4467-9343-db91082c0f64.tmp\n",
      "25/05/05 11:18:57 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.18.f1feb5d7-5610-46cc-877d-164beade927e.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/18\n",
      "25/05/05 11:18:57 INFO MicroBatchExecution: Committed offsets for batch 18. Metadata OffsetSeqMetadata(0,1746458337120,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:18:57 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.18.ff81113b-3363-4f26-90da-5aff027bf12a.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/18\n",
      "25/05/05 11:18:57 INFO MicroBatchExecution: Committed offsets for batch 18. Metadata OffsetSeqMetadata(0,1746458337122,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:18:57 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.18.56354649-7b21-4467-9343-db91082c0f64.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/18\n",
      "25/05/05 11:18:57 INFO MicroBatchExecution: Committed offsets for batch 18. Metadata OffsetSeqMetadata(0,1746458337123,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:18:57 INFO IncrementalExecution: Current batch timestamp = 1746458337120\n",
      "25/05/05 11:18:57 INFO IncrementalExecution: Current batch timestamp = 1746458337122\n",
      "25/05/05 11:18:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:57 INFO IncrementalExecution: Current batch timestamp = 1746458337123\n",
      "25/05/05 11:18:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:57 INFO IncrementalExecution: Current batch timestamp = 1746458337120\n",
      "25/05/05 11:18:57 INFO IncrementalExecution: Current batch timestamp = 1746458337122\n",
      "25/05/05 11:18:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:57 INFO IncrementalExecution: Current batch timestamp = 1746458337123\n",
      "25/05/05 11:18:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:57 INFO IncrementalExecution: Current batch timestamp = 1746458337120\n",
      "25/05/05 11:18:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:57 INFO IncrementalExecution: Current batch timestamp = 1746458337123\n",
      "25/05/05 11:18:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:18:57 INFO CodeGenerator: Code generated in 5.217 ms\n",
      "25/05/05 11:18:57 INFO CodeGenerator: Code generated in 6.215459 ms\n",
      "25/05/05 11:18:57 INFO CodeGenerator: Code generated in 6.286042 ms\n",
      "25/05/05 11:18:57 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 18, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@5e4665e3]. The input RDD has 1 partitions.\n",
      "25/05/05 11:18:57 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:57 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 18, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:18:57 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:57 INFO DAGScheduler: Got job 54 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:18:57 INFO DAGScheduler: Final stage: ResultStage 53 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:18:57 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:18:57 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:18:57 INFO DAGScheduler: Submitting ResultStage 53 (MapPartitionsRDD[298] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:18:57 INFO MemoryStore: Block broadcast_71 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:18:57 INFO MemoryStore: Block broadcast_71_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:18:57 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:57 INFO SparkContext: Created broadcast 71 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:18:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 53 (MapPartitionsRDD[298] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:18:57 INFO TaskSchedulerImpl: Adding task set 53.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:18:57 INFO DAGScheduler: Got job 55 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:18:57 INFO DAGScheduler: Final stage: ResultStage 54 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:18:57 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:18:57 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:18:57 INFO DAGScheduler: Submitting ResultStage 54 (MapPartitionsRDD[299] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:18:57 INFO TaskSetManager: Starting task 0.0 in stage 53.0 (TID 53) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:18:57 INFO Executor: Running task 0.0 in stage 53.0 (TID 53)\n",
      "25/05/05 11:18:57 INFO MemoryStore: Block broadcast_72 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:18:57 INFO MemoryStore: Block broadcast_72_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:18:57 INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:18:57 INFO SparkContext: Created broadcast 72 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:18:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 54 (MapPartitionsRDD[299] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:18:57 INFO TaskSchedulerImpl: Adding task set 54.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:18:57 INFO TaskSetManager: Starting task 0.0 in stage 54.0 (TID 54) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:18:57 INFO Executor: Running task 0.0 in stage 54.0 (TID 54)\n",
      "25/05/05 11:18:57 INFO CodeGenerator: Code generated in 5.106625 ms\n",
      "25/05/05 11:18:57 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3249 untilOffset=3250, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=18 taskId=53 partitionId=0\n",
      "25/05/05 11:18:57 INFO CodeGenerator: Code generated in 5.636459 ms\n",
      "25/05/05 11:18:57 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:57 INFO DAGScheduler: Got job 56 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:18:57 INFO DAGScheduler: Final stage: ResultStage 55 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:18:57 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:18:57 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:18:57 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3249 untilOffset=3250, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=18 taskId=54 partitionId=0\n",
      "25/05/05 11:18:57 INFO DAGScheduler: Submitting ResultStage 55 (MapPartitionsRDD[304] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:18:57 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3249 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:57 INFO MemoryStore: Block broadcast_73 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:18:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:57 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3249 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:57 INFO MemoryStore: Block broadcast_73_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:18:57 INFO BlockManagerInfo: Added broadcast_73_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:18:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:57 INFO SparkContext: Created broadcast 73 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:18:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 55 (MapPartitionsRDD[304] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:18:57 INFO TaskSchedulerImpl: Adding task set 55.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:18:57 INFO TaskSetManager: Starting task 0.0 in stage 55.0 (TID 55) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:18:57 INFO Executor: Running task 0.0 in stage 55.0 (TID 55)\n",
      "25/05/05 11:18:57 INFO CodeGenerator: Code generated in 7.021541 ms\n",
      "25/05/05 11:18:57 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3249 untilOffset=3250, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=18 taskId=55 partitionId=0\n",
      "25/05/05 11:18:57 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3249 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3250, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3250, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:57 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:18:57 INFO DataWritingSparkTask: Committed partition 0 (task 54, attempt 0, stage 54.0)\n",
      "25/05/05 11:18:57 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 506421416 nanos, during time span of 510915041 nanos.\n",
      "25/05/05 11:18:57 INFO Executor: Finished task 0.0 in stage 54.0 (TID 54). 2188 bytes result sent to driver\n",
      "25/05/05 11:18:57 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:18:57 INFO DataWritingSparkTask: Committed partition 0 (task 53, attempt 0, stage 53.0)\n",
      "25/05/05 11:18:57 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 511326166 nanos, during time span of 519083750 nanos.\n",
      "25/05/05 11:18:57 INFO Executor: Finished task 0.0 in stage 53.0 (TID 53). 3559 bytes result sent to driver\n",
      "25/05/05 11:18:57 INFO TaskSetManager: Finished task 0.0 in stage 54.0 (TID 54) in 542 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:18:57 INFO TaskSchedulerImpl: Removed TaskSet 54.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:18:57 INFO TaskSetManager: Finished task 0.0 in stage 53.0 (TID 53) in 547 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:18:57 INFO TaskSchedulerImpl: Removed TaskSet 53.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:18:57 INFO DAGScheduler: ResultStage 54 (start at NativeMethodAccessorImpl.java:0) finished in 0.547 s\n",
      "25/05/05 11:18:57 INFO DAGScheduler: Job 55 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:18:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 54: Stage finished\n",
      "25/05/05 11:18:57 INFO DAGScheduler: Job 55 finished: start at NativeMethodAccessorImpl.java:0, took 0.551706 s\n",
      "25/05/05 11:18:57 INFO DAGScheduler: ResultStage 53 (start at NativeMethodAccessorImpl.java:0) finished in 0.551 s\n",
      "25/05/05 11:18:57 INFO DAGScheduler: Job 54 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:18:57 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 18, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:18:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 53: Stage finished\n",
      "25/05/05 11:18:57 INFO DAGScheduler: Job 54 finished: start at NativeMethodAccessorImpl.java:0, took 0.554329 s\n",
      "25/05/05 11:18:57 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 18, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@5e4665e3] is committing.\n",
      "25/05/05 11:18:57 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 18, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@5e4665e3] committed.\n",
      "25/05/05 11:18:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:18:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3250, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:18:57 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:18:57 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:18:57 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:18:57 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/18 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.18.80b49f66-0e8d-4ccf-9ad6-1693db1c7193.tmp\n",
      "25/05/05 11:18:57 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 18, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:18:57 INFO connection: Opened connection [connectionId{localValue:35, serverValue:4405}] to localhost:27017\n",
      "25/05/05 11:18:57 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=4853250}\n",
      "25/05/05 11:18:57 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.18.80b49f66-0e8d-4ccf-9ad6-1693db1c7193.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/18\n",
      "25/05/05 11:18:57 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:18:57.119Z\",\n",
      "  \"batchId\" : 18,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.461988304093567,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 580,\n",
      "    \"commitOffsets\" : 34,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 7,\n",
      "    \"triggerExecution\" : 684,\n",
      "    \"walCommit\" : 61\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3249\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3250\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3250\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.461988304093567,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:18:57 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/18 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.18.3314f54c-c82f-43db-85fb-4642aed7b755.tmp\n",
      "25/05/05 11:18:57 INFO connection: Opened connection [connectionId{localValue:36, serverValue:4406}] to localhost:27017\n",
      "25/05/05 11:18:57 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:18:57 INFO connection: Closed connection [connectionId{localValue:36, serverValue:4406}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:18:57 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 507601333 nanos, during time span of 542671667 nanos.\n",
      "25/05/05 11:18:57 INFO Executor: Finished task 0.0 in stage 55.0 (TID 55). 1645 bytes result sent to driver\n",
      "25/05/05 11:18:57 INFO TaskSetManager: Finished task 0.0 in stage 55.0 (TID 55) in 564 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:18:57 INFO TaskSchedulerImpl: Removed TaskSet 55.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:18:57 INFO DAGScheduler: ResultStage 55 (start at NativeMethodAccessorImpl.java:0) finished in 0.571 s\n",
      "25/05/05 11:18:57 INFO DAGScheduler: Job 56 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:18:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 55: Stage finished\n",
      "25/05/05 11:18:57 INFO DAGScheduler: Job 56 finished: start at NativeMethodAccessorImpl.java:0, took 0.572647 s\n",
      "25/05/05 11:18:57 INFO MemoryStore: Block broadcast_74 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:18:57 INFO MemoryStore: Block broadcast_74_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:18:57 INFO BlockManagerInfo: Added broadcast_74_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:18:57 INFO SparkContext: Created broadcast 74 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:18:57 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/18 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.18.cf42128d-421c-4c7c-8aa7-71b29320d6e4.tmp\n",
      "25/05/05 11:18:57 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.18.3314f54c-c82f-43db-85fb-4642aed7b755.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/18\n",
      "25/05/05 11:18:57 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:18:57.122Z\",\n",
      "  \"batchId\" : 18,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.4124293785310735,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 604,\n",
      "    \"commitOffsets\" : 35,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 6,\n",
      "    \"triggerExecution\" : 708,\n",
      "    \"walCommit\" : 61\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3249\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3250\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3250\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.4124293785310735,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:18:57 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.18.cf42128d-421c-4c7c-8aa7-71b29320d6e4.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/18\n",
      "25/05/05 11:18:57 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:18:57.121Z\",\n",
      "  \"batchId\" : 18,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.3908205841446455,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 623,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 7,\n",
      "    \"triggerExecution\" : 719,\n",
      "    \"walCommit\" : 60\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3249\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3250\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3250\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.3908205841446455,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 18\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R023|R045|00-00-00|34 ST-HERALD SQ|05/05/2025|11:18:55|REGULAR|6          |11        |2025-05-05 11:18:57.123|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:19:03 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/19 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.19.7e710b80-7d65-4f4f-99c8-ab4af10c6c92.tmp\n",
      "25/05/05 11:19:03 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/19 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.19.a7b94729-943a-4aea-9154-03b3d144ba5a.tmp\n",
      "25/05/05 11:19:03 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/19 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.19.a5c2faf9-353c-4565-bac7-6b5aa322c455.tmp\n",
      "25/05/05 11:19:03 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.19.7e710b80-7d65-4f4f-99c8-ab4af10c6c92.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/19\n",
      "25/05/05 11:19:03 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.19.a7b94729-943a-4aea-9154-03b3d144ba5a.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/19\n",
      "25/05/05 11:19:03 INFO MicroBatchExecution: Committed offsets for batch 19. Metadata OffsetSeqMetadata(0,1746458343624,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:19:03 INFO MicroBatchExecution: Committed offsets for batch 19. Metadata OffsetSeqMetadata(0,1746458343624,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:19:03 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.19.a5c2faf9-353c-4565-bac7-6b5aa322c455.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/19\n",
      "25/05/05 11:19:03 INFO MicroBatchExecution: Committed offsets for batch 19. Metadata OffsetSeqMetadata(0,1746458343630,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:19:03 INFO IncrementalExecution: Current batch timestamp = 1746458343624\n",
      "25/05/05 11:19:03 INFO IncrementalExecution: Current batch timestamp = 1746458343624\n",
      "25/05/05 11:19:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:03 INFO IncrementalExecution: Current batch timestamp = 1746458343630\n",
      "25/05/05 11:19:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:03 INFO IncrementalExecution: Current batch timestamp = 1746458343624\n",
      "25/05/05 11:19:03 INFO IncrementalExecution: Current batch timestamp = 1746458343624\n",
      "25/05/05 11:19:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:03 INFO IncrementalExecution: Current batch timestamp = 1746458343630\n",
      "25/05/05 11:19:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:03 INFO IncrementalExecution: Current batch timestamp = 1746458343624\n",
      "25/05/05 11:19:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:03 INFO BlockManagerInfo: Removed broadcast_67_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:19:03 INFO BlockManagerInfo: Removed broadcast_71_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:03 INFO IncrementalExecution: Current batch timestamp = 1746458343630\n",
      "25/05/05 11:19:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:03 INFO BlockManagerInfo: Removed broadcast_69_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:03 INFO BlockManagerInfo: Removed broadcast_70_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:19:03 INFO CodeGenerator: Code generated in 7.905833 ms\n",
      "25/05/05 11:19:03 INFO BlockManagerInfo: Removed broadcast_68_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 19, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:19:03 INFO BlockManagerInfo: Removed broadcast_74_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:19:03 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:03 INFO DAGScheduler: Got job 57 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:19:03 INFO DAGScheduler: Final stage: ResultStage 56 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:19:03 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:19:03 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:19:03 INFO DAGScheduler: Submitting ResultStage 56 (MapPartitionsRDD[310] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:19:03 INFO MemoryStore: Block broadcast_75 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:03 INFO BlockManagerInfo: Removed broadcast_73_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:03 INFO MemoryStore: Block broadcast_75_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.3 MiB)\n",
      "25/05/05 11:19:03 INFO BlockManagerInfo: Added broadcast_75_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:03 INFO SparkContext: Created broadcast 75 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:19:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 56 (MapPartitionsRDD[310] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:19:03 INFO TaskSchedulerImpl: Adding task set 56.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:19:03 INFO TaskSetManager: Starting task 0.0 in stage 56.0 (TID 56) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:19:03 INFO Executor: Running task 0.0 in stage 56.0 (TID 56)\n",
      "25/05/05 11:19:03 INFO BlockManagerInfo: Removed broadcast_72_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:03 INFO CodeGenerator: Code generated in 8.278708 ms\n",
      "25/05/05 11:19:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 19, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6a434925]. The input RDD has 1 partitions.\n",
      "25/05/05 11:19:03 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:03 INFO DAGScheduler: Got job 58 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:19:03 INFO DAGScheduler: Final stage: ResultStage 57 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:19:03 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:19:03 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:19:03 INFO CodeGenerator: Code generated in 6.070834 ms\n",
      "25/05/05 11:19:03 INFO DAGScheduler: Submitting ResultStage 57 (MapPartitionsRDD[315] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:19:03 INFO MemoryStore: Block broadcast_76 stored as values in memory (estimated size 16.2 KiB, free 366.3 MiB)\n",
      "25/05/05 11:19:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3250 untilOffset=3251, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=19 taskId=56 partitionId=0\n",
      "25/05/05 11:19:03 INFO MemoryStore: Block broadcast_76_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.3 MiB)\n",
      "25/05/05 11:19:03 INFO BlockManagerInfo: Added broadcast_76_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:03 INFO SparkContext: Created broadcast 76 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:19:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 57 (MapPartitionsRDD[315] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:19:03 INFO TaskSchedulerImpl: Adding task set 57.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:19:03 INFO TaskSetManager: Starting task 0.0 in stage 57.0 (TID 57) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:19:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3250 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:03 INFO Executor: Running task 0.0 in stage 57.0 (TID 57)\n",
      "25/05/05 11:19:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:03 INFO CodeGenerator: Code generated in 5.250875 ms\n",
      "25/05/05 11:19:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3250 untilOffset=3251, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=19 taskId=57 partitionId=0\n",
      "25/05/05 11:19:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3250 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:03 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:03 INFO DAGScheduler: Got job 59 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:19:03 INFO DAGScheduler: Final stage: ResultStage 58 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:19:03 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:19:03 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:19:03 INFO DAGScheduler: Submitting ResultStage 58 (MapPartitionsRDD[320] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:19:03 INFO MemoryStore: Block broadcast_77 stored as values in memory (estimated size 47.1 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:03 INFO MemoryStore: Block broadcast_77_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:03 INFO BlockManagerInfo: Added broadcast_77_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:03 INFO SparkContext: Created broadcast 77 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:19:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 58 (MapPartitionsRDD[320] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:19:03 INFO TaskSchedulerImpl: Adding task set 58.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:19:03 INFO TaskSetManager: Starting task 0.0 in stage 58.0 (TID 58) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:19:03 INFO Executor: Running task 0.0 in stage 58.0 (TID 58)\n",
      "25/05/05 11:19:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3250 untilOffset=3251, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=19 taskId=58 partitionId=0\n",
      "25/05/05 11:19:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3250 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3251, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:04 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:19:04 INFO DataWritingSparkTask: Committed partition 0 (task 56, attempt 0, stage 56.0)\n",
      "25/05/05 11:19:04 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 505949792 nanos, during time span of 506379458 nanos.\n",
      "25/05/05 11:19:04 INFO Executor: Finished task 0.0 in stage 56.0 (TID 56). 2188 bytes result sent to driver\n",
      "25/05/05 11:19:04 INFO TaskSetManager: Finished task 0.0 in stage 56.0 (TID 56) in 522 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:19:04 INFO TaskSchedulerImpl: Removed TaskSet 56.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:19:04 INFO DAGScheduler: ResultStage 56 (start at NativeMethodAccessorImpl.java:0) finished in 0.524 s\n",
      "25/05/05 11:19:04 INFO DAGScheduler: Job 57 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:19:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 56: Stage finished\n",
      "25/05/05 11:19:04 INFO DAGScheduler: Job 57 finished: start at NativeMethodAccessorImpl.java:0, took 0.525518 s\n",
      "25/05/05 11:19:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 19, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:19:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3251, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 19, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:19:04 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:19:04 INFO DataWritingSparkTask: Committed partition 0 (task 57, attempt 0, stage 57.0)\n",
      "25/05/05 11:19:04 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 506446791 nanos, during time span of 508099750 nanos.\n",
      "25/05/05 11:19:04 INFO Executor: Finished task 0.0 in stage 57.0 (TID 57). 3554 bytes result sent to driver\n",
      "25/05/05 11:19:04 INFO TaskSetManager: Finished task 0.0 in stage 57.0 (TID 57) in 521 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:19:04 INFO TaskSchedulerImpl: Removed TaskSet 57.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:19:04 INFO DAGScheduler: ResultStage 57 (start at NativeMethodAccessorImpl.java:0) finished in 0.525 s\n",
      "25/05/05 11:19:04 INFO DAGScheduler: Job 58 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:19:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 57: Stage finished\n",
      "25/05/05 11:19:04 INFO DAGScheduler: Job 58 finished: start at NativeMethodAccessorImpl.java:0, took 0.527922 s\n",
      "25/05/05 11:19:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 19, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6a434925] is committing.\n",
      "25/05/05 11:19:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 19, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6a434925] committed.\n",
      "25/05/05 11:19:04 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/19 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.19.6176ee14-76af-4296-85fb-10f1917de3b6.tmp\n",
      "25/05/05 11:19:04 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/19 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.19.95e30729-7b90-4e89-90f3-d07f0ff67575.tmp\n",
      "25/05/05 11:19:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3251, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:04 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:19:04 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:19:04 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:19:04 INFO connection: Opened connection [connectionId{localValue:37, serverValue:4407}] to localhost:27017\n",
      "25/05/05 11:19:04 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=429292}\n",
      "25/05/05 11:19:04 INFO connection: Opened connection [connectionId{localValue:38, serverValue:4408}] to localhost:27017\n",
      "25/05/05 11:19:04 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.19.6176ee14-76af-4296-85fb-10f1917de3b6.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/19\n",
      "25/05/05 11:19:04 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:19:03.623Z\",\n",
      "  \"batchId\" : 19,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.5698587127158556,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 568,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 6,\n",
      "    \"triggerExecution\" : 637,\n",
      "    \"walCommit\" : 33\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3250\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3251\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3251\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.5698587127158556,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:19:04 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.19.95e30729-7b90-4e89-90f3-d07f0ff67575.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/19\n",
      "25/05/05 11:19:04 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:19:03.626Z\",\n",
      "  \"batchId\" : 19,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.5698587127158556,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 569,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 4,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 637,\n",
      "    \"walCommit\" : 30\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3250\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3251\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3251\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.5698587127158556,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:19:04 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:19:04 INFO connection: Closed connection [connectionId{localValue:38, serverValue:4408}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:19:04 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 506415375 nanos, during time span of 524293917 nanos.\n",
      "25/05/05 11:19:04 INFO Executor: Finished task 0.0 in stage 58.0 (TID 58). 1645 bytes result sent to driver\n",
      "25/05/05 11:19:04 INFO TaskSetManager: Finished task 0.0 in stage 58.0 (TID 58) in 531 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:19:04 INFO TaskSchedulerImpl: Removed TaskSet 58.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:19:04 INFO DAGScheduler: ResultStage 58 (start at NativeMethodAccessorImpl.java:0) finished in 0.536 s\n",
      "25/05/05 11:19:04 INFO DAGScheduler: Job 59 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:19:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 58: Stage finished\n",
      "25/05/05 11:19:04 INFO DAGScheduler: Job 59 finished: start at NativeMethodAccessorImpl.java:0, took 0.537463 s\n",
      "25/05/05 11:19:04 INFO MemoryStore: Block broadcast_78 stored as values in memory (estimated size 248.0 B, free 366.2 MiB)\n",
      "25/05/05 11:19:04 INFO MemoryStore: Block broadcast_78_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.2 MiB)\n",
      "25/05/05 11:19:04 INFO BlockManagerInfo: Added broadcast_78_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:19:04 INFO SparkContext: Created broadcast 78 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:04 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/19 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.19.a675eaae-4917-445c-832d-a94e0fc88ae9.tmp\n",
      "25/05/05 11:19:04 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.19.a675eaae-4917-445c-832d-a94e0fc88ae9.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/19\n",
      "25/05/05 11:19:04 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:19:03.623Z\",\n",
      "  \"batchId\" : 19,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.4814814814814814,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 606,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 6,\n",
      "    \"triggerExecution\" : 675,\n",
      "    \"walCommit\" : 33\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3250\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3251\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3251\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.4814814814814814,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 19\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|B013|R081|02-00-00|161/YANKEE STAD|05/05/2025|11:19:02|REGULAR|9          |9         |2025-05-05 11:19:03.624|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:19:10 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/20 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.20.c6e19ac0-551c-4afd-a6a2-1854fb5cb7a8.tmp\n",
      "25/05/05 11:19:10 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/20 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.20.db560276-b4a2-4caa-b09f-3b7d944aabed.tmp\n",
      "25/05/05 11:19:10 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/20 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.20.9bfc061a-5a7c-404a-96ff-aadff112dc6b.tmp\n",
      "25/05/05 11:19:10 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.20.db560276-b4a2-4caa-b09f-3b7d944aabed.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/20\n",
      "25/05/05 11:19:10 INFO MicroBatchExecution: Committed offsets for batch 20. Metadata OffsetSeqMetadata(0,1746458350188,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:19:10 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.20.c6e19ac0-551c-4afd-a6a2-1854fb5cb7a8.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/20\n",
      "25/05/05 11:19:10 INFO MicroBatchExecution: Committed offsets for batch 20. Metadata OffsetSeqMetadata(0,1746458350188,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:19:10 INFO IncrementalExecution: Current batch timestamp = 1746458350188\n",
      "25/05/05 11:19:10 INFO IncrementalExecution: Current batch timestamp = 1746458350188\n",
      "25/05/05 11:19:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:10 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.20.9bfc061a-5a7c-404a-96ff-aadff112dc6b.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/20\n",
      "25/05/05 11:19:10 INFO MicroBatchExecution: Committed offsets for batch 20. Metadata OffsetSeqMetadata(0,1746458350188,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:19:10 INFO IncrementalExecution: Current batch timestamp = 1746458350188\n",
      "25/05/05 11:19:10 INFO IncrementalExecution: Current batch timestamp = 1746458350188\n",
      "25/05/05 11:19:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:10 INFO IncrementalExecution: Current batch timestamp = 1746458350188\n",
      "25/05/05 11:19:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:10 INFO IncrementalExecution: Current batch timestamp = 1746458350188\n",
      "25/05/05 11:19:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:10 INFO IncrementalExecution: Current batch timestamp = 1746458350188\n",
      "25/05/05 11:19:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:10 INFO IncrementalExecution: Current batch timestamp = 1746458350188\n",
      "25/05/05 11:19:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:10 INFO CodeGenerator: Code generated in 14.626125 ms\n",
      "25/05/05 11:19:10 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 20, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@423df918]. The input RDD has 1 partitions.\n",
      "25/05/05 11:19:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:10 INFO DAGScheduler: Got job 60 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:19:10 INFO DAGScheduler: Final stage: ResultStage 59 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:19:10 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:19:10 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:19:10 INFO DAGScheduler: Submitting ResultStage 59 (MapPartitionsRDD[328] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:19:10 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 20, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:19:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:10 INFO MemoryStore: Block broadcast_79 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:10 INFO MemoryStore: Block broadcast_79_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:10 INFO BlockManagerInfo: Added broadcast_79_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:10 INFO SparkContext: Created broadcast 79 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:19:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 59 (MapPartitionsRDD[328] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:19:10 INFO TaskSchedulerImpl: Adding task set 59.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:19:10 INFO DAGScheduler: Got job 61 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:19:10 INFO DAGScheduler: Final stage: ResultStage 60 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:19:10 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:19:10 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:19:10 INFO DAGScheduler: Submitting ResultStage 60 (MapPartitionsRDD[331] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:19:10 INFO TaskSetManager: Starting task 0.0 in stage 59.0 (TID 59) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:19:10 INFO Executor: Running task 0.0 in stage 59.0 (TID 59)\n",
      "25/05/05 11:19:10 INFO MemoryStore: Block broadcast_80 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:10 INFO MemoryStore: Block broadcast_80_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:10 INFO BlockManagerInfo: Added broadcast_80_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:10 INFO SparkContext: Created broadcast 80 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:19:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 60 (MapPartitionsRDD[331] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:19:10 INFO TaskSchedulerImpl: Adding task set 60.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:19:10 INFO TaskSetManager: Starting task 0.0 in stage 60.0 (TID 60) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:19:10 INFO Executor: Running task 0.0 in stage 60.0 (TID 60)\n",
      "25/05/05 11:19:10 INFO CodeGenerator: Code generated in 5.251791 ms\n",
      "25/05/05 11:19:10 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3251 untilOffset=3252, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=20 taskId=60 partitionId=0\n",
      "25/05/05 11:19:10 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3251 untilOffset=3252, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=20 taskId=59 partitionId=0\n",
      "25/05/05 11:19:10 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3251 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:10 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3251 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:10 INFO DAGScheduler: Got job 62 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:19:10 INFO DAGScheduler: Final stage: ResultStage 61 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:19:10 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:19:10 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:19:10 INFO DAGScheduler: Submitting ResultStage 61 (MapPartitionsRDD[336] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:19:10 INFO MemoryStore: Block broadcast_81 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:19:10 INFO MemoryStore: Block broadcast_81_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.1 MiB)\n",
      "25/05/05 11:19:10 INFO BlockManagerInfo: Added broadcast_81_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:19:10 INFO SparkContext: Created broadcast 81 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:19:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 61 (MapPartitionsRDD[336] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:19:10 INFO TaskSchedulerImpl: Adding task set 61.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:19:10 INFO TaskSetManager: Starting task 0.0 in stage 61.0 (TID 61) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:19:10 INFO Executor: Running task 0.0 in stage 61.0 (TID 61)\n",
      "25/05/05 11:19:10 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3251 untilOffset=3252, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=20 taskId=61 partitionId=0\n",
      "25/05/05 11:19:10 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3251 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3252, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3252, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:10 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:19:10 INFO DataWritingSparkTask: Committed partition 0 (task 60, attempt 0, stage 60.0)\n",
      "25/05/05 11:19:10 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 504234792 nanos, during time span of 504811042 nanos.\n",
      "25/05/05 11:19:10 INFO Executor: Finished task 0.0 in stage 60.0 (TID 60). 2188 bytes result sent to driver\n",
      "25/05/05 11:19:10 INFO TaskSetManager: Finished task 0.0 in stage 60.0 (TID 60) in 514 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:19:10 INFO TaskSchedulerImpl: Removed TaskSet 60.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:19:10 INFO DAGScheduler: ResultStage 60 (start at NativeMethodAccessorImpl.java:0) finished in 0.517 s\n",
      "25/05/05 11:19:10 INFO DAGScheduler: Job 61 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:19:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 60: Stage finished\n",
      "25/05/05 11:19:10 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:19:10 INFO DataWritingSparkTask: Committed partition 0 (task 59, attempt 0, stage 59.0)\n",
      "25/05/05 11:19:10 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 503146250 nanos, during time span of 505703375 nanos.\n",
      "25/05/05 11:19:10 INFO DAGScheduler: Job 61 finished: start at NativeMethodAccessorImpl.java:0, took 0.520170 s\n",
      "25/05/05 11:19:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 20, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:19:10 INFO Executor: Finished task 0.0 in stage 59.0 (TID 59). 3553 bytes result sent to driver\n",
      "25/05/05 11:19:10 INFO TaskSetManager: Finished task 0.0 in stage 59.0 (TID 59) in 519 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:19:10 INFO TaskSchedulerImpl: Removed TaskSet 59.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:19:10 INFO DAGScheduler: ResultStage 59 (start at NativeMethodAccessorImpl.java:0) finished in 0.522 s\n",
      "25/05/05 11:19:10 INFO DAGScheduler: Job 60 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:19:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 59: Stage finished\n",
      "25/05/05 11:19:10 INFO DAGScheduler: Job 60 finished: start at NativeMethodAccessorImpl.java:0, took 0.522636 s\n",
      "25/05/05 11:19:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 20, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@423df918] is committing.\n",
      "25/05/05 11:19:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 20, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@423df918] committed.\n",
      "25/05/05 11:19:10 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/20 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.20.df8b8f7e-0457-4fc0-9f1a-f15444c68e4d.tmp\n",
      "25/05/05 11:19:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 20, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:19:10 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/20 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.20.b14731a9-51a4-4f1c-9c8f-2bb2c00587ce.tmp\n",
      "25/05/05 11:19:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3252, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:10 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:19:10 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:19:10 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:19:10 INFO connection: Opened connection [connectionId{localValue:39, serverValue:4409}] to localhost:27017\n",
      "25/05/05 11:19:10 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=341334}\n",
      "25/05/05 11:19:10 INFO connection: Opened connection [connectionId{localValue:40, serverValue:4410}] to localhost:27017\n",
      "25/05/05 11:19:10 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:19:10 INFO connection: Closed connection [connectionId{localValue:40, serverValue:4410}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:19:10 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 502966334 nanos, during time span of 510330417 nanos.\n",
      "25/05/05 11:19:10 INFO Executor: Finished task 0.0 in stage 61.0 (TID 61). 1645 bytes result sent to driver\n",
      "25/05/05 11:19:10 INFO TaskSetManager: Finished task 0.0 in stage 61.0 (TID 61) in 518 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:19:10 INFO TaskSchedulerImpl: Removed TaskSet 61.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:19:10 INFO DAGScheduler: ResultStage 61 (start at NativeMethodAccessorImpl.java:0) finished in 0.525 s\n",
      "25/05/05 11:19:10 INFO DAGScheduler: Job 62 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:19:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 61: Stage finished\n",
      "25/05/05 11:19:10 INFO DAGScheduler: Job 62 finished: start at NativeMethodAccessorImpl.java:0, took 0.526759 s\n",
      "25/05/05 11:19:10 INFO MemoryStore: Block broadcast_82 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:19:10 INFO MemoryStore: Block broadcast_82_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:19:10 INFO BlockManagerInfo: Added broadcast_82_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:19:10 INFO SparkContext: Created broadcast 82 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:10 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.20.df8b8f7e-0457-4fc0-9f1a-f15444c68e4d.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/20\n",
      "25/05/05 11:19:10 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:19:10.186Z\",\n",
      "  \"batchId\" : 20,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.5797788309636651,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 545,\n",
      "    \"commitOffsets\" : 35,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 6,\n",
      "    \"triggerExecution\" : 633,\n",
      "    \"walCommit\" : 45\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3251\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3252\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3252\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.5797788309636651,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:19:10 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/20 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.20.fdae3444-000c-46f9-b8f7-38c5991617c8.tmp\n",
      "25/05/05 11:19:10 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.20.b14731a9-51a4-4f1c-9c8f-2bb2c00587ce.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/20\n",
      "25/05/05 11:19:10 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:19:10.187Z\",\n",
      "  \"batchId\" : 20,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.5600624024960998,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 560,\n",
      "    \"commitOffsets\" : 34,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 7,\n",
      "    \"triggerExecution\" : 641,\n",
      "    \"walCommit\" : 38\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3251\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3252\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3252\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.5600624024960998,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:19:10 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.20.fdae3444-000c-46f9-b8f7-38c5991617c8.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/20\n",
      "25/05/05 11:19:10 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:19:10.187Z\",\n",
      "  \"batchId\" : 20,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "  \"processedRowsPerSecond\" : 1.51285930408472,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 584,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 9,\n",
      "    \"triggerExecution\" : 661,\n",
      "    \"walCommit\" : 35\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3251\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3252\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3252\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "    \"processedRowsPerSecond\" : 1.51285930408472,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 20\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION  |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A002|R170|00-00-00|FULTON ST|05/05/2025|11:19:09|REGULAR|9          |11        |2025-05-05 11:19:10.188|\n",
      "+----+----+--------+---------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:19:12 INFO BlockManagerInfo: Removed broadcast_76_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:19:12 INFO BlockManagerInfo: Removed broadcast_78_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:19:12 INFO BlockManagerInfo: Removed broadcast_75_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:12 INFO BlockManagerInfo: Removed broadcast_82_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:19:12 INFO BlockManagerInfo: Removed broadcast_77_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:12 INFO BlockManagerInfo: Removed broadcast_80_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:12 INFO BlockManagerInfo: Removed broadcast_81_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:12 INFO BlockManagerInfo: Removed broadcast_79_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:14 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/21 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.21.880596e6-0acd-4a10-92d5-4aa2778cd868.tmp\n",
      "25/05/05 11:19:14 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/21 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.21.bbf62d43-b7fc-420d-be36-3207b9c601e6.tmp\n",
      "25/05/05 11:19:14 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/21 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.21.d8a81814-c30e-4255-819a-cc7ea02eede1.tmp\n",
      "25/05/05 11:19:14 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.21.880596e6-0acd-4a10-92d5-4aa2778cd868.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/21\n",
      "25/05/05 11:19:14 INFO MicroBatchExecution: Committed offsets for batch 21. Metadata OffsetSeqMetadata(0,1746458354739,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:19:14 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.21.bbf62d43-b7fc-420d-be36-3207b9c601e6.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/21\n",
      "25/05/05 11:19:14 INFO MicroBatchExecution: Committed offsets for batch 21. Metadata OffsetSeqMetadata(0,1746458354739,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:19:14 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.21.d8a81814-c30e-4255-819a-cc7ea02eede1.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/21\n",
      "25/05/05 11:19:14 INFO MicroBatchExecution: Committed offsets for batch 21. Metadata OffsetSeqMetadata(0,1746458354739,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:19:14 INFO IncrementalExecution: Current batch timestamp = 1746458354739\n",
      "25/05/05 11:19:14 INFO IncrementalExecution: Current batch timestamp = 1746458354739\n",
      "25/05/05 11:19:14 INFO IncrementalExecution: Current batch timestamp = 1746458354739\n",
      "25/05/05 11:19:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:14 INFO IncrementalExecution: Current batch timestamp = 1746458354739\n",
      "25/05/05 11:19:14 INFO IncrementalExecution: Current batch timestamp = 1746458354739\n",
      "25/05/05 11:19:14 INFO IncrementalExecution: Current batch timestamp = 1746458354739\n",
      "25/05/05 11:19:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:14 INFO IncrementalExecution: Current batch timestamp = 1746458354739\n",
      "25/05/05 11:19:14 INFO IncrementalExecution: Current batch timestamp = 1746458354739\n",
      "25/05/05 11:19:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:14 INFO CodeGenerator: Code generated in 10.613666 ms\n",
      "25/05/05 11:19:14 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 21, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@abec69b]. The input RDD has 1 partitions.\n",
      "25/05/05 11:19:14 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:14 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 21, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:19:14 INFO DAGScheduler: Got job 63 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:19:14 INFO DAGScheduler: Final stage: ResultStage 62 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:19:14 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:14 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:19:14 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:19:14 INFO DAGScheduler: Submitting ResultStage 62 (MapPartitionsRDD[346] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:19:14 INFO MemoryStore: Block broadcast_83 stored as values in memory (estimated size 16.2 KiB, free 366.3 MiB)\n",
      "25/05/05 11:19:14 INFO MemoryStore: Block broadcast_83_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.3 MiB)\n",
      "25/05/05 11:19:14 INFO BlockManagerInfo: Added broadcast_83_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:14 INFO SparkContext: Created broadcast 83 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:19:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 62 (MapPartitionsRDD[346] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:19:14 INFO TaskSchedulerImpl: Adding task set 62.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:19:14 INFO DAGScheduler: Got job 64 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:19:14 INFO DAGScheduler: Final stage: ResultStage 63 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:19:14 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:19:14 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:19:14 INFO DAGScheduler: Submitting ResultStage 63 (MapPartitionsRDD[347] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:19:14 INFO TaskSetManager: Starting task 0.0 in stage 62.0 (TID 62) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:19:14 INFO Executor: Running task 0.0 in stage 62.0 (TID 62)\n",
      "25/05/05 11:19:14 INFO MemoryStore: Block broadcast_84 stored as values in memory (estimated size 15.2 KiB, free 366.3 MiB)\n",
      "25/05/05 11:19:14 INFO MemoryStore: Block broadcast_84_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.3 MiB)\n",
      "25/05/05 11:19:14 INFO BlockManagerInfo: Added broadcast_84_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:14 INFO SparkContext: Created broadcast 84 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:19:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 63 (MapPartitionsRDD[347] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:19:14 INFO TaskSchedulerImpl: Adding task set 63.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:19:14 INFO TaskSetManager: Starting task 0.0 in stage 63.0 (TID 63) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:19:14 INFO Executor: Running task 0.0 in stage 63.0 (TID 63)\n",
      "25/05/05 11:19:14 INFO CodeGenerator: Code generated in 7.667833 ms\n",
      "25/05/05 11:19:14 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3252 untilOffset=3253, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=21 taskId=63 partitionId=0\n",
      "25/05/05 11:19:14 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3252 untilOffset=3253, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=21 taskId=62 partitionId=0\n",
      "25/05/05 11:19:14 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3252 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:14 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3252 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:14 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:14 INFO DAGScheduler: Got job 65 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:19:14 INFO DAGScheduler: Final stage: ResultStage 64 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:19:14 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:19:14 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:19:14 INFO DAGScheduler: Submitting ResultStage 64 (MapPartitionsRDD[352] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:19:14 INFO MemoryStore: Block broadcast_85 stored as values in memory (estimated size 47.1 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:14 INFO MemoryStore: Block broadcast_85_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:14 INFO BlockManagerInfo: Added broadcast_85_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:14 INFO SparkContext: Created broadcast 85 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:19:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 64 (MapPartitionsRDD[352] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:19:14 INFO TaskSchedulerImpl: Adding task set 64.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:19:14 INFO TaskSetManager: Starting task 0.0 in stage 64.0 (TID 64) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:19:14 INFO Executor: Running task 0.0 in stage 64.0 (TID 64)\n",
      "25/05/05 11:19:14 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3252 untilOffset=3253, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=21 taskId=64 partitionId=0\n",
      "25/05/05 11:19:14 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3252 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:15 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:15 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:15 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:15 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3253, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:15 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:15 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3253, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:15 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:19:15 INFO DataWritingSparkTask: Committed partition 0 (task 63, attempt 0, stage 63.0)\n",
      "25/05/05 11:19:15 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 506590083 nanos, during time span of 507690584 nanos.\n",
      "25/05/05 11:19:15 INFO Executor: Finished task 0.0 in stage 63.0 (TID 63). 2188 bytes result sent to driver\n",
      "25/05/05 11:19:15 INFO TaskSetManager: Finished task 0.0 in stage 63.0 (TID 63) in 524 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:19:15 INFO TaskSchedulerImpl: Removed TaskSet 63.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:19:15 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:19:15 INFO DataWritingSparkTask: Committed partition 0 (task 62, attempt 0, stage 62.0)\n",
      "25/05/05 11:19:15 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 505116834 nanos, during time span of 514667417 nanos.\n",
      "25/05/05 11:19:15 INFO DAGScheduler: ResultStage 63 (start at NativeMethodAccessorImpl.java:0) finished in 0.533 s\n",
      "25/05/05 11:19:15 INFO DAGScheduler: Job 64 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:19:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 63: Stage finished\n",
      "25/05/05 11:19:15 INFO Executor: Finished task 0.0 in stage 62.0 (TID 62). 3559 bytes result sent to driver\n",
      "25/05/05 11:19:15 INFO DAGScheduler: Job 64 finished: start at NativeMethodAccessorImpl.java:0, took 0.540170 s\n",
      "25/05/05 11:19:15 INFO TaskSetManager: Finished task 0.0 in stage 62.0 (TID 62) in 537 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:19:15 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 21, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:19:15 INFO TaskSchedulerImpl: Removed TaskSet 62.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:19:15 INFO DAGScheduler: ResultStage 62 (start at NativeMethodAccessorImpl.java:0) finished in 0.542 s\n",
      "25/05/05 11:19:15 INFO DAGScheduler: Job 63 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:19:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 62: Stage finished\n",
      "25/05/05 11:19:15 INFO DAGScheduler: Job 63 finished: start at NativeMethodAccessorImpl.java:0, took 0.546890 s\n",
      "25/05/05 11:19:15 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 21, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@abec69b] is committing.\n",
      "25/05/05 11:19:15 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 21, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@abec69b] committed.\n",
      "25/05/05 11:19:15 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:15 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:15 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3253, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:15 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:19:15 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:19:15 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:19:15 INFO connection: Opened connection [connectionId{localValue:41, serverValue:4411}] to localhost:27017\n",
      "25/05/05 11:19:15 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=762291}\n",
      "25/05/05 11:19:15 INFO connection: Opened connection [connectionId{localValue:42, serverValue:4412}] to localhost:27017\n",
      "25/05/05 11:19:15 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 21, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:19:15 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:19:15 INFO connection: Closed connection [connectionId{localValue:42, serverValue:4412}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:19:15 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 509632292 nanos, during time span of 521329333 nanos.\n",
      "25/05/05 11:19:15 INFO Executor: Finished task 0.0 in stage 64.0 (TID 64). 1645 bytes result sent to driver\n",
      "25/05/05 11:19:15 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/21 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.21.57d937d0-0ff1-4c08-addb-e02ade31e1c9.tmp\n",
      "25/05/05 11:19:15 INFO TaskSetManager: Finished task 0.0 in stage 64.0 (TID 64) in 531 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:19:15 INFO TaskSchedulerImpl: Removed TaskSet 64.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:19:15 INFO DAGScheduler: ResultStage 64 (start at NativeMethodAccessorImpl.java:0) finished in 0.537 s\n",
      "25/05/05 11:19:15 INFO DAGScheduler: Job 65 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:19:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 64: Stage finished\n",
      "25/05/05 11:19:15 INFO DAGScheduler: Job 65 finished: start at NativeMethodAccessorImpl.java:0, took 0.537732 s\n",
      "25/05/05 11:19:15 INFO MemoryStore: Block broadcast_86 stored as values in memory (estimated size 248.0 B, free 366.2 MiB)\n",
      "25/05/05 11:19:15 INFO MemoryStore: Block broadcast_86_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.2 MiB)\n",
      "25/05/05 11:19:15 INFO BlockManagerInfo: Added broadcast_86_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:19:15 INFO SparkContext: Created broadcast 86 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:15 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/21 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.21.26373838-a77e-4f3e-b062-88be29915c66.tmp\n",
      "25/05/05 11:19:15 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/21 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.21.bcfdbd89-d363-4406-af82-6426504c334e.tmp\n",
      "25/05/05 11:19:15 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.21.57d937d0-0ff1-4c08-addb-e02ade31e1c9.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/21\n",
      "25/05/05 11:19:15 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:19:14.738Z\",\n",
      "  \"batchId\" : 21,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.506024096385542,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 568,\n",
      "    \"commitOffsets\" : 56,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 664,\n",
      "    \"walCommit\" : 32\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3252\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3253\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3253\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.506024096385542,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:19:15 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.21.26373838-a77e-4f3e-b062-88be29915c66.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/21\n",
      "25/05/05 11:19:15 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:19:14.738Z\",\n",
      "  \"batchId\" : 21,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.4947683109118086,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 588,\n",
      "    \"commitOffsets\" : 42,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 669,\n",
      "    \"walCommit\" : 32\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3252\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3253\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3253\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.4947683109118086,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:19:15 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.21.bcfdbd89-d363-4406-af82-6426504c334e.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/21\n",
      "25/05/05 11:19:15 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:19:14.738Z\",\n",
      "  \"batchId\" : 21,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.488095238095238,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 596,\n",
      "    \"commitOffsets\" : 37,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 6,\n",
      "    \"triggerExecution\" : 672,\n",
      "    \"walCommit\" : 31\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3252\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3253\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3253\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.488095238095238,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 21\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|B013|R081|02-00-00|161/YANKEE STAD|05/05/2025|11:19:13|REGULAR|11         |14        |2025-05-05 11:19:14.739|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:19:21 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/22 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.22.8641b4ee-abc0-4a59-97fa-f9bf236aa851.tmp\n",
      "25/05/05 11:19:21 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/22 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.22.99edc95c-6f97-4f96-bfd1-de9ed2900446.tmp\n",
      "25/05/05 11:19:21 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/22 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.22.6150f998-ff5e-44d2-9d8d-b1b4f14218d9.tmp\n",
      "25/05/05 11:19:21 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.22.99edc95c-6f97-4f96-bfd1-de9ed2900446.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/22\n",
      "25/05/05 11:19:21 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.22.6150f998-ff5e-44d2-9d8d-b1b4f14218d9.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/22\n",
      "25/05/05 11:19:21 INFO MicroBatchExecution: Committed offsets for batch 22. Metadata OffsetSeqMetadata(0,1746458361331,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:19:21 INFO MicroBatchExecution: Committed offsets for batch 22. Metadata OffsetSeqMetadata(0,1746458361331,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:19:21 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.22.8641b4ee-abc0-4a59-97fa-f9bf236aa851.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/22\n",
      "25/05/05 11:19:21 INFO MicroBatchExecution: Committed offsets for batch 22. Metadata OffsetSeqMetadata(0,1746458361332,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:19:21 INFO IncrementalExecution: Current batch timestamp = 1746458361331\n",
      "25/05/05 11:19:21 INFO IncrementalExecution: Current batch timestamp = 1746458361332\n",
      "25/05/05 11:19:21 INFO IncrementalExecution: Current batch timestamp = 1746458361331\n",
      "25/05/05 11:19:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:21 INFO IncrementalExecution: Current batch timestamp = 1746458361331\n",
      "25/05/05 11:19:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:21 INFO IncrementalExecution: Current batch timestamp = 1746458361332\n",
      "25/05/05 11:19:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:21 INFO IncrementalExecution: Current batch timestamp = 1746458361331\n",
      "25/05/05 11:19:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:21 INFO IncrementalExecution: Current batch timestamp = 1746458361332\n",
      "25/05/05 11:19:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:21 INFO IncrementalExecution: Current batch timestamp = 1746458361331\n",
      "25/05/05 11:19:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:21 INFO CodeGenerator: Code generated in 11.543958 ms\n",
      "25/05/05 11:19:21 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 22, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:19:21 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:21 INFO DAGScheduler: Got job 66 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:19:21 INFO DAGScheduler: Final stage: ResultStage 65 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:19:21 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:19:21 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:19:21 INFO DAGScheduler: Submitting ResultStage 65 (MapPartitionsRDD[360] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:19:21 INFO MemoryStore: Block broadcast_87 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:21 INFO MemoryStore: Block broadcast_87_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:21 INFO BlockManagerInfo: Added broadcast_87_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:21 INFO SparkContext: Created broadcast 87 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:19:21 INFO CodeGenerator: Code generated in 11.170958 ms\n",
      "25/05/05 11:19:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 65 (MapPartitionsRDD[360] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:19:21 INFO TaskSchedulerImpl: Adding task set 65.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:19:21 INFO TaskSetManager: Starting task 0.0 in stage 65.0 (TID 65) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:19:21 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 22, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@52c147cf]. The input RDD has 1 partitions.\n",
      "25/05/05 11:19:21 INFO Executor: Running task 0.0 in stage 65.0 (TID 65)\n",
      "25/05/05 11:19:21 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:21 INFO DAGScheduler: Got job 67 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:19:21 INFO DAGScheduler: Final stage: ResultStage 66 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:19:21 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:19:21 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:19:21 INFO DAGScheduler: Submitting ResultStage 66 (MapPartitionsRDD[363] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:19:21 INFO MemoryStore: Block broadcast_88 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:21 INFO MemoryStore: Block broadcast_88_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:21 INFO BlockManagerInfo: Added broadcast_88_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:21 INFO SparkContext: Created broadcast 88 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:19:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 66 (MapPartitionsRDD[363] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:19:21 INFO TaskSchedulerImpl: Adding task set 66.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:19:21 INFO TaskSetManager: Starting task 0.0 in stage 66.0 (TID 66) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:19:21 INFO Executor: Running task 0.0 in stage 66.0 (TID 66)\n",
      "25/05/05 11:19:21 INFO CodeGenerator: Code generated in 10.193875 ms\n",
      "25/05/05 11:19:21 INFO CodeGenerator: Code generated in 8.056417 ms\n",
      "25/05/05 11:19:21 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3253 untilOffset=3254, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=22 taskId=65 partitionId=0\n",
      "25/05/05 11:19:21 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3253 untilOffset=3254, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=22 taskId=66 partitionId=0\n",
      "25/05/05 11:19:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3253 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3253 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:21 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:21 INFO DAGScheduler: Got job 68 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:19:21 INFO DAGScheduler: Final stage: ResultStage 67 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:19:21 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:19:21 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:19:21 INFO BlockManagerInfo: Removed broadcast_85_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:21 INFO DAGScheduler: Submitting ResultStage 67 (MapPartitionsRDD[368] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:19:21 INFO MemoryStore: Block broadcast_89 stored as values in memory (estimated size 47.1 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:21 INFO MemoryStore: Block broadcast_89_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:21 INFO BlockManagerInfo: Removed broadcast_86_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:19:21 INFO BlockManagerInfo: Added broadcast_89_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:21 INFO SparkContext: Created broadcast 89 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:19:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 67 (MapPartitionsRDD[368] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:19:21 INFO TaskSchedulerImpl: Adding task set 67.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:19:21 INFO TaskSetManager: Starting task 0.0 in stage 67.0 (TID 67) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:19:21 INFO Executor: Running task 0.0 in stage 67.0 (TID 67)\n",
      "25/05/05 11:19:21 INFO BlockManagerInfo: Removed broadcast_84_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:21 INFO BlockManagerInfo: Removed broadcast_83_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:21 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3253 untilOffset=3254, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=22 taskId=67 partitionId=0\n",
      "25/05/05 11:19:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3253 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3254, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3254, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:21 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:19:21 INFO DataWritingSparkTask: Committed partition 0 (task 65, attempt 0, stage 65.0)\n",
      "25/05/05 11:19:21 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 508742542 nanos, during time span of 509365459 nanos.\n",
      "25/05/05 11:19:21 INFO Executor: Finished task 0.0 in stage 65.0 (TID 65). 2188 bytes result sent to driver\n",
      "25/05/05 11:19:21 INFO TaskSetManager: Finished task 0.0 in stage 65.0 (TID 65) in 533 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:19:21 INFO TaskSchedulerImpl: Removed TaskSet 65.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:19:21 INFO DAGScheduler: ResultStage 65 (start at NativeMethodAccessorImpl.java:0) finished in 0.540 s\n",
      "25/05/05 11:19:21 INFO DAGScheduler: Job 66 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:19:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 65: Stage finished\n",
      "25/05/05 11:19:21 INFO DAGScheduler: Job 66 finished: start at NativeMethodAccessorImpl.java:0, took 0.541883 s\n",
      "25/05/05 11:19:21 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 22, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:19:21 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:19:21 INFO DataWritingSparkTask: Committed partition 0 (task 66, attempt 0, stage 66.0)\n",
      "25/05/05 11:19:21 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 509465250 nanos, during time span of 511515042 nanos.\n",
      "25/05/05 11:19:21 INFO Executor: Finished task 0.0 in stage 66.0 (TID 66). 3600 bytes result sent to driver\n",
      "25/05/05 11:19:21 INFO TaskSetManager: Finished task 0.0 in stage 66.0 (TID 66) in 531 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:19:21 INFO TaskSchedulerImpl: Removed TaskSet 66.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:19:21 INFO DAGScheduler: ResultStage 66 (start at NativeMethodAccessorImpl.java:0) finished in 0.533 s\n",
      "25/05/05 11:19:21 INFO DAGScheduler: Job 67 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:19:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 66: Stage finished\n",
      "25/05/05 11:19:21 INFO DAGScheduler: Job 67 finished: start at NativeMethodAccessorImpl.java:0, took 0.535309 s\n",
      "25/05/05 11:19:21 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 22, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@52c147cf] is committing.\n",
      "25/05/05 11:19:21 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 22, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@52c147cf] committed.\n",
      "25/05/05 11:19:21 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/22 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.22.123dea9a-149d-410f-9389-91061613eac8.tmp\n",
      "25/05/05 11:19:21 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 22, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:19:21 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/22 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.22.8366f087-d51e-45a3-a40f-05f39eab2cd4.tmp\n",
      "25/05/05 11:19:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3254, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:21 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:19:21 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:19:21 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:19:21 INFO connection: Opened connection [connectionId{localValue:43, serverValue:4413}] to localhost:27017\n",
      "25/05/05 11:19:21 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=321084}\n",
      "25/05/05 11:19:21 INFO connection: Opened connection [connectionId{localValue:44, serverValue:4414}] to localhost:27017\n",
      "25/05/05 11:19:21 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:19:21 INFO connection: Closed connection [connectionId{localValue:44, serverValue:4414}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:19:21 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 509522916 nanos, during time span of 517106125 nanos.\n",
      "25/05/05 11:19:21 INFO Executor: Finished task 0.0 in stage 67.0 (TID 67). 1645 bytes result sent to driver\n",
      "25/05/05 11:19:21 INFO TaskSetManager: Finished task 0.0 in stage 67.0 (TID 67) in 529 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:19:21 INFO TaskSchedulerImpl: Removed TaskSet 67.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:19:21 INFO DAGScheduler: ResultStage 67 (start at NativeMethodAccessorImpl.java:0) finished in 0.536 s\n",
      "25/05/05 11:19:21 INFO DAGScheduler: Job 68 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:19:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 67: Stage finished\n",
      "25/05/05 11:19:21 INFO DAGScheduler: Job 68 finished: start at NativeMethodAccessorImpl.java:0, took 0.538041 s\n",
      "25/05/05 11:19:21 INFO MemoryStore: Block broadcast_90 stored as values in memory (estimated size 248.0 B, free 366.2 MiB)\n",
      "25/05/05 11:19:21 INFO MemoryStore: Block broadcast_90_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.2 MiB)\n",
      "25/05/05 11:19:21 INFO BlockManagerInfo: Added broadcast_90_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:19:21 INFO SparkContext: Created broadcast 90 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:21 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/22 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.22.8393abe0-fdec-4859-8b37-9acb05d76a0e.tmp\n",
      "25/05/05 11:19:21 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.22.123dea9a-149d-410f-9389-91061613eac8.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/22\n",
      "25/05/05 11:19:21 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:19:21.330Z\",\n",
      "  \"batchId\" : 22,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.4992503748125936,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 568,\n",
      "    \"commitOffsets\" : 52,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 12,\n",
      "    \"triggerExecution\" : 667,\n",
      "    \"walCommit\" : 33\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3253\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3254\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3254\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.4992503748125936,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:19:22 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.22.8366f087-d51e-45a3-a40f-05f39eab2cd4.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/22\n",
      "25/05/05 11:19:22 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:19:21.330Z\",\n",
      "  \"batchId\" : 22,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.488095238095238,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 581,\n",
      "    \"commitOffsets\" : 44,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 13,\n",
      "    \"triggerExecution\" : 672,\n",
      "    \"walCommit\" : 33\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3253\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3254\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3254\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.488095238095238,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:19:22 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.22.8393abe0-fdec-4859-8b37-9acb05d76a0e.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/22\n",
      "25/05/05 11:19:22 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:19:21.330Z\",\n",
      "  \"batchId\" : 22,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.443001443001443,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 618,\n",
      "    \"commitOffsets\" : 35,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 6,\n",
      "    \"triggerExecution\" : 693,\n",
      "    \"walCommit\" : 33\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3253\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3254\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3254\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.443001443001443,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 22\n",
      "-------------------------------------------\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION      |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R016|R032|00-00-01|TIME SQ-42 ST|05/05/2025|11:19:20|REGULAR|10         |13        |2025-05-05 11:19:21.331|\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:19:26 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/23 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.23.9c1722d5-9bf9-4f94-90e0-be39c144b560.tmp\n",
      "25/05/05 11:19:26 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/23 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.23.804ef8c6-97ac-42f4-bd75-5b6b3ab53d08.tmp\n",
      "25/05/05 11:19:26 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/23 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.23.d22a1847-96e5-4075-81cd-299e45972a45.tmp\n",
      "25/05/05 11:19:26 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.23.d22a1847-96e5-4075-81cd-299e45972a45.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/23\n",
      "25/05/05 11:19:26 INFO MicroBatchExecution: Committed offsets for batch 23. Metadata OffsetSeqMetadata(0,1746458366937,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:19:26 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.23.9c1722d5-9bf9-4f94-90e0-be39c144b560.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/23\n",
      "25/05/05 11:19:26 INFO MicroBatchExecution: Committed offsets for batch 23. Metadata OffsetSeqMetadata(0,1746458366939,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:19:26 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.23.804ef8c6-97ac-42f4-bd75-5b6b3ab53d08.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/23\n",
      "25/05/05 11:19:26 INFO MicroBatchExecution: Committed offsets for batch 23. Metadata OffsetSeqMetadata(0,1746458366939,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:19:26 INFO IncrementalExecution: Current batch timestamp = 1746458366937\n",
      "25/05/05 11:19:26 INFO IncrementalExecution: Current batch timestamp = 1746458366939\n",
      "25/05/05 11:19:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:26 INFO IncrementalExecution: Current batch timestamp = 1746458366939\n",
      "25/05/05 11:19:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:26 INFO IncrementalExecution: Current batch timestamp = 1746458366939\n",
      "25/05/05 11:19:26 INFO IncrementalExecution: Current batch timestamp = 1746458366937\n",
      "25/05/05 11:19:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:26 INFO IncrementalExecution: Current batch timestamp = 1746458366939\n",
      "25/05/05 11:19:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:26 INFO IncrementalExecution: Current batch timestamp = 1746458366939\n",
      "25/05/05 11:19:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:26 INFO IncrementalExecution: Current batch timestamp = 1746458366939\n",
      "25/05/05 11:19:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:27 INFO CodeGenerator: Code generated in 7.101667 ms\n",
      "25/05/05 11:19:27 INFO CodeGenerator: Code generated in 10.803084 ms\n",
      "25/05/05 11:19:27 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 23, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@62070938]. The input RDD has 1 partitions.\n",
      "25/05/05 11:19:27 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 23, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:19:27 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:27 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:27 INFO DAGScheduler: Got job 69 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:19:27 INFO DAGScheduler: Final stage: ResultStage 68 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:19:27 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:19:27 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:19:27 INFO DAGScheduler: Submitting ResultStage 68 (MapPartitionsRDD[378] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:19:27 INFO MemoryStore: Block broadcast_91 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:27 INFO MemoryStore: Block broadcast_91_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:27 INFO BlockManagerInfo: Added broadcast_91_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:27 INFO SparkContext: Created broadcast 91 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:19:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 68 (MapPartitionsRDD[378] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:19:27 INFO TaskSchedulerImpl: Adding task set 68.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:19:27 INFO DAGScheduler: Got job 70 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:19:27 INFO DAGScheduler: Final stage: ResultStage 69 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:19:27 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:19:27 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:19:27 INFO TaskSetManager: Starting task 0.0 in stage 68.0 (TID 68) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:19:27 INFO DAGScheduler: Submitting ResultStage 69 (MapPartitionsRDD[379] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:19:27 INFO Executor: Running task 0.0 in stage 68.0 (TID 68)\n",
      "25/05/05 11:19:27 INFO MemoryStore: Block broadcast_92 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:27 INFO MemoryStore: Block broadcast_92_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:27 INFO BlockManagerInfo: Added broadcast_92_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:27 INFO SparkContext: Created broadcast 92 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:19:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 69 (MapPartitionsRDD[379] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:19:27 INFO TaskSchedulerImpl: Adding task set 69.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:19:27 INFO TaskSetManager: Starting task 0.0 in stage 69.0 (TID 69) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:19:27 INFO Executor: Running task 0.0 in stage 69.0 (TID 69)\n",
      "25/05/05 11:19:27 INFO CodeGenerator: Code generated in 6.152291 ms\n",
      "25/05/05 11:19:27 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3254 untilOffset=3255, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=23 taskId=69 partitionId=0\n",
      "25/05/05 11:19:27 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3254 untilOffset=3255, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=23 taskId=68 partitionId=0\n",
      "25/05/05 11:19:27 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3254 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:27 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3254 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:27 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:27 INFO DAGScheduler: Got job 71 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:19:27 INFO DAGScheduler: Final stage: ResultStage 70 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:19:27 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:19:27 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:19:27 INFO DAGScheduler: Submitting ResultStage 70 (MapPartitionsRDD[384] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:19:27 INFO MemoryStore: Block broadcast_93 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:19:27 INFO MemoryStore: Block broadcast_93_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.1 MiB)\n",
      "25/05/05 11:19:27 INFO BlockManagerInfo: Added broadcast_93_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:19:27 INFO SparkContext: Created broadcast 93 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:19:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 70 (MapPartitionsRDD[384] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:19:27 INFO TaskSchedulerImpl: Adding task set 70.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:19:27 INFO TaskSetManager: Starting task 0.0 in stage 70.0 (TID 70) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:19:27 INFO Executor: Running task 0.0 in stage 70.0 (TID 70)\n",
      "25/05/05 11:19:27 INFO CodeGenerator: Code generated in 5.646125 ms\n",
      "25/05/05 11:19:27 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3254 untilOffset=3255, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=23 taskId=70 partitionId=0\n",
      "25/05/05 11:19:27 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3254 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3255, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3255, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:27 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:19:27 INFO DataWritingSparkTask: Committed partition 0 (task 69, attempt 0, stage 69.0)\n",
      "25/05/05 11:19:27 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 501932333 nanos, during time span of 502336083 nanos.\n",
      "25/05/05 11:19:27 INFO Executor: Finished task 0.0 in stage 69.0 (TID 69). 2188 bytes result sent to driver\n",
      "25/05/05 11:19:27 INFO TaskSetManager: Finished task 0.0 in stage 69.0 (TID 69) in 515 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:19:27 INFO TaskSchedulerImpl: Removed TaskSet 69.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:19:27 INFO DAGScheduler: ResultStage 69 (start at NativeMethodAccessorImpl.java:0) finished in 0.517 s\n",
      "25/05/05 11:19:27 INFO DAGScheduler: Job 70 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:19:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 69: Stage finished\n",
      "25/05/05 11:19:27 INFO DAGScheduler: Job 70 finished: start at NativeMethodAccessorImpl.java:0, took 0.521197 s\n",
      "25/05/05 11:19:27 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 23, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:19:27 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:19:27 INFO DataWritingSparkTask: Committed partition 0 (task 68, attempt 0, stage 68.0)\n",
      "25/05/05 11:19:27 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 501890375 nanos, during time span of 503847708 nanos.\n",
      "25/05/05 11:19:27 INFO Executor: Finished task 0.0 in stage 68.0 (TID 68). 3557 bytes result sent to driver\n",
      "25/05/05 11:19:27 INFO TaskSetManager: Finished task 0.0 in stage 68.0 (TID 68) in 520 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:19:27 INFO TaskSchedulerImpl: Removed TaskSet 68.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:19:27 INFO DAGScheduler: ResultStage 68 (start at NativeMethodAccessorImpl.java:0) finished in 0.522 s\n",
      "25/05/05 11:19:27 INFO DAGScheduler: Job 69 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:19:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 68: Stage finished\n",
      "25/05/05 11:19:27 INFO DAGScheduler: Job 69 finished: start at NativeMethodAccessorImpl.java:0, took 0.523021 s\n",
      "25/05/05 11:19:27 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 23, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@62070938] is committing.\n",
      "25/05/05 11:19:27 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 23, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@62070938] committed.\n",
      "25/05/05 11:19:27 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 23, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:19:27 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/23 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.23.d0640305-73be-4f97-b00d-6d7596706133.tmp\n",
      "25/05/05 11:19:27 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/23 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.23.84a616a0-176e-423d-8b45-45b1c623c945.tmp\n",
      "25/05/05 11:19:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3255, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:27 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:19:27 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:19:27 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:19:27 INFO connection: Opened connection [connectionId{localValue:45, serverValue:4415}] to localhost:27017\n",
      "25/05/05 11:19:27 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=280875}\n",
      "25/05/05 11:19:27 INFO connection: Opened connection [connectionId{localValue:46, serverValue:4416}] to localhost:27017\n",
      "25/05/05 11:19:27 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:19:27 INFO connection: Closed connection [connectionId{localValue:46, serverValue:4416}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:19:27 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 502572709 nanos, during time span of 508558542 nanos.\n",
      "25/05/05 11:19:27 INFO Executor: Finished task 0.0 in stage 70.0 (TID 70). 1645 bytes result sent to driver\n",
      "25/05/05 11:19:27 INFO TaskSetManager: Finished task 0.0 in stage 70.0 (TID 70) in 522 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:19:27 INFO TaskSchedulerImpl: Removed TaskSet 70.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:19:27 INFO DAGScheduler: ResultStage 70 (start at NativeMethodAccessorImpl.java:0) finished in 0.527 s\n",
      "25/05/05 11:19:27 INFO DAGScheduler: Job 71 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:19:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 70: Stage finished\n",
      "25/05/05 11:19:27 INFO DAGScheduler: Job 71 finished: start at NativeMethodAccessorImpl.java:0, took 0.527949 s\n",
      "25/05/05 11:19:27 INFO MemoryStore: Block broadcast_94 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:19:27 INFO MemoryStore: Block broadcast_94_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:19:27 INFO BlockManagerInfo: Added broadcast_94_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:19:27 INFO SparkContext: Created broadcast 94 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:27 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.23.d0640305-73be-4f97-b00d-6d7596706133.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/23\n",
      "25/05/05 11:19:27 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:19:26.931Z\",\n",
      "  \"batchId\" : 23,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "  \"processedRowsPerSecond\" : 1.567398119122257,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 554,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 8,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 638,\n",
      "    \"walCommit\" : 42\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3254\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3255\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3255\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "    \"processedRowsPerSecond\" : 1.567398119122257,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:19:27 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/23 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.23.b5c9fa3e-3a33-4ee3-b7f3-653ad9125494.tmp\n",
      "25/05/05 11:19:27 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.23.84a616a0-176e-423d-8b45-45b1c623c945.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/23\n",
      "25/05/05 11:19:27 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:19:26.932Z\",\n",
      "  \"batchId\" : 23,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 55.55555555555556,\n",
      "  \"processedRowsPerSecond\" : 1.5503875968992247,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 557,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 7,\n",
      "    \"queryPlanning\" : 7,\n",
      "    \"triggerExecution\" : 645,\n",
      "    \"walCommit\" : 44\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3254\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3255\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3255\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 55.55555555555556,\n",
      "    \"processedRowsPerSecond\" : 1.5503875968992247,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:19:27 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.23.b5c9fa3e-3a33-4ee3-b7f3-653ad9125494.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/23\n",
      "25/05/05 11:19:27 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:19:26.931Z\",\n",
      "  \"batchId\" : 23,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 62.5,\n",
      "  \"processedRowsPerSecond\" : 1.5082956259426847,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 582,\n",
      "    \"commitOffsets\" : 26,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 6,\n",
      "    \"queryPlanning\" : 6,\n",
      "    \"triggerExecution\" : 663,\n",
      "    \"walCommit\" : 43\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3254\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3255\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3255\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 62.5,\n",
      "    \"processedRowsPerSecond\" : 1.5082956259426847,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 23\n",
      "-------------------------------------------\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION      |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R016|R032|00-00-01|TIME SQ-42 ST|05/05/2025|11:19:25|REGULAR|5          |4         |2025-05-05 11:19:26.939|\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:19:32 INFO BlockManagerInfo: Removed broadcast_94_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:19:32 INFO BlockManagerInfo: Removed broadcast_89_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:32 INFO BlockManagerInfo: Removed broadcast_90_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:19:32 INFO BlockManagerInfo: Removed broadcast_93_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:32 INFO BlockManagerInfo: Removed broadcast_92_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:32 INFO BlockManagerInfo: Removed broadcast_91_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:32 INFO BlockManagerInfo: Removed broadcast_88_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:32 INFO BlockManagerInfo: Removed broadcast_87_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:33 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/24 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.24.d46a98a2-9203-4cef-983d-15dfb9d3bada.tmp\n",
      "25/05/05 11:19:33 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/24 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.24.fe5f9a59-5bf9-4324-9703-b78e8add45f4.tmp\n",
      "25/05/05 11:19:33 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/24 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.24.69edea49-20f1-4ebc-81d9-031199ab1b6c.tmp\n",
      "25/05/05 11:19:33 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.24.69edea49-20f1-4ebc-81d9-031199ab1b6c.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/24\n",
      "25/05/05 11:19:33 INFO MicroBatchExecution: Committed offsets for batch 24. Metadata OffsetSeqMetadata(0,1746458373608,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:19:33 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.24.fe5f9a59-5bf9-4324-9703-b78e8add45f4.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/24\n",
      "25/05/05 11:19:33 INFO MicroBatchExecution: Committed offsets for batch 24. Metadata OffsetSeqMetadata(0,1746458373607,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:19:33 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.24.d46a98a2-9203-4cef-983d-15dfb9d3bada.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/24\n",
      "25/05/05 11:19:33 INFO MicroBatchExecution: Committed offsets for batch 24. Metadata OffsetSeqMetadata(0,1746458373605,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:19:33 INFO IncrementalExecution: Current batch timestamp = 1746458373607\n",
      "25/05/05 11:19:33 INFO IncrementalExecution: Current batch timestamp = 1746458373608\n",
      "25/05/05 11:19:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:33 INFO IncrementalExecution: Current batch timestamp = 1746458373605\n",
      "25/05/05 11:19:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:33 INFO IncrementalExecution: Current batch timestamp = 1746458373607\n",
      "25/05/05 11:19:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:33 INFO IncrementalExecution: Current batch timestamp = 1746458373608\n",
      "25/05/05 11:19:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:33 INFO IncrementalExecution: Current batch timestamp = 1746458373608\n",
      "25/05/05 11:19:33 INFO IncrementalExecution: Current batch timestamp = 1746458373605\n",
      "25/05/05 11:19:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:33 INFO IncrementalExecution: Current batch timestamp = 1746458373605\n",
      "25/05/05 11:19:33 INFO CodeGenerator: Code generated in 5.45525 ms\n",
      "25/05/05 11:19:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:33 INFO CodeGenerator: Code generated in 4.562791 ms\n",
      "25/05/05 11:19:33 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 24, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:19:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:33 INFO DAGScheduler: Got job 72 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:19:33 INFO DAGScheduler: Final stage: ResultStage 71 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:19:33 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:19:33 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:19:33 INFO DAGScheduler: Submitting ResultStage 71 (MapPartitionsRDD[392] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:19:33 INFO MemoryStore: Block broadcast_95 stored as values in memory (estimated size 15.2 KiB, free 366.3 MiB)\n",
      "25/05/05 11:19:33 INFO MemoryStore: Block broadcast_95_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.3 MiB)\n",
      "25/05/05 11:19:33 INFO BlockManagerInfo: Added broadcast_95_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:33 INFO SparkContext: Created broadcast 95 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:19:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 71 (MapPartitionsRDD[392] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:19:33 INFO TaskSchedulerImpl: Adding task set 71.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:19:33 INFO TaskSetManager: Starting task 0.0 in stage 71.0 (TID 71) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:19:33 INFO Executor: Running task 0.0 in stage 71.0 (TID 71)\n",
      "25/05/05 11:19:33 INFO CodeGenerator: Code generated in 8.546375 ms\n",
      "25/05/05 11:19:33 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 24, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@7f8cdbfa]. The input RDD has 1 partitions.\n",
      "25/05/05 11:19:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:33 INFO DAGScheduler: Got job 73 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:19:33 INFO DAGScheduler: Final stage: ResultStage 72 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:19:33 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:19:33 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:19:33 INFO DAGScheduler: Submitting ResultStage 72 (MapPartitionsRDD[395] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:19:33 INFO MemoryStore: Block broadcast_96 stored as values in memory (estimated size 16.2 KiB, free 366.3 MiB)\n",
      "25/05/05 11:19:33 INFO MemoryStore: Block broadcast_96_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.3 MiB)\n",
      "25/05/05 11:19:33 INFO BlockManagerInfo: Added broadcast_96_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:33 INFO SparkContext: Created broadcast 96 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:19:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 72 (MapPartitionsRDD[395] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:19:33 INFO TaskSchedulerImpl: Adding task set 72.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:19:33 INFO CodeGenerator: Code generated in 5.723375 ms\n",
      "25/05/05 11:19:33 INFO TaskSetManager: Starting task 0.0 in stage 72.0 (TID 72) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:19:33 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3255 untilOffset=3256, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=24 taskId=71 partitionId=0\n",
      "25/05/05 11:19:33 INFO Executor: Running task 0.0 in stage 72.0 (TID 72)\n",
      "25/05/05 11:19:33 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3255 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:33 INFO CodeGenerator: Code generated in 9.587583 ms\n",
      "25/05/05 11:19:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:33 INFO DAGScheduler: Got job 74 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:19:33 INFO DAGScheduler: Final stage: ResultStage 73 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:19:33 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:19:33 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:19:33 INFO DAGScheduler: Submitting ResultStage 73 (MapPartitionsRDD[400] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:19:33 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3255 untilOffset=3256, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=24 taskId=72 partitionId=0\n",
      "25/05/05 11:19:33 INFO MemoryStore: Block broadcast_97 stored as values in memory (estimated size 47.1 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:33 INFO MemoryStore: Block broadcast_97_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:33 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3255 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:33 INFO BlockManagerInfo: Added broadcast_97_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:33 INFO SparkContext: Created broadcast 97 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:19:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 73 (MapPartitionsRDD[400] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:19:33 INFO TaskSchedulerImpl: Adding task set 73.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:19:33 INFO TaskSetManager: Starting task 0.0 in stage 73.0 (TID 73) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:19:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:33 INFO Executor: Running task 0.0 in stage 73.0 (TID 73)\n",
      "25/05/05 11:19:33 INFO CodeGenerator: Code generated in 6.746042 ms\n",
      "25/05/05 11:19:33 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3255 untilOffset=3256, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=24 taskId=73 partitionId=0\n",
      "25/05/05 11:19:33 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3255 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3256, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:34 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:19:34 INFO DataWritingSparkTask: Committed partition 0 (task 71, attempt 0, stage 71.0)\n",
      "25/05/05 11:19:34 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 502585875 nanos, during time span of 503065292 nanos.\n",
      "25/05/05 11:19:34 INFO Executor: Finished task 0.0 in stage 71.0 (TID 71). 2188 bytes result sent to driver\n",
      "25/05/05 11:19:34 INFO TaskSetManager: Finished task 0.0 in stage 71.0 (TID 71) in 517 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:19:34 INFO TaskSchedulerImpl: Removed TaskSet 71.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:19:34 INFO DAGScheduler: ResultStage 71 (start at NativeMethodAccessorImpl.java:0) finished in 0.520 s\n",
      "25/05/05 11:19:34 INFO DAGScheduler: Job 72 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:19:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 71: Stage finished\n",
      "25/05/05 11:19:34 INFO DAGScheduler: Job 72 finished: start at NativeMethodAccessorImpl.java:0, took 0.521490 s\n",
      "25/05/05 11:19:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 24, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:19:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 24, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:19:34 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/24 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.24.8765de5e-f754-46fe-8316-08d53fb482c6.tmp\n",
      "25/05/05 11:19:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3256, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:34 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:19:34 INFO DataWritingSparkTask: Committed partition 0 (task 72, attempt 0, stage 72.0)\n",
      "25/05/05 11:19:34 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 503888583 nanos, during time span of 506033291 nanos.\n",
      "25/05/05 11:19:34 INFO Executor: Finished task 0.0 in stage 72.0 (TID 72). 3559 bytes result sent to driver\n",
      "25/05/05 11:19:34 INFO TaskSetManager: Finished task 0.0 in stage 72.0 (TID 72) in 530 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:19:34 INFO TaskSchedulerImpl: Removed TaskSet 72.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:19:34 INFO DAGScheduler: ResultStage 72 (start at NativeMethodAccessorImpl.java:0) finished in 0.534 s\n",
      "25/05/05 11:19:34 INFO DAGScheduler: Job 73 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:19:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 72: Stage finished\n",
      "25/05/05 11:19:34 INFO DAGScheduler: Job 73 finished: start at NativeMethodAccessorImpl.java:0, took 0.534273 s\n",
      "25/05/05 11:19:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 24, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@7f8cdbfa] is committing.\n",
      "25/05/05 11:19:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 24, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@7f8cdbfa] committed.\n",
      "25/05/05 11:19:34 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/24 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.24.1ff04a7a-042f-42ff-bf7c-c50b0a1eb037.tmp\n",
      "25/05/05 11:19:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3256, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:34 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:19:34 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:19:34 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:19:34 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.24.8765de5e-f754-46fe-8316-08d53fb482c6.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/24\n",
      "25/05/05 11:19:34 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:19:33.606Z\",\n",
      "  \"batchId\" : 24,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.6233766233766234,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 549,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 6,\n",
      "    \"triggerExecution\" : 616,\n",
      "    \"walCommit\" : 30\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3255\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3256\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3256\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.6233766233766234,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:19:34 INFO connection: Opened connection [connectionId{localValue:47, serverValue:4417}] to localhost:27017\n",
      "25/05/05 11:19:34 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=430500}\n",
      "25/05/05 11:19:34 INFO connection: Opened connection [connectionId{localValue:48, serverValue:4418}] to localhost:27017\n",
      "25/05/05 11:19:34 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:19:34 INFO connection: Closed connection [connectionId{localValue:48, serverValue:4418}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:19:34 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 506597292 nanos, during time span of 513278334 nanos.\n",
      "25/05/05 11:19:34 INFO Executor: Finished task 0.0 in stage 73.0 (TID 73). 1645 bytes result sent to driver\n",
      "25/05/05 11:19:34 INFO TaskSetManager: Finished task 0.0 in stage 73.0 (TID 73) in 528 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:19:34 INFO TaskSchedulerImpl: Removed TaskSet 73.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:19:34 INFO DAGScheduler: ResultStage 73 (start at NativeMethodAccessorImpl.java:0) finished in 0.539 s\n",
      "25/05/05 11:19:34 INFO DAGScheduler: Job 74 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:19:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 73: Stage finished\n",
      "25/05/05 11:19:34 INFO DAGScheduler: Job 74 finished: start at NativeMethodAccessorImpl.java:0, took 0.541068 s\n",
      "25/05/05 11:19:34 INFO MemoryStore: Block broadcast_98 stored as values in memory (estimated size 248.0 B, free 366.2 MiB)\n",
      "25/05/05 11:19:34 INFO MemoryStore: Block broadcast_98_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.2 MiB)\n",
      "25/05/05 11:19:34 INFO BlockManagerInfo: Added broadcast_98_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:19:34 INFO SparkContext: Created broadcast 98 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:34 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/24 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.24.8f353a82-d393-43a4-97b5-87dbb59a84c4.tmp\n",
      "25/05/05 11:19:34 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.24.1ff04a7a-042f-42ff-bf7c-c50b0a1eb037.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/24\n",
      "25/05/05 11:19:34 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:19:33.603Z\",\n",
      "  \"batchId\" : 24,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.5527950310559007,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 559,\n",
      "    \"commitOffsets\" : 40,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 7,\n",
      "    \"triggerExecution\" : 644,\n",
      "    \"walCommit\" : 36\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3255\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3256\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3256\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.5527950310559007,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:19:34 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.24.8f353a82-d393-43a4-97b5-87dbb59a84c4.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/24\n",
      "25/05/05 11:19:34 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:19:33.606Z\",\n",
      "  \"batchId\" : 24,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.5151515151515151,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 590,\n",
      "    \"commitOffsets\" : 32,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 660,\n",
      "    \"walCommit\" : 31\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3255\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3256\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3256\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.5151515151515151,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 24\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R023|R045|00-00-00|34 ST-HERALD SQ|05/05/2025|11:19:32|REGULAR|8          |14        |2025-05-05 11:19:33.608|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:19:39 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/25 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.25.47bbdba3-d5ec-4f0e-816c-1b2435399a83.tmp\n",
      "25/05/05 11:19:39 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/25 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.25.cfd1ae74-c634-485c-9bf6-99c26329664b.tmp\n",
      "25/05/05 11:19:39 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/25 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.25.2aab6d3b-a326-44c6-965a-280a2fca6ded.tmp\n",
      "25/05/05 11:19:39 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.25.47bbdba3-d5ec-4f0e-816c-1b2435399a83.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/25\n",
      "25/05/05 11:19:39 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.25.cfd1ae74-c634-485c-9bf6-99c26329664b.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/25\n",
      "25/05/05 11:19:39 INFO MicroBatchExecution: Committed offsets for batch 25. Metadata OffsetSeqMetadata(0,1746458379301,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:19:39 INFO MicroBatchExecution: Committed offsets for batch 25. Metadata OffsetSeqMetadata(0,1746458379300,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:19:39 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.25.2aab6d3b-a326-44c6-965a-280a2fca6ded.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/25\n",
      "25/05/05 11:19:39 INFO MicroBatchExecution: Committed offsets for batch 25. Metadata OffsetSeqMetadata(0,1746458379300,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:19:39 INFO IncrementalExecution: Current batch timestamp = 1746458379301\n",
      "25/05/05 11:19:39 INFO IncrementalExecution: Current batch timestamp = 1746458379300\n",
      "25/05/05 11:19:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:39 INFO IncrementalExecution: Current batch timestamp = 1746458379300\n",
      "25/05/05 11:19:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:39 INFO IncrementalExecution: Current batch timestamp = 1746458379300\n",
      "25/05/05 11:19:39 INFO IncrementalExecution: Current batch timestamp = 1746458379301\n",
      "25/05/05 11:19:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:39 INFO IncrementalExecution: Current batch timestamp = 1746458379300\n",
      "25/05/05 11:19:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:39 INFO IncrementalExecution: Current batch timestamp = 1746458379300\n",
      "25/05/05 11:19:39 INFO IncrementalExecution: Current batch timestamp = 1746458379301\n",
      "25/05/05 11:19:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:39 INFO CodeGenerator: Code generated in 8.16725 ms\n",
      "25/05/05 11:19:39 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 25, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:19:39 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:39 INFO DAGScheduler: Got job 75 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:19:39 INFO DAGScheduler: Final stage: ResultStage 74 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:19:39 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:19:39 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:19:39 INFO DAGScheduler: Submitting ResultStage 74 (MapPartitionsRDD[408] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:19:39 INFO CodeGenerator: Code generated in 4.883958 ms\n",
      "25/05/05 11:19:39 INFO MemoryStore: Block broadcast_99 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:39 INFO MemoryStore: Block broadcast_99_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:39 INFO BlockManagerInfo: Added broadcast_99_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:39 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 25, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@18407d95]. The input RDD has 1 partitions.\n",
      "25/05/05 11:19:39 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:39 INFO SparkContext: Created broadcast 99 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:19:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 74 (MapPartitionsRDD[408] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:19:39 INFO TaskSchedulerImpl: Adding task set 74.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:19:39 INFO DAGScheduler: Got job 76 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:19:39 INFO DAGScheduler: Final stage: ResultStage 75 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:19:39 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:19:39 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:19:39 INFO DAGScheduler: Submitting ResultStage 75 (MapPartitionsRDD[411] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:19:39 INFO MemoryStore: Block broadcast_100 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:39 INFO TaskSetManager: Starting task 0.0 in stage 74.0 (TID 74) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:19:39 INFO MemoryStore: Block broadcast_100_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:39 INFO BlockManagerInfo: Added broadcast_100_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:39 INFO Executor: Running task 0.0 in stage 74.0 (TID 74)\n",
      "25/05/05 11:19:39 INFO SparkContext: Created broadcast 100 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:19:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 75 (MapPartitionsRDD[411] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:19:39 INFO TaskSchedulerImpl: Adding task set 75.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:19:39 INFO TaskSetManager: Starting task 0.0 in stage 75.0 (TID 75) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:19:39 INFO Executor: Running task 0.0 in stage 75.0 (TID 75)\n",
      "25/05/05 11:19:39 INFO CodeGenerator: Code generated in 6.756 ms\n",
      "25/05/05 11:19:39 INFO CodeGenerator: Code generated in 4.667708 ms\n",
      "25/05/05 11:19:39 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3256 untilOffset=3257, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=25 taskId=74 partitionId=0\n",
      "25/05/05 11:19:39 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3256 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:39 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3256 untilOffset=3257, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=25 taskId=75 partitionId=0\n",
      "25/05/05 11:19:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:39 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3256 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:39 INFO BlockManagerInfo: Removed broadcast_96_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:39 INFO BlockManagerInfo: Removed broadcast_98_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:19:39 INFO BlockManagerInfo: Removed broadcast_95_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:39 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:39 INFO DAGScheduler: Got job 77 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:19:39 INFO DAGScheduler: Final stage: ResultStage 76 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:19:39 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:19:39 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:19:39 INFO DAGScheduler: Submitting ResultStage 76 (MapPartitionsRDD[416] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:19:39 INFO BlockManagerInfo: Removed broadcast_97_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:39 INFO MemoryStore: Block broadcast_101 stored as values in memory (estimated size 47.1 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:39 INFO MemoryStore: Block broadcast_101_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:39 INFO BlockManagerInfo: Added broadcast_101_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:39 INFO SparkContext: Created broadcast 101 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:19:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 76 (MapPartitionsRDD[416] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:19:39 INFO TaskSchedulerImpl: Adding task set 76.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:19:39 INFO TaskSetManager: Starting task 0.0 in stage 76.0 (TID 76) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:19:39 INFO Executor: Running task 0.0 in stage 76.0 (TID 76)\n",
      "25/05/05 11:19:39 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3256 untilOffset=3257, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=25 taskId=76 partitionId=0\n",
      "25/05/05 11:19:39 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3256 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3257, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:39 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:19:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:39 INFO DataWritingSparkTask: Committed partition 0 (task 74, attempt 0, stage 74.0)\n",
      "25/05/05 11:19:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:39 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 504991584 nanos, during time span of 505435208 nanos.\n",
      "25/05/05 11:19:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3257, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:39 INFO Executor: Finished task 0.0 in stage 74.0 (TID 74). 2223 bytes result sent to driver\n",
      "25/05/05 11:19:39 INFO TaskSetManager: Finished task 0.0 in stage 74.0 (TID 74) in 521 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:19:39 INFO TaskSchedulerImpl: Removed TaskSet 74.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:19:39 INFO DAGScheduler: ResultStage 74 (start at NativeMethodAccessorImpl.java:0) finished in 0.525 s\n",
      "25/05/05 11:19:39 INFO DAGScheduler: Job 75 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:19:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 74: Stage finished\n",
      "25/05/05 11:19:39 INFO DAGScheduler: Job 75 finished: start at NativeMethodAccessorImpl.java:0, took 0.525700 s\n",
      "25/05/05 11:19:39 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 25, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:19:39 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:19:39 INFO DataWritingSparkTask: Committed partition 0 (task 75, attempt 0, stage 75.0)\n",
      "25/05/05 11:19:39 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 502208542 nanos, during time span of 504153791 nanos.\n",
      "25/05/05 11:19:39 INFO Executor: Finished task 0.0 in stage 75.0 (TID 75). 3592 bytes result sent to driver\n",
      "25/05/05 11:19:39 INFO TaskSetManager: Finished task 0.0 in stage 75.0 (TID 75) in 519 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:19:39 INFO TaskSchedulerImpl: Removed TaskSet 75.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:19:39 INFO DAGScheduler: ResultStage 75 (start at NativeMethodAccessorImpl.java:0) finished in 0.524 s\n",
      "25/05/05 11:19:39 INFO DAGScheduler: Job 76 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:19:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 75: Stage finished\n",
      "25/05/05 11:19:39 INFO DAGScheduler: Job 76 finished: start at NativeMethodAccessorImpl.java:0, took 0.525765 s\n",
      "25/05/05 11:19:39 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 25, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@18407d95] is committing.\n",
      "25/05/05 11:19:39 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 25, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@18407d95] committed.\n",
      "25/05/05 11:19:39 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/25 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.25.92b1c3fb-b207-41ae-b70b-e1fa9d03fcde.tmp\n",
      "25/05/05 11:19:39 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 25, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:19:39 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/25 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.25.0f07fb71-7c87-492c-9f62-49e0a3462baa.tmp\n",
      "25/05/05 11:19:39 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.25.92b1c3fb-b207-41ae-b70b-e1fa9d03fcde.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/25\n",
      "25/05/05 11:19:39 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:19:39.299Z\",\n",
      "  \"batchId\" : 25,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 62.5,\n",
      "  \"processedRowsPerSecond\" : 1.5772870662460567,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 549,\n",
      "    \"commitOffsets\" : 35,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 6,\n",
      "    \"triggerExecution\" : 634,\n",
      "    \"walCommit\" : 40\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3256\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3257\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3257\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 62.5,\n",
      "    \"processedRowsPerSecond\" : 1.5772870662460567,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:19:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3257, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:39 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:19:39 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:19:39 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:19:39 INFO connection: Opened connection [connectionId{localValue:49, serverValue:4419}] to localhost:27017\n",
      "25/05/05 11:19:39 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=508250}\n",
      "25/05/05 11:19:39 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.25.0f07fb71-7c87-492c-9f62-49e0a3462baa.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/25\n",
      "25/05/05 11:19:39 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:19:39.298Z\",\n",
      "  \"batchId\" : 25,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.5527950310559007,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 558,\n",
      "    \"commitOffsets\" : 35,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 6,\n",
      "    \"triggerExecution\" : 644,\n",
      "    \"walCommit\" : 43\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3256\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3257\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3257\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.5527950310559007,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:19:39 INFO connection: Opened connection [connectionId{localValue:50, serverValue:4420}] to localhost:27017\n",
      "25/05/05 11:19:39 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:19:39 INFO connection: Closed connection [connectionId{localValue:50, serverValue:4420}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:19:39 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 503108834 nanos, during time span of 515027834 nanos.\n",
      "25/05/05 11:19:39 INFO Executor: Finished task 0.0 in stage 76.0 (TID 76). 1645 bytes result sent to driver\n",
      "25/05/05 11:19:39 INFO TaskSetManager: Finished task 0.0 in stage 76.0 (TID 76) in 522 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:19:39 INFO TaskSchedulerImpl: Removed TaskSet 76.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:19:39 INFO DAGScheduler: ResultStage 76 (start at NativeMethodAccessorImpl.java:0) finished in 0.527 s\n",
      "25/05/05 11:19:39 INFO DAGScheduler: Job 77 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:19:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 76: Stage finished\n",
      "25/05/05 11:19:39 INFO DAGScheduler: Job 77 finished: start at NativeMethodAccessorImpl.java:0, took 0.528669 s\n",
      "25/05/05 11:19:39 INFO MemoryStore: Block broadcast_102 stored as values in memory (estimated size 248.0 B, free 366.2 MiB)\n",
      "25/05/05 11:19:39 INFO MemoryStore: Block broadcast_102_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.2 MiB)\n",
      "25/05/05 11:19:39 INFO BlockManagerInfo: Added broadcast_102_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:19:39 INFO SparkContext: Created broadcast 102 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:39 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/25 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.25.d05a10b1-2fcf-4321-b046-a99198f7b117.tmp\n",
      "25/05/05 11:19:39 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.25.d05a10b1-2fcf-4321-b046-a99198f7b117.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/25\n",
      "25/05/05 11:19:39 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:19:39.299Z\",\n",
      "  \"batchId\" : 25,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 62.5,\n",
      "  \"processedRowsPerSecond\" : 1.4662756598240467,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 605,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 6,\n",
      "    \"triggerExecution\" : 682,\n",
      "    \"walCommit\" : 41\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3256\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3257\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3257\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 62.5,\n",
      "    \"processedRowsPerSecond\" : 1.4662756598240467,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 25\n",
      "-------------------------------------------\n",
      "+----+----+--------+-------+----------+--------+-------+-----------+----------+---------------------+\n",
      "|C/A |UNIT|SCP     |STATION|DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time          |\n",
      "+----+----+--------+-------+----------+--------+-------+-----------+----------+---------------------+\n",
      "|R029|R044|00-00-00|23 ST  |05/05/2025|11:19:38|REGULAR|13         |7         |2025-05-05 11:19:39.3|\n",
      "+----+----+--------+-------+----------+--------+-------+-----------+----------+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:19:43 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/26 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.26.6fda3bf6-8f9c-46fb-9f56-af28abc782e0.tmp\n",
      "25/05/05 11:19:43 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/26 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.26.2d1d8e0a-b2f5-4a82-a203-205348a90834.tmp\n",
      "25/05/05 11:19:43 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/26 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.26.e7d64bab-f911-4cdc-aa23-e3aa702a9dd2.tmp\n",
      "25/05/05 11:19:43 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.26.6fda3bf6-8f9c-46fb-9f56-af28abc782e0.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/26\n",
      "25/05/05 11:19:43 INFO MicroBatchExecution: Committed offsets for batch 26. Metadata OffsetSeqMetadata(0,1746458383823,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:19:43 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.26.e7d64bab-f911-4cdc-aa23-e3aa702a9dd2.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/26\n",
      "25/05/05 11:19:43 INFO MicroBatchExecution: Committed offsets for batch 26. Metadata OffsetSeqMetadata(0,1746458383824,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:19:43 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.26.2d1d8e0a-b2f5-4a82-a203-205348a90834.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/26\n",
      "25/05/05 11:19:43 INFO MicroBatchExecution: Committed offsets for batch 26. Metadata OffsetSeqMetadata(0,1746458383824,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:19:43 INFO IncrementalExecution: Current batch timestamp = 1746458383823\n",
      "25/05/05 11:19:43 INFO IncrementalExecution: Current batch timestamp = 1746458383824\n",
      "25/05/05 11:19:43 INFO IncrementalExecution: Current batch timestamp = 1746458383824\n",
      "25/05/05 11:19:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:43 INFO IncrementalExecution: Current batch timestamp = 1746458383824\n",
      "25/05/05 11:19:43 INFO IncrementalExecution: Current batch timestamp = 1746458383823\n",
      "25/05/05 11:19:43 INFO IncrementalExecution: Current batch timestamp = 1746458383824\n",
      "25/05/05 11:19:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:43 INFO IncrementalExecution: Current batch timestamp = 1746458383823\n",
      "25/05/05 11:19:43 INFO IncrementalExecution: Current batch timestamp = 1746458383824\n",
      "25/05/05 11:19:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:43 INFO CodeGenerator: Code generated in 4.75975 ms\n",
      "25/05/05 11:19:43 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 26, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:19:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:43 INFO DAGScheduler: Got job 78 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:19:43 INFO DAGScheduler: Final stage: ResultStage 77 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:19:43 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:19:43 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:19:43 INFO DAGScheduler: Submitting ResultStage 77 (MapPartitionsRDD[422] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:19:43 INFO MemoryStore: Block broadcast_103 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:43 INFO MemoryStore: Block broadcast_103_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:43 INFO BlockManagerInfo: Added broadcast_103_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:43 INFO CodeGenerator: Code generated in 4.335208 ms\n",
      "25/05/05 11:19:43 INFO SparkContext: Created broadcast 103 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:19:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 77 (MapPartitionsRDD[422] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:19:43 INFO TaskSchedulerImpl: Adding task set 77.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:19:43 INFO TaskSetManager: Starting task 0.0 in stage 77.0 (TID 77) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:19:43 INFO Executor: Running task 0.0 in stage 77.0 (TID 77)\n",
      "25/05/05 11:19:43 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 26, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@46ba33a0]. The input RDD has 1 partitions.\n",
      "25/05/05 11:19:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:43 INFO DAGScheduler: Got job 79 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:19:43 INFO DAGScheduler: Final stage: ResultStage 78 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:19:43 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:19:43 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:19:43 INFO DAGScheduler: Submitting ResultStage 78 (MapPartitionsRDD[427] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:19:43 INFO MemoryStore: Block broadcast_104 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:43 INFO MemoryStore: Block broadcast_104_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:43 INFO BlockManagerInfo: Added broadcast_104_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:43 INFO SparkContext: Created broadcast 104 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:19:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 78 (MapPartitionsRDD[427] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:19:43 INFO TaskSchedulerImpl: Adding task set 78.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:19:43 INFO TaskSetManager: Starting task 0.0 in stage 78.0 (TID 78) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:19:43 INFO Executor: Running task 0.0 in stage 78.0 (TID 78)\n",
      "25/05/05 11:19:43 INFO CodeGenerator: Code generated in 4.756291 ms\n",
      "25/05/05 11:19:43 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3257 untilOffset=3258, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=26 taskId=77 partitionId=0\n",
      "25/05/05 11:19:43 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3257 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:43 INFO CodeGenerator: Code generated in 4.536875 ms\n",
      "25/05/05 11:19:43 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3257 untilOffset=3258, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=26 taskId=78 partitionId=0\n",
      "25/05/05 11:19:43 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3257 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:43 INFO DAGScheduler: Got job 80 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:19:43 INFO DAGScheduler: Final stage: ResultStage 79 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:19:43 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:19:43 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:19:43 INFO DAGScheduler: Submitting ResultStage 79 (MapPartitionsRDD[432] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:19:43 INFO MemoryStore: Block broadcast_105 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:19:43 INFO MemoryStore: Block broadcast_105_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:19:43 INFO BlockManagerInfo: Added broadcast_105_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:19:43 INFO SparkContext: Created broadcast 105 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:19:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 79 (MapPartitionsRDD[432] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:19:43 INFO TaskSchedulerImpl: Adding task set 79.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:19:43 INFO TaskSetManager: Starting task 0.0 in stage 79.0 (TID 79) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:19:43 INFO Executor: Running task 0.0 in stage 79.0 (TID 79)\n",
      "25/05/05 11:19:43 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3257 untilOffset=3258, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=26 taskId=79 partitionId=0\n",
      "25/05/05 11:19:43 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3257 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3258, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:44 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:19:44 INFO DataWritingSparkTask: Committed partition 0 (task 77, attempt 0, stage 77.0)\n",
      "25/05/05 11:19:44 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 504775792 nanos, during time span of 505347625 nanos.\n",
      "25/05/05 11:19:44 INFO Executor: Finished task 0.0 in stage 77.0 (TID 77). 2145 bytes result sent to driver\n",
      "25/05/05 11:19:44 INFO TaskSetManager: Finished task 0.0 in stage 77.0 (TID 77) in 515 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:19:44 INFO TaskSchedulerImpl: Removed TaskSet 77.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:19:44 INFO DAGScheduler: ResultStage 77 (start at NativeMethodAccessorImpl.java:0) finished in 0.517 s\n",
      "25/05/05 11:19:44 INFO DAGScheduler: Job 78 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:19:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 77: Stage finished\n",
      "25/05/05 11:19:44 INFO DAGScheduler: Job 78 finished: start at NativeMethodAccessorImpl.java:0, took 0.518037 s\n",
      "25/05/05 11:19:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 26, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:19:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3258, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:44 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:19:44 INFO DataWritingSparkTask: Committed partition 0 (task 78, attempt 0, stage 78.0)\n",
      "25/05/05 11:19:44 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 503635167 nanos, during time span of 505422625 nanos.\n",
      "25/05/05 11:19:44 INFO Executor: Finished task 0.0 in stage 78.0 (TID 78). 3559 bytes result sent to driver\n",
      "25/05/05 11:19:44 INFO TaskSetManager: Finished task 0.0 in stage 78.0 (TID 78) in 517 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:19:44 INFO TaskSchedulerImpl: Removed TaskSet 78.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:19:44 INFO DAGScheduler: ResultStage 78 (start at NativeMethodAccessorImpl.java:0) finished in 0.519 s\n",
      "25/05/05 11:19:44 INFO DAGScheduler: Job 79 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:19:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 78: Stage finished\n",
      "25/05/05 11:19:44 INFO DAGScheduler: Job 79 finished: start at NativeMethodAccessorImpl.java:0, took 0.520084 s\n",
      "25/05/05 11:19:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 26, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@46ba33a0] is committing.\n",
      "25/05/05 11:19:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 26, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@46ba33a0] committed.\n",
      "25/05/05 11:19:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 26, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:19:44 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/26 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.26.7b8b2d5f-b75a-4bc5-88f4-5e8604352f43.tmp\n",
      "25/05/05 11:19:44 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/26 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.26.f01dd32b-6baa-4deb-9dc1-82595fbe7afe.tmp\n",
      "25/05/05 11:19:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3258, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:44 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:19:44 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:19:44 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:19:44 INFO connection: Opened connection [connectionId{localValue:51, serverValue:4421}] to localhost:27017\n",
      "25/05/05 11:19:44 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=296542}\n",
      "25/05/05 11:19:44 INFO connection: Opened connection [connectionId{localValue:52, serverValue:4422}] to localhost:27017\n",
      "25/05/05 11:19:44 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:19:44 INFO connection: Closed connection [connectionId{localValue:52, serverValue:4422}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:19:44 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 504919541 nanos, during time span of 512256958 nanos.\n",
      "25/05/05 11:19:44 INFO Executor: Finished task 0.0 in stage 79.0 (TID 79). 1645 bytes result sent to driver\n",
      "25/05/05 11:19:44 INFO TaskSetManager: Finished task 0.0 in stage 79.0 (TID 79) in 519 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:19:44 INFO TaskSchedulerImpl: Removed TaskSet 79.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:19:44 INFO DAGScheduler: ResultStage 79 (start at NativeMethodAccessorImpl.java:0) finished in 0.522 s\n",
      "25/05/05 11:19:44 INFO DAGScheduler: Job 80 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:19:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 79: Stage finished\n",
      "25/05/05 11:19:44 INFO DAGScheduler: Job 80 finished: start at NativeMethodAccessorImpl.java:0, took 0.523473 s\n",
      "25/05/05 11:19:44 INFO MemoryStore: Block broadcast_106 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:19:44 INFO MemoryStore: Block broadcast_106_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:19:44 INFO BlockManagerInfo: Added broadcast_106_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:19:44 INFO SparkContext: Created broadcast 106 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:44 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/26 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.26.aea3b1f0-d25c-42e0-91f8-81760f969df5.tmp\n",
      "25/05/05 11:19:44 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.26.7b8b2d5f-b75a-4bc5-88f4-5e8604352f43.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/26\n",
      "25/05/05 11:19:44 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:19:43.822Z\",\n",
      "  \"batchId\" : 26,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.6025641025641026,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 537,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 624,\n",
      "    \"walCommit\" : 51\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3257\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3258\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3258\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.6025641025641026,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:19:44 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.26.f01dd32b-6baa-4deb-9dc1-82595fbe7afe.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/26\n",
      "25/05/05 11:19:44 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:19:43.824Z\",\n",
      "  \"batchId\" : 26,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.6,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 539,\n",
      "    \"commitOffsets\" : 31,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 0,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 625,\n",
      "    \"walCommit\" : 52\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3257\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3258\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3258\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.6,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:19:44 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.26.aea3b1f0-d25c-42e0-91f8-81760f969df5.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/26\n",
      "25/05/05 11:19:44 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:19:43.824Z\",\n",
      "  \"batchId\" : 26,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.5625,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 559,\n",
      "    \"commitOffsets\" : 26,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 0,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 640,\n",
      "    \"walCommit\" : 51\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3257\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3258\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3258\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.5625,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 26\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A001|R079|01-00-00|WORLD TRADE CTR|05/05/2025|11:19:42|REGULAR|9          |7         |2025-05-05 11:19:43.824|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:19:48 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/27 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.27.16242378-f89d-4a65-bd25-8a6bf4b02af5.tmp\n",
      "25/05/05 11:19:48 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/27 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.27.7f46c019-cb0e-4334-bc05-379b76944b1c.tmp\n",
      "25/05/05 11:19:48 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/27 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.27.f48821f6-8380-4886-9c0d-2e3072008739.tmp\n",
      "25/05/05 11:19:48 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.27.16242378-f89d-4a65-bd25-8a6bf4b02af5.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/27\n",
      "25/05/05 11:19:48 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.27.7f46c019-cb0e-4334-bc05-379b76944b1c.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/27\n",
      "25/05/05 11:19:48 INFO MicroBatchExecution: Committed offsets for batch 27. Metadata OffsetSeqMetadata(0,1746458388401,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:19:48 INFO MicroBatchExecution: Committed offsets for batch 27. Metadata OffsetSeqMetadata(0,1746458388401,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:19:48 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.27.f48821f6-8380-4886-9c0d-2e3072008739.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/27\n",
      "25/05/05 11:19:48 INFO MicroBatchExecution: Committed offsets for batch 27. Metadata OffsetSeqMetadata(0,1746458388403,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:19:48 INFO IncrementalExecution: Current batch timestamp = 1746458388401\n",
      "25/05/05 11:19:48 INFO IncrementalExecution: Current batch timestamp = 1746458388401\n",
      "25/05/05 11:19:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:48 INFO IncrementalExecution: Current batch timestamp = 1746458388403\n",
      "25/05/05 11:19:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:48 INFO IncrementalExecution: Current batch timestamp = 1746458388401\n",
      "25/05/05 11:19:48 INFO IncrementalExecution: Current batch timestamp = 1746458388401\n",
      "25/05/05 11:19:48 INFO IncrementalExecution: Current batch timestamp = 1746458388403\n",
      "25/05/05 11:19:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:48 INFO BlockManagerInfo: Removed broadcast_101_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:48 INFO IncrementalExecution: Current batch timestamp = 1746458388401\n",
      "25/05/05 11:19:48 INFO IncrementalExecution: Current batch timestamp = 1746458388401\n",
      "25/05/05 11:19:48 INFO BlockManagerInfo: Removed broadcast_104_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:48 INFO CodeGenerator: Code generated in 5.964417 ms\n",
      "25/05/05 11:19:48 INFO BlockManagerInfo: Removed broadcast_103_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:48 INFO CodeGenerator: Code generated in 7.106709 ms\n",
      "25/05/05 11:19:48 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 27, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@594b1ed3]. The input RDD has 1 partitions.\n",
      "25/05/05 11:19:48 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:48 INFO BlockManagerInfo: Removed broadcast_105_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:48 INFO DAGScheduler: Got job 81 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:19:48 INFO DAGScheduler: Final stage: ResultStage 80 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:19:48 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:19:48 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:19:48 INFO DAGScheduler: Submitting ResultStage 80 (MapPartitionsRDD[440] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:19:48 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 27, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:19:48 INFO MemoryStore: Block broadcast_107 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:48 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:48 INFO MemoryStore: Block broadcast_107_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:48 INFO BlockManagerInfo: Added broadcast_107_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:48 INFO SparkContext: Created broadcast 107 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:19:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 80 (MapPartitionsRDD[440] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:19:48 INFO TaskSchedulerImpl: Adding task set 80.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:19:48 INFO DAGScheduler: Got job 82 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:19:48 INFO DAGScheduler: Final stage: ResultStage 81 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:19:48 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:19:48 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:19:48 INFO DAGScheduler: Submitting ResultStage 81 (MapPartitionsRDD[443] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:19:48 INFO TaskSetManager: Starting task 0.0 in stage 80.0 (TID 80) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:19:48 INFO MemoryStore: Block broadcast_108 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:48 INFO MemoryStore: Block broadcast_108_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:48 INFO Executor: Running task 0.0 in stage 80.0 (TID 80)\n",
      "25/05/05 11:19:48 INFO BlockManagerInfo: Added broadcast_108_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:48 INFO SparkContext: Created broadcast 108 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:19:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 81 (MapPartitionsRDD[443] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:19:48 INFO TaskSchedulerImpl: Adding task set 81.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:19:48 INFO TaskSetManager: Starting task 0.0 in stage 81.0 (TID 81) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:19:48 INFO BlockManagerInfo: Removed broadcast_106_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:19:48 INFO Executor: Running task 0.0 in stage 81.0 (TID 81)\n",
      "25/05/05 11:19:48 INFO BlockManagerInfo: Removed broadcast_100_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:48 INFO CodeGenerator: Code generated in 7.661458 ms\n",
      "25/05/05 11:19:48 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3258 untilOffset=3259, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=27 taskId=81 partitionId=0\n",
      "25/05/05 11:19:48 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3258 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:48 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3258 untilOffset=3259, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=27 taskId=80 partitionId=0\n",
      "25/05/05 11:19:48 INFO BlockManagerInfo: Removed broadcast_99_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:48 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3258 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:48 INFO BlockManagerInfo: Removed broadcast_102_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:19:48 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:48 INFO DAGScheduler: Got job 83 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:19:48 INFO DAGScheduler: Final stage: ResultStage 82 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:19:48 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:19:48 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:19:48 INFO DAGScheduler: Submitting ResultStage 82 (MapPartitionsRDD[448] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:19:48 INFO MemoryStore: Block broadcast_109 stored as values in memory (estimated size 47.1 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:48 INFO MemoryStore: Block broadcast_109_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:48 INFO BlockManagerInfo: Added broadcast_109_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:48 INFO SparkContext: Created broadcast 109 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:19:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 82 (MapPartitionsRDD[448] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:19:48 INFO TaskSchedulerImpl: Adding task set 82.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:19:48 INFO TaskSetManager: Starting task 0.0 in stage 82.0 (TID 82) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:19:48 INFO Executor: Running task 0.0 in stage 82.0 (TID 82)\n",
      "25/05/05 11:19:48 INFO CodeGenerator: Code generated in 5.682208 ms\n",
      "25/05/05 11:19:48 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3258 untilOffset=3259, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=27 taskId=82 partitionId=0\n",
      "25/05/05 11:19:48 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3258 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3259, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:49 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:19:49 INFO DataWritingSparkTask: Committed partition 0 (task 81, attempt 0, stage 81.0)\n",
      "25/05/05 11:19:49 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 503873917 nanos, during time span of 504263166 nanos.\n",
      "25/05/05 11:19:49 INFO Executor: Finished task 0.0 in stage 81.0 (TID 81). 2137 bytes result sent to driver\n",
      "25/05/05 11:19:49 INFO TaskSetManager: Finished task 0.0 in stage 81.0 (TID 81) in 516 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:19:49 INFO TaskSchedulerImpl: Removed TaskSet 81.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:19:49 INFO DAGScheduler: ResultStage 81 (start at NativeMethodAccessorImpl.java:0) finished in 0.518 s\n",
      "25/05/05 11:19:49 INFO DAGScheduler: Job 82 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:19:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 81: Stage finished\n",
      "25/05/05 11:19:49 INFO DAGScheduler: Job 82 finished: start at NativeMethodAccessorImpl.java:0, took 0.520938 s\n",
      "25/05/05 11:19:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 27, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:19:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3259, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:49 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:19:49 INFO DataWritingSparkTask: Committed partition 0 (task 80, attempt 0, stage 80.0)\n",
      "25/05/05 11:19:49 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 503685375 nanos, during time span of 505572333 nanos.\n",
      "25/05/05 11:19:49 INFO Executor: Finished task 0.0 in stage 80.0 (TID 80). 3549 bytes result sent to driver\n",
      "25/05/05 11:19:49 INFO TaskSetManager: Finished task 0.0 in stage 80.0 (TID 80) in 525 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:19:49 INFO TaskSchedulerImpl: Removed TaskSet 80.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:19:49 INFO DAGScheduler: ResultStage 80 (start at NativeMethodAccessorImpl.java:0) finished in 0.529 s\n",
      "25/05/05 11:19:49 INFO DAGScheduler: Job 81 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:19:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 80: Stage finished\n",
      "25/05/05 11:19:49 INFO DAGScheduler: Job 81 finished: start at NativeMethodAccessorImpl.java:0, took 0.530009 s\n",
      "25/05/05 11:19:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 27, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@594b1ed3] is committing.\n",
      "25/05/05 11:19:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 27, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@594b1ed3] committed.\n",
      "25/05/05 11:19:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 27, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:19:49 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/27 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.27.68a69211-1669-4a67-81f3-6d4364e69bed.tmp\n",
      "25/05/05 11:19:49 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/27 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.27.eba34352-229e-4c9f-bf02-10a9f0643bd7.tmp\n",
      "25/05/05 11:19:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3259, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:49 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:19:49 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:19:49 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:19:49 INFO connection: Opened connection [connectionId{localValue:53, serverValue:4423}] to localhost:27017\n",
      "25/05/05 11:19:49 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.27.68a69211-1669-4a67-81f3-6d4364e69bed.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/27\n",
      "25/05/05 11:19:49 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:19:48.398Z\",\n",
      "  \"batchId\" : 27,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.5174506828528072,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 564,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 7,\n",
      "    \"triggerExecution\" : 659,\n",
      "    \"walCommit\" : 56\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3258\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3259\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3259\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.5174506828528072,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:19:49 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2266667}\n",
      "25/05/05 11:19:49 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.27.eba34352-229e-4c9f-bf02-10a9f0643bd7.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/27\n",
      "25/05/05 11:19:49 INFO connection: Opened connection [connectionId{localValue:54, serverValue:4424}] to localhost:27017\n",
      "25/05/05 11:19:49 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:19:48.398Z\",\n",
      "  \"batchId\" : 27,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.510574018126888,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 568,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 7,\n",
      "    \"triggerExecution\" : 662,\n",
      "    \"walCommit\" : 56\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3258\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3259\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3259\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.510574018126888,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:19:49 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:19:49 INFO connection: Closed connection [connectionId{localValue:54, serverValue:4424}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:19:49 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 505493667 nanos, during time span of 514110917 nanos.\n",
      "25/05/05 11:19:49 INFO Executor: Finished task 0.0 in stage 82.0 (TID 82). 1645 bytes result sent to driver\n",
      "25/05/05 11:19:49 INFO TaskSetManager: Finished task 0.0 in stage 82.0 (TID 82) in 528 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:19:49 INFO TaskSchedulerImpl: Removed TaskSet 82.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:19:49 INFO DAGScheduler: ResultStage 82 (start at NativeMethodAccessorImpl.java:0) finished in 0.535 s\n",
      "25/05/05 11:19:49 INFO DAGScheduler: Job 83 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:19:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 82: Stage finished\n",
      "25/05/05 11:19:49 INFO DAGScheduler: Job 83 finished: start at NativeMethodAccessorImpl.java:0, took 0.536723 s\n",
      "25/05/05 11:19:49 INFO MemoryStore: Block broadcast_110 stored as values in memory (estimated size 248.0 B, free 366.2 MiB)\n",
      "25/05/05 11:19:49 INFO MemoryStore: Block broadcast_110_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.2 MiB)\n",
      "25/05/05 11:19:49 INFO BlockManagerInfo: Added broadcast_110_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:19:49 INFO SparkContext: Created broadcast 110 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:49 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/27 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.27.81d4a347-6412-4a77-a15a-eb9fae5544f0.tmp\n",
      "25/05/05 11:19:49 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.27.81d4a347-6412-4a77-a15a-eb9fae5544f0.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/27\n",
      "25/05/05 11:19:49 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:19:48.400Z\",\n",
      "  \"batchId\" : 27,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.4367816091954024,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 600,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 696,\n",
      "    \"walCommit\" : 57\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3258\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3259\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3259\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.4367816091954024,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 27\n",
      "-------------------------------------------\n",
      "+----+----+--------+-------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION|DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+-------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R029|R044|00-00-00|23 ST  |05/05/2025|11:19:47|REGULAR|11         |8         |2025-05-05 11:19:48.401|\n",
      "+----+----+--------+-------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:19:52 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/28 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.28.633713ee-8a07-42b6-9e7d-0da8fe7fead7.tmp\n",
      "25/05/05 11:19:52 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/28 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.28.cdb6ce64-c0c3-4317-9833-5563e158cf52.tmp\n",
      "25/05/05 11:19:52 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/28 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.28.cd5c14bf-3aeb-4cbf-806a-41105103ace5.tmp\n",
      "25/05/05 11:19:53 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.28.633713ee-8a07-42b6-9e7d-0da8fe7fead7.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/28\n",
      "25/05/05 11:19:53 INFO MicroBatchExecution: Committed offsets for batch 28. Metadata OffsetSeqMetadata(0,1746458392975,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:19:53 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.28.cdb6ce64-c0c3-4317-9833-5563e158cf52.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/28\n",
      "25/05/05 11:19:53 INFO MicroBatchExecution: Committed offsets for batch 28. Metadata OffsetSeqMetadata(0,1746458392976,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:19:53 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.28.cd5c14bf-3aeb-4cbf-806a-41105103ace5.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/28\n",
      "25/05/05 11:19:53 INFO MicroBatchExecution: Committed offsets for batch 28. Metadata OffsetSeqMetadata(0,1746458392973,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:19:53 INFO IncrementalExecution: Current batch timestamp = 1746458392975\n",
      "25/05/05 11:19:53 INFO IncrementalExecution: Current batch timestamp = 1746458392976\n",
      "25/05/05 11:19:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:53 INFO IncrementalExecution: Current batch timestamp = 1746458392973\n",
      "25/05/05 11:19:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:53 INFO IncrementalExecution: Current batch timestamp = 1746458392975\n",
      "25/05/05 11:19:53 INFO IncrementalExecution: Current batch timestamp = 1746458392976\n",
      "25/05/05 11:19:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:53 INFO IncrementalExecution: Current batch timestamp = 1746458392973\n",
      "25/05/05 11:19:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:53 INFO IncrementalExecution: Current batch timestamp = 1746458392975\n",
      "25/05/05 11:19:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:53 INFO IncrementalExecution: Current batch timestamp = 1746458392976\n",
      "25/05/05 11:19:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:53 INFO CodeGenerator: Code generated in 7.696667 ms\n",
      "25/05/05 11:19:53 INFO CodeGenerator: Code generated in 7.685417 ms\n",
      "25/05/05 11:19:53 INFO CodeGenerator: Code generated in 9.016625 ms\n",
      "25/05/05 11:19:53 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 28, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:19:53 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 28, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@69e5e07]. The input RDD has 1 partitions.\n",
      "25/05/05 11:19:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:53 INFO DAGScheduler: Got job 84 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:19:53 INFO DAGScheduler: Final stage: ResultStage 83 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:19:53 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:19:53 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:19:53 INFO DAGScheduler: Submitting ResultStage 83 (MapPartitionsRDD[458] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:19:53 INFO MemoryStore: Block broadcast_111 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:53 INFO MemoryStore: Block broadcast_111_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:53 INFO BlockManagerInfo: Added broadcast_111_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:53 INFO SparkContext: Created broadcast 111 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:19:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 83 (MapPartitionsRDD[458] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:19:53 INFO TaskSchedulerImpl: Adding task set 83.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:19:53 INFO DAGScheduler: Got job 85 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:19:53 INFO DAGScheduler: Final stage: ResultStage 84 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:19:53 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:19:53 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:19:53 INFO TaskSetManager: Starting task 0.0 in stage 83.0 (TID 83) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:19:53 INFO DAGScheduler: Submitting ResultStage 84 (MapPartitionsRDD[459] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:19:53 INFO MemoryStore: Block broadcast_112 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:53 INFO MemoryStore: Block broadcast_112_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:53 INFO Executor: Running task 0.0 in stage 83.0 (TID 83)\n",
      "25/05/05 11:19:53 INFO BlockManagerInfo: Added broadcast_112_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:53 INFO SparkContext: Created broadcast 112 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:19:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 84 (MapPartitionsRDD[459] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:19:53 INFO TaskSchedulerImpl: Adding task set 84.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:19:53 INFO TaskSetManager: Starting task 0.0 in stage 84.0 (TID 84) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:19:53 INFO Executor: Running task 0.0 in stage 84.0 (TID 84)\n",
      "25/05/05 11:19:53 INFO CodeGenerator: Code generated in 5.316875 ms\n",
      "25/05/05 11:19:53 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3259 untilOffset=3260, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=28 taskId=83 partitionId=0\n",
      "25/05/05 11:19:53 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3259 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:53 INFO CodeGenerator: Code generated in 4.603083 ms\n",
      "25/05/05 11:19:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:53 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3259 untilOffset=3260, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=28 taskId=84 partitionId=0\n",
      "25/05/05 11:19:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:53 INFO DAGScheduler: Got job 86 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:19:53 INFO DAGScheduler: Final stage: ResultStage 85 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:19:53 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:19:53 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:19:53 INFO DAGScheduler: Submitting ResultStage 85 (MapPartitionsRDD[464] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:19:53 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3259 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:53 INFO MemoryStore: Block broadcast_113 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:19:53 INFO MemoryStore: Block broadcast_113_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:19:53 INFO BlockManagerInfo: Added broadcast_113_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:19:53 INFO SparkContext: Created broadcast 113 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:19:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 85 (MapPartitionsRDD[464] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:19:53 INFO TaskSchedulerImpl: Adding task set 85.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:19:53 INFO TaskSetManager: Starting task 0.0 in stage 85.0 (TID 85) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:19:53 INFO Executor: Running task 0.0 in stage 85.0 (TID 85)\n",
      "25/05/05 11:19:53 INFO CodeGenerator: Code generated in 4.611125 ms\n",
      "25/05/05 11:19:53 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3259 untilOffset=3260, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=28 taskId=85 partitionId=0\n",
      "25/05/05 11:19:53 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3259 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3260, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:53 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:19:53 INFO DataWritingSparkTask: Committed partition 0 (task 83, attempt 0, stage 83.0)\n",
      "25/05/05 11:19:53 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 503878083 nanos, during time span of 504663083 nanos.\n",
      "25/05/05 11:19:53 INFO Executor: Finished task 0.0 in stage 83.0 (TID 83). 2145 bytes result sent to driver\n",
      "25/05/05 11:19:53 INFO TaskSetManager: Finished task 0.0 in stage 83.0 (TID 83) in 518 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:19:53 INFO TaskSchedulerImpl: Removed TaskSet 83.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:19:53 INFO DAGScheduler: ResultStage 83 (start at NativeMethodAccessorImpl.java:0) finished in 0.521 s\n",
      "25/05/05 11:19:53 INFO DAGScheduler: Job 84 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:19:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 83: Stage finished\n",
      "25/05/05 11:19:53 INFO DAGScheduler: Job 84 finished: start at NativeMethodAccessorImpl.java:0, took 0.523142 s\n",
      "25/05/05 11:19:53 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 28, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:19:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3260, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:53 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 28, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:19:53 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:19:53 INFO DataWritingSparkTask: Committed partition 0 (task 84, attempt 0, stage 84.0)\n",
      "25/05/05 11:19:53 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 506746708 nanos, during time span of 508642375 nanos.\n",
      "25/05/05 11:19:53 INFO Executor: Finished task 0.0 in stage 84.0 (TID 84). 3559 bytes result sent to driver\n",
      "25/05/05 11:19:53 INFO TaskSetManager: Finished task 0.0 in stage 84.0 (TID 84) in 529 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:19:53 INFO TaskSchedulerImpl: Removed TaskSet 84.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:19:53 INFO DAGScheduler: ResultStage 84 (start at NativeMethodAccessorImpl.java:0) finished in 0.533 s\n",
      "25/05/05 11:19:53 INFO DAGScheduler: Job 85 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:19:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 84: Stage finished\n",
      "25/05/05 11:19:53 INFO DAGScheduler: Job 85 finished: start at NativeMethodAccessorImpl.java:0, took 0.537605 s\n",
      "25/05/05 11:19:53 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 28, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@69e5e07] is committing.\n",
      "25/05/05 11:19:53 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 28, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@69e5e07] committed.\n",
      "25/05/05 11:19:53 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/28 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.28.03de12b2-fbca-45d5-94cb-675ff5b8d870.tmp\n",
      "25/05/05 11:19:53 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/28 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.28.985e7e27-df4c-4def-9e1d-499af6a8b8d4.tmp\n",
      "25/05/05 11:19:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3260, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:53 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:19:53 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:19:53 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:19:53 INFO connection: Opened connection [connectionId{localValue:55, serverValue:4425}] to localhost:27017\n",
      "25/05/05 11:19:53 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=309875}\n",
      "25/05/05 11:19:53 INFO connection: Opened connection [connectionId{localValue:56, serverValue:4426}] to localhost:27017\n",
      "25/05/05 11:19:53 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:19:53 INFO connection: Closed connection [connectionId{localValue:56, serverValue:4426}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:19:53 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 504306750 nanos, during time span of 519275292 nanos.\n",
      "25/05/05 11:19:53 INFO Executor: Finished task 0.0 in stage 85.0 (TID 85). 1645 bytes result sent to driver\n",
      "25/05/05 11:19:53 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.28.03de12b2-fbca-45d5-94cb-675ff5b8d870.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/28\n",
      "25/05/05 11:19:53 INFO TaskSetManager: Finished task 0.0 in stage 85.0 (TID 85) in 533 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:19:53 INFO TaskSchedulerImpl: Removed TaskSet 85.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:19:53 INFO DAGScheduler: ResultStage 85 (start at NativeMethodAccessorImpl.java:0) finished in 0.541 s\n",
      "25/05/05 11:19:53 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:19:52.972Z\",\n",
      "  \"batchId\" : 28,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.5384615384615383,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 563,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 4,\n",
      "    \"queryPlanning\" : 7,\n",
      "    \"triggerExecution\" : 650,\n",
      "    \"walCommit\" : 46\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3259\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3260\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3260\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.5384615384615383,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:19:53 INFO DAGScheduler: Job 86 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:19:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 85: Stage finished\n",
      "25/05/05 11:19:53 INFO DAGScheduler: Job 86 finished: start at NativeMethodAccessorImpl.java:0, took 0.544505 s\n",
      "25/05/05 11:19:53 INFO MemoryStore: Block broadcast_114 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:19:53 INFO MemoryStore: Block broadcast_114_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:19:53 INFO BlockManagerInfo: Added broadcast_114_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:19:53 INFO SparkContext: Created broadcast 114 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:53 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.28.985e7e27-df4c-4def-9e1d-499af6a8b8d4.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/28\n",
      "25/05/05 11:19:53 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:19:52.972Z\",\n",
      "  \"batchId\" : 28,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.5267175572519083,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 567,\n",
      "    \"commitOffsets\" : 31,\n",
      "    \"getBatch\" : 1,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 7,\n",
      "    \"triggerExecution\" : 655,\n",
      "    \"walCommit\" : 46\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3259\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3260\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3260\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.5267175572519083,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:19:53 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/28 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.28.955a6de2-4625-4a04-9285-47c34aee8eee.tmp\n",
      "25/05/05 11:19:53 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.28.955a6de2-4625-4a04-9285-47c34aee8eee.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/28\n",
      "25/05/05 11:19:53 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:19:52.970Z\",\n",
      "  \"batchId\" : 28,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 47.61904761904761,\n",
      "  \"processedRowsPerSecond\" : 1.4598540145985401,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 593,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 9,\n",
      "    \"triggerExecution\" : 685,\n",
      "    \"walCommit\" : 49\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3259\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3260\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3260\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 47.61904761904761,\n",
      "    \"processedRowsPerSecond\" : 1.4598540145985401,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 28\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R023|R045|00-00-00|34 ST-HERALD SQ|05/05/2025|11:19:51|REGULAR|6          |9         |2025-05-05 11:19:52.976|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:19:53 INFO BlockManagerInfo: Removed broadcast_111_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:19:53 INFO BlockManagerInfo: Removed broadcast_110_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:19:53 INFO BlockManagerInfo: Removed broadcast_112_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:53 INFO BlockManagerInfo: Removed broadcast_108_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:53 INFO BlockManagerInfo: Removed broadcast_107_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:53 INFO BlockManagerInfo: Removed broadcast_113_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:53 INFO BlockManagerInfo: Removed broadcast_114_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:19:53 INFO BlockManagerInfo: Removed broadcast_109_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:57 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/29 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.29.edc0b1e6-fab6-4277-a252-6e21e26b1630.tmp\n",
      "25/05/05 11:19:57 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/29 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.29.4464558b-b03a-4779-bde1-342a5c2cc914.tmp\n",
      "25/05/05 11:19:57 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/29 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.29.496ac512-e0ea-46a0-ab27-fcb3edc5332d.tmp\n",
      "25/05/05 11:19:57 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.29.edc0b1e6-fab6-4277-a252-6e21e26b1630.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/29\n",
      "25/05/05 11:19:57 INFO MicroBatchExecution: Committed offsets for batch 29. Metadata OffsetSeqMetadata(0,1746458397550,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:19:57 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.29.4464558b-b03a-4779-bde1-342a5c2cc914.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/29\n",
      "25/05/05 11:19:57 INFO MicroBatchExecution: Committed offsets for batch 29. Metadata OffsetSeqMetadata(0,1746458397553,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:19:57 INFO IncrementalExecution: Current batch timestamp = 1746458397550\n",
      "25/05/05 11:19:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:57 INFO IncrementalExecution: Current batch timestamp = 1746458397553\n",
      "25/05/05 11:19:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:57 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.29.496ac512-e0ea-46a0-ab27-fcb3edc5332d.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/29\n",
      "25/05/05 11:19:57 INFO MicroBatchExecution: Committed offsets for batch 29. Metadata OffsetSeqMetadata(0,1746458397554,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:19:57 INFO IncrementalExecution: Current batch timestamp = 1746458397550\n",
      "25/05/05 11:19:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:57 INFO IncrementalExecution: Current batch timestamp = 1746458397553\n",
      "25/05/05 11:19:57 INFO IncrementalExecution: Current batch timestamp = 1746458397554\n",
      "25/05/05 11:19:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:57 INFO IncrementalExecution: Current batch timestamp = 1746458397550\n",
      "25/05/05 11:19:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:57 INFO IncrementalExecution: Current batch timestamp = 1746458397554\n",
      "25/05/05 11:19:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:57 INFO IncrementalExecution: Current batch timestamp = 1746458397553\n",
      "25/05/05 11:19:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:19:57 INFO CodeGenerator: Code generated in 8.971583 ms\n",
      "25/05/05 11:19:57 INFO CodeGenerator: Code generated in 18.879875 ms\n",
      "25/05/05 11:19:57 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 29, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:19:57 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:57 INFO DAGScheduler: Got job 87 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:19:57 INFO DAGScheduler: Final stage: ResultStage 86 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:19:57 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:19:57 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:19:57 INFO DAGScheduler: Submitting ResultStage 86 (MapPartitionsRDD[472] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:19:57 INFO CodeGenerator: Code generated in 9.100875 ms\n",
      "25/05/05 11:19:57 INFO MemoryStore: Block broadcast_115 stored as values in memory (estimated size 15.2 KiB, free 366.3 MiB)\n",
      "25/05/05 11:19:57 INFO MemoryStore: Block broadcast_115_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.3 MiB)\n",
      "25/05/05 11:19:57 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 29, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@34cbaab7]. The input RDD has 1 partitions.\n",
      "25/05/05 11:19:57 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:57 INFO BlockManagerInfo: Added broadcast_115_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:57 INFO SparkContext: Created broadcast 115 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:19:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 86 (MapPartitionsRDD[472] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:19:57 INFO TaskSchedulerImpl: Adding task set 86.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:19:57 INFO DAGScheduler: Got job 88 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:19:57 INFO DAGScheduler: Final stage: ResultStage 87 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:19:57 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:19:57 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:19:57 INFO DAGScheduler: Submitting ResultStage 87 (MapPartitionsRDD[475] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:19:57 INFO MemoryStore: Block broadcast_116 stored as values in memory (estimated size 16.2 KiB, free 366.3 MiB)\n",
      "25/05/05 11:19:57 INFO MemoryStore: Block broadcast_116_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.3 MiB)\n",
      "25/05/05 11:19:57 INFO TaskSetManager: Starting task 0.0 in stage 86.0 (TID 86) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:19:57 INFO BlockManagerInfo: Added broadcast_116_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:57 INFO Executor: Running task 0.0 in stage 86.0 (TID 86)\n",
      "25/05/05 11:19:57 INFO SparkContext: Created broadcast 116 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:19:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 87 (MapPartitionsRDD[475] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:19:57 INFO TaskSchedulerImpl: Adding task set 87.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:19:57 INFO TaskSetManager: Starting task 0.0 in stage 87.0 (TID 87) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:19:57 INFO Executor: Running task 0.0 in stage 87.0 (TID 87)\n",
      "25/05/05 11:19:57 INFO CodeGenerator: Code generated in 13.342541 ms\n",
      "25/05/05 11:19:57 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3260 untilOffset=3261, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=29 taskId=86 partitionId=0\n",
      "25/05/05 11:19:57 INFO CodeGenerator: Code generated in 14.146833 ms\n",
      "25/05/05 11:19:57 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3260 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:57 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3260 untilOffset=3261, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=29 taskId=87 partitionId=0\n",
      "25/05/05 11:19:57 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3260 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:57 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:57 INFO DAGScheduler: Got job 89 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:19:57 INFO DAGScheduler: Final stage: ResultStage 88 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:19:57 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:19:57 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:19:57 INFO DAGScheduler: Submitting ResultStage 88 (MapPartitionsRDD[480] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:19:57 INFO MemoryStore: Block broadcast_117 stored as values in memory (estimated size 47.1 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:57 INFO MemoryStore: Block broadcast_117_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.2 MiB)\n",
      "25/05/05 11:19:57 INFO BlockManagerInfo: Added broadcast_117_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:19:57 INFO SparkContext: Created broadcast 117 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:19:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 88 (MapPartitionsRDD[480] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:19:57 INFO TaskSchedulerImpl: Adding task set 88.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:19:57 INFO TaskSetManager: Starting task 0.0 in stage 88.0 (TID 88) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:19:57 INFO Executor: Running task 0.0 in stage 88.0 (TID 88)\n",
      "25/05/05 11:19:57 INFO CodeGenerator: Code generated in 8.74925 ms\n",
      "25/05/05 11:19:57 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3260 untilOffset=3261, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=29 taskId=88 partitionId=0\n",
      "25/05/05 11:19:57 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3260 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3261, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:58 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:19:58 INFO DataWritingSparkTask: Committed partition 0 (task 86, attempt 0, stage 86.0)\n",
      "25/05/05 11:19:58 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 506960583 nanos, during time span of 507364667 nanos.\n",
      "25/05/05 11:19:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:58 INFO Executor: Finished task 0.0 in stage 86.0 (TID 86). 2188 bytes result sent to driver\n",
      "25/05/05 11:19:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3261, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:58 INFO TaskSetManager: Finished task 0.0 in stage 86.0 (TID 86) in 530 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:19:58 INFO TaskSchedulerImpl: Removed TaskSet 86.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:19:58 INFO DAGScheduler: ResultStage 86 (start at NativeMethodAccessorImpl.java:0) finished in 0.536 s\n",
      "25/05/05 11:19:58 INFO DAGScheduler: Job 87 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:19:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 86: Stage finished\n",
      "25/05/05 11:19:58 INFO DAGScheduler: Job 87 finished: start at NativeMethodAccessorImpl.java:0, took 0.537514 s\n",
      "25/05/05 11:19:58 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 29, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:19:58 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:19:58 INFO DataWritingSparkTask: Committed partition 0 (task 87, attempt 0, stage 87.0)\n",
      "25/05/05 11:19:58 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 503797958 nanos, during time span of 505665875 nanos.\n",
      "25/05/05 11:19:58 INFO Executor: Finished task 0.0 in stage 87.0 (TID 87). 3559 bytes result sent to driver\n",
      "25/05/05 11:19:58 INFO TaskSetManager: Finished task 0.0 in stage 87.0 (TID 87) in 530 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:19:58 INFO TaskSchedulerImpl: Removed TaskSet 87.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:19:58 INFO DAGScheduler: ResultStage 87 (start at NativeMethodAccessorImpl.java:0) finished in 0.533 s\n",
      "25/05/05 11:19:58 INFO DAGScheduler: Job 88 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:19:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 87: Stage finished\n",
      "25/05/05 11:19:58 INFO DAGScheduler: Job 88 finished: start at NativeMethodAccessorImpl.java:0, took 0.535983 s\n",
      "25/05/05 11:19:58 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 29, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@34cbaab7] is committing.\n",
      "25/05/05 11:19:58 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 29, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@34cbaab7] committed.\n",
      "25/05/05 11:19:58 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 29, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:19:58 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/29 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.29.8d681fd0-826d-4077-8cf0-2e19d752cd20.tmp\n",
      "25/05/05 11:19:58 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/29 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.29.68caaaff-8e52-44e2-89c0-47419a75347d.tmp\n",
      "25/05/05 11:19:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:19:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3261, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:19:58 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:19:58 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:19:58 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:19:58 INFO connection: Opened connection [connectionId{localValue:57, serverValue:4427}] to localhost:27017\n",
      "25/05/05 11:19:58 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=419458}\n",
      "25/05/05 11:19:58 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.29.8d681fd0-826d-4077-8cf0-2e19d752cd20.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/29\n",
      "25/05/05 11:19:58 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:19:57.551Z\",\n",
      "  \"batchId\" : 29,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.5220700152207,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 576,\n",
      "    \"commitOffsets\" : 31,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 6,\n",
      "    \"triggerExecution\" : 657,\n",
      "    \"walCommit\" : 41\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3260\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3261\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3261\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.5220700152207,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:19:58 INFO connection: Opened connection [connectionId{localValue:58, serverValue:4428}] to localhost:27017\n",
      "25/05/05 11:19:58 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.29.68caaaff-8e52-44e2-89c0-47419a75347d.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/29\n",
      "25/05/05 11:19:58 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:19:57.549Z\",\n",
      "  \"batchId\" : 29,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.506024096385542,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 585,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 9,\n",
      "    \"triggerExecution\" : 664,\n",
      "    \"walCommit\" : 38\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3260\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3261\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3261\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.506024096385542,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:19:58 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:19:58 INFO connection: Closed connection [connectionId{localValue:58, serverValue:4428}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:19:58 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 503042458 nanos, during time span of 517420584 nanos.\n",
      "25/05/05 11:19:58 INFO Executor: Finished task 0.0 in stage 88.0 (TID 88). 1645 bytes result sent to driver\n",
      "25/05/05 11:19:58 INFO TaskSetManager: Finished task 0.0 in stage 88.0 (TID 88) in 534 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:19:58 INFO TaskSchedulerImpl: Removed TaskSet 88.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:19:58 INFO DAGScheduler: ResultStage 88 (start at NativeMethodAccessorImpl.java:0) finished in 0.541 s\n",
      "25/05/05 11:19:58 INFO DAGScheduler: Job 89 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:19:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 88: Stage finished\n",
      "25/05/05 11:19:58 INFO DAGScheduler: Job 89 finished: start at NativeMethodAccessorImpl.java:0, took 0.542135 s\n",
      "25/05/05 11:19:58 INFO MemoryStore: Block broadcast_118 stored as values in memory (estimated size 248.0 B, free 366.2 MiB)\n",
      "25/05/05 11:19:58 INFO MemoryStore: Block broadcast_118_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.2 MiB)\n",
      "25/05/05 11:19:58 INFO BlockManagerInfo: Added broadcast_118_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:19:58 INFO SparkContext: Created broadcast 118 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:19:58 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/29 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.29.1589daac-ecae-421b-b02d-1b595d3954fe.tmp\n",
      "25/05/05 11:19:58 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.29.1589daac-ecae-421b-b02d-1b595d3954fe.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/29\n",
      "25/05/05 11:19:58 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:19:57.553Z\",\n",
      "  \"batchId\" : 29,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.4388489208633095,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 610,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 7,\n",
      "    \"triggerExecution\" : 695,\n",
      "    \"walCommit\" : 45\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3260\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3261\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3261\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.4388489208633095,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 29\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time           |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+----------------------+\n",
      "|A001|R079|01-00-00|WORLD TRADE CTR|05/05/2025|11:19:56|REGULAR|11         |10        |2025-05-05 11:19:57.55|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:20:04 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/30 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.30.135379a9-86b9-432c-9c88-2ea4cd81cd76.tmp\n",
      "25/05/05 11:20:04 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/30 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.30.fd5f95e1-d3bd-4854-b71b-f5e1286d6621.tmp\n",
      "25/05/05 11:20:04 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/30 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.30.8478c535-d90a-4a2b-a6bc-ad5097c5d20e.tmp\n",
      "25/05/05 11:20:04 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.30.8478c535-d90a-4a2b-a6bc-ad5097c5d20e.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/30\n",
      "25/05/05 11:20:04 INFO MicroBatchExecution: Committed offsets for batch 30. Metadata OffsetSeqMetadata(0,1746458404131,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:20:04 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.30.fd5f95e1-d3bd-4854-b71b-f5e1286d6621.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/30\n",
      "25/05/05 11:20:04 INFO MicroBatchExecution: Committed offsets for batch 30. Metadata OffsetSeqMetadata(0,1746458404131,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:20:04 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.30.135379a9-86b9-432c-9c88-2ea4cd81cd76.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/30\n",
      "25/05/05 11:20:04 INFO MicroBatchExecution: Committed offsets for batch 30. Metadata OffsetSeqMetadata(0,1746458404131,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:20:04 INFO BlockManagerInfo: Removed broadcast_117_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:04 INFO IncrementalExecution: Current batch timestamp = 1746458404131\n",
      "25/05/05 11:20:04 INFO IncrementalExecution: Current batch timestamp = 1746458404131\n",
      "25/05/05 11:20:04 INFO BlockManagerInfo: Removed broadcast_115_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:04 INFO IncrementalExecution: Current batch timestamp = 1746458404131\n",
      "25/05/05 11:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:04 INFO BlockManagerInfo: Removed broadcast_118_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:20:04 INFO IncrementalExecution: Current batch timestamp = 1746458404131\n",
      "25/05/05 11:20:04 INFO IncrementalExecution: Current batch timestamp = 1746458404131\n",
      "25/05/05 11:20:04 INFO IncrementalExecution: Current batch timestamp = 1746458404131\n",
      "25/05/05 11:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:04 INFO BlockManagerInfo: Removed broadcast_116_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:04 INFO IncrementalExecution: Current batch timestamp = 1746458404131\n",
      "25/05/05 11:20:04 INFO IncrementalExecution: Current batch timestamp = 1746458404131\n",
      "25/05/05 11:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:04 INFO CodeGenerator: Code generated in 4.707916 ms\n",
      "25/05/05 11:20:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 30, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@f77cbc1]. The input RDD has 1 partitions.\n",
      "25/05/05 11:20:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 30, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:20:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:04 INFO DAGScheduler: Got job 90 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:20:04 INFO DAGScheduler: Final stage: ResultStage 89 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:20:04 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:20:04 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:20:04 INFO DAGScheduler: Submitting ResultStage 89 (MapPartitionsRDD[490] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:20:04 INFO MemoryStore: Block broadcast_119 stored as values in memory (estimated size 16.2 KiB, free 366.3 MiB)\n",
      "25/05/05 11:20:04 INFO MemoryStore: Block broadcast_119_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.3 MiB)\n",
      "25/05/05 11:20:04 INFO BlockManagerInfo: Added broadcast_119_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:04 INFO SparkContext: Created broadcast 119 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:20:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 89 (MapPartitionsRDD[490] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:20:04 INFO TaskSchedulerImpl: Adding task set 89.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:20:04 INFO DAGScheduler: Got job 91 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:20:04 INFO DAGScheduler: Final stage: ResultStage 90 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:20:04 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:20:04 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:20:04 INFO DAGScheduler: Submitting ResultStage 90 (MapPartitionsRDD[491] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:20:04 INFO TaskSetManager: Starting task 0.0 in stage 89.0 (TID 89) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:20:04 INFO Executor: Running task 0.0 in stage 89.0 (TID 89)\n",
      "25/05/05 11:20:04 INFO MemoryStore: Block broadcast_120 stored as values in memory (estimated size 15.2 KiB, free 366.3 MiB)\n",
      "25/05/05 11:20:04 INFO MemoryStore: Block broadcast_120_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.3 MiB)\n",
      "25/05/05 11:20:04 INFO BlockManagerInfo: Added broadcast_120_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:04 INFO SparkContext: Created broadcast 120 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:20:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 90 (MapPartitionsRDD[491] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:20:04 INFO TaskSchedulerImpl: Adding task set 90.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:20:04 INFO TaskSetManager: Starting task 0.0 in stage 90.0 (TID 90) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:20:04 INFO Executor: Running task 0.0 in stage 90.0 (TID 90)\n",
      "25/05/05 11:20:04 INFO CodeGenerator: Code generated in 4.376375 ms\n",
      "25/05/05 11:20:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3261 untilOffset=3262, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=30 taskId=90 partitionId=0\n",
      "25/05/05 11:20:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3261 untilOffset=3262, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=30 taskId=89 partitionId=0\n",
      "25/05/05 11:20:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3261 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3261 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:04 INFO DAGScheduler: Got job 92 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:20:04 INFO DAGScheduler: Final stage: ResultStage 91 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:20:04 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:20:04 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:20:04 INFO DAGScheduler: Submitting ResultStage 91 (MapPartitionsRDD[496] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:20:04 INFO MemoryStore: Block broadcast_121 stored as values in memory (estimated size 47.1 KiB, free 366.2 MiB)\n",
      "25/05/05 11:20:04 INFO MemoryStore: Block broadcast_121_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:20:04 INFO BlockManagerInfo: Added broadcast_121_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:04 INFO SparkContext: Created broadcast 121 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:20:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 91 (MapPartitionsRDD[496] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:20:04 INFO TaskSchedulerImpl: Adding task set 91.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:20:04 INFO TaskSetManager: Starting task 0.0 in stage 91.0 (TID 91) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:20:04 INFO Executor: Running task 0.0 in stage 91.0 (TID 91)\n",
      "25/05/05 11:20:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3261 untilOffset=3262, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=30 taskId=91 partitionId=0\n",
      "25/05/05 11:20:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3261 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3262, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3262, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:04 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:20:04 INFO DataWritingSparkTask: Committed partition 0 (task 90, attempt 0, stage 90.0)\n",
      "25/05/05 11:20:04 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 502769000 nanos, during time span of 503185875 nanos.\n",
      "25/05/05 11:20:04 INFO Executor: Finished task 0.0 in stage 90.0 (TID 90). 2145 bytes result sent to driver\n",
      "25/05/05 11:20:04 INFO TaskSetManager: Finished task 0.0 in stage 90.0 (TID 90) in 511 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:20:04 INFO TaskSchedulerImpl: Removed TaskSet 90.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:20:04 INFO DAGScheduler: ResultStage 90 (start at NativeMethodAccessorImpl.java:0) finished in 0.513 s\n",
      "25/05/05 11:20:04 INFO DAGScheduler: Job 91 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:20:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 90: Stage finished\n",
      "25/05/05 11:20:04 INFO DAGScheduler: Job 91 finished: start at NativeMethodAccessorImpl.java:0, took 0.516100 s\n",
      "25/05/05 11:20:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 30, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:20:04 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:20:04 INFO DataWritingSparkTask: Committed partition 0 (task 89, attempt 0, stage 89.0)\n",
      "25/05/05 11:20:04 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 501849292 nanos, during time span of 503661292 nanos.\n",
      "25/05/05 11:20:04 INFO Executor: Finished task 0.0 in stage 89.0 (TID 89). 3516 bytes result sent to driver\n",
      "25/05/05 11:20:04 INFO TaskSetManager: Finished task 0.0 in stage 89.0 (TID 89) in 515 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:20:04 INFO TaskSchedulerImpl: Removed TaskSet 89.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:20:04 INFO DAGScheduler: ResultStage 89 (start at NativeMethodAccessorImpl.java:0) finished in 0.516 s\n",
      "25/05/05 11:20:04 INFO DAGScheduler: Job 90 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:20:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 89: Stage finished\n",
      "25/05/05 11:20:04 INFO DAGScheduler: Job 90 finished: start at NativeMethodAccessorImpl.java:0, took 0.517532 s\n",
      "25/05/05 11:20:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 30, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@f77cbc1] is committing.\n",
      "25/05/05 11:20:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 30, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@f77cbc1] committed.\n",
      "25/05/05 11:20:04 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/30 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.30.80409d83-bfc8-44e4-927f-892ce93fb4a8.tmp\n",
      "25/05/05 11:20:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 30, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:20:04 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/30 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.30.e00cae72-598f-474b-aaf3-cfb3e5c13bea.tmp\n",
      "25/05/05 11:20:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3262, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:04 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:20:04 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:20:04 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:20:04 INFO connection: Opened connection [connectionId{localValue:59, serverValue:4429}] to localhost:27017\n",
      "25/05/05 11:20:04 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=264958}\n",
      "25/05/05 11:20:04 INFO connection: Opened connection [connectionId{localValue:60, serverValue:4430}] to localhost:27017\n",
      "25/05/05 11:20:04 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:20:04 INFO connection: Closed connection [connectionId{localValue:60, serverValue:4430}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:20:04 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 505972584 nanos, during time span of 511841250 nanos.\n",
      "25/05/05 11:20:04 INFO Executor: Finished task 0.0 in stage 91.0 (TID 91). 1645 bytes result sent to driver\n",
      "25/05/05 11:20:04 INFO TaskSetManager: Finished task 0.0 in stage 91.0 (TID 91) in 518 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:20:04 INFO TaskSchedulerImpl: Removed TaskSet 91.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:20:04 INFO DAGScheduler: ResultStage 91 (start at NativeMethodAccessorImpl.java:0) finished in 0.522 s\n",
      "25/05/05 11:20:04 INFO DAGScheduler: Job 92 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:20:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 91: Stage finished\n",
      "25/05/05 11:20:04 INFO DAGScheduler: Job 92 finished: start at NativeMethodAccessorImpl.java:0, took 0.522803 s\n",
      "25/05/05 11:20:04 INFO MemoryStore: Block broadcast_122 stored as values in memory (estimated size 248.0 B, free 366.2 MiB)\n",
      "25/05/05 11:20:04 INFO MemoryStore: Block broadcast_122_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.2 MiB)\n",
      "25/05/05 11:20:04 INFO BlockManagerInfo: Added broadcast_122_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:20:04 INFO SparkContext: Created broadcast 122 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:04 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.30.80409d83-bfc8-44e4-927f-892ce93fb4a8.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/30\n",
      "25/05/05 11:20:04 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:20:04.130Z\",\n",
      "  \"batchId\" : 30,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "  \"processedRowsPerSecond\" : 1.6366612111292962,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 531,\n",
      "    \"commitOffsets\" : 33,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 611,\n",
      "    \"walCommit\" : 41\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3261\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3262\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3262\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "    \"processedRowsPerSecond\" : 1.6366612111292962,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:20:04 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.30.e00cae72-598f-474b-aaf3-cfb3e5c13bea.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/30\n",
      "25/05/05 11:20:04 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:20:04.130Z\",\n",
      "  \"batchId\" : 30,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.6313213703099512,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 537,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 613,\n",
      "    \"walCommit\" : 41\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3261\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3262\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3262\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.6313213703099512,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:20:04 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/30 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.30.440d9307-6b8e-4d30-b625-0505a231a1fe.tmp\n",
      "25/05/05 11:20:04 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.30.440d9307-6b8e-4d30-b625-0505a231a1fe.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/30\n",
      "25/05/05 11:20:04 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:20:04.130Z\",\n",
      "  \"batchId\" : 30,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.5649452269170578,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 557,\n",
      "    \"commitOffsets\" : 34,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 639,\n",
      "    \"walCommit\" : 41\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3261\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3262\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3262\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.5649452269170578,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 30\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|B013|R081|02-00-00|161/YANKEE STAD|05/05/2025|11:20:02|REGULAR|5          |12        |2025-05-05 11:20:04.131|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:20:10 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/31 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.31.42dc27ad-2e2b-4425-9c5d-26377ca5e59b.tmp\n",
      "25/05/05 11:20:10 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/31 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.31.07684819-d18c-41e1-b318-a271a0f94a4e.tmp\n",
      "25/05/05 11:20:10 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/31 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.31.ef663a72-b02c-42f3-8d80-f02fe4bfa4d6.tmp\n",
      "25/05/05 11:20:10 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.31.07684819-d18c-41e1-b318-a271a0f94a4e.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/31\n",
      "25/05/05 11:20:10 INFO MicroBatchExecution: Committed offsets for batch 31. Metadata OffsetSeqMetadata(0,1746458410680,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:20:10 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.31.42dc27ad-2e2b-4425-9c5d-26377ca5e59b.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/31\n",
      "25/05/05 11:20:10 INFO MicroBatchExecution: Committed offsets for batch 31. Metadata OffsetSeqMetadata(0,1746458410680,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:20:10 INFO IncrementalExecution: Current batch timestamp = 1746458410680\n",
      "25/05/05 11:20:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:10 INFO IncrementalExecution: Current batch timestamp = 1746458410680\n",
      "25/05/05 11:20:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:10 INFO IncrementalExecution: Current batch timestamp = 1746458410680\n",
      "25/05/05 11:20:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:10 INFO IncrementalExecution: Current batch timestamp = 1746458410680\n",
      "25/05/05 11:20:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:10 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.31.ef663a72-b02c-42f3-8d80-f02fe4bfa4d6.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/31\n",
      "25/05/05 11:20:10 INFO MicroBatchExecution: Committed offsets for batch 31. Metadata OffsetSeqMetadata(0,1746458410693,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:20:10 INFO IncrementalExecution: Current batch timestamp = 1746458410680\n",
      "25/05/05 11:20:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:10 INFO IncrementalExecution: Current batch timestamp = 1746458410680\n",
      "25/05/05 11:20:10 INFO IncrementalExecution: Current batch timestamp = 1746458410693\n",
      "25/05/05 11:20:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:10 INFO IncrementalExecution: Current batch timestamp = 1746458410693\n",
      "25/05/05 11:20:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:10 INFO CodeGenerator: Code generated in 6.93275 ms\n",
      "25/05/05 11:20:10 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 31, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@1918a91]. The input RDD has 1 partitions.\n",
      "25/05/05 11:20:10 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 31, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:20:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:10 INFO DAGScheduler: Got job 93 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:20:10 INFO DAGScheduler: Final stage: ResultStage 92 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:20:10 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:20:10 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:20:10 INFO DAGScheduler: Submitting ResultStage 92 (MapPartitionsRDD[502] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:20:10 INFO MemoryStore: Block broadcast_123 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:20:10 INFO MemoryStore: Block broadcast_123_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:20:10 INFO CodeGenerator: Code generated in 6.246333 ms\n",
      "25/05/05 11:20:10 INFO BlockManagerInfo: Added broadcast_123_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:10 INFO SparkContext: Created broadcast 123 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:20:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 92 (MapPartitionsRDD[502] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:20:10 INFO TaskSchedulerImpl: Adding task set 92.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:20:10 INFO DAGScheduler: Got job 94 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:20:10 INFO DAGScheduler: Final stage: ResultStage 93 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:20:10 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:20:10 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:20:10 INFO DAGScheduler: Submitting ResultStage 93 (MapPartitionsRDD[503] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:20:10 INFO MemoryStore: Block broadcast_124 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:20:10 INFO TaskSetManager: Starting task 0.0 in stage 92.0 (TID 92) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:20:10 INFO Executor: Running task 0.0 in stage 92.0 (TID 92)\n",
      "25/05/05 11:20:10 INFO MemoryStore: Block broadcast_124_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:20:10 INFO BlockManagerInfo: Added broadcast_124_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:10 INFO SparkContext: Created broadcast 124 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:20:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 93 (MapPartitionsRDD[503] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:20:10 INFO TaskSchedulerImpl: Adding task set 93.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:20:10 INFO TaskSetManager: Starting task 0.0 in stage 93.0 (TID 93) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:20:10 INFO Executor: Running task 0.0 in stage 93.0 (TID 93)\n",
      "25/05/05 11:20:10 INFO CodeGenerator: Code generated in 8.4455 ms\n",
      "25/05/05 11:20:10 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3262 untilOffset=3263, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=31 taskId=93 partitionId=0\n",
      "25/05/05 11:20:10 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3262 untilOffset=3263, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=31 taskId=92 partitionId=0\n",
      "25/05/05 11:20:10 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3262 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:10 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3262 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:10 INFO DAGScheduler: Got job 95 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:20:10 INFO DAGScheduler: Final stage: ResultStage 94 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:20:10 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:20:10 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:20:10 INFO DAGScheduler: Submitting ResultStage 94 (MapPartitionsRDD[512] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:20:10 INFO MemoryStore: Block broadcast_125 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:20:10 INFO MemoryStore: Block broadcast_125_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.1 MiB)\n",
      "25/05/05 11:20:10 INFO BlockManagerInfo: Added broadcast_125_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:20:10 INFO SparkContext: Created broadcast 125 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:20:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 94 (MapPartitionsRDD[512] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:20:10 INFO BlockManagerInfo: Removed broadcast_122_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:20:10 INFO TaskSchedulerImpl: Adding task set 94.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:20:10 INFO TaskSetManager: Starting task 0.0 in stage 94.0 (TID 94) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:20:10 INFO Executor: Running task 0.0 in stage 94.0 (TID 94)\n",
      "25/05/05 11:20:10 INFO BlockManagerInfo: Removed broadcast_120_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:20:10 INFO BlockManagerInfo: Removed broadcast_119_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:10 INFO BlockManagerInfo: Removed broadcast_121_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:10 INFO CodeGenerator: Code generated in 5.072166 ms\n",
      "25/05/05 11:20:10 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3262 untilOffset=3263, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=31 taskId=94 partitionId=0\n",
      "25/05/05 11:20:10 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3262 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3263, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3263, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:11 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:20:11 INFO DataWritingSparkTask: Committed partition 0 (task 93, attempt 0, stage 93.0)\n",
      "25/05/05 11:20:11 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 506707625 nanos, during time span of 507316167 nanos.\n",
      "25/05/05 11:20:11 INFO Executor: Finished task 0.0 in stage 93.0 (TID 93). 2231 bytes result sent to driver\n",
      "25/05/05 11:20:11 INFO TaskSetManager: Finished task 0.0 in stage 93.0 (TID 93) in 527 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:20:11 INFO TaskSchedulerImpl: Removed TaskSet 93.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:20:11 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:20:11 INFO DAGScheduler: ResultStage 93 (start at NativeMethodAccessorImpl.java:0) finished in 0.531 s\n",
      "25/05/05 11:20:11 INFO DataWritingSparkTask: Committed partition 0 (task 92, attempt 0, stage 92.0)\n",
      "25/05/05 11:20:11 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 503801000 nanos, during time span of 505950083 nanos.\n",
      "25/05/05 11:20:11 INFO DAGScheduler: Job 94 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:20:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 93: Stage finished\n",
      "25/05/05 11:20:11 INFO DAGScheduler: Job 94 finished: start at NativeMethodAccessorImpl.java:0, took 0.535633 s\n",
      "25/05/05 11:20:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 31, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:20:11 INFO Executor: Finished task 0.0 in stage 92.0 (TID 92). 3602 bytes result sent to driver\n",
      "25/05/05 11:20:11 INFO TaskSetManager: Finished task 0.0 in stage 92.0 (TID 92) in 532 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:20:11 INFO TaskSchedulerImpl: Removed TaskSet 92.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:20:11 INFO DAGScheduler: ResultStage 92 (start at NativeMethodAccessorImpl.java:0) finished in 0.536 s\n",
      "25/05/05 11:20:11 INFO DAGScheduler: Job 93 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:20:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 92: Stage finished\n",
      "25/05/05 11:20:11 INFO DAGScheduler: Job 93 finished: start at NativeMethodAccessorImpl.java:0, took 0.536962 s\n",
      "25/05/05 11:20:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 31, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@1918a91] is committing.\n",
      "25/05/05 11:20:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 31, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@1918a91] committed.\n",
      "25/05/05 11:20:11 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/31 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.31.2d709a9f-8aa4-4698-9129-f8e77029917e.tmp\n",
      "25/05/05 11:20:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 31, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:20:11 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/31 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.31.dbacff0b-fac3-42cf-bc02-7c5ee4948427.tmp\n",
      "25/05/05 11:20:11 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.31.2d709a9f-8aa4-4698-9129-f8e77029917e.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/31\n",
      "25/05/05 11:20:11 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:20:10.679Z\",\n",
      "  \"batchId\" : 31,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.557632398753894,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 564,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 7,\n",
      "    \"triggerExecution\" : 642,\n",
      "    \"walCommit\" : 40\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3262\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3263\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3263\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.557632398753894,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:20:11 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.31.dbacff0b-fac3-42cf-bc02-7c5ee4948427.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/31\n",
      "25/05/05 11:20:11 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:20:10.679Z\",\n",
      "  \"batchId\" : 31,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.5455950540958268,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 570,\n",
      "    \"commitOffsets\" : 26,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 6,\n",
      "    \"triggerExecution\" : 647,\n",
      "    \"walCommit\" : 43\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3262\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3263\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3263\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.5455950540958268,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:20:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3263, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:11 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:20:11 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:20:11 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:20:11 INFO connection: Opened connection [connectionId{localValue:61, serverValue:4431}] to localhost:27017\n",
      "25/05/05 11:20:11 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=270792}\n",
      "25/05/05 11:20:11 INFO connection: Opened connection [connectionId{localValue:62, serverValue:4432}] to localhost:27017\n",
      "25/05/05 11:20:11 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:20:11 INFO connection: Closed connection [connectionId{localValue:62, serverValue:4432}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:20:11 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 506138375 nanos, during time span of 515193250 nanos.\n",
      "25/05/05 11:20:11 INFO Executor: Finished task 0.0 in stage 94.0 (TID 94). 1645 bytes result sent to driver\n",
      "25/05/05 11:20:11 INFO TaskSetManager: Finished task 0.0 in stage 94.0 (TID 94) in 530 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:20:11 INFO TaskSchedulerImpl: Removed TaskSet 94.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:20:11 INFO DAGScheduler: ResultStage 94 (start at NativeMethodAccessorImpl.java:0) finished in 0.548 s\n",
      "25/05/05 11:20:11 INFO DAGScheduler: Job 95 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:20:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 94: Stage finished\n",
      "25/05/05 11:20:11 INFO DAGScheduler: Job 95 finished: start at NativeMethodAccessorImpl.java:0, took 0.548748 s\n",
      "25/05/05 11:20:11 INFO MemoryStore: Block broadcast_126 stored as values in memory (estimated size 248.0 B, free 366.2 MiB)\n",
      "25/05/05 11:20:11 INFO MemoryStore: Block broadcast_126_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.2 MiB)\n",
      "25/05/05 11:20:11 INFO BlockManagerInfo: Added broadcast_126_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:20:11 INFO SparkContext: Created broadcast 126 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:11 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/31 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.31.4b16c94a-22f0-4a79-b370-7507b6cdc157.tmp\n",
      "25/05/05 11:20:11 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.31.4b16c94a-22f0-4a79-b370-7507b6cdc157.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/31\n",
      "25/05/05 11:20:11 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:20:10.692Z\",\n",
      "  \"batchId\" : 31,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.4598540145985401,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 607,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 7,\n",
      "    \"triggerExecution\" : 685,\n",
      "    \"walCommit\" : 41\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3262\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3263\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3263\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.4598540145985401,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 31\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time           |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+----------------------+\n",
      "|B013|R081|02-00-00|161/YANKEE STAD|05/05/2025|11:20:09|REGULAR|12         |10        |2025-05-05 11:20:10.68|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:20:17 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/32 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.32.97ffbce1-de87-40e8-b0fc-25771cc7832b.tmp\n",
      "25/05/05 11:20:17 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/32 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.32.ad6ee655-e855-4a07-8341-a58971144052.tmp\n",
      "25/05/05 11:20:17 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/32 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.32.a30d9f82-f331-46cc-9b23-40b4325597fa.tmp\n",
      "25/05/05 11:20:17 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.32.97ffbce1-de87-40e8-b0fc-25771cc7832b.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/32\n",
      "25/05/05 11:20:17 INFO MicroBatchExecution: Committed offsets for batch 32. Metadata OffsetSeqMetadata(0,1746458417206,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:20:17 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.32.ad6ee655-e855-4a07-8341-a58971144052.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/32\n",
      "25/05/05 11:20:17 INFO MicroBatchExecution: Committed offsets for batch 32. Metadata OffsetSeqMetadata(0,1746458417207,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:20:17 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.32.a30d9f82-f331-46cc-9b23-40b4325597fa.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/32\n",
      "25/05/05 11:20:17 INFO MicroBatchExecution: Committed offsets for batch 32. Metadata OffsetSeqMetadata(0,1746458417209,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:20:17 INFO IncrementalExecution: Current batch timestamp = 1746458417206\n",
      "25/05/05 11:20:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:17 INFO IncrementalExecution: Current batch timestamp = 1746458417209\n",
      "25/05/05 11:20:17 INFO IncrementalExecution: Current batch timestamp = 1746458417207\n",
      "25/05/05 11:20:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:17 INFO IncrementalExecution: Current batch timestamp = 1746458417209\n",
      "25/05/05 11:20:17 INFO IncrementalExecution: Current batch timestamp = 1746458417206\n",
      "25/05/05 11:20:17 INFO IncrementalExecution: Current batch timestamp = 1746458417207\n",
      "25/05/05 11:20:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:17 INFO IncrementalExecution: Current batch timestamp = 1746458417209\n",
      "25/05/05 11:20:17 INFO IncrementalExecution: Current batch timestamp = 1746458417207\n",
      "25/05/05 11:20:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:17 INFO CodeGenerator: Code generated in 5.550125 ms\n",
      "25/05/05 11:20:17 INFO CodeGenerator: Code generated in 4.215333 ms\n",
      "25/05/05 11:20:17 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 32, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@211e7f28]. The input RDD has 1 partitions.\n",
      "25/05/05 11:20:17 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:17 INFO CodeGenerator: Code generated in 6.092708 ms\n",
      "25/05/05 11:20:17 INFO DAGScheduler: Got job 96 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:20:17 INFO DAGScheduler: Final stage: ResultStage 95 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:20:17 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:20:17 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:20:17 INFO DAGScheduler: Submitting ResultStage 95 (MapPartitionsRDD[520] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:20:17 INFO MemoryStore: Block broadcast_127 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:20:17 INFO MemoryStore: Block broadcast_127_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:20:17 INFO BlockManagerInfo: Added broadcast_127_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:17 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 32, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:20:17 INFO SparkContext: Created broadcast 127 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:20:17 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 95 (MapPartitionsRDD[520] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:20:17 INFO TaskSchedulerImpl: Adding task set 95.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:20:17 INFO DAGScheduler: Got job 97 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:20:17 INFO DAGScheduler: Final stage: ResultStage 96 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:20:17 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:20:17 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:20:17 INFO DAGScheduler: Submitting ResultStage 96 (MapPartitionsRDD[523] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:20:17 INFO TaskSetManager: Starting task 0.0 in stage 95.0 (TID 95) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:20:17 INFO Executor: Running task 0.0 in stage 95.0 (TID 95)\n",
      "25/05/05 11:20:17 INFO MemoryStore: Block broadcast_128 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:20:17 INFO MemoryStore: Block broadcast_128_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:20:17 INFO BlockManagerInfo: Added broadcast_128_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:17 INFO SparkContext: Created broadcast 128 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:20:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 96 (MapPartitionsRDD[523] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:20:17 INFO TaskSchedulerImpl: Adding task set 96.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:20:17 INFO TaskSetManager: Starting task 0.0 in stage 96.0 (TID 96) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:20:17 INFO Executor: Running task 0.0 in stage 96.0 (TID 96)\n",
      "25/05/05 11:20:17 INFO CodeGenerator: Code generated in 4.647 ms\n",
      "25/05/05 11:20:17 INFO CodeGenerator: Code generated in 4.408208 ms\n",
      "25/05/05 11:20:17 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3263 untilOffset=3264, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=32 taskId=96 partitionId=0\n",
      "25/05/05 11:20:17 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3263 untilOffset=3264, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=32 taskId=95 partitionId=0\n",
      "25/05/05 11:20:17 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3263 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:17 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3263 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:17 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:17 INFO DAGScheduler: Got job 98 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:20:17 INFO DAGScheduler: Final stage: ResultStage 97 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:20:17 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:20:17 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:20:17 INFO DAGScheduler: Submitting ResultStage 97 (MapPartitionsRDD[528] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:20:17 INFO MemoryStore: Block broadcast_129 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:20:17 INFO MemoryStore: Block broadcast_129_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.1 MiB)\n",
      "25/05/05 11:20:17 INFO BlockManagerInfo: Added broadcast_129_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:20:17 INFO SparkContext: Created broadcast 129 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:20:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 97 (MapPartitionsRDD[528] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:20:17 INFO TaskSchedulerImpl: Adding task set 97.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:20:17 INFO TaskSetManager: Starting task 0.0 in stage 97.0 (TID 97) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:20:17 INFO Executor: Running task 0.0 in stage 97.0 (TID 97)\n",
      "25/05/05 11:20:17 INFO CodeGenerator: Code generated in 4.057875 ms\n",
      "25/05/05 11:20:17 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3263 untilOffset=3264, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=32 taskId=97 partitionId=0\n",
      "25/05/05 11:20:17 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3263 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3264, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3264, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:17 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:20:17 INFO DataWritingSparkTask: Committed partition 0 (task 96, attempt 0, stage 96.0)\n",
      "25/05/05 11:20:17 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 505128875 nanos, during time span of 505508167 nanos.\n",
      "25/05/05 11:20:17 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:20:17 INFO DataWritingSparkTask: Committed partition 0 (task 95, attempt 0, stage 95.0)\n",
      "25/05/05 11:20:17 INFO Executor: Finished task 0.0 in stage 96.0 (TID 96). 2145 bytes result sent to driver\n",
      "25/05/05 11:20:17 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 504192209 nanos, during time span of 506156667 nanos.\n",
      "25/05/05 11:20:17 INFO Executor: Finished task 0.0 in stage 95.0 (TID 95). 3514 bytes result sent to driver\n",
      "25/05/05 11:20:17 INFO TaskSetManager: Finished task 0.0 in stage 96.0 (TID 96) in 516 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:20:17 INFO TaskSchedulerImpl: Removed TaskSet 96.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:20:17 INFO TaskSetManager: Finished task 0.0 in stage 95.0 (TID 95) in 518 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:20:17 INFO TaskSchedulerImpl: Removed TaskSet 95.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:20:17 INFO DAGScheduler: ResultStage 96 (start at NativeMethodAccessorImpl.java:0) finished in 0.517 s\n",
      "25/05/05 11:20:17 INFO DAGScheduler: Job 97 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:20:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 96: Stage finished\n",
      "25/05/05 11:20:17 INFO DAGScheduler: ResultStage 95 (start at NativeMethodAccessorImpl.java:0) finished in 0.520 s\n",
      "25/05/05 11:20:17 INFO DAGScheduler: Job 96 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:20:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 95: Stage finished\n",
      "25/05/05 11:20:17 INFO DAGScheduler: Job 97 finished: start at NativeMethodAccessorImpl.java:0, took 0.518648 s\n",
      "25/05/05 11:20:17 INFO DAGScheduler: Job 96 finished: start at NativeMethodAccessorImpl.java:0, took 0.520677 s\n",
      "25/05/05 11:20:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 32, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:20:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 32, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@211e7f28] is committing.\n",
      "25/05/05 11:20:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 32, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@211e7f28] committed.\n",
      "25/05/05 11:20:17 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/32 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.32.8eb8e12f-c87e-40fa-982e-45c75f6936c4.tmp\n",
      "25/05/05 11:20:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 32, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:20:17 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/32 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.32.be393d88-3685-42f6-a929-e16a3d0beaf5.tmp\n",
      "25/05/05 11:20:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3264, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:17 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:20:17 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:20:17 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:20:17 INFO connection: Opened connection [connectionId{localValue:63, serverValue:4433}] to localhost:27017\n",
      "25/05/05 11:20:17 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=290542}\n",
      "25/05/05 11:20:17 INFO connection: Opened connection [connectionId{localValue:64, serverValue:4434}] to localhost:27017\n",
      "25/05/05 11:20:17 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:20:17 INFO connection: Closed connection [connectionId{localValue:64, serverValue:4434}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:20:17 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 508463167 nanos, during time span of 513310375 nanos.\n",
      "25/05/05 11:20:17 INFO Executor: Finished task 0.0 in stage 97.0 (TID 97). 1645 bytes result sent to driver\n",
      "25/05/05 11:20:17 INFO TaskSetManager: Finished task 0.0 in stage 97.0 (TID 97) in 525 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:20:17 INFO TaskSchedulerImpl: Removed TaskSet 97.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:20:17 INFO DAGScheduler: ResultStage 97 (start at NativeMethodAccessorImpl.java:0) finished in 0.529 s\n",
      "25/05/05 11:20:17 INFO DAGScheduler: Job 98 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:20:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 97: Stage finished\n",
      "25/05/05 11:20:17 INFO DAGScheduler: Job 98 finished: start at NativeMethodAccessorImpl.java:0, took 0.530019 s\n",
      "25/05/05 11:20:17 INFO MemoryStore: Block broadcast_130 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:20:17 INFO MemoryStore: Block broadcast_130_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:20:17 INFO BlockManagerInfo: Added broadcast_130_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:20:17 INFO SparkContext: Created broadcast 130 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:17 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.32.8eb8e12f-c87e-40fa-982e-45c75f6936c4.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/32\n",
      "25/05/05 11:20:17 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:20:17.208Z\",\n",
      "  \"batchId\" : 32,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 62.5,\n",
      "  \"processedRowsPerSecond\" : 1.6025641025641026,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 539,\n",
      "    \"commitOffsets\" : 33,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 624,\n",
      "    \"walCommit\" : 47\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3263\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3264\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3264\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 62.5,\n",
      "    \"processedRowsPerSecond\" : 1.6025641025641026,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:20:17 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/32 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.32.98403553-771e-4f8a-9c8e-88a03d995498.tmp\n",
      "25/05/05 11:20:17 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.32.be393d88-3685-42f6-a929-e16a3d0beaf5.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/32\n",
      "25/05/05 11:20:17 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:20:17.206Z\",\n",
      "  \"batchId\" : 32,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.5822784810126582,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 548,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 632,\n",
      "    \"walCommit\" : 49\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3263\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3264\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3264\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.5822784810126582,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:20:17 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.32.98403553-771e-4f8a-9c8e-88a03d995498.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/32\n",
      "25/05/05 11:20:17 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:20:17.205Z\",\n",
      "  \"batchId\" : 32,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.5267175572519083,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 567,\n",
      "    \"commitOffsets\" : 32,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 655,\n",
      "    \"walCommit\" : 48\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3263\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3264\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3264\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.5267175572519083,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 32\n",
      "-------------------------------------------\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION      |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R016|R032|00-00-01|TIME SQ-42 ST|05/05/2025|11:20:16|REGULAR|12         |10        |2025-05-05 11:20:17.207|\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:20:18 INFO BlockManagerInfo: Removed broadcast_129_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:18 INFO BlockManagerInfo: Removed broadcast_127_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:18 INFO BlockManagerInfo: Removed broadcast_128_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:18 INFO BlockManagerInfo: Removed broadcast_125_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:18 INFO BlockManagerInfo: Removed broadcast_124_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:18 INFO BlockManagerInfo: Removed broadcast_126_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:20:18 INFO BlockManagerInfo: Removed broadcast_123_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:18 INFO BlockManagerInfo: Removed broadcast_130_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:20:21 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/33 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.33.ccbd4302-0861-4d6f-b92f-028c64d90515.tmp\n",
      "25/05/05 11:20:21 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/33 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.33.ab2c78b0-f1ec-4180-8832-74bba33776ab.tmp\n",
      "25/05/05 11:20:21 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/33 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.33.34b9c49c-4c20-44d6-bb6b-f808806a8ea3.tmp\n",
      "25/05/05 11:20:21 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.33.ccbd4302-0861-4d6f-b92f-028c64d90515.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/33\n",
      "25/05/05 11:20:21 INFO MicroBatchExecution: Committed offsets for batch 33. Metadata OffsetSeqMetadata(0,1746458421727,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:20:21 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.33.ab2c78b0-f1ec-4180-8832-74bba33776ab.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/33\n",
      "25/05/05 11:20:21 INFO MicroBatchExecution: Committed offsets for batch 33. Metadata OffsetSeqMetadata(0,1746458421741,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:20:21 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.33.34b9c49c-4c20-44d6-bb6b-f808806a8ea3.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/33\n",
      "25/05/05 11:20:21 INFO MicroBatchExecution: Committed offsets for batch 33. Metadata OffsetSeqMetadata(0,1746458421741,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:20:21 INFO IncrementalExecution: Current batch timestamp = 1746458421727\n",
      "25/05/05 11:20:21 INFO IncrementalExecution: Current batch timestamp = 1746458421741\n",
      "25/05/05 11:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:21 INFO IncrementalExecution: Current batch timestamp = 1746458421741\n",
      "25/05/05 11:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:21 INFO IncrementalExecution: Current batch timestamp = 1746458421741\n",
      "25/05/05 11:20:21 INFO IncrementalExecution: Current batch timestamp = 1746458421741\n",
      "25/05/05 11:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:21 INFO IncrementalExecution: Current batch timestamp = 1746458421727\n",
      "25/05/05 11:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:21 INFO IncrementalExecution: Current batch timestamp = 1746458421741\n",
      "25/05/05 11:20:21 INFO IncrementalExecution: Current batch timestamp = 1746458421741\n",
      "25/05/05 11:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:21 INFO CodeGenerator: Code generated in 4.373375 ms\n",
      "25/05/05 11:20:21 INFO CodeGenerator: Code generated in 4.51675 ms\n",
      "25/05/05 11:20:21 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 33, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:20:21 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 33, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@5581b32a]. The input RDD has 1 partitions.\n",
      "25/05/05 11:20:21 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:21 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:21 INFO DAGScheduler: Got job 99 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:20:21 INFO DAGScheduler: Final stage: ResultStage 98 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:20:21 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:20:21 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:20:21 INFO DAGScheduler: Submitting ResultStage 98 (MapPartitionsRDD[538] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:20:21 INFO MemoryStore: Block broadcast_131 stored as values in memory (estimated size 15.2 KiB, free 366.3 MiB)\n",
      "25/05/05 11:20:21 INFO MemoryStore: Block broadcast_131_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.3 MiB)\n",
      "25/05/05 11:20:21 INFO BlockManagerInfo: Added broadcast_131_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:21 INFO SparkContext: Created broadcast 131 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:20:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 98 (MapPartitionsRDD[538] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:20:21 INFO TaskSchedulerImpl: Adding task set 98.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:20:21 INFO DAGScheduler: Got job 100 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:20:21 INFO DAGScheduler: Final stage: ResultStage 99 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:20:21 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:20:21 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:20:21 INFO DAGScheduler: Submitting ResultStage 99 (MapPartitionsRDD[539] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:20:21 INFO TaskSetManager: Starting task 0.0 in stage 98.0 (TID 98) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:20:21 INFO Executor: Running task 0.0 in stage 98.0 (TID 98)\n",
      "25/05/05 11:20:21 INFO MemoryStore: Block broadcast_132 stored as values in memory (estimated size 16.2 KiB, free 366.3 MiB)\n",
      "25/05/05 11:20:21 INFO MemoryStore: Block broadcast_132_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.3 MiB)\n",
      "25/05/05 11:20:21 INFO BlockManagerInfo: Added broadcast_132_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:21 INFO SparkContext: Created broadcast 132 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:20:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 99 (MapPartitionsRDD[539] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:20:21 INFO TaskSchedulerImpl: Adding task set 99.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:20:21 INFO TaskSetManager: Starting task 0.0 in stage 99.0 (TID 99) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:20:21 INFO Executor: Running task 0.0 in stage 99.0 (TID 99)\n",
      "25/05/05 11:20:21 INFO CodeGenerator: Code generated in 4.09375 ms\n",
      "25/05/05 11:20:21 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3264 untilOffset=3265, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=33 taskId=98 partitionId=0\n",
      "25/05/05 11:20:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3264 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:21 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3264 untilOffset=3265, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=33 taskId=99 partitionId=0\n",
      "25/05/05 11:20:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3264 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:21 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:21 INFO DAGScheduler: Got job 101 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:20:21 INFO DAGScheduler: Final stage: ResultStage 100 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:20:21 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:20:21 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:20:21 INFO DAGScheduler: Submitting ResultStage 100 (MapPartitionsRDD[544] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:20:21 INFO MemoryStore: Block broadcast_133 stored as values in memory (estimated size 47.1 KiB, free 366.2 MiB)\n",
      "25/05/05 11:20:21 INFO MemoryStore: Block broadcast_133_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:20:21 INFO BlockManagerInfo: Added broadcast_133_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:21 INFO SparkContext: Created broadcast 133 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:20:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 100 (MapPartitionsRDD[544] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:20:21 INFO TaskSchedulerImpl: Adding task set 100.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:20:21 INFO TaskSetManager: Starting task 0.0 in stage 100.0 (TID 100) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:20:21 INFO Executor: Running task 0.0 in stage 100.0 (TID 100)\n",
      "25/05/05 11:20:21 INFO CodeGenerator: Code generated in 4.107375 ms\n",
      "25/05/05 11:20:21 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3264 untilOffset=3265, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=33 taskId=100 partitionId=0\n",
      "25/05/05 11:20:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3264 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3265, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3265, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:22 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:20:22 INFO DataWritingSparkTask: Committed partition 0 (task 98, attempt 0, stage 98.0)\n",
      "25/05/05 11:20:22 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 505956708 nanos, during time span of 506382917 nanos.\n",
      "25/05/05 11:20:22 INFO Executor: Finished task 0.0 in stage 98.0 (TID 98). 2137 bytes result sent to driver\n",
      "25/05/05 11:20:22 INFO TaskSetManager: Finished task 0.0 in stage 98.0 (TID 98) in 515 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:20:22 INFO TaskSchedulerImpl: Removed TaskSet 98.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:20:22 INFO DAGScheduler: ResultStage 98 (start at NativeMethodAccessorImpl.java:0) finished in 0.517 s\n",
      "25/05/05 11:20:22 INFO DAGScheduler: Job 99 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:20:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 98: Stage finished\n",
      "25/05/05 11:20:22 INFO DAGScheduler: Job 99 finished: start at NativeMethodAccessorImpl.java:0, took 0.518056 s\n",
      "25/05/05 11:20:22 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:20:22 INFO DataWritingSparkTask: Committed partition 0 (task 99, attempt 0, stage 99.0)\n",
      "25/05/05 11:20:22 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 503636334 nanos, during time span of 505334125 nanos.\n",
      "25/05/05 11:20:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 33, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:20:22 INFO Executor: Finished task 0.0 in stage 99.0 (TID 99). 3507 bytes result sent to driver\n",
      "25/05/05 11:20:22 INFO TaskSetManager: Finished task 0.0 in stage 99.0 (TID 99) in 515 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:20:22 INFO TaskSchedulerImpl: Removed TaskSet 99.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:20:22 INFO DAGScheduler: ResultStage 99 (start at NativeMethodAccessorImpl.java:0) finished in 0.516 s\n",
      "25/05/05 11:20:22 INFO DAGScheduler: Job 100 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:20:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 99: Stage finished\n",
      "25/05/05 11:20:22 INFO DAGScheduler: Job 100 finished: start at NativeMethodAccessorImpl.java:0, took 0.518975 s\n",
      "25/05/05 11:20:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 33, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@5581b32a] is committing.\n",
      "25/05/05 11:20:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 33, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@5581b32a] committed.\n",
      "25/05/05 11:20:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 33, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:20:22 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/33 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.33.1c7f8d70-263c-4c4b-a631-c705b93332c3.tmp\n",
      "25/05/05 11:20:22 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/33 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.33.ad436d51-cb7a-4b17-9c86-a84f220165d8.tmp\n",
      "25/05/05 11:20:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3265, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:22 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:20:22 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:20:22 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:20:22 INFO connection: Opened connection [connectionId{localValue:65, serverValue:4435}] to localhost:27017\n",
      "25/05/05 11:20:22 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=301042}\n",
      "25/05/05 11:20:22 INFO connection: Opened connection [connectionId{localValue:66, serverValue:4436}] to localhost:27017\n",
      "25/05/05 11:20:22 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:20:22 INFO connection: Closed connection [connectionId{localValue:66, serverValue:4436}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:20:22 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 505076292 nanos, during time span of 509915917 nanos.\n",
      "25/05/05 11:20:22 INFO Executor: Finished task 0.0 in stage 100.0 (TID 100). 1645 bytes result sent to driver\n",
      "25/05/05 11:20:22 INFO TaskSetManager: Finished task 0.0 in stage 100.0 (TID 100) in 521 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:20:22 INFO TaskSchedulerImpl: Removed TaskSet 100.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:20:22 INFO DAGScheduler: ResultStage 100 (start at NativeMethodAccessorImpl.java:0) finished in 0.524 s\n",
      "25/05/05 11:20:22 INFO DAGScheduler: Job 101 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:20:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 100: Stage finished\n",
      "25/05/05 11:20:22 INFO DAGScheduler: Job 101 finished: start at NativeMethodAccessorImpl.java:0, took 0.525974 s\n",
      "25/05/05 11:20:22 INFO MemoryStore: Block broadcast_134 stored as values in memory (estimated size 248.0 B, free 366.2 MiB)\n",
      "25/05/05 11:20:22 INFO MemoryStore: Block broadcast_134_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.2 MiB)\n",
      "25/05/05 11:20:22 INFO BlockManagerInfo: Added broadcast_134_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:20:22 INFO SparkContext: Created broadcast 134 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:22 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.33.1c7f8d70-263c-4c4b-a631-c705b93332c3.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/33\n",
      "25/05/05 11:20:22 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/33 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.33.9747ac41-2e3b-4e29-8107-40d00fb1d77b.tmp\n",
      "25/05/05 11:20:22 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:20:21.738Z\",\n",
      "  \"batchId\" : 33,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 40.0,\n",
      "  \"processedRowsPerSecond\" : 1.5923566878980893,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 537,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 628,\n",
      "    \"walCommit\" : 55\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3264\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3265\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3265\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 40.0,\n",
      "    \"processedRowsPerSecond\" : 1.5923566878980893,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:20:22 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.33.ad436d51-cb7a-4b17-9c86-a84f220165d8.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/33\n",
      "25/05/05 11:20:22 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:20:21.738Z\",\n",
      "  \"batchId\" : 33,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 40.0,\n",
      "  \"processedRowsPerSecond\" : 1.5772870662460567,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 543,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 634,\n",
      "    \"walCommit\" : 56\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3264\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3265\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3265\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 40.0,\n",
      "    \"processedRowsPerSecond\" : 1.5772870662460567,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:20:22 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.33.9747ac41-2e3b-4e29-8107-40d00fb1d77b.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/33\n",
      "25/05/05 11:20:22 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:20:21.726Z\",\n",
      "  \"batchId\" : 33,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.51285930408472,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 559,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 661,\n",
      "    \"walCommit\" : 68\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3264\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3265\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3265\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.51285930408472,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 33\n",
      "-------------------------------------------\n",
      "+----+----+--------+-------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION|DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+-------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A004|R033|00-00-00|125 ST |05/05/2025|11:20:20|REGULAR|6          |8         |2025-05-05 11:20:21.741|\n",
      "+----+----+--------+-------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:20:28 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/34 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.34.1e4e89b3-788e-441e-84d2-8d78b6ed265c.tmp\n",
      "25/05/05 11:20:28 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/34 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.34.1f375fa7-62fe-4a85-a69d-57df4c4bd789.tmp\n",
      "25/05/05 11:20:28 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/34 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.34.98fc9a99-1d1d-47ea-ba43-7126bd98368a.tmp\n",
      "25/05/05 11:20:28 INFO BlockManagerInfo: Removed broadcast_134_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:20:28 INFO BlockManagerInfo: Removed broadcast_133_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:28 INFO BlockManagerInfo: Removed broadcast_131_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:28 INFO BlockManagerInfo: Removed broadcast_132_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:28 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.34.98fc9a99-1d1d-47ea-ba43-7126bd98368a.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/34\n",
      "25/05/05 11:20:28 INFO MicroBatchExecution: Committed offsets for batch 34. Metadata OffsetSeqMetadata(0,1746458428253,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:20:28 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.34.1e4e89b3-788e-441e-84d2-8d78b6ed265c.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/34\n",
      "25/05/05 11:20:28 INFO MicroBatchExecution: Committed offsets for batch 34. Metadata OffsetSeqMetadata(0,1746458428255,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:20:28 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.34.1f375fa7-62fe-4a85-a69d-57df4c4bd789.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/34\n",
      "25/05/05 11:20:28 INFO MicroBatchExecution: Committed offsets for batch 34. Metadata OffsetSeqMetadata(0,1746458428253,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:20:28 INFO IncrementalExecution: Current batch timestamp = 1746458428253\n",
      "25/05/05 11:20:28 INFO IncrementalExecution: Current batch timestamp = 1746458428255\n",
      "25/05/05 11:20:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:28 INFO IncrementalExecution: Current batch timestamp = 1746458428253\n",
      "25/05/05 11:20:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:28 INFO IncrementalExecution: Current batch timestamp = 1746458428253\n",
      "25/05/05 11:20:28 INFO IncrementalExecution: Current batch timestamp = 1746458428255\n",
      "25/05/05 11:20:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:28 INFO IncrementalExecution: Current batch timestamp = 1746458428253\n",
      "25/05/05 11:20:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:28 INFO IncrementalExecution: Current batch timestamp = 1746458428253\n",
      "25/05/05 11:20:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:28 INFO IncrementalExecution: Current batch timestamp = 1746458428253\n",
      "25/05/05 11:20:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:28 INFO CodeGenerator: Code generated in 4.231084 ms\n",
      "25/05/05 11:20:28 INFO CodeGenerator: Code generated in 3.826416 ms\n",
      "25/05/05 11:20:28 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 34, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@5b7682fe]. The input RDD has 1 partitions.\n",
      "25/05/05 11:20:28 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 34, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:20:28 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:28 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:28 INFO DAGScheduler: Got job 103 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:20:28 INFO DAGScheduler: Final stage: ResultStage 101 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:20:28 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:20:28 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:20:28 INFO DAGScheduler: Submitting ResultStage 101 (MapPartitionsRDD[554] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:20:28 INFO MemoryStore: Block broadcast_135 stored as values in memory (estimated size 16.2 KiB, free 366.3 MiB)\n",
      "25/05/05 11:20:28 INFO MemoryStore: Block broadcast_135_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.3 MiB)\n",
      "25/05/05 11:20:28 INFO BlockManagerInfo: Added broadcast_135_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:28 INFO SparkContext: Created broadcast 135 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:20:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 101 (MapPartitionsRDD[554] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:20:28 INFO TaskSchedulerImpl: Adding task set 101.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:20:28 INFO DAGScheduler: Got job 102 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:20:28 INFO DAGScheduler: Final stage: ResultStage 102 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:20:28 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:20:28 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:20:28 INFO DAGScheduler: Submitting ResultStage 102 (MapPartitionsRDD[555] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:20:28 INFO TaskSetManager: Starting task 0.0 in stage 101.0 (TID 101) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:20:28 INFO Executor: Running task 0.0 in stage 101.0 (TID 101)\n",
      "25/05/05 11:20:28 INFO MemoryStore: Block broadcast_136 stored as values in memory (estimated size 15.2 KiB, free 366.3 MiB)\n",
      "25/05/05 11:20:28 INFO MemoryStore: Block broadcast_136_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.3 MiB)\n",
      "25/05/05 11:20:28 INFO BlockManagerInfo: Added broadcast_136_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:28 INFO SparkContext: Created broadcast 136 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:20:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 102 (MapPartitionsRDD[555] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:20:28 INFO TaskSchedulerImpl: Adding task set 102.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:20:28 INFO TaskSetManager: Starting task 0.0 in stage 102.0 (TID 102) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:20:28 INFO Executor: Running task 0.0 in stage 102.0 (TID 102)\n",
      "25/05/05 11:20:28 INFO CodeGenerator: Code generated in 4.254458 ms\n",
      "25/05/05 11:20:28 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3265 untilOffset=3266, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=34 taskId=102 partitionId=0\n",
      "25/05/05 11:20:28 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3265 untilOffset=3266, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=34 taskId=101 partitionId=0\n",
      "25/05/05 11:20:28 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3265 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:28 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3265 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:28 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:28 INFO DAGScheduler: Got job 104 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:20:28 INFO DAGScheduler: Final stage: ResultStage 103 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:20:28 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:20:28 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:20:28 INFO DAGScheduler: Submitting ResultStage 103 (MapPartitionsRDD[560] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:20:28 INFO MemoryStore: Block broadcast_137 stored as values in memory (estimated size 47.1 KiB, free 366.2 MiB)\n",
      "25/05/05 11:20:28 INFO MemoryStore: Block broadcast_137_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:20:28 INFO BlockManagerInfo: Added broadcast_137_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:28 INFO SparkContext: Created broadcast 137 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:20:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 103 (MapPartitionsRDD[560] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:20:28 INFO TaskSchedulerImpl: Adding task set 103.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:20:28 INFO TaskSetManager: Starting task 0.0 in stage 103.0 (TID 103) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:20:28 INFO Executor: Running task 0.0 in stage 103.0 (TID 103)\n",
      "25/05/05 11:20:28 INFO CodeGenerator: Code generated in 3.873208 ms\n",
      "25/05/05 11:20:28 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3265 untilOffset=3266, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=34 taskId=103 partitionId=0\n",
      "25/05/05 11:20:28 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3265 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3266, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3266, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:28 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:20:28 INFO DataWritingSparkTask: Committed partition 0 (task 102, attempt 0, stage 102.0)\n",
      "25/05/05 11:20:28 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 506750542 nanos, during time span of 507128625 nanos.\n",
      "25/05/05 11:20:28 INFO Executor: Finished task 0.0 in stage 102.0 (TID 102). 2145 bytes result sent to driver\n",
      "25/05/05 11:20:28 INFO TaskSetManager: Finished task 0.0 in stage 102.0 (TID 102) in 514 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:20:28 INFO TaskSchedulerImpl: Removed TaskSet 102.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:20:28 INFO DAGScheduler: ResultStage 102 (start at NativeMethodAccessorImpl.java:0) finished in 0.517 s\n",
      "25/05/05 11:20:28 INFO DAGScheduler: Job 102 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:20:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 102: Stage finished\n",
      "25/05/05 11:20:28 INFO DAGScheduler: Job 102 finished: start at NativeMethodAccessorImpl.java:0, took 0.518974 s\n",
      "25/05/05 11:20:28 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 34, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:20:28 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:20:28 INFO DataWritingSparkTask: Committed partition 0 (task 101, attempt 0, stage 101.0)\n",
      "25/05/05 11:20:28 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 505325083 nanos, during time span of 507425542 nanos.\n",
      "25/05/05 11:20:28 INFO Executor: Finished task 0.0 in stage 101.0 (TID 101). 3516 bytes result sent to driver\n",
      "25/05/05 11:20:28 INFO TaskSetManager: Finished task 0.0 in stage 101.0 (TID 101) in 518 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:20:28 INFO TaskSchedulerImpl: Removed TaskSet 101.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:20:28 INFO DAGScheduler: ResultStage 101 (start at NativeMethodAccessorImpl.java:0) finished in 0.520 s\n",
      "25/05/05 11:20:28 INFO DAGScheduler: Job 103 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:20:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 101: Stage finished\n",
      "25/05/05 11:20:28 INFO DAGScheduler: Job 103 finished: start at NativeMethodAccessorImpl.java:0, took 0.520236 s\n",
      "25/05/05 11:20:28 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 34, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@5b7682fe] is committing.\n",
      "25/05/05 11:20:28 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 34, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@5b7682fe] committed.\n",
      "25/05/05 11:20:28 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 34, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:20:28 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/34 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.34.67002005-2715-4085-99ed-61239f634405.tmp\n",
      "25/05/05 11:20:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:28 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/34 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.34.7a69202b-6077-4949-9ae4-591c24e7f911.tmp\n",
      "25/05/05 11:20:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3266, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:28 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:20:28 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:20:28 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:20:28 INFO connection: Opened connection [connectionId{localValue:67, serverValue:4437}] to localhost:27017\n",
      "25/05/05 11:20:28 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=339084}\n",
      "25/05/05 11:20:28 INFO connection: Opened connection [connectionId{localValue:68, serverValue:4438}] to localhost:27017\n",
      "25/05/05 11:20:28 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:20:28 INFO connection: Closed connection [connectionId{localValue:68, serverValue:4438}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:20:28 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 502924875 nanos, during time span of 511567042 nanos.\n",
      "25/05/05 11:20:28 INFO Executor: Finished task 0.0 in stage 103.0 (TID 103). 1645 bytes result sent to driver\n",
      "25/05/05 11:20:28 INFO TaskSetManager: Finished task 0.0 in stage 103.0 (TID 103) in 523 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:20:28 INFO TaskSchedulerImpl: Removed TaskSet 103.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:20:28 INFO DAGScheduler: ResultStage 103 (start at NativeMethodAccessorImpl.java:0) finished in 0.527 s\n",
      "25/05/05 11:20:28 INFO DAGScheduler: Job 104 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:20:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 103: Stage finished\n",
      "25/05/05 11:20:28 INFO DAGScheduler: Job 104 finished: start at NativeMethodAccessorImpl.java:0, took 0.527906 s\n",
      "25/05/05 11:20:28 INFO MemoryStore: Block broadcast_138 stored as values in memory (estimated size 248.0 B, free 366.2 MiB)\n",
      "25/05/05 11:20:28 INFO MemoryStore: Block broadcast_138_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.2 MiB)\n",
      "25/05/05 11:20:28 INFO BlockManagerInfo: Added broadcast_138_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:20:28 INFO SparkContext: Created broadcast 138 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:28 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.34.67002005-2715-4085-99ed-61239f634405.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/34\n",
      "25/05/05 11:20:28 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:20:28.249Z\",\n",
      "  \"batchId\" : 34,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.5625,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 535,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 4,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 640,\n",
      "    \"walCommit\" : 65\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3265\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3266\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3266\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.5625,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:20:28 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/34 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.34.4b955b5f-b078-4261-9280-49783befc076.tmp\n",
      "25/05/05 11:20:28 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.34.7a69202b-6077-4949-9ae4-591c24e7f911.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/34\n",
      "25/05/05 11:20:28 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:20:28.249Z\",\n",
      "  \"batchId\" : 34,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.547987616099071,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 541,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 4,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 646,\n",
      "    \"walCommit\" : 67\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3265\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3266\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3266\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.547987616099071,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:20:28 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.34.4b955b5f-b078-4261-9280-49783befc076.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/34\n",
      "25/05/05 11:20:28 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:20:28.249Z\",\n",
      "  \"batchId\" : 34,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.5037593984962405,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 560,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 6,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 665,\n",
      "    \"walCommit\" : 64\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3265\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3266\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3266\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.5037593984962405,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 34\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R023|R045|00-00-00|34 ST-HERALD SQ|05/05/2025|11:20:27|REGULAR|9          |7         |2025-05-05 11:20:28.253|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:20:33 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/35 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.35.27ae5244-7bf7-4cad-93c2-7c70077b1fd7.tmp\n",
      "25/05/05 11:20:33 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/35 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.35.7c76a5ed-564d-44ef-ae96-bf43c30c997c.tmp\n",
      "25/05/05 11:20:33 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/35 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.35.e3cdc46a-7b73-4a78-ab78-c4e7876a729d.tmp\n",
      "25/05/05 11:20:33 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.35.e3cdc46a-7b73-4a78-ab78-c4e7876a729d.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/35\n",
      "25/05/05 11:20:33 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.35.7c76a5ed-564d-44ef-ae96-bf43c30c997c.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/35\n",
      "25/05/05 11:20:33 INFO MicroBatchExecution: Committed offsets for batch 35. Metadata OffsetSeqMetadata(0,1746458433746,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:20:33 INFO MicroBatchExecution: Committed offsets for batch 35. Metadata OffsetSeqMetadata(0,1746458433747,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:20:33 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.35.27ae5244-7bf7-4cad-93c2-7c70077b1fd7.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/35\n",
      "25/05/05 11:20:33 INFO MicroBatchExecution: Committed offsets for batch 35. Metadata OffsetSeqMetadata(0,1746458433747,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:20:33 INFO IncrementalExecution: Current batch timestamp = 1746458433746\n",
      "25/05/05 11:20:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:33 INFO IncrementalExecution: Current batch timestamp = 1746458433747\n",
      "25/05/05 11:20:33 INFO IncrementalExecution: Current batch timestamp = 1746458433747\n",
      "25/05/05 11:20:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:33 INFO IncrementalExecution: Current batch timestamp = 1746458433747\n",
      "25/05/05 11:20:33 INFO IncrementalExecution: Current batch timestamp = 1746458433747\n",
      "25/05/05 11:20:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:33 INFO IncrementalExecution: Current batch timestamp = 1746458433746\n",
      "25/05/05 11:20:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:33 INFO IncrementalExecution: Current batch timestamp = 1746458433747\n",
      "25/05/05 11:20:33 INFO IncrementalExecution: Current batch timestamp = 1746458433747\n",
      "25/05/05 11:20:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:33 INFO CodeGenerator: Code generated in 4.698167 ms\n",
      "25/05/05 11:20:33 INFO CodeGenerator: Code generated in 4.637625 ms\n",
      "25/05/05 11:20:33 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 35, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@5845ee42]. The input RDD has 1 partitions.\n",
      "25/05/05 11:20:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:33 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 35, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:20:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:33 INFO DAGScheduler: Got job 105 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:20:33 INFO DAGScheduler: Final stage: ResultStage 104 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:20:33 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:20:33 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:20:33 INFO DAGScheduler: Submitting ResultStage 104 (MapPartitionsRDD[569] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:20:33 INFO MemoryStore: Block broadcast_139 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:20:33 INFO MemoryStore: Block broadcast_139_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:20:33 INFO BlockManagerInfo: Added broadcast_139_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:33 INFO SparkContext: Created broadcast 139 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:20:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 104 (MapPartitionsRDD[569] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:20:33 INFO TaskSchedulerImpl: Adding task set 104.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:20:33 INFO DAGScheduler: Got job 106 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:20:33 INFO DAGScheduler: Final stage: ResultStage 105 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:20:33 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:20:33 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:20:33 INFO DAGScheduler: Submitting ResultStage 105 (MapPartitionsRDD[571] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:20:33 INFO TaskSetManager: Starting task 0.0 in stage 104.0 (TID 104) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:20:33 INFO Executor: Running task 0.0 in stage 104.0 (TID 104)\n",
      "25/05/05 11:20:33 INFO MemoryStore: Block broadcast_140 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:20:33 INFO MemoryStore: Block broadcast_140_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:20:33 INFO BlockManagerInfo: Added broadcast_140_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:33 INFO SparkContext: Created broadcast 140 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:20:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 105 (MapPartitionsRDD[571] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:20:33 INFO TaskSchedulerImpl: Adding task set 105.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:20:33 INFO TaskSetManager: Starting task 0.0 in stage 105.0 (TID 105) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:20:33 INFO Executor: Running task 0.0 in stage 105.0 (TID 105)\n",
      "25/05/05 11:20:33 INFO CodeGenerator: Code generated in 5.575291 ms\n",
      "25/05/05 11:20:33 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3266 untilOffset=3267, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=35 taskId=105 partitionId=0\n",
      "25/05/05 11:20:33 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3266 untilOffset=3267, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=35 taskId=104 partitionId=0\n",
      "25/05/05 11:20:33 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3266 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:33 INFO BlockManagerInfo: Removed broadcast_138_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:20:33 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3266 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:33 INFO DAGScheduler: Got job 107 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:20:33 INFO DAGScheduler: Final stage: ResultStage 106 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:20:33 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:20:33 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:20:33 INFO DAGScheduler: Submitting ResultStage 106 (MapPartitionsRDD[576] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:20:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:33 INFO BlockManagerInfo: Removed broadcast_137_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:33 INFO MemoryStore: Block broadcast_141 stored as values in memory (estimated size 47.1 KiB, free 366.2 MiB)\n",
      "25/05/05 11:20:33 INFO MemoryStore: Block broadcast_141_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.2 MiB)\n",
      "25/05/05 11:20:33 INFO BlockManagerInfo: Removed broadcast_135_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:33 INFO BlockManagerInfo: Added broadcast_141_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:33 INFO SparkContext: Created broadcast 141 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:20:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 106 (MapPartitionsRDD[576] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:20:33 INFO TaskSchedulerImpl: Adding task set 106.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:20:33 INFO TaskSetManager: Starting task 0.0 in stage 106.0 (TID 106) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:20:33 INFO Executor: Running task 0.0 in stage 106.0 (TID 106)\n",
      "25/05/05 11:20:33 INFO BlockManagerInfo: Removed broadcast_136_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:33 INFO CodeGenerator: Code generated in 4.868834 ms\n",
      "25/05/05 11:20:33 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3266 untilOffset=3267, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=35 taskId=106 partitionId=0\n",
      "25/05/05 11:20:33 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3266 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3267, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3267, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:34 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:20:34 INFO DataWritingSparkTask: Committed partition 0 (task 105, attempt 0, stage 105.0)\n",
      "25/05/05 11:20:34 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 504054292 nanos, during time span of 504522708 nanos.\n",
      "25/05/05 11:20:34 INFO Executor: Finished task 0.0 in stage 105.0 (TID 105). 2188 bytes result sent to driver\n",
      "25/05/05 11:20:34 INFO TaskSetManager: Finished task 0.0 in stage 105.0 (TID 105) in 521 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:20:34 INFO TaskSchedulerImpl: Removed TaskSet 105.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:20:34 INFO DAGScheduler: ResultStage 105 (start at NativeMethodAccessorImpl.java:0) finished in 0.524 s\n",
      "25/05/05 11:20:34 INFO DAGScheduler: Job 106 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:20:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 105: Stage finished\n",
      "25/05/05 11:20:34 INFO DAGScheduler: Job 106 finished: start at NativeMethodAccessorImpl.java:0, took 0.527832 s\n",
      "25/05/05 11:20:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 35, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:20:34 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:20:34 INFO DataWritingSparkTask: Committed partition 0 (task 104, attempt 0, stage 104.0)\n",
      "25/05/05 11:20:34 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 502341000 nanos, during time span of 504398542 nanos.\n",
      "25/05/05 11:20:34 INFO Executor: Finished task 0.0 in stage 104.0 (TID 104). 3554 bytes result sent to driver\n",
      "25/05/05 11:20:34 INFO TaskSetManager: Finished task 0.0 in stage 104.0 (TID 104) in 525 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:20:34 INFO TaskSchedulerImpl: Removed TaskSet 104.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:20:34 INFO DAGScheduler: ResultStage 104 (start at NativeMethodAccessorImpl.java:0) finished in 0.528 s\n",
      "25/05/05 11:20:34 INFO DAGScheduler: Job 105 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:20:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 104: Stage finished\n",
      "25/05/05 11:20:34 INFO DAGScheduler: Job 105 finished: start at NativeMethodAccessorImpl.java:0, took 0.529776 s\n",
      "25/05/05 11:20:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 35, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@5845ee42] is committing.\n",
      "25/05/05 11:20:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 35, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@5845ee42] committed.\n",
      "25/05/05 11:20:34 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/35 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.35.6b0cec26-0f1c-4abb-ae44-c4efde3bcda8.tmp\n",
      "25/05/05 11:20:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 35, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:20:34 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/35 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.35.fc6b780e-c14d-4300-9f26-6573d7eff1d7.tmp\n",
      "25/05/05 11:20:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3267, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:34 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:20:34 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:20:34 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:20:34 INFO connection: Opened connection [connectionId{localValue:69, serverValue:4439}] to localhost:27017\n",
      "25/05/05 11:20:34 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1418916}\n",
      "25/05/05 11:20:34 INFO connection: Opened connection [connectionId{localValue:70, serverValue:4440}] to localhost:27017\n",
      "25/05/05 11:20:34 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:20:34 INFO connection: Closed connection [connectionId{localValue:70, serverValue:4440}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:20:34 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 504254959 nanos, during time span of 512657167 nanos.\n",
      "25/05/05 11:20:34 INFO Executor: Finished task 0.0 in stage 106.0 (TID 106). 1645 bytes result sent to driver\n",
      "25/05/05 11:20:34 INFO TaskSetManager: Finished task 0.0 in stage 106.0 (TID 106) in 525 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:20:34 INFO TaskSchedulerImpl: Removed TaskSet 106.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:20:34 INFO DAGScheduler: ResultStage 106 (start at NativeMethodAccessorImpl.java:0) finished in 0.530 s\n",
      "25/05/05 11:20:34 INFO DAGScheduler: Job 107 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:20:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 106: Stage finished\n",
      "25/05/05 11:20:34 INFO DAGScheduler: Job 107 finished: start at NativeMethodAccessorImpl.java:0, took 0.531821 s\n",
      "25/05/05 11:20:34 INFO MemoryStore: Block broadcast_142 stored as values in memory (estimated size 248.0 B, free 366.2 MiB)\n",
      "25/05/05 11:20:34 INFO MemoryStore: Block broadcast_142_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.2 MiB)\n",
      "25/05/05 11:20:34 INFO BlockManagerInfo: Added broadcast_142_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:20:34 INFO SparkContext: Created broadcast 142 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:34 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/35 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.35.ffabd1ef-3cd6-4bf0-b1bc-6784d2fd0088.tmp\n",
      "25/05/05 11:20:34 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.35.6b0cec26-0f1c-4abb-ae44-c4efde3bcda8.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/35\n",
      "25/05/05 11:20:34 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:20:33.738Z\",\n",
      "  \"batchId\" : 35,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 37.03703703703704,\n",
      "  \"processedRowsPerSecond\" : 1.5625,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 549,\n",
      "    \"commitOffsets\" : 39,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 9,\n",
      "    \"queryPlanning\" : 6,\n",
      "    \"triggerExecution\" : 640,\n",
      "    \"walCommit\" : 37\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3266\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3267\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3267\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 37.03703703703704,\n",
      "    \"processedRowsPerSecond\" : 1.5625,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:20:34 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.35.fc6b780e-c14d-4300-9f26-6573d7eff1d7.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/35\n",
      "25/05/05 11:20:34 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:20:33.739Z\",\n",
      "  \"batchId\" : 35,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 62.5,\n",
      "  \"processedRowsPerSecond\" : 1.5432098765432098,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 556,\n",
      "    \"commitOffsets\" : 41,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 8,\n",
      "    \"queryPlanning\" : 6,\n",
      "    \"triggerExecution\" : 648,\n",
      "    \"walCommit\" : 37\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3266\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3267\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3267\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 62.5,\n",
      "    \"processedRowsPerSecond\" : 1.5432098765432098,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:20:34 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.35.ffabd1ef-3cd6-4bf0-b1bc-6784d2fd0088.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/35\n",
      "25/05/05 11:20:34 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:20:33.738Z\",\n",
      "  \"batchId\" : 35,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 40.0,\n",
      "  \"processedRowsPerSecond\" : 1.510574018126888,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 579,\n",
      "    \"commitOffsets\" : 31,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 7,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 662,\n",
      "    \"walCommit\" : 38\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3266\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3267\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3267\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 40.0,\n",
      "    \"processedRowsPerSecond\" : 1.510574018126888,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 35\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|B013|R081|02-00-00|161/YANKEE STAD|05/05/2025|11:20:32|REGULAR|9          |9         |2025-05-05 11:20:33.747|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:20:40 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/36 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.36.e3dded6c-8b78-48e6-974d-f386d85ba246.tmp\n",
      "25/05/05 11:20:40 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/36 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.36.faf9ddec-71ca-460b-8c78-49777ec5bb9e.tmp\n",
      "25/05/05 11:20:40 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/36 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.36.de26a5be-2785-47ed-adf6-bb41cc578d5a.tmp\n",
      "25/05/05 11:20:40 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.36.e3dded6c-8b78-48e6-974d-f386d85ba246.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/36\n",
      "25/05/05 11:20:40 INFO MicroBatchExecution: Committed offsets for batch 36. Metadata OffsetSeqMetadata(0,1746458440302,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:20:40 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.36.de26a5be-2785-47ed-adf6-bb41cc578d5a.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/36\n",
      "25/05/05 11:20:40 INFO MicroBatchExecution: Committed offsets for batch 36. Metadata OffsetSeqMetadata(0,1746458440302,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:20:40 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.36.faf9ddec-71ca-460b-8c78-49777ec5bb9e.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/36\n",
      "25/05/05 11:20:40 INFO MicroBatchExecution: Committed offsets for batch 36. Metadata OffsetSeqMetadata(0,1746458440304,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:20:40 INFO IncrementalExecution: Current batch timestamp = 1746458440302\n",
      "25/05/05 11:20:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:40 INFO IncrementalExecution: Current batch timestamp = 1746458440302\n",
      "25/05/05 11:20:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:40 INFO IncrementalExecution: Current batch timestamp = 1746458440304\n",
      "25/05/05 11:20:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:40 INFO IncrementalExecution: Current batch timestamp = 1746458440302\n",
      "25/05/05 11:20:40 INFO IncrementalExecution: Current batch timestamp = 1746458440302\n",
      "25/05/05 11:20:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:40 INFO IncrementalExecution: Current batch timestamp = 1746458440304\n",
      "25/05/05 11:20:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:40 INFO IncrementalExecution: Current batch timestamp = 1746458440302\n",
      "25/05/05 11:20:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:40 INFO IncrementalExecution: Current batch timestamp = 1746458440304\n",
      "25/05/05 11:20:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:40 INFO CodeGenerator: Code generated in 4.965042 ms\n",
      "25/05/05 11:20:40 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 36, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@65d02498]. The input RDD has 1 partitions.\n",
      "25/05/05 11:20:40 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:40 INFO DAGScheduler: Got job 108 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:20:40 INFO DAGScheduler: Final stage: ResultStage 107 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:20:40 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:20:40 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:20:40 INFO DAGScheduler: Submitting ResultStage 107 (MapPartitionsRDD[584] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:20:40 INFO MemoryStore: Block broadcast_143 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:20:40 INFO MemoryStore: Block broadcast_143_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:20:40 INFO BlockManagerInfo: Added broadcast_143_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:40 INFO SparkContext: Created broadcast 143 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:20:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 107 (MapPartitionsRDD[584] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:20:40 INFO TaskSchedulerImpl: Adding task set 107.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:20:40 INFO TaskSetManager: Starting task 0.0 in stage 107.0 (TID 107) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:20:40 INFO CodeGenerator: Code generated in 6.784666 ms\n",
      "25/05/05 11:20:40 INFO Executor: Running task 0.0 in stage 107.0 (TID 107)\n",
      "25/05/05 11:20:40 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 36, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:20:40 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:40 INFO DAGScheduler: Got job 109 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:20:40 INFO DAGScheduler: Final stage: ResultStage 108 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:20:40 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:20:40 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:20:40 INFO DAGScheduler: Submitting ResultStage 108 (MapPartitionsRDD[587] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:20:40 INFO MemoryStore: Block broadcast_144 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:20:40 INFO MemoryStore: Block broadcast_144_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:20:40 INFO BlockManagerInfo: Added broadcast_144_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:40 INFO SparkContext: Created broadcast 144 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:20:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 108 (MapPartitionsRDD[587] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:20:40 INFO TaskSchedulerImpl: Adding task set 108.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:20:40 INFO TaskSetManager: Starting task 0.0 in stage 108.0 (TID 108) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:20:40 INFO Executor: Running task 0.0 in stage 108.0 (TID 108)\n",
      "25/05/05 11:20:40 INFO CodeGenerator: Code generated in 5.12325 ms\n",
      "25/05/05 11:20:40 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3267 untilOffset=3268, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=36 taskId=107 partitionId=0\n",
      "25/05/05 11:20:40 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3267 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:40 INFO CodeGenerator: Code generated in 5.2725 ms\n",
      "25/05/05 11:20:40 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3267 untilOffset=3268, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=36 taskId=108 partitionId=0\n",
      "25/05/05 11:20:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:40 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3267 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:40 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:40 INFO DAGScheduler: Got job 110 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:20:40 INFO DAGScheduler: Final stage: ResultStage 109 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:20:40 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:20:40 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:20:40 INFO DAGScheduler: Submitting ResultStage 109 (MapPartitionsRDD[592] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:20:40 INFO MemoryStore: Block broadcast_145 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:20:40 INFO MemoryStore: Block broadcast_145_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.1 MiB)\n",
      "25/05/05 11:20:40 INFO BlockManagerInfo: Added broadcast_145_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:20:40 INFO SparkContext: Created broadcast 145 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:20:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 109 (MapPartitionsRDD[592] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:20:40 INFO TaskSchedulerImpl: Adding task set 109.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:20:40 INFO TaskSetManager: Starting task 0.0 in stage 109.0 (TID 109) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:20:40 INFO Executor: Running task 0.0 in stage 109.0 (TID 109)\n",
      "25/05/05 11:20:40 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3267 untilOffset=3268, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=36 taskId=109 partitionId=0\n",
      "25/05/05 11:20:40 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3267 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3268, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:40 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:20:40 INFO DataWritingSparkTask: Committed partition 0 (task 107, attempt 0, stage 107.0)\n",
      "25/05/05 11:20:40 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 504008666 nanos, during time span of 505803292 nanos.\n",
      "25/05/05 11:20:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:40 INFO Executor: Finished task 0.0 in stage 107.0 (TID 107). 3559 bytes result sent to driver\n",
      "25/05/05 11:20:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3268, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:40 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:20:40 INFO DataWritingSparkTask: Committed partition 0 (task 108, attempt 0, stage 108.0)\n",
      "25/05/05 11:20:40 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 502117166 nanos, during time span of 502441083 nanos.\n",
      "25/05/05 11:20:40 INFO TaskSetManager: Finished task 0.0 in stage 107.0 (TID 107) in 519 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:20:40 INFO TaskSchedulerImpl: Removed TaskSet 107.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:20:40 INFO DAGScheduler: ResultStage 107 (start at NativeMethodAccessorImpl.java:0) finished in 0.521 s\n",
      "25/05/05 11:20:40 INFO DAGScheduler: Job 108 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:20:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 107: Stage finished\n",
      "25/05/05 11:20:40 INFO Executor: Finished task 0.0 in stage 108.0 (TID 108). 2188 bytes result sent to driver\n",
      "25/05/05 11:20:40 INFO DAGScheduler: Job 108 finished: start at NativeMethodAccessorImpl.java:0, took 0.522150 s\n",
      "25/05/05 11:20:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 36, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@65d02498] is committing.\n",
      "25/05/05 11:20:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 36, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@65d02498] committed.\n",
      "25/05/05 11:20:40 INFO TaskSetManager: Finished task 0.0 in stage 108.0 (TID 108) in 514 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:20:40 INFO TaskSchedulerImpl: Removed TaskSet 108.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:20:40 INFO DAGScheduler: ResultStage 108 (start at NativeMethodAccessorImpl.java:0) finished in 0.516 s\n",
      "25/05/05 11:20:40 INFO DAGScheduler: Job 109 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:20:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 108: Stage finished\n",
      "25/05/05 11:20:40 INFO DAGScheduler: Job 109 finished: start at NativeMethodAccessorImpl.java:0, took 0.517130 s\n",
      "25/05/05 11:20:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 36, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:20:40 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/36 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.36.213bb82f-32ec-4457-b47c-f2b33861cae7.tmp\n",
      "25/05/05 11:20:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 36, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:20:40 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/36 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.36.f06e07cc-abbe-4d32-99ec-337dde7c8f7f.tmp\n",
      "25/05/05 11:20:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3268, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:40 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:20:40 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:20:40 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:20:40 INFO connection: Opened connection [connectionId{localValue:71, serverValue:4441}] to localhost:27017\n",
      "25/05/05 11:20:40 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=279792}\n",
      "25/05/05 11:20:40 INFO connection: Opened connection [connectionId{localValue:72, serverValue:4442}] to localhost:27017\n",
      "25/05/05 11:20:40 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:20:40 INFO connection: Closed connection [connectionId{localValue:72, serverValue:4442}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:20:40 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 502077334 nanos, during time span of 510051833 nanos.\n",
      "25/05/05 11:20:40 INFO Executor: Finished task 0.0 in stage 109.0 (TID 109). 1645 bytes result sent to driver\n",
      "25/05/05 11:20:40 INFO TaskSetManager: Finished task 0.0 in stage 109.0 (TID 109) in 517 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:20:40 INFO TaskSchedulerImpl: Removed TaskSet 109.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:20:40 INFO DAGScheduler: ResultStage 109 (start at NativeMethodAccessorImpl.java:0) finished in 0.532 s\n",
      "25/05/05 11:20:40 INFO DAGScheduler: Job 110 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:20:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 109: Stage finished\n",
      "25/05/05 11:20:40 INFO DAGScheduler: Job 110 finished: start at NativeMethodAccessorImpl.java:0, took 0.533017 s\n",
      "25/05/05 11:20:40 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.36.213bb82f-32ec-4457-b47c-f2b33861cae7.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/36\n",
      "25/05/05 11:20:40 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:20:40.300Z\",\n",
      "  \"batchId\" : 36,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.5772870662460567,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 539,\n",
      "    \"commitOffsets\" : 31,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 6,\n",
      "    \"triggerExecution\" : 634,\n",
      "    \"walCommit\" : 56\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3267\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3268\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3268\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.5772870662460567,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:20:40 INFO MemoryStore: Block broadcast_146 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:20:40 INFO MemoryStore: Block broadcast_146_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:20:40 INFO BlockManagerInfo: Added broadcast_146_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:20:40 INFO SparkContext: Created broadcast 146 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:40 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/36 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.36.0a9f9fbb-078b-4a81-9ce9-37a842bacde0.tmp\n",
      "25/05/05 11:20:40 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.36.f06e07cc-abbe-4d32-99ec-337dde7c8f7f.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/36\n",
      "25/05/05 11:20:40 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:20:40.301Z\",\n",
      "  \"batchId\" : 36,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.5600624024960998,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 548,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 1,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 641,\n",
      "    \"walCommit\" : 56\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3267\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3268\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3268\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.5600624024960998,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:20:40 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.36.0a9f9fbb-078b-4a81-9ce9-37a842bacde0.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/36\n",
      "25/05/05 11:20:40 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:20:40.301Z\",\n",
      "  \"batchId\" : 36,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.510574018126888,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 571,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 6,\n",
      "    \"triggerExecution\" : 662,\n",
      "    \"walCommit\" : 56\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3267\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3268\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3268\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.510574018126888,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:20:41 INFO BlockManagerInfo: Removed broadcast_140_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:20:41 INFO BlockManagerInfo: Removed broadcast_141_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:41 INFO BlockManagerInfo: Removed broadcast_142_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:20:41 INFO BlockManagerInfo: Removed broadcast_146_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:20:41 INFO BlockManagerInfo: Removed broadcast_145_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:41 INFO BlockManagerInfo: Removed broadcast_139_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:41 INFO BlockManagerInfo: Removed broadcast_143_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:41 INFO BlockManagerInfo: Removed broadcast_144_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 36\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A001|R079|01-00-00|WORLD TRADE CTR|05/05/2025|11:20:39|REGULAR|8          |9         |2025-05-05 11:20:40.304|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:20:44 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/37 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.37.166cef64-3660-4524-81e5-a723b6271d77.tmp\n",
      "25/05/05 11:20:44 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/37 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.37.17d9d7ed-70ec-498a-a36a-6213142cb2ea.tmp\n",
      "25/05/05 11:20:44 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/37 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.37.bd8e3c62-b75a-4ad6-a59d-e68c8e720e83.tmp\n",
      "25/05/05 11:20:44 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.37.bd8e3c62-b75a-4ad6-a59d-e68c8e720e83.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/37\n",
      "25/05/05 11:20:44 INFO MicroBatchExecution: Committed offsets for batch 37. Metadata OffsetSeqMetadata(0,1746458444836,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:20:44 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.37.17d9d7ed-70ec-498a-a36a-6213142cb2ea.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/37\n",
      "25/05/05 11:20:44 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.37.166cef64-3660-4524-81e5-a723b6271d77.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/37\n",
      "25/05/05 11:20:44 INFO MicroBatchExecution: Committed offsets for batch 37. Metadata OffsetSeqMetadata(0,1746458444836,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:20:44 INFO MicroBatchExecution: Committed offsets for batch 37. Metadata OffsetSeqMetadata(0,1746458444823,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:20:44 INFO IncrementalExecution: Current batch timestamp = 1746458444823\n",
      "25/05/05 11:20:44 INFO IncrementalExecution: Current batch timestamp = 1746458444836\n",
      "25/05/05 11:20:44 INFO IncrementalExecution: Current batch timestamp = 1746458444836\n",
      "25/05/05 11:20:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:44 INFO IncrementalExecution: Current batch timestamp = 1746458444823\n",
      "25/05/05 11:20:44 INFO IncrementalExecution: Current batch timestamp = 1746458444836\n",
      "25/05/05 11:20:44 INFO IncrementalExecution: Current batch timestamp = 1746458444836\n",
      "25/05/05 11:20:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:44 INFO IncrementalExecution: Current batch timestamp = 1746458444823\n",
      "25/05/05 11:20:44 INFO IncrementalExecution: Current batch timestamp = 1746458444836\n",
      "25/05/05 11:20:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:44 INFO CodeGenerator: Code generated in 4.445584 ms\n",
      "25/05/05 11:20:44 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 37, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@26ce1866]. The input RDD has 1 partitions.\n",
      "25/05/05 11:20:44 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:44 INFO DAGScheduler: Got job 111 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:20:44 INFO DAGScheduler: Final stage: ResultStage 110 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:20:44 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:20:44 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:20:44 INFO DAGScheduler: Submitting ResultStage 110 (MapPartitionsRDD[600] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:20:44 INFO MemoryStore: Block broadcast_147 stored as values in memory (estimated size 16.2 KiB, free 366.3 MiB)\n",
      "25/05/05 11:20:44 INFO MemoryStore: Block broadcast_147_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.3 MiB)\n",
      "25/05/05 11:20:44 INFO BlockManagerInfo: Added broadcast_147_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:44 INFO CodeGenerator: Code generated in 3.871834 ms\n",
      "25/05/05 11:20:44 INFO SparkContext: Created broadcast 147 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:20:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 110 (MapPartitionsRDD[600] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:20:44 INFO TaskSchedulerImpl: Adding task set 110.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:20:44 INFO TaskSetManager: Starting task 0.0 in stage 110.0 (TID 110) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:20:44 INFO Executor: Running task 0.0 in stage 110.0 (TID 110)\n",
      "25/05/05 11:20:44 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 37, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:20:44 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:44 INFO DAGScheduler: Got job 112 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:20:44 INFO DAGScheduler: Final stage: ResultStage 111 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:20:44 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:20:44 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:20:44 INFO DAGScheduler: Submitting ResultStage 111 (MapPartitionsRDD[603] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:20:44 INFO MemoryStore: Block broadcast_148 stored as values in memory (estimated size 15.2 KiB, free 366.3 MiB)\n",
      "25/05/05 11:20:44 INFO MemoryStore: Block broadcast_148_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.3 MiB)\n",
      "25/05/05 11:20:44 INFO BlockManagerInfo: Added broadcast_148_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:44 INFO SparkContext: Created broadcast 148 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:20:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 111 (MapPartitionsRDD[603] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:20:44 INFO TaskSchedulerImpl: Adding task set 111.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:20:44 INFO TaskSetManager: Starting task 0.0 in stage 111.0 (TID 111) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:20:44 INFO Executor: Running task 0.0 in stage 111.0 (TID 111)\n",
      "25/05/05 11:20:44 INFO CodeGenerator: Code generated in 6.054958 ms\n",
      "25/05/05 11:20:44 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3268 untilOffset=3269, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=37 taskId=110 partitionId=0\n",
      "25/05/05 11:20:44 INFO CodeGenerator: Code generated in 3.883291 ms\n",
      "25/05/05 11:20:44 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3268 untilOffset=3269, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=37 taskId=111 partitionId=0\n",
      "25/05/05 11:20:44 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3268 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:44 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3268 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:44 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:44 INFO DAGScheduler: Got job 113 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:20:44 INFO DAGScheduler: Final stage: ResultStage 112 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:20:44 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:20:44 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:20:44 INFO DAGScheduler: Submitting ResultStage 112 (MapPartitionsRDD[608] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:20:44 INFO MemoryStore: Block broadcast_149 stored as values in memory (estimated size 47.1 KiB, free 366.2 MiB)\n",
      "25/05/05 11:20:44 INFO MemoryStore: Block broadcast_149_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.2 MiB)\n",
      "25/05/05 11:20:44 INFO BlockManagerInfo: Added broadcast_149_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:44 INFO SparkContext: Created broadcast 149 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:20:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 112 (MapPartitionsRDD[608] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:20:44 INFO TaskSchedulerImpl: Adding task set 112.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:20:44 INFO TaskSetManager: Starting task 0.0 in stage 112.0 (TID 112) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:20:44 INFO Executor: Running task 0.0 in stage 112.0 (TID 112)\n",
      "25/05/05 11:20:44 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3268 untilOffset=3269, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=37 taskId=112 partitionId=0\n",
      "25/05/05 11:20:44 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3268 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3269, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3269, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:45 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:20:45 INFO DataWritingSparkTask: Committed partition 0 (task 111, attempt 0, stage 111.0)\n",
      "25/05/05 11:20:45 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 506779166 nanos, during time span of 507190750 nanos.\n",
      "25/05/05 11:20:45 INFO Executor: Finished task 0.0 in stage 111.0 (TID 111). 2145 bytes result sent to driver\n",
      "25/05/05 11:20:45 INFO TaskSetManager: Finished task 0.0 in stage 111.0 (TID 111) in 516 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:20:45 INFO TaskSchedulerImpl: Removed TaskSet 111.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:20:45 INFO DAGScheduler: ResultStage 111 (start at NativeMethodAccessorImpl.java:0) finished in 0.519 s\n",
      "25/05/05 11:20:45 INFO DAGScheduler: Job 112 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:20:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 111: Stage finished\n",
      "25/05/05 11:20:45 INFO DAGScheduler: Job 112 finished: start at NativeMethodAccessorImpl.java:0, took 0.519383 s\n",
      "25/05/05 11:20:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 37, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:20:45 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:20:45 INFO DataWritingSparkTask: Committed partition 0 (task 110, attempt 0, stage 110.0)\n",
      "25/05/05 11:20:45 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 507174584 nanos, during time span of 509091375 nanos.\n",
      "25/05/05 11:20:45 INFO Executor: Finished task 0.0 in stage 110.0 (TID 110). 3516 bytes result sent to driver\n",
      "25/05/05 11:20:45 INFO TaskSetManager: Finished task 0.0 in stage 110.0 (TID 110) in 521 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:20:45 INFO TaskSchedulerImpl: Removed TaskSet 110.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:20:45 INFO DAGScheduler: ResultStage 110 (start at NativeMethodAccessorImpl.java:0) finished in 0.523 s\n",
      "25/05/05 11:20:45 INFO DAGScheduler: Job 111 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:20:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 110: Stage finished\n",
      "25/05/05 11:20:45 INFO DAGScheduler: Job 111 finished: start at NativeMethodAccessorImpl.java:0, took 0.524044 s\n",
      "25/05/05 11:20:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 37, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@26ce1866] is committing.\n",
      "25/05/05 11:20:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 37, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@26ce1866] committed.\n",
      "25/05/05 11:20:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 37, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:20:45 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/37 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.37.dbeafc2a-6bfd-4408-b15a-40cfbab59f80.tmp\n",
      "25/05/05 11:20:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3269, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:45 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:20:45 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:20:45 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:20:45 INFO connection: Opened connection [connectionId{localValue:73, serverValue:4443}] to localhost:27017\n",
      "25/05/05 11:20:45 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=384666}\n",
      "25/05/05 11:20:45 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/37 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.37.187471c0-9b87-4b5d-8335-f3a82d83ea1f.tmp\n",
      "25/05/05 11:20:45 INFO connection: Opened connection [connectionId{localValue:74, serverValue:4444}] to localhost:27017\n",
      "25/05/05 11:20:45 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:20:45 INFO connection: Closed connection [connectionId{localValue:74, serverValue:4444}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:20:45 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 503624375 nanos, during time span of 508870250 nanos.\n",
      "25/05/05 11:20:45 INFO Executor: Finished task 0.0 in stage 112.0 (TID 112). 1645 bytes result sent to driver\n",
      "25/05/05 11:20:45 INFO TaskSetManager: Finished task 0.0 in stage 112.0 (TID 112) in 515 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:20:45 INFO TaskSchedulerImpl: Removed TaskSet 112.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:20:45 INFO DAGScheduler: ResultStage 112 (start at NativeMethodAccessorImpl.java:0) finished in 0.519 s\n",
      "25/05/05 11:20:45 INFO DAGScheduler: Job 113 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:20:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 112: Stage finished\n",
      "25/05/05 11:20:45 INFO DAGScheduler: Job 113 finished: start at NativeMethodAccessorImpl.java:0, took 0.519880 s\n",
      "25/05/05 11:20:45 INFO MemoryStore: Block broadcast_150 stored as values in memory (estimated size 248.0 B, free 366.2 MiB)\n",
      "25/05/05 11:20:45 INFO MemoryStore: Block broadcast_150_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.2 MiB)\n",
      "25/05/05 11:20:45 INFO BlockManagerInfo: Added broadcast_150_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:20:45 INFO SparkContext: Created broadcast 150 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:45 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/37 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.37.7a258bb1-2698-44f5-ad5f-c39b89c9dc1a.tmp\n",
      "25/05/05 11:20:45 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.37.dbeafc2a-6bfd-4408-b15a-40cfbab59f80.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/37\n",
      "25/05/05 11:20:45 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:20:44.826Z\",\n",
      "  \"batchId\" : 37,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.589825119236884,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 537,\n",
      "    \"commitOffsets\" : 35,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 10,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 629,\n",
      "    \"walCommit\" : 41\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3268\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3269\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3269\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.589825119236884,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:20:45 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.37.187471c0-9b87-4b5d-8335-f3a82d83ea1f.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/37\n",
      "25/05/05 11:20:45 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:20:44.822Z\",\n",
      "  \"batchId\" : 37,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "  \"processedRowsPerSecond\" : 1.574803149606299,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 543,\n",
      "    \"commitOffsets\" : 31,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 635,\n",
      "    \"walCommit\" : 54\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3268\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3269\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3269\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "    \"processedRowsPerSecond\" : 1.574803149606299,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:20:45 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.37.7a258bb1-2698-44f5-ad5f-c39b89c9dc1a.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/37\n",
      "25/05/05 11:20:45 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:20:44.826Z\",\n",
      "  \"batchId\" : 37,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.5600624024960998,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 555,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 10,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 641,\n",
      "    \"walCommit\" : 41\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3268\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3269\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3269\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.5600624024960998,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 37\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R023|R045|00-00-00|34 ST-HERALD SQ|05/05/2025|11:20:43|REGULAR|13         |3         |2025-05-05 11:20:44.823|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:20:49 INFO BlockManagerInfo: Removed broadcast_150_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:20:49 INFO BlockManagerInfo: Removed broadcast_149_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:49 INFO BlockManagerInfo: Removed broadcast_147_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:49 INFO BlockManagerInfo: Removed broadcast_148_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:51 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/38 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.38.f14eaf3f-449f-4948-b9cd-104a7d3938dd.tmp\n",
      "25/05/05 11:20:51 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/38 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.38.85b28b6c-c14b-43f1-b1b2-820a989e6483.tmp\n",
      "25/05/05 11:20:51 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/38 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.38.b08a3559-1e1f-4057-ae0f-3876d323d1c9.tmp\n",
      "25/05/05 11:20:51 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.38.f14eaf3f-449f-4948-b9cd-104a7d3938dd.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/38\n",
      "25/05/05 11:20:51 INFO MicroBatchExecution: Committed offsets for batch 38. Metadata OffsetSeqMetadata(0,1746458451327,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:20:51 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.38.85b28b6c-c14b-43f1-b1b2-820a989e6483.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/38\n",
      "25/05/05 11:20:51 INFO MicroBatchExecution: Committed offsets for batch 38. Metadata OffsetSeqMetadata(0,1746458451327,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:20:51 INFO IncrementalExecution: Current batch timestamp = 1746458451327\n",
      "25/05/05 11:20:51 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.38.b08a3559-1e1f-4057-ae0f-3876d323d1c9.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/38\n",
      "25/05/05 11:20:51 INFO MicroBatchExecution: Committed offsets for batch 38. Metadata OffsetSeqMetadata(0,1746458451338,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:20:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:51 INFO IncrementalExecution: Current batch timestamp = 1746458451327\n",
      "25/05/05 11:20:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:51 INFO IncrementalExecution: Current batch timestamp = 1746458451338\n",
      "25/05/05 11:20:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:51 INFO IncrementalExecution: Current batch timestamp = 1746458451327\n",
      "25/05/05 11:20:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:51 INFO IncrementalExecution: Current batch timestamp = 1746458451327\n",
      "25/05/05 11:20:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:51 INFO IncrementalExecution: Current batch timestamp = 1746458451338\n",
      "25/05/05 11:20:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:51 INFO IncrementalExecution: Current batch timestamp = 1746458451338\n",
      "25/05/05 11:20:51 INFO IncrementalExecution: Current batch timestamp = 1746458451327\n",
      "25/05/05 11:20:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:51 INFO CodeGenerator: Code generated in 4.473542 ms\n",
      "25/05/05 11:20:51 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 38, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@2c028493]. The input RDD has 1 partitions.\n",
      "25/05/05 11:20:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:51 INFO DAGScheduler: Got job 114 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:20:51 INFO DAGScheduler: Final stage: ResultStage 113 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:20:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:20:51 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:20:51 INFO DAGScheduler: Submitting ResultStage 113 (MapPartitionsRDD[616] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:20:51 INFO MemoryStore: Block broadcast_151 stored as values in memory (estimated size 16.2 KiB, free 366.3 MiB)\n",
      "25/05/05 11:20:51 INFO MemoryStore: Block broadcast_151_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.3 MiB)\n",
      "25/05/05 11:20:51 INFO BlockManagerInfo: Added broadcast_151_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:51 INFO SparkContext: Created broadcast 151 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:20:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 113 (MapPartitionsRDD[616] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:20:51 INFO TaskSchedulerImpl: Adding task set 113.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:20:51 INFO CodeGenerator: Code generated in 5.014666 ms\n",
      "25/05/05 11:20:51 INFO TaskSetManager: Starting task 0.0 in stage 113.0 (TID 113) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:20:51 INFO Executor: Running task 0.0 in stage 113.0 (TID 113)\n",
      "25/05/05 11:20:51 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 38, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:20:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:51 INFO DAGScheduler: Got job 115 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:20:51 INFO DAGScheduler: Final stage: ResultStage 114 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:20:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:20:51 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:20:51 INFO DAGScheduler: Submitting ResultStage 114 (MapPartitionsRDD[619] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:20:51 INFO MemoryStore: Block broadcast_152 stored as values in memory (estimated size 15.2 KiB, free 366.3 MiB)\n",
      "25/05/05 11:20:51 INFO MemoryStore: Block broadcast_152_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.3 MiB)\n",
      "25/05/05 11:20:51 INFO BlockManagerInfo: Added broadcast_152_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:51 INFO SparkContext: Created broadcast 152 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:20:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 114 (MapPartitionsRDD[619] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:20:51 INFO TaskSchedulerImpl: Adding task set 114.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:20:51 INFO TaskSetManager: Starting task 0.0 in stage 114.0 (TID 114) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:20:51 INFO Executor: Running task 0.0 in stage 114.0 (TID 114)\n",
      "25/05/05 11:20:51 INFO CodeGenerator: Code generated in 4.202 ms\n",
      "25/05/05 11:20:51 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3269 untilOffset=3270, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=38 taskId=113 partitionId=0\n",
      "25/05/05 11:20:51 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3269 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:51 INFO CodeGenerator: Code generated in 17.885625 ms\n",
      "25/05/05 11:20:51 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3269 untilOffset=3270, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=38 taskId=114 partitionId=0\n",
      "25/05/05 11:20:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:51 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3269 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:51 INFO DAGScheduler: Got job 116 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:20:51 INFO DAGScheduler: Final stage: ResultStage 115 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:20:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:20:51 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:20:51 INFO DAGScheduler: Submitting ResultStage 115 (MapPartitionsRDD[624] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:20:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:51 INFO MemoryStore: Block broadcast_153 stored as values in memory (estimated size 47.1 KiB, free 366.2 MiB)\n",
      "25/05/05 11:20:51 INFO MemoryStore: Block broadcast_153_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.2 MiB)\n",
      "25/05/05 11:20:51 INFO BlockManagerInfo: Added broadcast_153_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:51 INFO SparkContext: Created broadcast 153 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:20:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 115 (MapPartitionsRDD[624] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:20:51 INFO TaskSchedulerImpl: Adding task set 115.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:20:51 INFO TaskSetManager: Starting task 0.0 in stage 115.0 (TID 115) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:20:51 INFO Executor: Running task 0.0 in stage 115.0 (TID 115)\n",
      "25/05/05 11:20:51 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3269 untilOffset=3270, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=38 taskId=115 partitionId=0\n",
      "25/05/05 11:20:51 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3269 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3270, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:51 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:20:51 INFO DataWritingSparkTask: Committed partition 0 (task 113, attempt 0, stage 113.0)\n",
      "25/05/05 11:20:51 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 506345000 nanos, during time span of 508289917 nanos.\n",
      "25/05/05 11:20:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:51 INFO Executor: Finished task 0.0 in stage 113.0 (TID 113). 3516 bytes result sent to driver\n",
      "25/05/05 11:20:51 INFO TaskSetManager: Finished task 0.0 in stage 113.0 (TID 113) in 529 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:20:51 INFO TaskSchedulerImpl: Removed TaskSet 113.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:20:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3270, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:51 INFO DAGScheduler: ResultStage 113 (start at NativeMethodAccessorImpl.java:0) finished in 0.532 s\n",
      "25/05/05 11:20:51 INFO DAGScheduler: Job 114 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:20:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 113: Stage finished\n",
      "25/05/05 11:20:51 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:20:51 INFO DataWritingSparkTask: Committed partition 0 (task 114, attempt 0, stage 114.0)\n",
      "25/05/05 11:20:51 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 503174208 nanos, during time span of 503604875 nanos.\n",
      "25/05/05 11:20:51 INFO DAGScheduler: Job 114 finished: start at NativeMethodAccessorImpl.java:0, took 0.532673 s\n",
      "25/05/05 11:20:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 38, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@2c028493] is committing.\n",
      "25/05/05 11:20:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 38, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@2c028493] committed.\n",
      "25/05/05 11:20:51 INFO Executor: Finished task 0.0 in stage 114.0 (TID 114). 2188 bytes result sent to driver\n",
      "25/05/05 11:20:51 INFO TaskSetManager: Finished task 0.0 in stage 114.0 (TID 114) in 527 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:20:51 INFO TaskSchedulerImpl: Removed TaskSet 114.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:20:51 INFO DAGScheduler: ResultStage 114 (start at NativeMethodAccessorImpl.java:0) finished in 0.528 s\n",
      "25/05/05 11:20:51 INFO DAGScheduler: Job 115 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:20:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 114: Stage finished\n",
      "25/05/05 11:20:51 INFO DAGScheduler: Job 115 finished: start at NativeMethodAccessorImpl.java:0, took 0.529382 s\n",
      "25/05/05 11:20:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 38, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:20:51 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/38 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.38.63d7855f-3b49-4d2e-8edd-145739d48b86.tmp\n",
      "25/05/05 11:20:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3270, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:51 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:20:51 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:20:51 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:20:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 38, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:20:51 INFO connection: Opened connection [connectionId{localValue:75, serverValue:4445}] to localhost:27017\n",
      "25/05/05 11:20:51 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=5821792}\n",
      "25/05/05 11:20:51 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/38 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.38.e8a79740-b034-460f-b565-ce3d36f56134.tmp\n",
      "25/05/05 11:20:51 INFO connection: Opened connection [connectionId{localValue:76, serverValue:4446}] to localhost:27017\n",
      "25/05/05 11:20:51 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:20:51 INFO connection: Closed connection [connectionId{localValue:76, serverValue:4446}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:20:51 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 504455500 nanos, during time span of 522006625 nanos.\n",
      "25/05/05 11:20:51 INFO Executor: Finished task 0.0 in stage 115.0 (TID 115). 1645 bytes result sent to driver\n",
      "25/05/05 11:20:51 INFO TaskSetManager: Finished task 0.0 in stage 115.0 (TID 115) in 529 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:20:51 INFO TaskSchedulerImpl: Removed TaskSet 115.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:20:51 INFO DAGScheduler: ResultStage 115 (start at NativeMethodAccessorImpl.java:0) finished in 0.534 s\n",
      "25/05/05 11:20:51 INFO DAGScheduler: Job 116 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:20:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 115: Stage finished\n",
      "25/05/05 11:20:51 INFO DAGScheduler: Job 116 finished: start at NativeMethodAccessorImpl.java:0, took 0.535274 s\n",
      "25/05/05 11:20:51 INFO MemoryStore: Block broadcast_154 stored as values in memory (estimated size 248.0 B, free 366.2 MiB)\n",
      "25/05/05 11:20:51 INFO MemoryStore: Block broadcast_154_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.2 MiB)\n",
      "25/05/05 11:20:51 INFO BlockManagerInfo: Added broadcast_154_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:20:51 INFO SparkContext: Created broadcast 154 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:51 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.38.63d7855f-3b49-4d2e-8edd-145739d48b86.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/38\n",
      "25/05/05 11:20:51 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:20:51.325Z\",\n",
      "  \"batchId\" : 38,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.5360983102918586,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 548,\n",
      "    \"commitOffsets\" : 47,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 7,\n",
      "    \"triggerExecution\" : 651,\n",
      "    \"walCommit\" : 45\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3269\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3270\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3270\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.5360983102918586,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:20:51 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/38 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.38.6c97d410-6de4-4da0-bf3e-e42c90f466ab.tmp\n",
      "25/05/05 11:20:51 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.38.e8a79740-b034-460f-b565-ce3d36f56134.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/38\n",
      "25/05/05 11:20:51 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:20:51.337Z\",\n",
      "  \"batchId\" : 38,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.5503875968992247,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 559,\n",
      "    \"commitOffsets\" : 40,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 8,\n",
      "    \"triggerExecution\" : 645,\n",
      "    \"walCommit\" : 37\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3269\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3270\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3270\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.5503875968992247,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:20:52 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.38.6c97d410-6de4-4da0-bf3e-e42c90f466ab.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/38\n",
      "25/05/05 11:20:52 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:20:51.325Z\",\n",
      "  \"batchId\" : 38,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.4771048744460855,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 596,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 8,\n",
      "    \"triggerExecution\" : 677,\n",
      "    \"walCommit\" : 43\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3269\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3270\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3270\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.4771048744460855,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 38\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R022|R033|01-00-00|42 ST-PORT AUTH|05/05/2025|11:20:50|REGULAR|13         |7         |2025-05-05 11:20:51.338|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:20:57 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/39 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.39.a7d978a1-3f4c-42a2-8518-ca70ee971194.tmp\n",
      "25/05/05 11:20:57 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/39 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.39.15010d93-8eaf-4eb8-8984-ea645fb0fcbb.tmp\n",
      "25/05/05 11:20:57 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/39 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.39.a88ce304-8ac9-4cb2-ac32-d10571a93d91.tmp\n",
      "25/05/05 11:20:57 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.39.a7d978a1-3f4c-42a2-8518-ca70ee971194.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/39\n",
      "25/05/05 11:20:57 INFO MicroBatchExecution: Committed offsets for batch 39. Metadata OffsetSeqMetadata(0,1746458457857,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:20:57 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.39.a88ce304-8ac9-4cb2-ac32-d10571a93d91.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/39\n",
      "25/05/05 11:20:57 INFO MicroBatchExecution: Committed offsets for batch 39. Metadata OffsetSeqMetadata(0,1746458457857,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:20:57 INFO IncrementalExecution: Current batch timestamp = 1746458457857\n",
      "25/05/05 11:20:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:57 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.39.15010d93-8eaf-4eb8-8984-ea645fb0fcbb.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/39\n",
      "25/05/05 11:20:57 INFO MicroBatchExecution: Committed offsets for batch 39. Metadata OffsetSeqMetadata(0,1746458457857,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:20:57 INFO IncrementalExecution: Current batch timestamp = 1746458457857\n",
      "25/05/05 11:20:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:57 INFO IncrementalExecution: Current batch timestamp = 1746458457857\n",
      "25/05/05 11:20:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:57 INFO BlockManagerInfo: Removed broadcast_153_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:57 INFO IncrementalExecution: Current batch timestamp = 1746458457857\n",
      "25/05/05 11:20:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:57 INFO IncrementalExecution: Current batch timestamp = 1746458457857\n",
      "25/05/05 11:20:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:57 INFO IncrementalExecution: Current batch timestamp = 1746458457857\n",
      "25/05/05 11:20:57 INFO IncrementalExecution: Current batch timestamp = 1746458457857\n",
      "25/05/05 11:20:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:57 INFO BlockManagerInfo: Removed broadcast_152_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:57 INFO BlockManagerInfo: Removed broadcast_151_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:57 INFO IncrementalExecution: Current batch timestamp = 1746458457857\n",
      "25/05/05 11:20:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:57 INFO BlockManagerInfo: Removed broadcast_154_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:20:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:20:57 INFO CodeGenerator: Code generated in 4.807166 ms\n",
      "25/05/05 11:20:57 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 39, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:20:57 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 39, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@12861863]. The input RDD has 1 partitions.\n",
      "25/05/05 11:20:57 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:57 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:57 INFO DAGScheduler: Got job 117 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:20:57 INFO DAGScheduler: Final stage: ResultStage 116 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:20:57 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:20:57 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:20:57 INFO DAGScheduler: Submitting ResultStage 116 (MapPartitionsRDD[632] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:20:57 INFO MemoryStore: Block broadcast_155 stored as values in memory (estimated size 15.2 KiB, free 366.3 MiB)\n",
      "25/05/05 11:20:57 INFO MemoryStore: Block broadcast_155_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.3 MiB)\n",
      "25/05/05 11:20:57 INFO BlockManagerInfo: Added broadcast_155_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:57 INFO SparkContext: Created broadcast 155 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:20:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 116 (MapPartitionsRDD[632] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:20:57 INFO TaskSchedulerImpl: Adding task set 116.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:20:57 INFO DAGScheduler: Got job 118 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:20:57 INFO DAGScheduler: Final stage: ResultStage 117 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:20:57 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:20:57 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:20:57 INFO TaskSetManager: Starting task 0.0 in stage 116.0 (TID 116) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:20:57 INFO DAGScheduler: Submitting ResultStage 117 (MapPartitionsRDD[635] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:20:57 INFO Executor: Running task 0.0 in stage 116.0 (TID 116)\n",
      "25/05/05 11:20:57 INFO MemoryStore: Block broadcast_156 stored as values in memory (estimated size 16.2 KiB, free 366.3 MiB)\n",
      "25/05/05 11:20:57 INFO MemoryStore: Block broadcast_156_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.3 MiB)\n",
      "25/05/05 11:20:57 INFO BlockManagerInfo: Added broadcast_156_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:57 INFO SparkContext: Created broadcast 156 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:20:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 117 (MapPartitionsRDD[635] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:20:57 INFO TaskSchedulerImpl: Adding task set 117.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:20:57 INFO TaskSetManager: Starting task 0.0 in stage 117.0 (TID 117) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:20:57 INFO Executor: Running task 0.0 in stage 117.0 (TID 117)\n",
      "25/05/05 11:20:57 INFO CodeGenerator: Code generated in 5.758042 ms\n",
      "25/05/05 11:20:57 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3270 untilOffset=3271, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=39 taskId=116 partitionId=0\n",
      "25/05/05 11:20:57 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3270 untilOffset=3271, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=39 taskId=117 partitionId=0\n",
      "25/05/05 11:20:57 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3270 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:57 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3270 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:57 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:57 INFO DAGScheduler: Got job 119 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:20:57 INFO DAGScheduler: Final stage: ResultStage 118 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:20:57 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:20:57 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:20:57 INFO DAGScheduler: Submitting ResultStage 118 (MapPartitionsRDD[640] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:20:57 INFO MemoryStore: Block broadcast_157 stored as values in memory (estimated size 47.1 KiB, free 366.2 MiB)\n",
      "25/05/05 11:20:57 INFO MemoryStore: Block broadcast_157_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:20:57 INFO BlockManagerInfo: Added broadcast_157_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:20:57 INFO SparkContext: Created broadcast 157 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:20:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 118 (MapPartitionsRDD[640] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:20:57 INFO TaskSchedulerImpl: Adding task set 118.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:20:57 INFO TaskSetManager: Starting task 0.0 in stage 118.0 (TID 118) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:20:57 INFO Executor: Running task 0.0 in stage 118.0 (TID 118)\n",
      "25/05/05 11:20:57 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3270 untilOffset=3271, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=39 taskId=118 partitionId=0\n",
      "25/05/05 11:20:57 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3270 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3271, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:58 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:20:58 INFO DataWritingSparkTask: Committed partition 0 (task 116, attempt 0, stage 116.0)\n",
      "25/05/05 11:20:58 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 510591666 nanos, during time span of 511072417 nanos.\n",
      "25/05/05 11:20:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3271, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:58 INFO Executor: Finished task 0.0 in stage 116.0 (TID 116). 2188 bytes result sent to driver\n",
      "25/05/05 11:20:58 INFO TaskSetManager: Finished task 0.0 in stage 116.0 (TID 116) in 525 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:20:58 INFO TaskSchedulerImpl: Removed TaskSet 116.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:20:58 INFO DAGScheduler: ResultStage 116 (start at NativeMethodAccessorImpl.java:0) finished in 0.528 s\n",
      "25/05/05 11:20:58 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:20:58 INFO DataWritingSparkTask: Committed partition 0 (task 117, attempt 0, stage 117.0)\n",
      "25/05/05 11:20:58 INFO DAGScheduler: Job 117 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:20:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 116: Stage finished\n",
      "25/05/05 11:20:58 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 508731709 nanos, during time span of 511759333 nanos.\n",
      "25/05/05 11:20:58 INFO DAGScheduler: Job 117 finished: start at NativeMethodAccessorImpl.java:0, took 0.528565 s\n",
      "25/05/05 11:20:58 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 39, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:20:58 INFO Executor: Finished task 0.0 in stage 117.0 (TID 117). 3516 bytes result sent to driver\n",
      "25/05/05 11:20:58 INFO TaskSetManager: Finished task 0.0 in stage 117.0 (TID 117) in 525 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:20:58 INFO TaskSchedulerImpl: Removed TaskSet 117.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:20:58 INFO DAGScheduler: ResultStage 117 (start at NativeMethodAccessorImpl.java:0) finished in 0.528 s\n",
      "25/05/05 11:20:58 INFO DAGScheduler: Job 118 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:20:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 117: Stage finished\n",
      "25/05/05 11:20:58 INFO DAGScheduler: Job 118 finished: start at NativeMethodAccessorImpl.java:0, took 0.531416 s\n",
      "25/05/05 11:20:58 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 39, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@12861863] is committing.\n",
      "25/05/05 11:20:58 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 39, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@12861863] committed.\n",
      "25/05/05 11:20:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:20:58 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 39, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:20:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3271, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:20:58 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:20:58 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/39 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.39.7ce5c33f-d0f0-4f67-ad35-5ce13713cf21.tmp\n",
      "25/05/05 11:20:58 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:20:58 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:20:58 INFO connection: Opened connection [connectionId{localValue:77, serverValue:4447}] to localhost:27017\n",
      "25/05/05 11:20:58 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=247333}\n",
      "25/05/05 11:20:58 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/39 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.39.4bcd35c8-2eb4-4a21-acec-796640735bbc.tmp\n",
      "25/05/05 11:20:58 INFO connection: Opened connection [connectionId{localValue:78, serverValue:4448}] to localhost:27017\n",
      "25/05/05 11:20:58 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:20:58 INFO connection: Closed connection [connectionId{localValue:78, serverValue:4448}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:20:58 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 507598875 nanos, during time span of 521062084 nanos.\n",
      "25/05/05 11:20:58 INFO Executor: Finished task 0.0 in stage 118.0 (TID 118). 1645 bytes result sent to driver\n",
      "25/05/05 11:20:58 INFO TaskSetManager: Finished task 0.0 in stage 118.0 (TID 118) in 529 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:20:58 INFO TaskSchedulerImpl: Removed TaskSet 118.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:20:58 INFO DAGScheduler: ResultStage 118 (start at NativeMethodAccessorImpl.java:0) finished in 0.535 s\n",
      "25/05/05 11:20:58 INFO DAGScheduler: Job 119 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:20:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 118: Stage finished\n",
      "25/05/05 11:20:58 INFO DAGScheduler: Job 119 finished: start at NativeMethodAccessorImpl.java:0, took 0.537022 s\n",
      "25/05/05 11:20:58 INFO MemoryStore: Block broadcast_158 stored as values in memory (estimated size 248.0 B, free 366.2 MiB)\n",
      "25/05/05 11:20:58 INFO MemoryStore: Block broadcast_158_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.2 MiB)\n",
      "25/05/05 11:20:58 INFO BlockManagerInfo: Added broadcast_158_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:20:58 INFO SparkContext: Created broadcast 158 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:20:58 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/39 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.39.8ffd343a-9473-4113-8269-54f18c3c79b7.tmp\n",
      "25/05/05 11:20:58 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.39.7ce5c33f-d0f0-4f67-ad35-5ce13713cf21.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/39\n",
      "25/05/05 11:20:58 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:20:57.856Z\",\n",
      "  \"batchId\" : 39,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.5503875968992247,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 545,\n",
      "    \"commitOffsets\" : 42,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 14,\n",
      "    \"triggerExecution\" : 645,\n",
      "    \"walCommit\" : 42\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3270\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3271\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3271\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.5503875968992247,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:20:58 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.39.4bcd35c8-2eb4-4a21-acec-796640735bbc.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/39\n",
      "25/05/05 11:20:58 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:20:57.857Z\",\n",
      "  \"batchId\" : 39,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.5408320493066254,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 573,\n",
      "    \"commitOffsets\" : 34,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 0,\n",
      "    \"queryPlanning\" : 8,\n",
      "    \"triggerExecution\" : 649,\n",
      "    \"walCommit\" : 33\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3270\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3271\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3271\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.5408320493066254,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:20:58 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.39.8ffd343a-9473-4113-8269-54f18c3c79b7.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/39\n",
      "25/05/05 11:20:58 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:20:57.856Z\",\n",
      "  \"batchId\" : 39,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.506024096385542,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 579,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 15,\n",
      "    \"triggerExecution\" : 664,\n",
      "    \"walCommit\" : 38\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3270\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3271\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3271\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.506024096385542,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 39\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R023|R045|00-00-00|34 ST-HERALD SQ|05/05/2025|11:20:56|REGULAR|12         |10        |2025-05-05 11:20:57.857|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:21:04 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/40 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.40.6825c140-538d-4679-b3ea-c3570d4ee74f.tmp\n",
      "25/05/05 11:21:04 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/40 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.40.84797eec-bc96-4694-aa2f-d441d96d2457.tmp\n",
      "25/05/05 11:21:04 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/40 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.40.5280d2b7-79a5-42d3-a714-85599e951ece.tmp\n",
      "25/05/05 11:21:04 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.40.6825c140-538d-4679-b3ea-c3570d4ee74f.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/40\n",
      "25/05/05 11:21:04 INFO MicroBatchExecution: Committed offsets for batch 40. Metadata OffsetSeqMetadata(0,1746458464346,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:21:04 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.40.84797eec-bc96-4694-aa2f-d441d96d2457.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/40\n",
      "25/05/05 11:21:04 INFO MicroBatchExecution: Committed offsets for batch 40. Metadata OffsetSeqMetadata(0,1746458464354,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:21:04 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.40.5280d2b7-79a5-42d3-a714-85599e951ece.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/40\n",
      "25/05/05 11:21:04 INFO MicroBatchExecution: Committed offsets for batch 40. Metadata OffsetSeqMetadata(0,1746458464354,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:21:04 INFO IncrementalExecution: Current batch timestamp = 1746458464354\n",
      "25/05/05 11:21:04 INFO IncrementalExecution: Current batch timestamp = 1746458464346\n",
      "25/05/05 11:21:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:04 INFO IncrementalExecution: Current batch timestamp = 1746458464354\n",
      "25/05/05 11:21:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:04 INFO IncrementalExecution: Current batch timestamp = 1746458464354\n",
      "25/05/05 11:21:04 INFO IncrementalExecution: Current batch timestamp = 1746458464346\n",
      "25/05/05 11:21:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:04 INFO IncrementalExecution: Current batch timestamp = 1746458464354\n",
      "25/05/05 11:21:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:04 INFO IncrementalExecution: Current batch timestamp = 1746458464346\n",
      "25/05/05 11:21:04 INFO IncrementalExecution: Current batch timestamp = 1746458464354\n",
      "25/05/05 11:21:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:04 INFO CodeGenerator: Code generated in 7.354292 ms\n",
      "25/05/05 11:21:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 40, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:21:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:04 INFO DAGScheduler: Got job 120 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:21:04 INFO DAGScheduler: Final stage: ResultStage 119 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:21:04 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:21:04 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:21:04 INFO DAGScheduler: Submitting ResultStage 119 (MapPartitionsRDD[646] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:21:04 INFO MemoryStore: Block broadcast_159 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:04 INFO MemoryStore: Block broadcast_159_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:04 INFO BlockManagerInfo: Added broadcast_159_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:04 INFO SparkContext: Created broadcast 159 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:21:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 119 (MapPartitionsRDD[646] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:21:04 INFO TaskSchedulerImpl: Adding task set 119.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:21:04 INFO TaskSetManager: Starting task 0.0 in stage 119.0 (TID 119) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:21:04 INFO Executor: Running task 0.0 in stage 119.0 (TID 119)\n",
      "25/05/05 11:21:04 INFO CodeGenerator: Code generated in 15.7325 ms\n",
      "25/05/05 11:21:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 40, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@a778311]. The input RDD has 1 partitions.\n",
      "25/05/05 11:21:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:04 INFO BlockManagerInfo: Removed broadcast_157_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:04 INFO DAGScheduler: Got job 121 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:21:04 INFO DAGScheduler: Final stage: ResultStage 120 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:21:04 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:21:04 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:21:04 INFO DAGScheduler: Submitting ResultStage 120 (MapPartitionsRDD[651] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:21:04 INFO MemoryStore: Block broadcast_160 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:04 INFO MemoryStore: Block broadcast_160_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:04 INFO BlockManagerInfo: Added broadcast_160_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:04 INFO SparkContext: Created broadcast 160 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:21:04 INFO CodeGenerator: Code generated in 5.975167 ms\n",
      "25/05/05 11:21:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 120 (MapPartitionsRDD[651] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:21:04 INFO TaskSchedulerImpl: Adding task set 120.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:21:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3271 untilOffset=3272, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=40 taskId=119 partitionId=0\n",
      "25/05/05 11:21:04 INFO TaskSetManager: Starting task 0.0 in stage 120.0 (TID 120) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:21:04 INFO Executor: Running task 0.0 in stage 120.0 (TID 120)\n",
      "25/05/05 11:21:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3271 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:04 INFO BlockManagerInfo: Removed broadcast_158_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:21:04 INFO BlockManagerInfo: Removed broadcast_156_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:04 INFO BlockManagerInfo: Removed broadcast_155_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:04 INFO DAGScheduler: Got job 122 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:21:04 INFO DAGScheduler: Final stage: ResultStage 121 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:21:04 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:21:04 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:21:04 INFO DAGScheduler: Submitting ResultStage 121 (MapPartitionsRDD[656] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:21:04 INFO CodeGenerator: Code generated in 24.45375 ms\n",
      "25/05/05 11:21:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:04 INFO MemoryStore: Block broadcast_161 stored as values in memory (estimated size 47.1 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:04 INFO MemoryStore: Block broadcast_161_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:04 INFO BlockManagerInfo: Added broadcast_161_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:04 INFO SparkContext: Created broadcast 161 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:21:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 121 (MapPartitionsRDD[656] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:21:04 INFO TaskSchedulerImpl: Adding task set 121.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:21:04 INFO TaskSetManager: Starting task 0.0 in stage 121.0 (TID 121) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:21:04 INFO Executor: Running task 0.0 in stage 121.0 (TID 121)\n",
      "25/05/05 11:21:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3271 untilOffset=3272, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=40 taskId=120 partitionId=0\n",
      "25/05/05 11:21:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3271 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3271 untilOffset=3272, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=40 taskId=121 partitionId=0\n",
      "25/05/05 11:21:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3271 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3272, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:04 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:21:04 INFO DataWritingSparkTask: Committed partition 0 (task 119, attempt 0, stage 119.0)\n",
      "25/05/05 11:21:04 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 528656667 nanos, during time span of 529162375 nanos.\n",
      "25/05/05 11:21:04 INFO Executor: Finished task 0.0 in stage 119.0 (TID 119). 2145 bytes result sent to driver\n",
      "25/05/05 11:21:04 INFO TaskSetManager: Finished task 0.0 in stage 119.0 (TID 119) in 540 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:21:04 INFO TaskSchedulerImpl: Removed TaskSet 119.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:21:04 INFO DAGScheduler: ResultStage 119 (start at NativeMethodAccessorImpl.java:0) finished in 0.550 s\n",
      "25/05/05 11:21:04 INFO DAGScheduler: Job 120 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:21:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 119: Stage finished\n",
      "25/05/05 11:21:04 INFO DAGScheduler: Job 120 finished: start at NativeMethodAccessorImpl.java:0, took 0.551594 s\n",
      "25/05/05 11:21:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 40, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:21:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3272, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:04 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:21:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:04 INFO DataWritingSparkTask: Committed partition 0 (task 120, attempt 0, stage 120.0)\n",
      "25/05/05 11:21:04 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 503900125 nanos, during time span of 505752000 nanos.\n",
      "25/05/05 11:21:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 40, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:21:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3272, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:04 INFO Executor: Finished task 0.0 in stage 120.0 (TID 120). 3559 bytes result sent to driver\n",
      "25/05/05 11:21:04 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:21:04 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:21:04 INFO TaskSetManager: Finished task 0.0 in stage 120.0 (TID 120) in 541 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:21:04 INFO TaskSchedulerImpl: Removed TaskSet 120.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:21:04 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:21:04 INFO DAGScheduler: ResultStage 120 (start at NativeMethodAccessorImpl.java:0) finished in 0.545 s\n",
      "25/05/05 11:21:04 INFO DAGScheduler: Job 121 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:21:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 120: Stage finished\n",
      "25/05/05 11:21:04 INFO DAGScheduler: Job 121 finished: start at NativeMethodAccessorImpl.java:0, took 0.546341 s\n",
      "25/05/05 11:21:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 40, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@a778311] is committing.\n",
      "25/05/05 11:21:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 40, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@a778311] committed.\n",
      "25/05/05 11:21:04 INFO connection: Opened connection [connectionId{localValue:79, serverValue:4449}] to localhost:27017\n",
      "25/05/05 11:21:04 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=257333}\n",
      "25/05/05 11:21:04 INFO connection: Opened connection [connectionId{localValue:80, serverValue:4450}] to localhost:27017\n",
      "25/05/05 11:21:04 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/40 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.40.34a896dc-0f01-4ac3-8b42-fa6506443f44.tmp\n",
      "25/05/05 11:21:04 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:21:04 INFO connection: Closed connection [connectionId{localValue:80, serverValue:4450}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:21:04 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 502106917 nanos, during time span of 511515709 nanos.\n",
      "25/05/05 11:21:05 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/40 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.40.05326c5f-aae2-42a6-90fa-4dff66bb6c8c.tmp\n",
      "25/05/05 11:21:05 INFO Executor: Finished task 0.0 in stage 121.0 (TID 121). 1645 bytes result sent to driver\n",
      "25/05/05 11:21:05 INFO TaskSetManager: Finished task 0.0 in stage 121.0 (TID 121) in 525 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:21:05 INFO TaskSchedulerImpl: Removed TaskSet 121.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:21:05 INFO DAGScheduler: ResultStage 121 (start at NativeMethodAccessorImpl.java:0) finished in 0.541 s\n",
      "25/05/05 11:21:05 INFO DAGScheduler: Job 122 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:21:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 121: Stage finished\n",
      "25/05/05 11:21:05 INFO DAGScheduler: Job 122 finished: start at NativeMethodAccessorImpl.java:0, took 0.542230 s\n",
      "25/05/05 11:21:05 INFO MemoryStore: Block broadcast_162 stored as values in memory (estimated size 248.0 B, free 366.2 MiB)\n",
      "25/05/05 11:21:05 INFO MemoryStore: Block broadcast_162_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.2 MiB)\n",
      "25/05/05 11:21:05 INFO BlockManagerInfo: Added broadcast_162_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:21:05 INFO SparkContext: Created broadcast 162 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:05 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/40 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.40.2905c838-2480-40da-8b7f-840a41ee9ea3.tmp\n",
      "25/05/05 11:21:05 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.40.34a896dc-0f01-4ac3-8b42-fa6506443f44.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/40\n",
      "25/05/05 11:21:05 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.40.05326c5f-aae2-42a6-90fa-4dff66bb6c8c.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/40\n",
      "25/05/05 11:21:05 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:21:04.345Z\",\n",
      "  \"batchId\" : 40,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.4577259475218658,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 578,\n",
      "    \"commitOffsets\" : 40,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 686,\n",
      "    \"walCommit\" : 62\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3271\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3272\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3272\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.4577259475218658,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:21:05 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:21:04.351Z\",\n",
      "  \"batchId\" : 40,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.4705882352941175,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 575,\n",
      "    \"commitOffsets\" : 42,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 680,\n",
      "    \"walCommit\" : 55\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3271\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3272\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3272\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.4705882352941175,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:21:05 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.40.2905c838-2480-40da-8b7f-840a41ee9ea3.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/40\n",
      "25/05/05 11:21:05 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:21:04.351Z\",\n",
      "  \"batchId\" : 40,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 62.5,\n",
      "  \"processedRowsPerSecond\" : 1.4534883720930234,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 596,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 688,\n",
      "    \"walCommit\" : 54\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3271\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3272\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3272\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 62.5,\n",
      "    \"processedRowsPerSecond\" : 1.4534883720930234,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 40\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R023|R045|00-00-00|34 ST-HERALD SQ|05/05/2025|11:21:03|REGULAR|4          |8         |2025-05-05 11:21:04.354|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:21:10 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/41 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.41.3297b3e5-da3d-4ea0-8949-f4e4c66f2392.tmp\n",
      "25/05/05 11:21:10 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/41 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.41.92b5188b-48d2-45db-93bc-d8546a1a9a8f.tmp\n",
      "25/05/05 11:21:10 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/41 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.41.f04c30a1-52d4-49c0-b404-a36277357391.tmp\n",
      "25/05/05 11:21:10 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.41.3297b3e5-da3d-4ea0-8949-f4e4c66f2392.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/41\n",
      "25/05/05 11:21:10 INFO MicroBatchExecution: Committed offsets for batch 41. Metadata OffsetSeqMetadata(0,1746458470852,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:21:10 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.41.92b5188b-48d2-45db-93bc-d8546a1a9a8f.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/41\n",
      "25/05/05 11:21:10 INFO MicroBatchExecution: Committed offsets for batch 41. Metadata OffsetSeqMetadata(0,1746458470853,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:21:10 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.41.f04c30a1-52d4-49c0-b404-a36277357391.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/41\n",
      "25/05/05 11:21:10 INFO MicroBatchExecution: Committed offsets for batch 41. Metadata OffsetSeqMetadata(0,1746458470852,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:21:10 INFO IncrementalExecution: Current batch timestamp = 1746458470852\n",
      "25/05/05 11:21:10 INFO IncrementalExecution: Current batch timestamp = 1746458470852\n",
      "25/05/05 11:21:10 INFO IncrementalExecution: Current batch timestamp = 1746458470853\n",
      "25/05/05 11:21:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:10 INFO IncrementalExecution: Current batch timestamp = 1746458470853\n",
      "25/05/05 11:21:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:10 INFO IncrementalExecution: Current batch timestamp = 1746458470852\n",
      "25/05/05 11:21:10 INFO IncrementalExecution: Current batch timestamp = 1746458470852\n",
      "25/05/05 11:21:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:10 INFO IncrementalExecution: Current batch timestamp = 1746458470852\n",
      "25/05/05 11:21:10 INFO IncrementalExecution: Current batch timestamp = 1746458470853\n",
      "25/05/05 11:21:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:10 INFO CodeGenerator: Code generated in 12.111333 ms\n",
      "25/05/05 11:21:10 INFO CodeGenerator: Code generated in 6.03675 ms\n",
      "25/05/05 11:21:10 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 41, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:21:10 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 41, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@40bbc67c]. The input RDD has 1 partitions.\n",
      "25/05/05 11:21:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:10 INFO DAGScheduler: Got job 123 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:21:10 INFO DAGScheduler: Final stage: ResultStage 122 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:21:10 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:21:10 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:21:10 INFO DAGScheduler: Submitting ResultStage 122 (MapPartitionsRDD[664] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:21:10 INFO MemoryStore: Block broadcast_163 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:10 INFO MemoryStore: Block broadcast_163_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:10 INFO BlockManagerInfo: Added broadcast_163_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:10 INFO SparkContext: Created broadcast 163 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:21:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 122 (MapPartitionsRDD[664] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:21:10 INFO TaskSchedulerImpl: Adding task set 122.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:21:10 INFO DAGScheduler: Got job 124 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:21:10 INFO DAGScheduler: Final stage: ResultStage 123 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:21:10 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:21:10 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:21:10 INFO DAGScheduler: Submitting ResultStage 123 (MapPartitionsRDD[665] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:21:10 INFO TaskSetManager: Starting task 0.0 in stage 122.0 (TID 122) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:21:10 INFO Executor: Running task 0.0 in stage 122.0 (TID 122)\n",
      "25/05/05 11:21:10 INFO MemoryStore: Block broadcast_164 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:10 INFO MemoryStore: Block broadcast_164_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:10 INFO BlockManagerInfo: Added broadcast_164_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:10 INFO SparkContext: Created broadcast 164 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:21:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 123 (MapPartitionsRDD[665] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:21:10 INFO TaskSchedulerImpl: Adding task set 123.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:21:10 INFO TaskSetManager: Starting task 0.0 in stage 123.0 (TID 123) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:21:10 INFO Executor: Running task 0.0 in stage 123.0 (TID 123)\n",
      "25/05/05 11:21:10 INFO CodeGenerator: Code generated in 4.730916 ms\n",
      "25/05/05 11:21:10 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3272 untilOffset=3273, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=41 taskId=122 partitionId=0\n",
      "25/05/05 11:21:10 INFO BlockManagerInfo: Removed broadcast_159_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:10 INFO CodeGenerator: Code generated in 9.848209 ms\n",
      "25/05/05 11:21:10 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3272 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:10 INFO BlockManagerInfo: Removed broadcast_161_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:10 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3272 untilOffset=3273, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=41 taskId=123 partitionId=0\n",
      "25/05/05 11:21:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:10 INFO BlockManagerInfo: Removed broadcast_162_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:21:10 INFO BlockManagerInfo: Removed broadcast_160_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:10 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3272 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:10 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:10 INFO DAGScheduler: Got job 125 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:21:10 INFO DAGScheduler: Final stage: ResultStage 124 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:21:10 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:21:10 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:21:10 INFO DAGScheduler: Submitting ResultStage 124 (MapPartitionsRDD[672] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:21:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:10 INFO MemoryStore: Block broadcast_165 stored as values in memory (estimated size 47.1 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:10 INFO MemoryStore: Block broadcast_165_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:10 INFO BlockManagerInfo: Added broadcast_165_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:10 INFO SparkContext: Created broadcast 165 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:21:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 124 (MapPartitionsRDD[672] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:21:10 INFO TaskSchedulerImpl: Adding task set 124.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:21:10 INFO TaskSetManager: Starting task 0.0 in stage 124.0 (TID 124) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:21:10 INFO Executor: Running task 0.0 in stage 124.0 (TID 124)\n",
      "25/05/05 11:21:10 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3272 untilOffset=3273, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=41 taskId=124 partitionId=0\n",
      "25/05/05 11:21:10 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3272 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:10 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3273, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:11 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:21:11 INFO DataWritingSparkTask: Committed partition 0 (task 122, attempt 0, stage 122.0)\n",
      "25/05/05 11:21:11 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 507592042 nanos, during time span of 508091375 nanos.\n",
      "25/05/05 11:21:11 INFO Executor: Finished task 0.0 in stage 122.0 (TID 122). 2188 bytes result sent to driver\n",
      "25/05/05 11:21:11 INFO TaskSetManager: Finished task 0.0 in stage 122.0 (TID 122) in 525 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:21:11 INFO TaskSchedulerImpl: Removed TaskSet 122.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:21:11 INFO DAGScheduler: ResultStage 122 (start at NativeMethodAccessorImpl.java:0) finished in 0.527 s\n",
      "25/05/05 11:21:11 INFO DAGScheduler: Job 123 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:21:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 122: Stage finished\n",
      "25/05/05 11:21:11 INFO DAGScheduler: Job 123 finished: start at NativeMethodAccessorImpl.java:0, took 0.530571 s\n",
      "25/05/05 11:21:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 41, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:21:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3273, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:11 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:21:11 INFO DataWritingSparkTask: Committed partition 0 (task 123, attempt 0, stage 123.0)\n",
      "25/05/05 11:21:11 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 505002584 nanos, during time span of 506745333 nanos.\n",
      "25/05/05 11:21:11 INFO Executor: Finished task 0.0 in stage 123.0 (TID 123). 3557 bytes result sent to driver\n",
      "25/05/05 11:21:11 INFO TaskSetManager: Finished task 0.0 in stage 123.0 (TID 123) in 527 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:21:11 INFO TaskSchedulerImpl: Removed TaskSet 123.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:21:11 INFO DAGScheduler: ResultStage 123 (start at NativeMethodAccessorImpl.java:0) finished in 0.530 s\n",
      "25/05/05 11:21:11 INFO DAGScheduler: Job 124 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:21:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 123: Stage finished\n",
      "25/05/05 11:21:11 INFO DAGScheduler: Job 124 finished: start at NativeMethodAccessorImpl.java:0, took 0.534467 s\n",
      "25/05/05 11:21:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 41, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@40bbc67c] is committing.\n",
      "25/05/05 11:21:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 41, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@40bbc67c] committed.\n",
      "25/05/05 11:21:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 41, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:21:11 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/41 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.41.06a56977-4614-4079-a62b-8f03e2f1c1e9.tmp\n",
      "25/05/05 11:21:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3273, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:11 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:21:11 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:21:11 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:21:11 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/41 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.41.7259d75c-e351-4a2d-bd1e-b81fce5862ce.tmp\n",
      "25/05/05 11:21:11 INFO connection: Opened connection [connectionId{localValue:81, serverValue:4451}] to localhost:27017\n",
      "25/05/05 11:21:11 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=283791}\n",
      "25/05/05 11:21:11 INFO connection: Opened connection [connectionId{localValue:82, serverValue:4452}] to localhost:27017\n",
      "25/05/05 11:21:11 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:21:11 INFO connection: Closed connection [connectionId{localValue:82, serverValue:4452}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:21:11 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 504465958 nanos, during time span of 509813333 nanos.\n",
      "25/05/05 11:21:11 INFO Executor: Finished task 0.0 in stage 124.0 (TID 124). 1645 bytes result sent to driver\n",
      "25/05/05 11:21:11 INFO TaskSetManager: Finished task 0.0 in stage 124.0 (TID 124) in 516 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:21:11 INFO TaskSchedulerImpl: Removed TaskSet 124.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:21:11 INFO DAGScheduler: ResultStage 124 (start at NativeMethodAccessorImpl.java:0) finished in 0.521 s\n",
      "25/05/05 11:21:11 INFO DAGScheduler: Job 125 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:21:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 124: Stage finished\n",
      "25/05/05 11:21:11 INFO DAGScheduler: Job 125 finished: start at NativeMethodAccessorImpl.java:0, took 0.521898 s\n",
      "25/05/05 11:21:11 INFO MemoryStore: Block broadcast_166 stored as values in memory (estimated size 248.0 B, free 366.2 MiB)\n",
      "25/05/05 11:21:11 INFO MemoryStore: Block broadcast_166_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.2 MiB)\n",
      "25/05/05 11:21:11 INFO BlockManagerInfo: Added broadcast_166_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:21:11 INFO SparkContext: Created broadcast 166 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:11 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/41 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.41.17c0256a-971a-4441-8257-fedefe4b94e9.tmp\n",
      "25/05/05 11:21:11 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.41.06a56977-4614-4079-a62b-8f03e2f1c1e9.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/41\n",
      "25/05/05 11:21:11 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:21:10.843Z\",\n",
      "  \"batchId\" : 41,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "  \"processedRowsPerSecond\" : 1.547987616099071,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 556,\n",
      "    \"commitOffsets\" : 31,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 9,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 646,\n",
      "    \"walCommit\" : 45\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3272\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3273\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3273\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "    \"processedRowsPerSecond\" : 1.547987616099071,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:21:11 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.41.7259d75c-e351-4a2d-bd1e-b81fce5862ce.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/41\n",
      "25/05/05 11:21:11 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:21:10.840Z\",\n",
      "  \"batchId\" : 41,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.5313935681470137,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 559,\n",
      "    \"commitOffsets\" : 31,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 13,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 653,\n",
      "    \"walCommit\" : 44\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3272\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3273\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3273\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.5313935681470137,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:21:11 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.41.17c0256a-971a-4441-8257-fedefe4b94e9.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/41\n",
      "25/05/05 11:21:11 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:21:10.840Z\",\n",
      "  \"batchId\" : 41,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.4992503748125936,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 573,\n",
      "    \"commitOffsets\" : 31,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 12,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 667,\n",
      "    \"walCommit\" : 45\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3272\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3273\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3273\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.4992503748125936,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 41\n",
      "-------------------------------------------\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION      |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R016|R032|00-00-01|TIME SQ-42 ST|05/05/2025|11:21:09|REGULAR|4          |11        |2025-05-05 11:21:10.853|\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:21:17 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/42 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.42.8c761954-9a25-4120-8817-97107ac4765e.tmp\n",
      "25/05/05 11:21:17 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/42 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.42.ef7ce229-316f-4591-aaab-6458ab2f9905.tmp\n",
      "25/05/05 11:21:17 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/42 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.42.9d719f81-c664-4e26-ac21-e41347b7486a.tmp\n",
      "25/05/05 11:21:17 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.42.ef7ce229-316f-4591-aaab-6458ab2f9905.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/42\n",
      "25/05/05 11:21:17 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.42.8c761954-9a25-4120-8817-97107ac4765e.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/42\n",
      "25/05/05 11:21:17 INFO MicroBatchExecution: Committed offsets for batch 42. Metadata OffsetSeqMetadata(0,1746458477342,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:21:17 INFO MicroBatchExecution: Committed offsets for batch 42. Metadata OffsetSeqMetadata(0,1746458477344,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:21:17 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.42.9d719f81-c664-4e26-ac21-e41347b7486a.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/42\n",
      "25/05/05 11:21:17 INFO MicroBatchExecution: Committed offsets for batch 42. Metadata OffsetSeqMetadata(0,1746458477344,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:21:17 INFO IncrementalExecution: Current batch timestamp = 1746458477344\n",
      "25/05/05 11:21:17 INFO IncrementalExecution: Current batch timestamp = 1746458477342\n",
      "25/05/05 11:21:17 INFO IncrementalExecution: Current batch timestamp = 1746458477344\n",
      "25/05/05 11:21:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:17 INFO IncrementalExecution: Current batch timestamp = 1746458477344\n",
      "25/05/05 11:21:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:17 INFO IncrementalExecution: Current batch timestamp = 1746458477344\n",
      "25/05/05 11:21:17 INFO IncrementalExecution: Current batch timestamp = 1746458477342\n",
      "25/05/05 11:21:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:17 INFO IncrementalExecution: Current batch timestamp = 1746458477344\n",
      "25/05/05 11:21:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:17 INFO IncrementalExecution: Current batch timestamp = 1746458477344\n",
      "25/05/05 11:21:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:17 INFO CodeGenerator: Code generated in 4.806708 ms\n",
      "25/05/05 11:21:17 INFO CodeGenerator: Code generated in 3.657708 ms\n",
      "25/05/05 11:21:17 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 42, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@45d417c7]. The input RDD has 1 partitions.\n",
      "25/05/05 11:21:17 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:17 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 42, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:21:17 INFO DAGScheduler: Got job 126 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:21:17 INFO DAGScheduler: Final stage: ResultStage 125 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:21:17 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:21:17 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:21:17 INFO DAGScheduler: Submitting ResultStage 125 (MapPartitionsRDD[680] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:21:17 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:17 INFO MemoryStore: Block broadcast_167 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:17 INFO MemoryStore: Block broadcast_167_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:17 INFO BlockManagerInfo: Added broadcast_167_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:17 INFO SparkContext: Created broadcast 167 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:21:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 125 (MapPartitionsRDD[680] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:21:17 INFO TaskSchedulerImpl: Adding task set 125.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:21:17 INFO DAGScheduler: Got job 127 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:21:17 INFO DAGScheduler: Final stage: ResultStage 126 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:21:17 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:21:17 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:21:17 INFO DAGScheduler: Submitting ResultStage 126 (MapPartitionsRDD[683] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:21:17 INFO MemoryStore: Block broadcast_168 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:17 INFO MemoryStore: Block broadcast_168_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:17 INFO TaskSetManager: Starting task 0.0 in stage 125.0 (TID 125) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:21:17 INFO BlockManagerInfo: Added broadcast_168_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:17 INFO SparkContext: Created broadcast 168 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:21:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 126 (MapPartitionsRDD[683] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:21:17 INFO TaskSchedulerImpl: Adding task set 126.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:21:17 INFO Executor: Running task 0.0 in stage 125.0 (TID 125)\n",
      "25/05/05 11:21:17 INFO TaskSetManager: Starting task 0.0 in stage 126.0 (TID 126) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:21:17 INFO Executor: Running task 0.0 in stage 126.0 (TID 126)\n",
      "25/05/05 11:21:17 INFO CodeGenerator: Code generated in 3.805583 ms\n",
      "25/05/05 11:21:17 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3273 untilOffset=3274, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=42 taskId=126 partitionId=0\n",
      "25/05/05 11:21:17 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:17 INFO DAGScheduler: Got job 128 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:21:17 INFO DAGScheduler: Final stage: ResultStage 127 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:21:17 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:21:17 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:21:17 INFO DAGScheduler: Submitting ResultStage 127 (MapPartitionsRDD[688] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:21:17 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3273 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:17 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3273 untilOffset=3274, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=42 taskId=125 partitionId=0\n",
      "25/05/05 11:21:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:17 INFO MemoryStore: Block broadcast_169 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:21:17 INFO MemoryStore: Block broadcast_169_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:21:17 INFO BlockManagerInfo: Added broadcast_169_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:21:17 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3273 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:17 INFO SparkContext: Created broadcast 169 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:21:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 127 (MapPartitionsRDD[688] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:21:17 INFO TaskSchedulerImpl: Adding task set 127.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:21:17 INFO BlockManagerInfo: Removed broadcast_166_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:21:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:17 INFO TaskSetManager: Starting task 0.0 in stage 127.0 (TID 127) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:21:17 INFO Executor: Running task 0.0 in stage 127.0 (TID 127)\n",
      "25/05/05 11:21:17 INFO BlockManagerInfo: Removed broadcast_165_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:17 INFO BlockManagerInfo: Removed broadcast_164_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:17 INFO BlockManagerInfo: Removed broadcast_163_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:17 INFO CodeGenerator: Code generated in 225.561333 ms\n",
      "25/05/05 11:21:17 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3273 untilOffset=3274, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=42 taskId=127 partitionId=0\n",
      "25/05/05 11:21:17 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3273 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3274, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:17 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:21:17 INFO DataWritingSparkTask: Committed partition 0 (task 126, attempt 0, stage 126.0)\n",
      "25/05/05 11:21:17 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 507258750 nanos, during time span of 507690667 nanos.\n",
      "25/05/05 11:21:17 INFO Executor: Finished task 0.0 in stage 126.0 (TID 126). 2231 bytes result sent to driver\n",
      "25/05/05 11:21:17 INFO TaskSetManager: Finished task 0.0 in stage 126.0 (TID 126) in 518 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:21:17 INFO TaskSchedulerImpl: Removed TaskSet 126.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:21:17 INFO DAGScheduler: ResultStage 126 (start at NativeMethodAccessorImpl.java:0) finished in 0.523 s\n",
      "25/05/05 11:21:17 INFO DAGScheduler: Job 127 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:21:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 126: Stage finished\n",
      "25/05/05 11:21:17 INFO DAGScheduler: Job 127 finished: start at NativeMethodAccessorImpl.java:0, took 0.524909 s\n",
      "25/05/05 11:21:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 42, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:21:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3274, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:17 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:21:17 INFO DataWritingSparkTask: Committed partition 0 (task 125, attempt 0, stage 125.0)\n",
      "25/05/05 11:21:17 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 505057500 nanos, during time span of 507682708 nanos.\n",
      "25/05/05 11:21:17 INFO Executor: Finished task 0.0 in stage 125.0 (TID 125). 3602 bytes result sent to driver\n",
      "25/05/05 11:21:17 INFO TaskSetManager: Finished task 0.0 in stage 125.0 (TID 125) in 534 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:21:17 INFO TaskSchedulerImpl: Removed TaskSet 125.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:21:17 INFO DAGScheduler: ResultStage 125 (start at NativeMethodAccessorImpl.java:0) finished in 0.537 s\n",
      "25/05/05 11:21:17 INFO DAGScheduler: Job 126 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:21:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 125: Stage finished\n",
      "25/05/05 11:21:17 INFO DAGScheduler: Job 126 finished: start at NativeMethodAccessorImpl.java:0, took 0.547190 s\n",
      "25/05/05 11:21:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 42, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@45d417c7] is committing.\n",
      "25/05/05 11:21:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 42, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@45d417c7] committed.\n",
      "25/05/05 11:21:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 42, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:21:17 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/42 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.42.c49118a6-dc5c-4d84-8326-fd2f00c06cb3.tmp\n",
      "25/05/05 11:21:17 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/42 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.42.94912322-21ef-42af-9737-5bb0b825a645.tmp\n",
      "25/05/05 11:21:17 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.42.c49118a6-dc5c-4d84-8326-fd2f00c06cb3.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/42\n",
      "25/05/05 11:21:17 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:21:17.343Z\",\n",
      "  \"batchId\" : 42,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.5384615384615383,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 567,\n",
      "    \"commitOffsets\" : 40,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 650,\n",
      "    \"walCommit\" : 36\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3273\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3274\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3274\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.5384615384615383,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:21:18 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.42.94912322-21ef-42af-9737-5bb0b825a645.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/42\n",
      "25/05/05 11:21:18 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:21:17.343Z\",\n",
      "  \"batchId\" : 42,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.5220700152207,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 575,\n",
      "    \"commitOffsets\" : 37,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 657,\n",
      "    \"walCommit\" : 37\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3273\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3274\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3274\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.5220700152207,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 42\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|B013|R081|02-00-00|161/YANKEE STAD|05/05/2025|11:21:16|REGULAR|7          |11        |2025-05-05 11:21:17.344|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:21:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3274, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:18 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:21:18 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:21:18 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:21:18 INFO connection: Opened connection [connectionId{localValue:83, serverValue:4453}] to localhost:27017\n",
      "25/05/05 11:21:18 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=252291}\n",
      "25/05/05 11:21:18 INFO connection: Opened connection [connectionId{localValue:84, serverValue:4454}] to localhost:27017\n",
      "25/05/05 11:21:18 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:21:18 INFO connection: Closed connection [connectionId{localValue:84, serverValue:4454}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:21:18 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 506051333 nanos, during time span of 520216791 nanos.\n",
      "25/05/05 11:21:18 INFO Executor: Finished task 0.0 in stage 127.0 (TID 127). 1688 bytes result sent to driver\n",
      "25/05/05 11:21:18 INFO TaskSetManager: Finished task 0.0 in stage 127.0 (TID 127) in 761 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:21:18 INFO TaskSchedulerImpl: Removed TaskSet 127.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:21:18 INFO DAGScheduler: ResultStage 127 (start at NativeMethodAccessorImpl.java:0) finished in 0.774 s\n",
      "25/05/05 11:21:18 INFO DAGScheduler: Job 128 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:21:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 127: Stage finished\n",
      "25/05/05 11:21:18 INFO DAGScheduler: Job 128 finished: start at NativeMethodAccessorImpl.java:0, took 0.775612 s\n",
      "25/05/05 11:21:18 INFO MemoryStore: Block broadcast_170 stored as values in memory (estimated size 248.0 B, free 366.2 MiB)\n",
      "25/05/05 11:21:18 INFO MemoryStore: Block broadcast_170_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.2 MiB)\n",
      "25/05/05 11:21:18 INFO BlockManagerInfo: Added broadcast_170_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:21:18 INFO SparkContext: Created broadcast 170 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:18 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/42 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.42.69c2df88-32fa-473c-b448-82ae64a2d2f0.tmp\n",
      "25/05/05 11:21:18 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.42.69c2df88-32fa-473c-b448-82ae64a2d2f0.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/42\n",
      "25/05/05 11:21:18 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:21:17.341Z\",\n",
      "  \"batchId\" : 42,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.1173184357541899,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 815,\n",
      "    \"commitOffsets\" : 32,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 6,\n",
      "    \"triggerExecution\" : 895,\n",
      "    \"walCommit\" : 38\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3273\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3274\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3274\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.1173184357541899,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:21:21 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/43 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.43.72ef6130-4847-413f-9045-03cbb9494dec.tmp\n",
      "25/05/05 11:21:21 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/43 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.43.fcaffdf1-6552-47c5-80bd-30079b7a01bc.tmp\n",
      "25/05/05 11:21:21 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/43 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.43.458f6b1a-be50-4090-91be-9516aac305f4.tmp\n",
      "25/05/05 11:21:21 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.43.72ef6130-4847-413f-9045-03cbb9494dec.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/43\n",
      "25/05/05 11:21:21 INFO MicroBatchExecution: Committed offsets for batch 43. Metadata OffsetSeqMetadata(0,1746458481792,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:21:21 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.43.458f6b1a-be50-4090-91be-9516aac305f4.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/43\n",
      "25/05/05 11:21:21 INFO MicroBatchExecution: Committed offsets for batch 43. Metadata OffsetSeqMetadata(0,1746458481792,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:21:21 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.43.fcaffdf1-6552-47c5-80bd-30079b7a01bc.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/43\n",
      "25/05/05 11:21:21 INFO MicroBatchExecution: Committed offsets for batch 43. Metadata OffsetSeqMetadata(0,1746458481794,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:21:21 INFO IncrementalExecution: Current batch timestamp = 1746458481792\n",
      "25/05/05 11:21:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:21 INFO IncrementalExecution: Current batch timestamp = 1746458481792\n",
      "25/05/05 11:21:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:21 INFO IncrementalExecution: Current batch timestamp = 1746458481794\n",
      "25/05/05 11:21:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:21 INFO IncrementalExecution: Current batch timestamp = 1746458481792\n",
      "25/05/05 11:21:21 INFO IncrementalExecution: Current batch timestamp = 1746458481792\n",
      "25/05/05 11:21:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:21 INFO IncrementalExecution: Current batch timestamp = 1746458481794\n",
      "25/05/05 11:21:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:21 INFO IncrementalExecution: Current batch timestamp = 1746458481792\n",
      "25/05/05 11:21:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:21 INFO IncrementalExecution: Current batch timestamp = 1746458481794\n",
      "25/05/05 11:21:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:21 INFO CodeGenerator: Code generated in 7.847791 ms\n",
      "25/05/05 11:21:21 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 43, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4ca044ca]. The input RDD has 1 partitions.\n",
      "25/05/05 11:21:21 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:21 INFO CodeGenerator: Code generated in 4.256292 ms\n",
      "25/05/05 11:21:21 INFO DAGScheduler: Got job 129 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:21:21 INFO DAGScheduler: Final stage: ResultStage 128 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:21:21 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:21:21 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:21:21 INFO DAGScheduler: Submitting ResultStage 128 (MapPartitionsRDD[696] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:21:21 INFO MemoryStore: Block broadcast_171 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:21 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 43, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:21:21 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:21 INFO MemoryStore: Block broadcast_171_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:21 INFO BlockManagerInfo: Added broadcast_171_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:21 INFO SparkContext: Created broadcast 171 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:21:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 128 (MapPartitionsRDD[696] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:21:21 INFO TaskSchedulerImpl: Adding task set 128.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:21:21 INFO DAGScheduler: Got job 130 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:21:21 INFO DAGScheduler: Final stage: ResultStage 129 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:21:21 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:21:21 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:21:21 INFO DAGScheduler: Submitting ResultStage 129 (MapPartitionsRDD[699] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:21:21 INFO TaskSetManager: Starting task 0.0 in stage 128.0 (TID 128) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:21:21 INFO Executor: Running task 0.0 in stage 128.0 (TID 128)\n",
      "25/05/05 11:21:21 INFO MemoryStore: Block broadcast_172 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:21 INFO MemoryStore: Block broadcast_172_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:21 INFO BlockManagerInfo: Added broadcast_172_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:21 INFO SparkContext: Created broadcast 172 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:21:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 129 (MapPartitionsRDD[699] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:21:21 INFO TaskSchedulerImpl: Adding task set 129.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:21:21 INFO TaskSetManager: Starting task 0.0 in stage 129.0 (TID 129) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:21:21 INFO Executor: Running task 0.0 in stage 129.0 (TID 129)\n",
      "25/05/05 11:21:21 INFO CodeGenerator: Code generated in 4.059125 ms\n",
      "25/05/05 11:21:21 INFO CodeGenerator: Code generated in 3.635542 ms\n",
      "25/05/05 11:21:21 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3274 untilOffset=3275, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=43 taskId=129 partitionId=0\n",
      "25/05/05 11:21:21 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3274 untilOffset=3275, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=43 taskId=128 partitionId=0\n",
      "25/05/05 11:21:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3274 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3274 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:21 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:21 INFO DAGScheduler: Got job 131 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:21:21 INFO DAGScheduler: Final stage: ResultStage 130 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:21:21 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:21:21 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:21:21 INFO DAGScheduler: Submitting ResultStage 130 (MapPartitionsRDD[704] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:21:21 INFO MemoryStore: Block broadcast_173 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:21:21 INFO MemoryStore: Block broadcast_173_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.1 MiB)\n",
      "25/05/05 11:21:21 INFO BlockManagerInfo: Added broadcast_173_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:21:21 INFO SparkContext: Created broadcast 173 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:21:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 130 (MapPartitionsRDD[704] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:21:21 INFO TaskSchedulerImpl: Adding task set 130.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:21:21 INFO TaskSetManager: Starting task 0.0 in stage 130.0 (TID 130) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:21:21 INFO Executor: Running task 0.0 in stage 130.0 (TID 130)\n",
      "25/05/05 11:21:21 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3274 untilOffset=3275, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=43 taskId=130 partitionId=0\n",
      "25/05/05 11:21:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3274 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3275, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3275, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:22 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:21:22 INFO DataWritingSparkTask: Committed partition 0 (task 129, attempt 0, stage 129.0)\n",
      "25/05/05 11:21:22 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 503962958 nanos, during time span of 504454000 nanos.\n",
      "25/05/05 11:21:22 INFO Executor: Finished task 0.0 in stage 129.0 (TID 129). 2145 bytes result sent to driver\n",
      "25/05/05 11:21:22 INFO TaskSetManager: Finished task 0.0 in stage 129.0 (TID 129) in 513 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:21:22 INFO TaskSchedulerImpl: Removed TaskSet 129.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:21:22 INFO DAGScheduler: ResultStage 129 (start at NativeMethodAccessorImpl.java:0) finished in 0.514 s\n",
      "25/05/05 11:21:22 INFO DAGScheduler: Job 130 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:21:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 129: Stage finished\n",
      "25/05/05 11:21:22 INFO DAGScheduler: Job 130 finished: start at NativeMethodAccessorImpl.java:0, took 0.515393 s\n",
      "25/05/05 11:21:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 43, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:21:22 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:21:22 INFO DataWritingSparkTask: Committed partition 0 (task 128, attempt 0, stage 128.0)\n",
      "25/05/05 11:21:22 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 502887042 nanos, during time span of 504708375 nanos.\n",
      "25/05/05 11:21:22 INFO Executor: Finished task 0.0 in stage 128.0 (TID 128). 3514 bytes result sent to driver\n",
      "25/05/05 11:21:22 INFO TaskSetManager: Finished task 0.0 in stage 128.0 (TID 128) in 515 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:21:22 INFO TaskSchedulerImpl: Removed TaskSet 128.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:21:22 INFO DAGScheduler: ResultStage 128 (start at NativeMethodAccessorImpl.java:0) finished in 0.517 s\n",
      "25/05/05 11:21:22 INFO DAGScheduler: Job 129 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:21:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 128: Stage finished\n",
      "25/05/05 11:21:22 INFO DAGScheduler: Job 129 finished: start at NativeMethodAccessorImpl.java:0, took 0.518313 s\n",
      "25/05/05 11:21:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 43, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4ca044ca] is committing.\n",
      "25/05/05 11:21:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 43, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4ca044ca] committed.\n",
      "25/05/05 11:21:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 43, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:21:22 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/43 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.43.5de467e6-7965-4444-921e-a68d5cb64cf4.tmp\n",
      "25/05/05 11:21:22 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/43 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.43.5b6fd729-4716-463c-81e1-af0b40f380e6.tmp\n",
      "25/05/05 11:21:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3275, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:22 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:21:22 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:21:22 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:21:22 INFO connection: Opened connection [connectionId{localValue:85, serverValue:4455}] to localhost:27017\n",
      "25/05/05 11:21:22 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=346709}\n",
      "25/05/05 11:21:22 INFO connection: Opened connection [connectionId{localValue:86, serverValue:4456}] to localhost:27017\n",
      "25/05/05 11:21:22 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:21:22 INFO connection: Closed connection [connectionId{localValue:86, serverValue:4456}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:21:22 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 504634666 nanos, during time span of 510797083 nanos.\n",
      "25/05/05 11:21:22 INFO Executor: Finished task 0.0 in stage 130.0 (TID 130). 1645 bytes result sent to driver\n",
      "25/05/05 11:21:22 INFO TaskSetManager: Finished task 0.0 in stage 130.0 (TID 130) in 517 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:21:22 INFO TaskSchedulerImpl: Removed TaskSet 130.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:21:22 INFO DAGScheduler: ResultStage 130 (start at NativeMethodAccessorImpl.java:0) finished in 0.520 s\n",
      "25/05/05 11:21:22 INFO DAGScheduler: Job 131 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:21:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 130: Stage finished\n",
      "25/05/05 11:21:22 INFO DAGScheduler: Job 131 finished: start at NativeMethodAccessorImpl.java:0, took 0.521273 s\n",
      "25/05/05 11:21:22 INFO MemoryStore: Block broadcast_174 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:21:22 INFO MemoryStore: Block broadcast_174_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:21:22 INFO BlockManagerInfo: Added broadcast_174_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:21:22 INFO SparkContext: Created broadcast 174 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:22 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.43.5de467e6-7965-4444-921e-a68d5cb64cf4.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/43\n",
      "25/05/05 11:21:22 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/43 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.43.8e63383e-891a-4239-9d3d-bb5acd5bdd6a.tmp\n",
      "25/05/05 11:21:22 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:21:21.791Z\",\n",
      "  \"batchId\" : 43,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "  \"processedRowsPerSecond\" : 1.6420361247947455,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 535,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 8,\n",
      "    \"triggerExecution\" : 609,\n",
      "    \"walCommit\" : 35\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3274\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3275\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3275\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "    \"processedRowsPerSecond\" : 1.6420361247947455,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:21:22 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.43.5b6fd729-4716-463c-81e1-af0b40f380e6.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/43\n",
      "25/05/05 11:21:22 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:21:21.791Z\",\n",
      "  \"batchId\" : 43,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.6313213703099512,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 542,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 613,\n",
      "    \"walCommit\" : 36\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3274\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3275\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3275\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.6313213703099512,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:21:22 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.43.8e63383e-891a-4239-9d3d-bb5acd5bdd6a.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/43\n",
      "25/05/05 11:21:22 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:21:21.791Z\",\n",
      "  \"batchId\" : 43,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.5923566878980893,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 560,\n",
      "    \"commitOffsets\" : 24,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 6,\n",
      "    \"triggerExecution\" : 628,\n",
      "    \"walCommit\" : 37\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3274\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3275\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3275\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.5923566878980893,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 43\n",
      "-------------------------------------------\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION      |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R016|R032|00-00-01|TIME SQ-42 ST|05/05/2025|11:21:20|REGULAR|4          |10        |2025-05-05 11:21:21.794|\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:21:23 INFO BlockManagerInfo: Removed broadcast_173_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:23 INFO BlockManagerInfo: Removed broadcast_170_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:21:23 INFO BlockManagerInfo: Removed broadcast_174_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:21:23 INFO BlockManagerInfo: Removed broadcast_172_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:23 INFO BlockManagerInfo: Removed broadcast_171_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:27 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/44 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.44.0ba859dd-6e99-43af-adec-e659511381aa.tmp\n",
      "25/05/05 11:21:27 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/44 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.44.02605e70-41ec-4f6b-aefa-d548a14024c4.tmp\n",
      "25/05/05 11:21:27 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/44 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.44.e3505cc4-d1d6-4982-bf0e-830620edf34b.tmp\n",
      "25/05/05 11:21:27 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.44.02605e70-41ec-4f6b-aefa-d548a14024c4.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/44\n",
      "25/05/05 11:21:27 INFO MicroBatchExecution: Committed offsets for batch 44. Metadata OffsetSeqMetadata(0,1746458487290,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:21:27 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.44.e3505cc4-d1d6-4982-bf0e-830620edf34b.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/44\n",
      "25/05/05 11:21:27 INFO MicroBatchExecution: Committed offsets for batch 44. Metadata OffsetSeqMetadata(0,1746458487290,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:21:27 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.44.0ba859dd-6e99-43af-adec-e659511381aa.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/44\n",
      "25/05/05 11:21:27 INFO MicroBatchExecution: Committed offsets for batch 44. Metadata OffsetSeqMetadata(0,1746458487288,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:21:27 INFO IncrementalExecution: Current batch timestamp = 1746458487290\n",
      "25/05/05 11:21:27 INFO IncrementalExecution: Current batch timestamp = 1746458487288\n",
      "25/05/05 11:21:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:27 INFO IncrementalExecution: Current batch timestamp = 1746458487290\n",
      "25/05/05 11:21:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:27 INFO IncrementalExecution: Current batch timestamp = 1746458487290\n",
      "25/05/05 11:21:27 INFO IncrementalExecution: Current batch timestamp = 1746458487288\n",
      "25/05/05 11:21:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:27 INFO IncrementalExecution: Current batch timestamp = 1746458487290\n",
      "25/05/05 11:21:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:27 INFO IncrementalExecution: Current batch timestamp = 1746458487290\n",
      "25/05/05 11:21:27 INFO IncrementalExecution: Current batch timestamp = 1746458487288\n",
      "25/05/05 11:21:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:27 INFO CodeGenerator: Code generated in 4.567084 ms\n",
      "25/05/05 11:21:27 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 44, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:21:27 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:27 INFO DAGScheduler: Got job 132 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:21:27 INFO DAGScheduler: Final stage: ResultStage 131 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:21:27 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:21:27 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:21:27 INFO DAGScheduler: Submitting ResultStage 131 (MapPartitionsRDD[712] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:21:27 INFO MemoryStore: Block broadcast_175 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:27 INFO MemoryStore: Block broadcast_175_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:27 INFO BlockManagerInfo: Added broadcast_175_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:27 INFO SparkContext: Created broadcast 175 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:21:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 131 (MapPartitionsRDD[712] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:21:27 INFO TaskSchedulerImpl: Adding task set 131.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:21:27 INFO CodeGenerator: Code generated in 4.563333 ms\n",
      "25/05/05 11:21:27 INFO TaskSetManager: Starting task 0.0 in stage 131.0 (TID 131) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:21:27 INFO Executor: Running task 0.0 in stage 131.0 (TID 131)\n",
      "25/05/05 11:21:27 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 44, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@54f01b6]. The input RDD has 1 partitions.\n",
      "25/05/05 11:21:27 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:27 INFO DAGScheduler: Got job 133 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:21:27 INFO DAGScheduler: Final stage: ResultStage 132 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:21:27 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:21:27 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:21:27 INFO DAGScheduler: Submitting ResultStage 132 (MapPartitionsRDD[715] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:21:27 INFO MemoryStore: Block broadcast_176 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:27 INFO MemoryStore: Block broadcast_176_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:27 INFO BlockManagerInfo: Added broadcast_176_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:27 INFO SparkContext: Created broadcast 176 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:21:27 INFO CodeGenerator: Code generated in 3.763625 ms\n",
      "25/05/05 11:21:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 132 (MapPartitionsRDD[715] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:21:27 INFO TaskSchedulerImpl: Adding task set 132.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:21:27 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3275 untilOffset=3276, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=44 taskId=131 partitionId=0\n",
      "25/05/05 11:21:27 INFO TaskSetManager: Starting task 0.0 in stage 132.0 (TID 132) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:21:27 INFO Executor: Running task 0.0 in stage 132.0 (TID 132)\n",
      "25/05/05 11:21:27 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3275 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:27 INFO CodeGenerator: Code generated in 40.137917 ms\n",
      "25/05/05 11:21:27 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3275 untilOffset=3276, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=44 taskId=132 partitionId=0\n",
      "25/05/05 11:21:27 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3275 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:27 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:27 INFO DAGScheduler: Got job 134 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:21:27 INFO DAGScheduler: Final stage: ResultStage 133 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:21:27 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:21:27 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:21:27 INFO DAGScheduler: Submitting ResultStage 133 (MapPartitionsRDD[720] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:21:27 INFO MemoryStore: Block broadcast_177 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:21:27 INFO MemoryStore: Block broadcast_177_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.1 MiB)\n",
      "25/05/05 11:21:27 INFO BlockManagerInfo: Added broadcast_177_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:21:27 INFO SparkContext: Created broadcast 177 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:21:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 133 (MapPartitionsRDD[720] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:21:27 INFO TaskSchedulerImpl: Adding task set 133.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:21:27 INFO TaskSetManager: Starting task 0.0 in stage 133.0 (TID 133) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:21:27 INFO Executor: Running task 0.0 in stage 133.0 (TID 133)\n",
      "25/05/05 11:21:27 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3275 untilOffset=3276, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=44 taskId=133 partitionId=0\n",
      "25/05/05 11:21:27 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3275 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3276, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:27 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:21:27 INFO DataWritingSparkTask: Committed partition 0 (task 131, attempt 0, stage 131.0)\n",
      "25/05/05 11:21:27 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 504500834 nanos, during time span of 504894708 nanos.\n",
      "25/05/05 11:21:27 INFO Executor: Finished task 0.0 in stage 131.0 (TID 131). 2145 bytes result sent to driver\n",
      "25/05/05 11:21:27 INFO TaskSetManager: Finished task 0.0 in stage 131.0 (TID 131) in 513 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:21:27 INFO TaskSchedulerImpl: Removed TaskSet 131.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:21:27 INFO DAGScheduler: ResultStage 131 (start at NativeMethodAccessorImpl.java:0) finished in 0.515 s\n",
      "25/05/05 11:21:27 INFO DAGScheduler: Job 132 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:21:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 131: Stage finished\n",
      "25/05/05 11:21:27 INFO DAGScheduler: Job 132 finished: start at NativeMethodAccessorImpl.java:0, took 0.516651 s\n",
      "25/05/05 11:21:27 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 44, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:21:27 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 44, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:21:27 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/44 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.44.30e8070f-1960-4ff7-97c4-77c5c4023b58.tmp\n",
      "25/05/05 11:21:27 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.44.30e8070f-1960-4ff7-97c4-77c5c4023b58.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/44\n",
      "25/05/05 11:21:27 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:21:27.288Z\",\n",
      "  \"batchId\" : 44,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.6447368421052633,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 541,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 6,\n",
      "    \"triggerExecution\" : 608,\n",
      "    \"walCommit\" : 32\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3275\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3276\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3276\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.6447368421052633,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:21:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3276, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:27 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:21:27 INFO DataWritingSparkTask: Committed partition 0 (task 132, attempt 0, stage 132.0)\n",
      "25/05/05 11:21:27 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 502647542 nanos, during time span of 504395042 nanos.\n",
      "25/05/05 11:21:27 INFO Executor: Finished task 0.0 in stage 132.0 (TID 132). 3559 bytes result sent to driver\n",
      "25/05/05 11:21:27 INFO TaskSetManager: Finished task 0.0 in stage 132.0 (TID 132) in 553 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:21:27 INFO TaskSchedulerImpl: Removed TaskSet 132.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:21:27 INFO DAGScheduler: ResultStage 132 (start at NativeMethodAccessorImpl.java:0) finished in 0.557 s\n",
      "25/05/05 11:21:27 INFO DAGScheduler: Job 133 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:21:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 132: Stage finished\n",
      "25/05/05 11:21:27 INFO DAGScheduler: Job 133 finished: start at NativeMethodAccessorImpl.java:0, took 0.558818 s\n",
      "25/05/05 11:21:27 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 44, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@54f01b6] is committing.\n",
      "25/05/05 11:21:27 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 44, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@54f01b6] committed.\n",
      "25/05/05 11:21:27 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/44 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.44.8ed06e4e-8173-4d87-ab3c-3cac836f1825.tmp\n",
      "25/05/05 11:21:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3276, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:27 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:21:27 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:21:27 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:21:27 INFO connection: Opened connection [connectionId{localValue:87, serverValue:4457}] to localhost:27017\n",
      "25/05/05 11:21:27 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=341750}\n",
      "25/05/05 11:21:27 INFO connection: Opened connection [connectionId{localValue:88, serverValue:4458}] to localhost:27017\n",
      "25/05/05 11:21:27 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:21:27 INFO connection: Closed connection [connectionId{localValue:88, serverValue:4458}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:21:27 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 504661250 nanos, during time span of 512360375 nanos.\n",
      "25/05/05 11:21:27 INFO Executor: Finished task 0.0 in stage 133.0 (TID 133). 1645 bytes result sent to driver\n",
      "25/05/05 11:21:27 INFO TaskSetManager: Finished task 0.0 in stage 133.0 (TID 133) in 520 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:21:27 INFO TaskSchedulerImpl: Removed TaskSet 133.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:21:27 INFO DAGScheduler: ResultStage 133 (start at NativeMethodAccessorImpl.java:0) finished in 0.526 s\n",
      "25/05/05 11:21:27 INFO DAGScheduler: Job 134 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:21:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 133: Stage finished\n",
      "25/05/05 11:21:27 INFO DAGScheduler: Job 134 finished: start at NativeMethodAccessorImpl.java:0, took 0.527934 s\n",
      "25/05/05 11:21:27 INFO MemoryStore: Block broadcast_178 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:21:27 INFO MemoryStore: Block broadcast_178_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:21:27 INFO BlockManagerInfo: Added broadcast_178_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:21:27 INFO SparkContext: Created broadcast 178 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:27 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.44.8ed06e4e-8173-4d87-ab3c-3cac836f1825.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/44\n",
      "25/05/05 11:21:27 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:21:27.287Z\",\n",
      "  \"batchId\" : 44,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.5432098765432098,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 577,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 6,\n",
      "    \"triggerExecution\" : 648,\n",
      "    \"walCommit\" : 34\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3275\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3276\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3276\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.5432098765432098,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:21:27 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/44 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.44.adb552fd-8e5d-4ac6-b486-fa14aa5160f3.tmp\n",
      "25/05/05 11:21:27 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.44.adb552fd-8e5d-4ac6-b486-fa14aa5160f3.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/44\n",
      "25/05/05 11:21:27 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:21:27.288Z\",\n",
      "  \"batchId\" : 44,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.4858841010401187,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 606,\n",
      "    \"commitOffsets\" : 26,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 7,\n",
      "    \"triggerExecution\" : 673,\n",
      "    \"walCommit\" : 32\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3275\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3276\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3276\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.4858841010401187,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 44\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time           |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+----------------------+\n",
      "|B013|R081|02-00-00|161/YANKEE STAD|05/05/2025|11:21:26|REGULAR|13         |14        |2025-05-05 11:21:27.29|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:21:31 INFO BlockManagerInfo: Removed broadcast_177_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:31 INFO BlockManagerInfo: Removed broadcast_176_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:31 INFO BlockManagerInfo: Removed broadcast_178_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:21:31 INFO BlockManagerInfo: Removed broadcast_175_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:34 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/45 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.45.44e107c4-4ae7-4815-b3c6-9afb071f514b.tmp\n",
      "25/05/05 11:21:34 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/45 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.45.0a9062f9-6d71-45bf-83f4-5b51721665fd.tmp\n",
      "25/05/05 11:21:34 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/45 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.45.a0d3da69-a9c6-4845-9e36-558a999a0172.tmp\n",
      "25/05/05 11:21:34 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.45.a0d3da69-a9c6-4845-9e36-558a999a0172.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/45\n",
      "25/05/05 11:21:34 INFO MicroBatchExecution: Committed offsets for batch 45. Metadata OffsetSeqMetadata(0,1746458494262,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:21:34 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.45.44e107c4-4ae7-4815-b3c6-9afb071f514b.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/45\n",
      "25/05/05 11:21:34 INFO MicroBatchExecution: Committed offsets for batch 45. Metadata OffsetSeqMetadata(0,1746458494261,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:21:34 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.45.0a9062f9-6d71-45bf-83f4-5b51721665fd.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/45\n",
      "25/05/05 11:21:34 INFO MicroBatchExecution: Committed offsets for batch 45. Metadata OffsetSeqMetadata(0,1746458494261,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:21:34 INFO IncrementalExecution: Current batch timestamp = 1746458494262\n",
      "25/05/05 11:21:34 INFO IncrementalExecution: Current batch timestamp = 1746458494261\n",
      "25/05/05 11:21:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:34 INFO IncrementalExecution: Current batch timestamp = 1746458494261\n",
      "25/05/05 11:21:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:34 INFO IncrementalExecution: Current batch timestamp = 1746458494261\n",
      "25/05/05 11:21:34 INFO IncrementalExecution: Current batch timestamp = 1746458494261\n",
      "25/05/05 11:21:34 INFO IncrementalExecution: Current batch timestamp = 1746458494262\n",
      "25/05/05 11:21:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:34 INFO IncrementalExecution: Current batch timestamp = 1746458494261\n",
      "25/05/05 11:21:34 INFO IncrementalExecution: Current batch timestamp = 1746458494261\n",
      "25/05/05 11:21:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:34 INFO CodeGenerator: Code generated in 6.953375 ms\n",
      "25/05/05 11:21:34 INFO CodeGenerator: Code generated in 3.291541 ms\n",
      "25/05/05 11:21:34 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 45, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:21:34 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 45, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@2a9f384c]. The input RDD has 1 partitions.\n",
      "25/05/05 11:21:34 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:34 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:34 INFO DAGScheduler: Got job 135 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:21:34 INFO DAGScheduler: Final stage: ResultStage 134 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:21:34 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:21:34 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:21:34 INFO DAGScheduler: Submitting ResultStage 134 (MapPartitionsRDD[728] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:21:34 INFO MemoryStore: Block broadcast_179 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:34 INFO MemoryStore: Block broadcast_179_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:34 INFO BlockManagerInfo: Added broadcast_179_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:34 INFO SparkContext: Created broadcast 179 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:21:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 134 (MapPartitionsRDD[728] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:21:34 INFO TaskSchedulerImpl: Adding task set 134.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:21:34 INFO DAGScheduler: Got job 136 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:21:34 INFO DAGScheduler: Final stage: ResultStage 135 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:21:34 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:21:34 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:21:34 INFO DAGScheduler: Submitting ResultStage 135 (MapPartitionsRDD[731] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:21:34 INFO TaskSetManager: Starting task 0.0 in stage 134.0 (TID 134) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:21:34 INFO MemoryStore: Block broadcast_180 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:34 INFO Executor: Running task 0.0 in stage 134.0 (TID 134)\n",
      "25/05/05 11:21:34 INFO MemoryStore: Block broadcast_180_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:34 INFO BlockManagerInfo: Added broadcast_180_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:34 INFO SparkContext: Created broadcast 180 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:21:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 135 (MapPartitionsRDD[731] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:21:34 INFO TaskSchedulerImpl: Adding task set 135.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:21:34 INFO TaskSetManager: Starting task 0.0 in stage 135.0 (TID 135) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:21:34 INFO Executor: Running task 0.0 in stage 135.0 (TID 135)\n",
      "25/05/05 11:21:34 INFO CodeGenerator: Code generated in 3.981208 ms\n",
      "25/05/05 11:21:34 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3276 untilOffset=3277, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=45 taskId=134 partitionId=0\n",
      "25/05/05 11:21:34 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3276 untilOffset=3277, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=45 taskId=135 partitionId=0\n",
      "25/05/05 11:21:34 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3276 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:34 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3276 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:34 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:34 INFO DAGScheduler: Got job 137 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:21:34 INFO DAGScheduler: Final stage: ResultStage 136 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:21:34 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:21:34 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:21:34 INFO DAGScheduler: Submitting ResultStage 136 (MapPartitionsRDD[736] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:21:34 INFO MemoryStore: Block broadcast_181 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:21:34 INFO MemoryStore: Block broadcast_181_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:21:34 INFO BlockManagerInfo: Added broadcast_181_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:21:34 INFO SparkContext: Created broadcast 181 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:21:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 136 (MapPartitionsRDD[736] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:21:34 INFO TaskSchedulerImpl: Adding task set 136.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:21:34 INFO TaskSetManager: Starting task 0.0 in stage 136.0 (TID 136) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:21:34 INFO Executor: Running task 0.0 in stage 136.0 (TID 136)\n",
      "25/05/05 11:21:34 INFO CodeGenerator: Code generated in 3.858416 ms\n",
      "25/05/05 11:21:34 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3276 untilOffset=3277, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=45 taskId=136 partitionId=0\n",
      "25/05/05 11:21:34 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3276 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3277, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3277, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:34 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:21:34 INFO DataWritingSparkTask: Committed partition 0 (task 134, attempt 0, stage 134.0)\n",
      "25/05/05 11:21:34 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 504969166 nanos, during time span of 505424208 nanos.\n",
      "25/05/05 11:21:34 INFO Executor: Finished task 0.0 in stage 134.0 (TID 134). 2145 bytes result sent to driver\n",
      "25/05/05 11:21:34 INFO TaskSetManager: Finished task 0.0 in stage 134.0 (TID 134) in 515 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:21:34 INFO TaskSchedulerImpl: Removed TaskSet 134.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:21:34 INFO DAGScheduler: ResultStage 134 (start at NativeMethodAccessorImpl.java:0) finished in 0.516 s\n",
      "25/05/05 11:21:34 INFO DAGScheduler: Job 135 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:21:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 134: Stage finished\n",
      "25/05/05 11:21:34 INFO DAGScheduler: Job 135 finished: start at NativeMethodAccessorImpl.java:0, took 0.517052 s\n",
      "25/05/05 11:21:34 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:21:34 INFO DataWritingSparkTask: Committed partition 0 (task 135, attempt 0, stage 135.0)\n",
      "25/05/05 11:21:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 45, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:21:34 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 503890166 nanos, during time span of 505629792 nanos.\n",
      "25/05/05 11:21:34 INFO Executor: Finished task 0.0 in stage 135.0 (TID 135). 3516 bytes result sent to driver\n",
      "25/05/05 11:21:34 INFO TaskSetManager: Finished task 0.0 in stage 135.0 (TID 135) in 514 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:21:34 INFO TaskSchedulerImpl: Removed TaskSet 135.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:21:34 INFO DAGScheduler: ResultStage 135 (start at NativeMethodAccessorImpl.java:0) finished in 0.516 s\n",
      "25/05/05 11:21:34 INFO DAGScheduler: Job 136 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:21:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 135: Stage finished\n",
      "25/05/05 11:21:34 INFO DAGScheduler: Job 136 finished: start at NativeMethodAccessorImpl.java:0, took 0.517982 s\n",
      "25/05/05 11:21:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 45, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@2a9f384c] is committing.\n",
      "25/05/05 11:21:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 45, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@2a9f384c] committed.\n",
      "25/05/05 11:21:34 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/45 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.45.04cc417d-48a9-4b51-a513-e2b8c532847b.tmp\n",
      "25/05/05 11:21:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 45, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:21:34 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/45 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.45.b1fa0939-c5a5-439e-a0b0-6e24577f43c0.tmp\n",
      "25/05/05 11:21:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3277, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:34 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:21:34 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:21:34 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:21:34 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.45.04cc417d-48a9-4b51-a513-e2b8c532847b.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/45\n",
      "25/05/05 11:21:34 INFO connection: Opened connection [connectionId{localValue:89, serverValue:4459}] to localhost:27017\n",
      "25/05/05 11:21:34 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:21:34.260Z\",\n",
      "  \"batchId\" : 45,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "  \"processedRowsPerSecond\" : 1.6750418760469012,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 531,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 597,\n",
      "    \"walCommit\" : 35\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3276\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3277\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3277\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "    \"processedRowsPerSecond\" : 1.6750418760469012,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:21:34 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=264125}\n",
      "25/05/05 11:21:34 INFO connection: Opened connection [connectionId{localValue:90, serverValue:4460}] to localhost:27017\n",
      "25/05/05 11:21:34 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:21:34 INFO connection: Closed connection [connectionId{localValue:90, serverValue:4460}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:21:34 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 507672459 nanos, during time span of 514576375 nanos.\n",
      "25/05/05 11:21:34 INFO Executor: Finished task 0.0 in stage 136.0 (TID 136). 1645 bytes result sent to driver\n",
      "25/05/05 11:21:34 INFO TaskSetManager: Finished task 0.0 in stage 136.0 (TID 136) in 525 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:21:34 INFO TaskSchedulerImpl: Removed TaskSet 136.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:21:34 INFO DAGScheduler: ResultStage 136 (start at NativeMethodAccessorImpl.java:0) finished in 0.529 s\n",
      "25/05/05 11:21:34 INFO DAGScheduler: Job 137 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:21:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 136: Stage finished\n",
      "25/05/05 11:21:34 INFO DAGScheduler: Job 137 finished: start at NativeMethodAccessorImpl.java:0, took 0.530489 s\n",
      "25/05/05 11:21:34 INFO MemoryStore: Block broadcast_182 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:21:34 INFO MemoryStore: Block broadcast_182_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:21:34 INFO BlockManagerInfo: Added broadcast_182_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:21:34 INFO SparkContext: Created broadcast 182 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:34 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.45.b1fa0939-c5a5-439e-a0b0-6e24577f43c0.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/45\n",
      "25/05/05 11:21:34 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:21:34.260Z\",\n",
      "  \"batchId\" : 45,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.6556291390728477,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 538,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 604,\n",
      "    \"walCommit\" : 33\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3276\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3277\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3277\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.6556291390728477,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:21:34 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/45 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.45.e3afeaaa-396b-4ea2-b6d8-e1034a241852.tmp\n",
      "25/05/05 11:21:34 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.45.e3afeaaa-396b-4ea2-b6d8-e1034a241852.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/45\n",
      "25/05/05 11:21:34 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:21:34.260Z\",\n",
      "  \"batchId\" : 45,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "  \"processedRowsPerSecond\" : 1.5847860538827259,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 565,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 631,\n",
      "    \"walCommit\" : 32\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3276\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3277\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3277\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "    \"processedRowsPerSecond\" : 1.5847860538827259,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 45\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R023|R045|00-00-00|34 ST-HERALD SQ|05/05/2025|11:21:30|REGULAR|6          |13        |2025-05-05 11:21:34.261|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:21:38 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/46 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.46.36f17790-98a3-4d97-b3bc-327113f6ed4a.tmp\n",
      "25/05/05 11:21:38 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/46 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.46.a85f18fa-f99c-49a5-b7fb-9cf92056755c.tmp\n",
      "25/05/05 11:21:38 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/46 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.46.6cd14aac-905a-4552-822f-2a0ca101a4b4.tmp\n",
      "25/05/05 11:21:38 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.46.36f17790-98a3-4d97-b3bc-327113f6ed4a.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/46\n",
      "25/05/05 11:21:38 INFO MicroBatchExecution: Committed offsets for batch 46. Metadata OffsetSeqMetadata(0,1746458498851,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:21:38 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.46.6cd14aac-905a-4552-822f-2a0ca101a4b4.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/46\n",
      "25/05/05 11:21:38 INFO MicroBatchExecution: Committed offsets for batch 46. Metadata OffsetSeqMetadata(0,1746458498851,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:21:38 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.46.a85f18fa-f99c-49a5-b7fb-9cf92056755c.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/46\n",
      "25/05/05 11:21:38 INFO MicroBatchExecution: Committed offsets for batch 46. Metadata OffsetSeqMetadata(0,1746458498865,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:21:38 INFO IncrementalExecution: Current batch timestamp = 1746458498851\n",
      "25/05/05 11:21:38 INFO IncrementalExecution: Current batch timestamp = 1746458498851\n",
      "25/05/05 11:21:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:38 INFO IncrementalExecution: Current batch timestamp = 1746458498865\n",
      "25/05/05 11:21:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:38 INFO IncrementalExecution: Current batch timestamp = 1746458498851\n",
      "25/05/05 11:21:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:38 INFO IncrementalExecution: Current batch timestamp = 1746458498851\n",
      "25/05/05 11:21:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:38 INFO IncrementalExecution: Current batch timestamp = 1746458498865\n",
      "25/05/05 11:21:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:38 INFO IncrementalExecution: Current batch timestamp = 1746458498851\n",
      "25/05/05 11:21:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:38 INFO IncrementalExecution: Current batch timestamp = 1746458498865\n",
      "25/05/05 11:21:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:38 INFO CodeGenerator: Code generated in 17.590375 ms\n",
      "25/05/05 11:21:38 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 46, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@185c10aa]. The input RDD has 1 partitions.\n",
      "25/05/05 11:21:38 INFO BlockManagerInfo: Removed broadcast_180_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:21:38 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:38 INFO DAGScheduler: Got job 138 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:21:38 INFO DAGScheduler: Final stage: ResultStage 137 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:21:38 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:21:38 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:21:38 INFO DAGScheduler: Submitting ResultStage 137 (MapPartitionsRDD[742] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:21:38 INFO MemoryStore: Block broadcast_183 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:21:38 INFO BlockManagerInfo: Removed broadcast_181_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:38 INFO MemoryStore: Block broadcast_183_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.1 MiB)\n",
      "25/05/05 11:21:38 INFO BlockManagerInfo: Added broadcast_183_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:38 INFO SparkContext: Created broadcast 183 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:21:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 137 (MapPartitionsRDD[742] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:21:38 INFO TaskSchedulerImpl: Adding task set 137.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:21:38 INFO TaskSetManager: Starting task 0.0 in stage 137.0 (TID 137) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:21:38 INFO Executor: Running task 0.0 in stage 137.0 (TID 137)\n",
      "25/05/05 11:21:38 INFO BlockManagerInfo: Removed broadcast_182_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:21:38 INFO CodeGenerator: Code generated in 4.704708 ms\n",
      "25/05/05 11:21:38 INFO BlockManagerInfo: Removed broadcast_179_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:38 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 46, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:21:38 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:38 INFO DAGScheduler: Got job 139 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:21:38 INFO DAGScheduler: Final stage: ResultStage 138 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:21:38 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:21:38 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:21:38 INFO DAGScheduler: Submitting ResultStage 138 (MapPartitionsRDD[747] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:21:38 INFO MemoryStore: Block broadcast_184 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:38 INFO MemoryStore: Block broadcast_184_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:38 INFO BlockManagerInfo: Added broadcast_184_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:38 INFO CodeGenerator: Code generated in 4.863541 ms\n",
      "25/05/05 11:21:38 INFO SparkContext: Created broadcast 184 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:21:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 138 (MapPartitionsRDD[747] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:21:38 INFO TaskSchedulerImpl: Adding task set 138.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:21:38 INFO TaskSetManager: Starting task 0.0 in stage 138.0 (TID 138) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:21:38 INFO Executor: Running task 0.0 in stage 138.0 (TID 138)\n",
      "25/05/05 11:21:38 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3277 untilOffset=3278, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=46 taskId=137 partitionId=0\n",
      "25/05/05 11:21:38 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3277 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:38 INFO CodeGenerator: Code generated in 5.466125 ms\n",
      "25/05/05 11:21:38 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3277 untilOffset=3278, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=46 taskId=138 partitionId=0\n",
      "25/05/05 11:21:38 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3277 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:38 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:38 INFO DAGScheduler: Got job 140 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:21:38 INFO DAGScheduler: Final stage: ResultStage 139 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:21:38 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:21:38 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:21:38 INFO DAGScheduler: Submitting ResultStage 139 (MapPartitionsRDD[752] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:21:38 INFO MemoryStore: Block broadcast_185 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:21:38 INFO MemoryStore: Block broadcast_185_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.1 MiB)\n",
      "25/05/05 11:21:38 INFO BlockManagerInfo: Added broadcast_185_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:21:38 INFO SparkContext: Created broadcast 185 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:21:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 139 (MapPartitionsRDD[752] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:21:38 INFO TaskSchedulerImpl: Adding task set 139.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:21:38 INFO TaskSetManager: Starting task 0.0 in stage 139.0 (TID 139) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:21:38 INFO Executor: Running task 0.0 in stage 139.0 (TID 139)\n",
      "25/05/05 11:21:38 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3277 untilOffset=3278, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=46 taskId=139 partitionId=0\n",
      "25/05/05 11:21:38 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3277 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3278, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3278, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:39 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:21:39 INFO DataWritingSparkTask: Committed partition 0 (task 137, attempt 0, stage 137.0)\n",
      "25/05/05 11:21:39 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 505547792 nanos, during time span of 507886833 nanos.\n",
      "25/05/05 11:21:39 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:21:39 INFO DataWritingSparkTask: Committed partition 0 (task 138, attempt 0, stage 138.0)\n",
      "25/05/05 11:21:39 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 502040333 nanos, during time span of 502441791 nanos.\n",
      "25/05/05 11:21:39 INFO Executor: Finished task 0.0 in stage 137.0 (TID 137). 3516 bytes result sent to driver\n",
      "25/05/05 11:21:39 INFO Executor: Finished task 0.0 in stage 138.0 (TID 138). 2145 bytes result sent to driver\n",
      "25/05/05 11:21:39 INFO TaskSetManager: Finished task 0.0 in stage 138.0 (TID 138) in 514 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:21:39 INFO TaskSchedulerImpl: Removed TaskSet 138.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:21:39 INFO TaskSetManager: Finished task 0.0 in stage 137.0 (TID 137) in 521 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:21:39 INFO DAGScheduler: ResultStage 138 (start at NativeMethodAccessorImpl.java:0) finished in 0.516 s\n",
      "25/05/05 11:21:39 INFO TaskSchedulerImpl: Removed TaskSet 137.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:21:39 INFO DAGScheduler: Job 139 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:21:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 138: Stage finished\n",
      "25/05/05 11:21:39 INFO DAGScheduler: Job 139 finished: start at NativeMethodAccessorImpl.java:0, took 0.517076 s\n",
      "25/05/05 11:21:39 INFO DAGScheduler: ResultStage 137 (start at NativeMethodAccessorImpl.java:0) finished in 0.522 s\n",
      "25/05/05 11:21:39 INFO DAGScheduler: Job 138 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:21:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 137: Stage finished\n",
      "25/05/05 11:21:39 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 46, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:21:39 INFO DAGScheduler: Job 138 finished: start at NativeMethodAccessorImpl.java:0, took 0.523642 s\n",
      "25/05/05 11:21:39 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 46, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@185c10aa] is committing.\n",
      "25/05/05 11:21:39 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 46, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@185c10aa] committed.\n",
      "25/05/05 11:21:39 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/46 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.46.5af0df7c-c23a-4478-b318-8a6a9f44bc8e.tmp\n",
      "25/05/05 11:21:39 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 46, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:21:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3278, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:39 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:21:39 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:21:39 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:21:39 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/46 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.46.07c7ad79-3031-4138-af64-5ac6c10a458d.tmp\n",
      "25/05/05 11:21:39 INFO connection: Opened connection [connectionId{localValue:91, serverValue:4461}] to localhost:27017\n",
      "25/05/05 11:21:39 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=312917}\n",
      "25/05/05 11:21:39 INFO connection: Opened connection [connectionId{localValue:92, serverValue:4462}] to localhost:27017\n",
      "25/05/05 11:21:39 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:21:39 INFO connection: Closed connection [connectionId{localValue:92, serverValue:4462}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:21:39 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 502418709 nanos, during time span of 508550041 nanos.\n",
      "25/05/05 11:21:39 INFO Executor: Finished task 0.0 in stage 139.0 (TID 139). 1645 bytes result sent to driver\n",
      "25/05/05 11:21:39 INFO TaskSetManager: Finished task 0.0 in stage 139.0 (TID 139) in 515 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:21:39 INFO TaskSchedulerImpl: Removed TaskSet 139.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:21:39 INFO DAGScheduler: ResultStage 139 (start at NativeMethodAccessorImpl.java:0) finished in 0.520 s\n",
      "25/05/05 11:21:39 INFO DAGScheduler: Job 140 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:21:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 139: Stage finished\n",
      "25/05/05 11:21:39 INFO DAGScheduler: Job 140 finished: start at NativeMethodAccessorImpl.java:0, took 0.520593 s\n",
      "25/05/05 11:21:39 INFO MemoryStore: Block broadcast_186 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:21:39 INFO MemoryStore: Block broadcast_186_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:21:39 INFO BlockManagerInfo: Added broadcast_186_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:21:39 INFO SparkContext: Created broadcast 186 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:39 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/46 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.46.64b0d262-1bb4-48e3-942c-24d202f08050.tmp\n",
      "25/05/05 11:21:39 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.46.5af0df7c-c23a-4478-b318-8a6a9f44bc8e.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/46\n",
      "25/05/05 11:21:39 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:21:38.849Z\",\n",
      "  \"batchId\" : 46,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.5552099533437014,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 549,\n",
      "    \"commitOffsets\" : 31,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 643,\n",
      "    \"walCommit\" : 56\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3277\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3278\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3278\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.5552099533437014,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:21:39 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.46.07c7ad79-3031-4138-af64-5ac6c10a458d.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/46\n",
      "25/05/05 11:21:39 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:21:38.858Z\",\n",
      "  \"batchId\" : 46,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 55.55555555555556,\n",
      "  \"processedRowsPerSecond\" : 1.5552099533437014,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 557,\n",
      "    \"commitOffsets\" : 31,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 6,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 643,\n",
      "    \"walCommit\" : 44\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3277\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3278\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3278\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 55.55555555555556,\n",
      "    \"processedRowsPerSecond\" : 1.5552099533437014,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:21:39 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.46.64b0d262-1bb4-48e3-942c-24d202f08050.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/46\n",
      "25/05/05 11:21:39 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:21:38.850Z\",\n",
      "  \"batchId\" : 46,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.5082956259426847,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 572,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 663,\n",
      "    \"walCommit\" : 56\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3277\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3278\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3278\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.5082956259426847,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 46\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A001|R079|01-00-00|WORLD TRADE CTR|05/05/2025|11:21:37|REGULAR|10         |14        |2025-05-05 11:21:38.865|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:21:45 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/47 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.47.64d3eed2-f21a-4ff6-9ad2-a32ac0ad7851.tmp\n",
      "25/05/05 11:21:45 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/47 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.47.243d926e-b6e9-484e-a814-5d8021cc3189.tmp\n",
      "25/05/05 11:21:45 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/47 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.47.30f8f198-a046-4e95-9a38-afd8a3fea810.tmp\n",
      "25/05/05 11:21:45 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.47.30f8f198-a046-4e95-9a38-afd8a3fea810.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/47\n",
      "25/05/05 11:21:45 INFO MicroBatchExecution: Committed offsets for batch 47. Metadata OffsetSeqMetadata(0,1746458505353,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:21:45 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.47.64d3eed2-f21a-4ff6-9ad2-a32ac0ad7851.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/47\n",
      "25/05/05 11:21:45 INFO MicroBatchExecution: Committed offsets for batch 47. Metadata OffsetSeqMetadata(0,1746458505342,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:21:45 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.47.243d926e-b6e9-484e-a814-5d8021cc3189.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/47\n",
      "25/05/05 11:21:45 INFO MicroBatchExecution: Committed offsets for batch 47. Metadata OffsetSeqMetadata(0,1746458505354,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:21:45 INFO IncrementalExecution: Current batch timestamp = 1746458505353\n",
      "25/05/05 11:21:45 INFO IncrementalExecution: Current batch timestamp = 1746458505342\n",
      "25/05/05 11:21:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:45 INFO IncrementalExecution: Current batch timestamp = 1746458505354\n",
      "25/05/05 11:21:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:45 INFO IncrementalExecution: Current batch timestamp = 1746458505353\n",
      "25/05/05 11:21:45 INFO IncrementalExecution: Current batch timestamp = 1746458505342\n",
      "25/05/05 11:21:45 INFO IncrementalExecution: Current batch timestamp = 1746458505354\n",
      "25/05/05 11:21:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:45 INFO IncrementalExecution: Current batch timestamp = 1746458505353\n",
      "25/05/05 11:21:45 INFO IncrementalExecution: Current batch timestamp = 1746458505354\n",
      "25/05/05 11:21:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:45 INFO CodeGenerator: Code generated in 5.0795 ms\n",
      "25/05/05 11:21:45 INFO CodeGenerator: Code generated in 3.992 ms\n",
      "25/05/05 11:21:45 INFO CodeGenerator: Code generated in 4.370166 ms\n",
      "25/05/05 11:21:45 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 47, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:21:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:45 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 47, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@2b8a89cc]. The input RDD has 1 partitions.\n",
      "25/05/05 11:21:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:45 INFO DAGScheduler: Got job 141 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:21:45 INFO DAGScheduler: Final stage: ResultStage 140 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:21:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:21:45 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:21:45 INFO DAGScheduler: Submitting ResultStage 140 (MapPartitionsRDD[762] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:21:45 INFO MemoryStore: Block broadcast_187 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:21:45 INFO MemoryStore: Block broadcast_187_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:21:45 INFO BlockManagerInfo: Added broadcast_187_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:21:45 INFO SparkContext: Created broadcast 187 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:21:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 140 (MapPartitionsRDD[762] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:21:45 INFO TaskSchedulerImpl: Adding task set 140.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:21:45 INFO DAGScheduler: Got job 142 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:21:45 INFO DAGScheduler: Final stage: ResultStage 141 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:21:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:21:45 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:21:45 INFO DAGScheduler: Submitting ResultStage 141 (MapPartitionsRDD[763] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:21:45 INFO TaskSetManager: Starting task 0.0 in stage 140.0 (TID 140) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:21:45 INFO Executor: Running task 0.0 in stage 140.0 (TID 140)\n",
      "25/05/05 11:21:45 INFO MemoryStore: Block broadcast_188 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:21:45 INFO MemoryStore: Block broadcast_188_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)\n",
      "25/05/05 11:21:45 INFO BlockManagerInfo: Added broadcast_188_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:21:45 INFO SparkContext: Created broadcast 188 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:21:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 141 (MapPartitionsRDD[763] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:21:45 INFO TaskSchedulerImpl: Adding task set 141.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:21:45 INFO TaskSetManager: Starting task 0.0 in stage 141.0 (TID 141) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:21:45 INFO Executor: Running task 0.0 in stage 141.0 (TID 141)\n",
      "25/05/05 11:21:45 INFO CodeGenerator: Code generated in 5.047041 ms\n",
      "25/05/05 11:21:45 INFO CodeGenerator: Code generated in 3.802875 ms\n",
      "25/05/05 11:21:45 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3278 untilOffset=3279, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=47 taskId=140 partitionId=0\n",
      "25/05/05 11:21:45 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3278 untilOffset=3279, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=47 taskId=141 partitionId=0\n",
      "25/05/05 11:21:45 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3278 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:45 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3278 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:45 INFO DAGScheduler: Got job 143 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:21:45 INFO DAGScheduler: Final stage: ResultStage 142 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:21:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:21:45 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:21:45 INFO DAGScheduler: Submitting ResultStage 142 (MapPartitionsRDD[768] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:21:45 INFO MemoryStore: Block broadcast_189 stored as values in memory (estimated size 47.1 KiB, free 366.0 MiB)\n",
      "25/05/05 11:21:45 INFO MemoryStore: Block broadcast_189_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:21:45 INFO BlockManagerInfo: Added broadcast_189_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:21:45 INFO SparkContext: Created broadcast 189 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:21:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 142 (MapPartitionsRDD[768] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:21:45 INFO TaskSchedulerImpl: Adding task set 142.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:21:45 INFO TaskSetManager: Starting task 0.0 in stage 142.0 (TID 142) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:21:45 INFO Executor: Running task 0.0 in stage 142.0 (TID 142)\n",
      "25/05/05 11:21:45 INFO CodeGenerator: Code generated in 4.238667 ms\n",
      "25/05/05 11:21:45 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3278 untilOffset=3279, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=47 taskId=142 partitionId=0\n",
      "25/05/05 11:21:45 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3278 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3279, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3279, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:45 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:21:45 INFO DataWritingSparkTask: Committed partition 0 (task 140, attempt 0, stage 140.0)\n",
      "25/05/05 11:21:45 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 504217250 nanos, during time span of 504621542 nanos.\n",
      "25/05/05 11:21:45 INFO Executor: Finished task 0.0 in stage 140.0 (TID 140). 2180 bytes result sent to driver\n",
      "25/05/05 11:21:45 INFO TaskSetManager: Finished task 0.0 in stage 140.0 (TID 140) in 517 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:21:45 INFO TaskSchedulerImpl: Removed TaskSet 140.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:21:45 INFO DAGScheduler: ResultStage 140 (start at NativeMethodAccessorImpl.java:0) finished in 0.518 s\n",
      "25/05/05 11:21:45 INFO DAGScheduler: Job 141 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:21:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 140: Stage finished\n",
      "25/05/05 11:21:45 INFO DAGScheduler: Job 141 finished: start at NativeMethodAccessorImpl.java:0, took 0.518999 s\n",
      "25/05/05 11:21:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 47, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:21:45 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:21:45 INFO DataWritingSparkTask: Committed partition 0 (task 141, attempt 0, stage 141.0)\n",
      "25/05/05 11:21:45 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 502728334 nanos, during time span of 504953166 nanos.\n",
      "25/05/05 11:21:45 INFO Executor: Finished task 0.0 in stage 141.0 (TID 141). 3504 bytes result sent to driver\n",
      "25/05/05 11:21:45 INFO TaskSetManager: Finished task 0.0 in stage 141.0 (TID 141) in 515 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:21:45 INFO TaskSchedulerImpl: Removed TaskSet 141.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:21:45 INFO DAGScheduler: ResultStage 141 (start at NativeMethodAccessorImpl.java:0) finished in 0.518 s\n",
      "25/05/05 11:21:45 INFO DAGScheduler: Job 142 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:21:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 141: Stage finished\n",
      "25/05/05 11:21:45 INFO DAGScheduler: Job 142 finished: start at NativeMethodAccessorImpl.java:0, took 0.520094 s\n",
      "25/05/05 11:21:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 47, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@2b8a89cc] is committing.\n",
      "25/05/05 11:21:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 47, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@2b8a89cc] committed.\n",
      "25/05/05 11:21:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 47, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:21:45 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/47 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.47.14907360-c3d8-4317-98cd-5ec656683a67.tmp\n",
      "25/05/05 11:21:45 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/47 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.47.98779422-d2cd-4b16-bd1d-f4ceae1e493d.tmp\n",
      "25/05/05 11:21:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3279, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:45 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:21:45 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:21:45 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:21:45 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.47.14907360-c3d8-4317-98cd-5ec656683a67.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/47\n",
      "25/05/05 11:21:45 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:21:45.339Z\",\n",
      "  \"batchId\" : 47,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "  \"processedRowsPerSecond\" : 1.5923566878980893,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 537,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 14,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 628,\n",
      "    \"walCommit\" : 43\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3278\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3279\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3279\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "    \"processedRowsPerSecond\" : 1.5923566878980893,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:21:45 INFO connection: Opened connection [connectionId{localValue:93, serverValue:4463}] to localhost:27017\n",
      "25/05/05 11:21:45 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=262083}\n",
      "25/05/05 11:21:45 INFO connection: Opened connection [connectionId{localValue:94, serverValue:4464}] to localhost:27017\n",
      "25/05/05 11:21:45 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.47.98779422-d2cd-4b16-bd1d-f4ceae1e493d.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/47\n",
      "25/05/05 11:21:45 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:21:45.339Z\",\n",
      "  \"batchId\" : 47,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "  \"processedRowsPerSecond\" : 1.5772870662460567,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 544,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 15,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 634,\n",
      "    \"walCommit\" : 44\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3278\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3279\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3279\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "    \"processedRowsPerSecond\" : 1.5772870662460567,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:21:45 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:21:45 INFO connection: Closed connection [connectionId{localValue:94, serverValue:4464}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:21:45 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 503662208 nanos, during time span of 523033959 nanos.\n",
      "25/05/05 11:21:45 INFO Executor: Finished task 0.0 in stage 142.0 (TID 142). 1645 bytes result sent to driver\n",
      "25/05/05 11:21:45 INFO TaskSetManager: Finished task 0.0 in stage 142.0 (TID 142) in 535 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:21:45 INFO TaskSchedulerImpl: Removed TaskSet 142.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:21:45 INFO DAGScheduler: ResultStage 142 (start at NativeMethodAccessorImpl.java:0) finished in 0.539 s\n",
      "25/05/05 11:21:45 INFO DAGScheduler: Job 143 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:21:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 142: Stage finished\n",
      "25/05/05 11:21:45 INFO DAGScheduler: Job 143 finished: start at NativeMethodAccessorImpl.java:0, took 0.540159 s\n",
      "25/05/05 11:21:45 INFO MemoryStore: Block broadcast_190 stored as values in memory (estimated size 248.0 B, free 366.0 MiB)\n",
      "25/05/05 11:21:45 INFO MemoryStore: Block broadcast_190_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.0 MiB)\n",
      "25/05/05 11:21:45 INFO BlockManagerInfo: Added broadcast_190_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:21:45 INFO SparkContext: Created broadcast 190 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:45 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/47 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.47.07fa1a85-3cf1-4069-b044-6c8aafc64db2.tmp\n",
      "25/05/05 11:21:45 INFO BlockManagerInfo: Removed broadcast_188_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:21:45 INFO BlockManagerInfo: Removed broadcast_187_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:21:45 INFO BlockManagerInfo: Removed broadcast_189_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:21:45 INFO BlockManagerInfo: Removed broadcast_190_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:21:45 INFO BlockManagerInfo: Removed broadcast_184_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:21:45 INFO BlockManagerInfo: Removed broadcast_185_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:45 INFO BlockManagerInfo: Removed broadcast_183_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:45 INFO BlockManagerInfo: Removed broadcast_186_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:21:46 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.47.07fa1a85-3cf1-4069-b044-6c8aafc64db2.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/47\n",
      "25/05/05 11:21:46 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:21:45.335Z\",\n",
      "  \"batchId\" : 47,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.4749262536873156,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 576,\n",
      "    \"commitOffsets\" : 35,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 7,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 678,\n",
      "    \"walCommit\" : 55\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3278\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3279\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3279\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.4749262536873156,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 47\n",
      "-------------------------------------------\n",
      "+----+----+--------+--------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+--------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R028|R088|00-00-00|CANAL ST|05/05/2025|11:21:44|REGULAR|13         |13        |2025-05-05 11:21:45.354|\n",
      "+----+----+--------+--------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:21:49 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/48 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.48.fbc3d2cc-9fc3-486c-9ec5-6c092e3c0e78.tmp\n",
      "25/05/05 11:21:49 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/48 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.48.c11175d2-38b2-4932-b71c-d111be645f88.tmp\n",
      "25/05/05 11:21:49 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/48 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.48.458304ae-3852-4e26-991e-5183448813bb.tmp\n",
      "25/05/05 11:21:49 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.48.c11175d2-38b2-4932-b71c-d111be645f88.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/48\n",
      "25/05/05 11:21:49 INFO MicroBatchExecution: Committed offsets for batch 48. Metadata OffsetSeqMetadata(0,1746458509867,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:21:49 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.48.fbc3d2cc-9fc3-486c-9ec5-6c092e3c0e78.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/48\n",
      "25/05/05 11:21:49 INFO MicroBatchExecution: Committed offsets for batch 48. Metadata OffsetSeqMetadata(0,1746458509866,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:21:49 INFO IncrementalExecution: Current batch timestamp = 1746458509867\n",
      "25/05/05 11:21:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:49 INFO IncrementalExecution: Current batch timestamp = 1746458509866\n",
      "25/05/05 11:21:49 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.48.458304ae-3852-4e26-991e-5183448813bb.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/48\n",
      "25/05/05 11:21:49 INFO MicroBatchExecution: Committed offsets for batch 48. Metadata OffsetSeqMetadata(0,1746458509868,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:21:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:49 INFO IncrementalExecution: Current batch timestamp = 1746458509866\n",
      "25/05/05 11:21:49 INFO IncrementalExecution: Current batch timestamp = 1746458509867\n",
      "25/05/05 11:21:49 INFO IncrementalExecution: Current batch timestamp = 1746458509868\n",
      "25/05/05 11:21:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:49 INFO IncrementalExecution: Current batch timestamp = 1746458509866\n",
      "25/05/05 11:21:49 INFO IncrementalExecution: Current batch timestamp = 1746458509867\n",
      "25/05/05 11:21:49 INFO IncrementalExecution: Current batch timestamp = 1746458509868\n",
      "25/05/05 11:21:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:49 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:49 INFO CodeGenerator: Code generated in 4.280292 ms\n",
      "25/05/05 11:21:49 INFO CodeGenerator: Code generated in 5.699958 ms\n",
      "25/05/05 11:21:49 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 48, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@3ea96bdf]. The input RDD has 1 partitions.\n",
      "25/05/05 11:21:49 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:49 INFO DAGScheduler: Got job 144 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:21:49 INFO DAGScheduler: Final stage: ResultStage 143 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:21:49 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:21:49 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:21:49 INFO CodeGenerator: Code generated in 7.08525 ms\n",
      "25/05/05 11:21:49 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 48, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:21:49 INFO DAGScheduler: Submitting ResultStage 143 (MapPartitionsRDD[772] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:21:49 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:49 INFO MemoryStore: Block broadcast_191 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:49 INFO MemoryStore: Block broadcast_191_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:49 INFO BlockManagerInfo: Added broadcast_191_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:49 INFO SparkContext: Created broadcast 191 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:21:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 143 (MapPartitionsRDD[772] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:21:49 INFO TaskSchedulerImpl: Adding task set 143.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:21:49 INFO DAGScheduler: Got job 145 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:21:49 INFO DAGScheduler: Final stage: ResultStage 144 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:21:49 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:21:49 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:21:49 INFO DAGScheduler: Submitting ResultStage 144 (MapPartitionsRDD[775] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:21:49 INFO TaskSetManager: Starting task 0.0 in stage 143.0 (TID 143) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:21:49 INFO MemoryStore: Block broadcast_192 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:49 INFO Executor: Running task 0.0 in stage 143.0 (TID 143)\n",
      "25/05/05 11:21:49 INFO MemoryStore: Block broadcast_192_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:49 INFO BlockManagerInfo: Added broadcast_192_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:49 INFO SparkContext: Created broadcast 192 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:21:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 144 (MapPartitionsRDD[775] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:21:49 INFO TaskSchedulerImpl: Adding task set 144.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:21:49 INFO TaskSetManager: Starting task 0.0 in stage 144.0 (TID 144) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:21:49 INFO Executor: Running task 0.0 in stage 144.0 (TID 144)\n",
      "25/05/05 11:21:49 INFO CodeGenerator: Code generated in 4.886541 ms\n",
      "25/05/05 11:21:49 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3279 untilOffset=3280, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=48 taskId=143 partitionId=0\n",
      "25/05/05 11:21:49 INFO CodeGenerator: Code generated in 4.058917 ms\n",
      "25/05/05 11:21:49 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3279 untilOffset=3280, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=48 taskId=144 partitionId=0\n",
      "25/05/05 11:21:49 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3279 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:49 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3279 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:49 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:49 INFO DAGScheduler: Got job 146 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:21:49 INFO DAGScheduler: Final stage: ResultStage 145 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:21:49 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:21:49 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:21:49 INFO DAGScheduler: Submitting ResultStage 145 (MapPartitionsRDD[784] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:21:49 INFO MemoryStore: Block broadcast_193 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:21:49 INFO MemoryStore: Block broadcast_193_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.1 MiB)\n",
      "25/05/05 11:21:49 INFO BlockManagerInfo: Added broadcast_193_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:21:49 INFO SparkContext: Created broadcast 193 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:21:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 145 (MapPartitionsRDD[784] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:21:49 INFO TaskSchedulerImpl: Adding task set 145.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:21:49 INFO TaskSetManager: Starting task 0.0 in stage 145.0 (TID 145) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:21:49 INFO Executor: Running task 0.0 in stage 145.0 (TID 145)\n",
      "25/05/05 11:21:49 INFO CodeGenerator: Code generated in 4.013667 ms\n",
      "25/05/05 11:21:49 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3279 untilOffset=3280, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=48 taskId=145 partitionId=0\n",
      "25/05/05 11:21:49 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3279 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3280, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3280, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:50 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:21:50 INFO DataWritingSparkTask: Committed partition 0 (task 144, attempt 0, stage 144.0)\n",
      "25/05/05 11:21:50 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 503896833 nanos, during time span of 504662334 nanos.\n",
      "25/05/05 11:21:50 INFO Executor: Finished task 0.0 in stage 144.0 (TID 144). 2137 bytes result sent to driver\n",
      "25/05/05 11:21:50 INFO TaskSetManager: Finished task 0.0 in stage 144.0 (TID 144) in 515 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:21:50 INFO TaskSchedulerImpl: Removed TaskSet 144.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:21:50 INFO DAGScheduler: ResultStage 144 (start at NativeMethodAccessorImpl.java:0) finished in 0.519 s\n",
      "25/05/05 11:21:50 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:21:50 INFO DataWritingSparkTask: Committed partition 0 (task 143, attempt 0, stage 143.0)\n",
      "25/05/05 11:21:50 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 505317250 nanos, during time span of 508125208 nanos.\n",
      "25/05/05 11:21:50 INFO DAGScheduler: Job 145 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:21:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 144: Stage finished\n",
      "25/05/05 11:21:50 INFO DAGScheduler: Job 145 finished: start at NativeMethodAccessorImpl.java:0, took 0.521359 s\n",
      "25/05/05 11:21:50 INFO Executor: Finished task 0.0 in stage 143.0 (TID 143). 3545 bytes result sent to driver\n",
      "25/05/05 11:21:50 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 48, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:21:50 INFO TaskSetManager: Finished task 0.0 in stage 143.0 (TID 143) in 520 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:21:50 INFO TaskSchedulerImpl: Removed TaskSet 143.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:21:50 INFO DAGScheduler: ResultStage 143 (start at NativeMethodAccessorImpl.java:0) finished in 0.523 s\n",
      "25/05/05 11:21:50 INFO DAGScheduler: Job 144 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:21:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 143: Stage finished\n",
      "25/05/05 11:21:50 INFO DAGScheduler: Job 144 finished: start at NativeMethodAccessorImpl.java:0, took 0.523869 s\n",
      "25/05/05 11:21:50 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 48, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@3ea96bdf] is committing.\n",
      "25/05/05 11:21:50 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 48, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@3ea96bdf] committed.\n",
      "25/05/05 11:21:50 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/48 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.48.19c685ca-bd45-41ee-86bc-3de0ebd1b658.tmp\n",
      "25/05/05 11:21:50 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 48, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:21:50 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/48 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.48.d29b6444-64bb-4ec9-b365-a4bf1b43b619.tmp\n",
      "25/05/05 11:21:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3280, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:50 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:21:50 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:21:50 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:21:50 INFO connection: Opened connection [connectionId{localValue:95, serverValue:4465}] to localhost:27017\n",
      "25/05/05 11:21:50 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=552042}\n",
      "25/05/05 11:21:50 INFO connection: Opened connection [connectionId{localValue:96, serverValue:4466}] to localhost:27017\n",
      "25/05/05 11:21:50 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:21:50 INFO connection: Closed connection [connectionId{localValue:96, serverValue:4466}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:21:50 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 507500500 nanos, during time span of 514623167 nanos.\n",
      "25/05/05 11:21:50 INFO Executor: Finished task 0.0 in stage 145.0 (TID 145). 1645 bytes result sent to driver\n",
      "25/05/05 11:21:50 INFO TaskSetManager: Finished task 0.0 in stage 145.0 (TID 145) in 526 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:21:50 INFO TaskSchedulerImpl: Removed TaskSet 145.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:21:50 INFO DAGScheduler: ResultStage 145 (start at NativeMethodAccessorImpl.java:0) finished in 0.530 s\n",
      "25/05/05 11:21:50 INFO DAGScheduler: Job 146 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:21:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 145: Stage finished\n",
      "25/05/05 11:21:50 INFO DAGScheduler: Job 146 finished: start at NativeMethodAccessorImpl.java:0, took 0.531331 s\n",
      "25/05/05 11:21:50 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.48.19c685ca-bd45-41ee-86bc-3de0ebd1b658.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/48\n",
      "25/05/05 11:21:50 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:21:49.865Z\",\n",
      "  \"batchId\" : 48,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.6129032258064517,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 540,\n",
      "    \"commitOffsets\" : 33,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 620,\n",
      "    \"walCommit\" : 42\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3279\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3280\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3280\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.6129032258064517,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:21:50 INFO MemoryStore: Block broadcast_194 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:21:50 INFO MemoryStore: Block broadcast_194_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:21:50 INFO BlockManagerInfo: Added broadcast_194_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:21:50 INFO SparkContext: Created broadcast 194 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:50 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/48 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.48.bac9aef5-95f2-45b3-bf9d-6be0e5a59124.tmp\n",
      "25/05/05 11:21:50 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.48.d29b6444-64bb-4ec9-b365-a4bf1b43b619.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/48\n",
      "25/05/05 11:21:50 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:21:49.865Z\",\n",
      "  \"batchId\" : 48,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "  \"processedRowsPerSecond\" : 1.5873015873015872,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 549,\n",
      "    \"commitOffsets\" : 34,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 630,\n",
      "    \"walCommit\" : 39\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3279\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3280\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3280\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "    \"processedRowsPerSecond\" : 1.5873015873015872,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:21:50 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.48.bac9aef5-95f2-45b3-bf9d-6be0e5a59124.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/48\n",
      "25/05/05 11:21:50 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:21:49.866Z\",\n",
      "  \"batchId\" : 48,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.5313935681470137,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 571,\n",
      "    \"commitOffsets\" : 32,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 653,\n",
      "    \"walCommit\" : 43\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3279\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3280\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3280\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.5313935681470137,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 48\n",
      "-------------------------------------------\n",
      "+----+----+--------+-------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION|DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+-------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A004|R033|00-00-00|125 ST |05/05/2025|11:21:48|REGULAR|4          |4         |2025-05-05 11:21:49.867|\n",
      "+----+----+--------+-------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:21:56 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/49 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.49.868f1bc7-5174-4979-8c7d-e9cb5036084a.tmp\n",
      "25/05/05 11:21:56 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/49 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.49.0beabe00-20ef-416e-8f62-e3b6111ffb42.tmp\n",
      "25/05/05 11:21:56 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/49 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.49.610f2eae-df6a-487c-91f7-50c16f6cbb55.tmp\n",
      "25/05/05 11:21:56 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.49.610f2eae-df6a-487c-91f7-50c16f6cbb55.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/49\n",
      "25/05/05 11:21:56 INFO MicroBatchExecution: Committed offsets for batch 49. Metadata OffsetSeqMetadata(0,1746458516461,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:21:56 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.49.868f1bc7-5174-4979-8c7d-e9cb5036084a.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/49\n",
      "25/05/05 11:21:56 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.49.0beabe00-20ef-416e-8f62-e3b6111ffb42.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/49\n",
      "25/05/05 11:21:56 INFO MicroBatchExecution: Committed offsets for batch 49. Metadata OffsetSeqMetadata(0,1746458516461,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:21:56 INFO MicroBatchExecution: Committed offsets for batch 49. Metadata OffsetSeqMetadata(0,1746458516466,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:21:56 INFO IncrementalExecution: Current batch timestamp = 1746458516466\n",
      "25/05/05 11:21:56 INFO IncrementalExecution: Current batch timestamp = 1746458516461\n",
      "25/05/05 11:21:56 INFO IncrementalExecution: Current batch timestamp = 1746458516461\n",
      "25/05/05 11:21:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:56 INFO IncrementalExecution: Current batch timestamp = 1746458516461\n",
      "25/05/05 11:21:56 INFO IncrementalExecution: Current batch timestamp = 1746458516461\n",
      "25/05/05 11:21:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:56 INFO IncrementalExecution: Current batch timestamp = 1746458516466\n",
      "25/05/05 11:21:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:56 INFO BlockManagerInfo: Removed broadcast_194_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:21:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:56 INFO IncrementalExecution: Current batch timestamp = 1746458516461\n",
      "25/05/05 11:21:56 INFO IncrementalExecution: Current batch timestamp = 1746458516461\n",
      "25/05/05 11:21:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:21:56 INFO BlockManagerInfo: Removed broadcast_192_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:21:56 INFO BlockManagerInfo: Removed broadcast_193_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:56 INFO BlockManagerInfo: Removed broadcast_191_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:56 INFO CodeGenerator: Code generated in 25.523416 ms\n",
      "25/05/05 11:21:56 INFO CodeGenerator: Code generated in 18.233958 ms\n",
      "25/05/05 11:21:56 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 49, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@eaa38ae]. The input RDD has 1 partitions.\n",
      "25/05/05 11:21:56 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 49, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:21:56 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:56 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:56 INFO DAGScheduler: Got job 147 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:21:56 INFO DAGScheduler: Final stage: ResultStage 146 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:21:56 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:21:56 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:21:56 INFO DAGScheduler: Submitting ResultStage 146 (MapPartitionsRDD[793] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:21:56 INFO MemoryStore: Block broadcast_195 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:56 INFO MemoryStore: Block broadcast_195_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:56 INFO BlockManagerInfo: Added broadcast_195_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:56 INFO SparkContext: Created broadcast 195 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:21:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 146 (MapPartitionsRDD[793] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:21:56 INFO TaskSchedulerImpl: Adding task set 146.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:21:56 INFO DAGScheduler: Got job 148 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:21:56 INFO DAGScheduler: Final stage: ResultStage 147 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:21:56 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:21:56 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:21:56 INFO DAGScheduler: Submitting ResultStage 147 (MapPartitionsRDD[792] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:21:56 INFO TaskSetManager: Starting task 0.0 in stage 146.0 (TID 146) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:21:56 INFO Executor: Running task 0.0 in stage 146.0 (TID 146)\n",
      "25/05/05 11:21:56 INFO MemoryStore: Block broadcast_196 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:56 INFO MemoryStore: Block broadcast_196_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:21:56 INFO BlockManagerInfo: Added broadcast_196_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:21:56 INFO SparkContext: Created broadcast 196 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:21:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 147 (MapPartitionsRDD[792] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:21:56 INFO TaskSchedulerImpl: Adding task set 147.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:21:56 INFO TaskSetManager: Starting task 0.0 in stage 147.0 (TID 147) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:21:56 INFO Executor: Running task 0.0 in stage 147.0 (TID 147)\n",
      "25/05/05 11:21:56 INFO CodeGenerator: Code generated in 5.624167 ms\n",
      "25/05/05 11:21:56 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3280 untilOffset=3281, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=49 taskId=147 partitionId=0\n",
      "25/05/05 11:21:56 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3280 untilOffset=3281, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=49 taskId=146 partitionId=0\n",
      "25/05/05 11:21:56 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3280 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:56 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3280 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:56 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:56 INFO DAGScheduler: Got job 149 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:21:56 INFO DAGScheduler: Final stage: ResultStage 148 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:21:56 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:21:56 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:21:56 INFO DAGScheduler: Submitting ResultStage 148 (MapPartitionsRDD[800] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:21:56 INFO MemoryStore: Block broadcast_197 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:21:56 INFO MemoryStore: Block broadcast_197_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.1 MiB)\n",
      "25/05/05 11:21:56 INFO BlockManagerInfo: Added broadcast_197_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:21:56 INFO SparkContext: Created broadcast 197 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:21:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 148 (MapPartitionsRDD[800] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:21:56 INFO TaskSchedulerImpl: Adding task set 148.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:21:56 INFO TaskSetManager: Starting task 0.0 in stage 148.0 (TID 148) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:21:56 INFO Executor: Running task 0.0 in stage 148.0 (TID 148)\n",
      "25/05/05 11:21:56 INFO CodeGenerator: Code generated in 4.436834 ms\n",
      "25/05/05 11:21:56 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3280 untilOffset=3281, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=49 taskId=148 partitionId=0\n",
      "25/05/05 11:21:56 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3280 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3281, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3281, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:57 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:21:57 INFO DataWritingSparkTask: Committed partition 0 (task 147, attempt 0, stage 147.0)\n",
      "25/05/05 11:21:57 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 504893500 nanos, during time span of 505518875 nanos.\n",
      "25/05/05 11:21:57 INFO Executor: Finished task 0.0 in stage 147.0 (TID 147). 2145 bytes result sent to driver\n",
      "25/05/05 11:21:57 INFO TaskSetManager: Finished task 0.0 in stage 147.0 (TID 147) in 517 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:21:57 INFO TaskSchedulerImpl: Removed TaskSet 147.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:21:57 INFO DAGScheduler: ResultStage 147 (start at NativeMethodAccessorImpl.java:0) finished in 0.521 s\n",
      "25/05/05 11:21:57 INFO DAGScheduler: Job 148 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:21:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 147: Stage finished\n",
      "25/05/05 11:21:57 INFO DAGScheduler: Job 148 finished: start at NativeMethodAccessorImpl.java:0, took 0.526198 s\n",
      "25/05/05 11:21:57 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:21:57 INFO DataWritingSparkTask: Committed partition 0 (task 146, attempt 0, stage 146.0)\n",
      "25/05/05 11:21:57 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 504162583 nanos, during time span of 506463500 nanos.\n",
      "25/05/05 11:21:57 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 49, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:21:57 INFO Executor: Finished task 0.0 in stage 146.0 (TID 146). 3558 bytes result sent to driver\n",
      "25/05/05 11:21:57 INFO TaskSetManager: Finished task 0.0 in stage 146.0 (TID 146) in 523 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:21:57 INFO TaskSchedulerImpl: Removed TaskSet 146.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:21:57 INFO DAGScheduler: ResultStage 146 (start at NativeMethodAccessorImpl.java:0) finished in 0.527 s\n",
      "25/05/05 11:21:57 INFO DAGScheduler: Job 147 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:21:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 146: Stage finished\n",
      "25/05/05 11:21:57 INFO DAGScheduler: Job 147 finished: start at NativeMethodAccessorImpl.java:0, took 0.527904 s\n",
      "25/05/05 11:21:57 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 49, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@eaa38ae] is committing.\n",
      "25/05/05 11:21:57 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 49, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@eaa38ae] committed.\n",
      "25/05/05 11:21:57 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 49, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:21:57 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/49 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.49.44cd5689-31ca-4fbc-8b12-514a74611434.tmp\n",
      "25/05/05 11:21:57 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/49 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.49.9cddea4e-7660-4b34-8104-d99f1afcb57f.tmp\n",
      "25/05/05 11:21:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:21:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3281, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:21:57 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:21:57 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:21:57 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:21:57 INFO connection: Opened connection [connectionId{localValue:97, serverValue:4467}] to localhost:27017\n",
      "25/05/05 11:21:57 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=382375}\n",
      "25/05/05 11:21:57 INFO connection: Opened connection [connectionId{localValue:98, serverValue:4468}] to localhost:27017\n",
      "25/05/05 11:21:57 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:21:57 INFO connection: Closed connection [connectionId{localValue:98, serverValue:4468}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:21:57 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 510220250 nanos, during time span of 532874041 nanos.\n",
      "25/05/05 11:21:57 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.49.44cd5689-31ca-4fbc-8b12-514a74611434.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/49\n",
      "25/05/05 11:21:57 INFO Executor: Finished task 0.0 in stage 148.0 (TID 148). 1645 bytes result sent to driver\n",
      "25/05/05 11:21:57 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:21:56.459Z\",\n",
      "  \"batchId\" : 49,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.4184397163120568,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 575,\n",
      "    \"commitOffsets\" : 54,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 8,\n",
      "    \"triggerExecution\" : 705,\n",
      "    \"walCommit\" : 66\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3280\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3281\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3281\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.4184397163120568,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:21:57 INFO TaskSetManager: Finished task 0.0 in stage 148.0 (TID 148) in 546 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:21:57 INFO TaskSchedulerImpl: Removed TaskSet 148.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:21:57 INFO DAGScheduler: ResultStage 148 (start at NativeMethodAccessorImpl.java:0) finished in 0.552 s\n",
      "25/05/05 11:21:57 INFO DAGScheduler: Job 149 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:21:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 148: Stage finished\n",
      "25/05/05 11:21:57 INFO DAGScheduler: Job 149 finished: start at NativeMethodAccessorImpl.java:0, took 0.552775 s\n",
      "25/05/05 11:21:57 INFO MemoryStore: Block broadcast_198 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:21:57 INFO MemoryStore: Block broadcast_198_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:21:57 INFO BlockManagerInfo: Added broadcast_198_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:21:57 INFO SparkContext: Created broadcast 198 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:21:57 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.49.9cddea4e-7660-4b34-8104-d99f1afcb57f.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/49\n",
      "25/05/05 11:21:57 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:21:56.459Z\",\n",
      "  \"batchId\" : 49,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "  \"processedRowsPerSecond\" : 1.3966480446927374,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 587,\n",
      "    \"commitOffsets\" : 53,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 9,\n",
      "    \"triggerExecution\" : 716,\n",
      "    \"walCommit\" : 64\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3280\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3281\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3281\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "    \"processedRowsPerSecond\" : 1.3966480446927374,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:21:57 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/49 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.49.c99da7e1-cf86-463c-8f58-38bba155b095.tmp\n",
      "25/05/05 11:21:57 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.49.c99da7e1-cf86-463c-8f58-38bba155b095.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/49\n",
      "25/05/05 11:21:57 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:21:56.459Z\",\n",
      "  \"batchId\" : 49,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 62.5,\n",
      "  \"processedRowsPerSecond\" : 1.3458950201884252,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 632,\n",
      "    \"commitOffsets\" : 34,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 7,\n",
      "    \"queryPlanning\" : 8,\n",
      "    \"triggerExecution\" : 743,\n",
      "    \"walCommit\" : 61\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3280\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3281\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3281\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 62.5,\n",
      "    \"processedRowsPerSecond\" : 1.3458950201884252,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 49\n",
      "-------------------------------------------\n",
      "+----+----+--------+--------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION       |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+--------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A002|R051|02-00-00|34 ST-PENN STA|05/05/2025|11:21:55|REGULAR|14         |10        |2025-05-05 11:21:56.461|\n",
      "+----+----+--------+--------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:22:01 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/50 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.50.13058140-fbaf-4638-947e-6c23b8607d84.tmp\n",
      "25/05/05 11:22:01 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/50 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.50.eecdf715-d1b3-48e8-a399-754ad41b5124.tmp\n",
      "25/05/05 11:22:01 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/50 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.50.43abe68e-ba23-427c-8e0f-2e434dc93f44.tmp\n",
      "25/05/05 11:22:02 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.50.eecdf715-d1b3-48e8-a399-754ad41b5124.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/50\n",
      "25/05/05 11:22:02 INFO MicroBatchExecution: Committed offsets for batch 50. Metadata OffsetSeqMetadata(0,1746458521960,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:22:02 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.50.43abe68e-ba23-427c-8e0f-2e434dc93f44.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/50\n",
      "25/05/05 11:22:02 INFO MicroBatchExecution: Committed offsets for batch 50. Metadata OffsetSeqMetadata(0,1746458521962,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:22:02 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.50.13058140-fbaf-4638-947e-6c23b8607d84.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/50\n",
      "25/05/05 11:22:02 INFO MicroBatchExecution: Committed offsets for batch 50. Metadata OffsetSeqMetadata(0,1746458521960,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:22:02 INFO IncrementalExecution: Current batch timestamp = 1746458521960\n",
      "25/05/05 11:22:02 INFO IncrementalExecution: Current batch timestamp = 1746458521962\n",
      "25/05/05 11:22:02 INFO IncrementalExecution: Current batch timestamp = 1746458521960\n",
      "25/05/05 11:22:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:02 INFO IncrementalExecution: Current batch timestamp = 1746458521960\n",
      "25/05/05 11:22:02 INFO IncrementalExecution: Current batch timestamp = 1746458521962\n",
      "25/05/05 11:22:02 INFO IncrementalExecution: Current batch timestamp = 1746458521960\n",
      "25/05/05 11:22:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:02 INFO IncrementalExecution: Current batch timestamp = 1746458521960\n",
      "25/05/05 11:22:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:02 INFO IncrementalExecution: Current batch timestamp = 1746458521960\n",
      "25/05/05 11:22:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:02 INFO CodeGenerator: Code generated in 5.560709 ms\n",
      "25/05/05 11:22:02 INFO CodeGenerator: Code generated in 4.768208 ms\n",
      "25/05/05 11:22:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 50, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6beb9901]. The input RDD has 1 partitions.\n",
      "25/05/05 11:22:02 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 50, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:22:02 INFO DAGScheduler: Got job 150 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:22:02 INFO DAGScheduler: Final stage: ResultStage 149 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:22:02 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:02 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:22:02 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:22:02 INFO DAGScheduler: Submitting ResultStage 149 (MapPartitionsRDD[810] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:22:02 INFO MemoryStore: Block broadcast_199 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:22:02 INFO MemoryStore: Block broadcast_199_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.1 MiB)\n",
      "25/05/05 11:22:02 INFO BlockManagerInfo: Added broadcast_199_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:02 INFO SparkContext: Created broadcast 199 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:22:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 149 (MapPartitionsRDD[810] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:22:02 INFO TaskSchedulerImpl: Adding task set 149.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:22:02 INFO DAGScheduler: Got job 151 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:22:02 INFO DAGScheduler: Final stage: ResultStage 150 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:22:02 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:22:02 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:22:02 INFO DAGScheduler: Submitting ResultStage 150 (MapPartitionsRDD[811] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:22:02 INFO TaskSetManager: Starting task 0.0 in stage 149.0 (TID 149) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:22:02 INFO Executor: Running task 0.0 in stage 149.0 (TID 149)\n",
      "25/05/05 11:22:02 INFO MemoryStore: Block broadcast_200 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:22:02 INFO MemoryStore: Block broadcast_200_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:22:02 INFO BlockManagerInfo: Added broadcast_200_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:02 INFO SparkContext: Created broadcast 200 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:22:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 150 (MapPartitionsRDD[811] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:22:02 INFO TaskSchedulerImpl: Adding task set 150.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:22:02 INFO TaskSetManager: Starting task 0.0 in stage 150.0 (TID 150) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:22:02 INFO Executor: Running task 0.0 in stage 150.0 (TID 150)\n",
      "25/05/05 11:22:02 INFO CodeGenerator: Code generated in 5.533292 ms\n",
      "25/05/05 11:22:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3281 untilOffset=3282, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=50 taskId=150 partitionId=0\n",
      "25/05/05 11:22:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3281 untilOffset=3282, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=50 taskId=149 partitionId=0\n",
      "25/05/05 11:22:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3281 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3281 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:02 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:02 INFO DAGScheduler: Got job 152 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:22:02 INFO DAGScheduler: Final stage: ResultStage 151 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:22:02 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:22:02 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:22:02 INFO DAGScheduler: Submitting ResultStage 151 (MapPartitionsRDD[816] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:22:02 INFO MemoryStore: Block broadcast_201 stored as values in memory (estimated size 47.1 KiB, free 366.0 MiB)\n",
      "25/05/05 11:22:02 INFO MemoryStore: Block broadcast_201_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:22:02 INFO BlockManagerInfo: Added broadcast_201_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:02 INFO SparkContext: Created broadcast 201 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:22:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 151 (MapPartitionsRDD[816] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:22:02 INFO TaskSchedulerImpl: Adding task set 151.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:22:02 INFO TaskSetManager: Starting task 0.0 in stage 151.0 (TID 151) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:22:02 INFO Executor: Running task 0.0 in stage 151.0 (TID 151)\n",
      "25/05/05 11:22:02 INFO CodeGenerator: Code generated in 5.867917 ms\n",
      "25/05/05 11:22:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3281 untilOffset=3282, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=50 taskId=151 partitionId=0\n",
      "25/05/05 11:22:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3281 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3282, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3282, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:02 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:22:02 INFO DataWritingSparkTask: Committed partition 0 (task 150, attempt 0, stage 150.0)\n",
      "25/05/05 11:22:02 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 505025291 nanos, during time span of 505435625 nanos.\n",
      "25/05/05 11:22:02 INFO Executor: Finished task 0.0 in stage 150.0 (TID 150). 2188 bytes result sent to driver\n",
      "25/05/05 11:22:02 INFO TaskSetManager: Finished task 0.0 in stage 150.0 (TID 150) in 514 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:22:02 INFO TaskSchedulerImpl: Removed TaskSet 150.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:22:02 INFO DAGScheduler: ResultStage 150 (start at NativeMethodAccessorImpl.java:0) finished in 0.518 s\n",
      "25/05/05 11:22:02 INFO DAGScheduler: Job 151 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:22:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 150: Stage finished\n",
      "25/05/05 11:22:02 INFO DAGScheduler: Job 151 finished: start at NativeMethodAccessorImpl.java:0, took 0.520789 s\n",
      "25/05/05 11:22:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 50, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:22:02 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:22:02 INFO DataWritingSparkTask: Committed partition 0 (task 149, attempt 0, stage 149.0)\n",
      "25/05/05 11:22:02 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 503327833 nanos, during time span of 505339959 nanos.\n",
      "25/05/05 11:22:02 INFO Executor: Finished task 0.0 in stage 149.0 (TID 149). 3557 bytes result sent to driver\n",
      "25/05/05 11:22:02 INFO TaskSetManager: Finished task 0.0 in stage 149.0 (TID 149) in 520 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:22:02 INFO TaskSchedulerImpl: Removed TaskSet 149.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:22:02 INFO DAGScheduler: ResultStage 149 (start at NativeMethodAccessorImpl.java:0) finished in 0.523 s\n",
      "25/05/05 11:22:02 INFO DAGScheduler: Job 150 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:22:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 149: Stage finished\n",
      "25/05/05 11:22:02 INFO DAGScheduler: Job 150 finished: start at NativeMethodAccessorImpl.java:0, took 0.524155 s\n",
      "25/05/05 11:22:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 50, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6beb9901] is committing.\n",
      "25/05/05 11:22:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 50, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6beb9901] committed.\n",
      "25/05/05 11:22:02 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/50 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.50.27d59459-8bbc-4d0e-a7cd-eedae143815d.tmp\n",
      "25/05/05 11:22:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 50, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:22:02 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/50 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.50.d7f0de8a-88c3-4bde-b420-32eed583f1c2.tmp\n",
      "25/05/05 11:22:02 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.50.27d59459-8bbc-4d0e-a7cd-eedae143815d.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/50\n",
      "25/05/05 11:22:02 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:22:01.958Z\",\n",
      "  \"batchId\" : 50,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.5552099533437014,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 542,\n",
      "    \"commitOffsets\" : 34,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 643,\n",
      "    \"walCommit\" : 60\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3281\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3282\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3282\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.5552099533437014,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:22:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3282, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:02 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:22:02 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:22:02 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:22:02 INFO connection: Opened connection [connectionId{localValue:99, serverValue:4469}] to localhost:27017\n",
      "25/05/05 11:22:02 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=442958}\n",
      "25/05/05 11:22:02 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.50.d7f0de8a-88c3-4bde-b420-32eed583f1c2.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/50\n",
      "25/05/05 11:22:02 INFO connection: Opened connection [connectionId{localValue:100, serverValue:4470}] to localhost:27017\n",
      "25/05/05 11:22:02 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:22:01.958Z\",\n",
      "  \"batchId\" : 50,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.5313935681470137,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 552,\n",
      "    \"commitOffsets\" : 34,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 653,\n",
      "    \"walCommit\" : 59\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3281\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3282\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3282\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.5313935681470137,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:22:02 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:22:02 INFO connection: Closed connection [connectionId{localValue:100, serverValue:4470}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:22:02 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 503420416 nanos, during time span of 514383042 nanos.\n",
      "25/05/05 11:22:02 INFO Executor: Finished task 0.0 in stage 151.0 (TID 151). 1645 bytes result sent to driver\n",
      "25/05/05 11:22:02 INFO TaskSetManager: Finished task 0.0 in stage 151.0 (TID 151) in 528 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:22:02 INFO TaskSchedulerImpl: Removed TaskSet 151.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:22:02 INFO DAGScheduler: ResultStage 151 (start at NativeMethodAccessorImpl.java:0) finished in 0.536 s\n",
      "25/05/05 11:22:02 INFO DAGScheduler: Job 152 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:22:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 151: Stage finished\n",
      "25/05/05 11:22:02 INFO DAGScheduler: Job 152 finished: start at NativeMethodAccessorImpl.java:0, took 0.538613 s\n",
      "25/05/05 11:22:02 INFO MemoryStore: Block broadcast_202 stored as values in memory (estimated size 248.0 B, free 366.0 MiB)\n",
      "25/05/05 11:22:02 INFO MemoryStore: Block broadcast_202_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.0 MiB)\n",
      "25/05/05 11:22:02 INFO BlockManagerInfo: Added broadcast_202_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:22:02 INFO SparkContext: Created broadcast 202 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:02 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/50 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.50.bced15c1-302e-4314-a1ef-b4ecb8e1f09c.tmp\n",
      "25/05/05 11:22:02 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.50.bced15c1-302e-4314-a1ef-b4ecb8e1f09c.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/50\n",
      "25/05/05 11:22:02 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:22:01.959Z\",\n",
      "  \"batchId\" : 50,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.451378809869376,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 594,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 689,\n",
      "    \"walCommit\" : 58\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3281\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3282\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3282\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.451378809869376,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 50\n",
      "-------------------------------------------\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+----------------------+\n",
      "|C/A |UNIT|SCP     |STATION      |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time           |\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+----------------------+\n",
      "|R016|R032|00-00-01|TIME SQ-42 ST|05/05/2025|11:22:00|REGULAR|6          |7         |2025-05-05 11:22:01.96|\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:22:04 INFO BlockManagerInfo: Removed broadcast_196_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:04 INFO BlockManagerInfo: Removed broadcast_200_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:04 INFO BlockManagerInfo: Removed broadcast_199_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:04 INFO BlockManagerInfo: Removed broadcast_202_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:22:04 INFO BlockManagerInfo: Removed broadcast_201_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:04 INFO BlockManagerInfo: Removed broadcast_197_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:22:04 INFO BlockManagerInfo: Removed broadcast_195_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:22:04 INFO BlockManagerInfo: Removed broadcast_198_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:22:08 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/51 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.51.bf90c231-75cd-4661-95bf-73470aac9312.tmp\n",
      "25/05/05 11:22:08 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/51 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.51.902984f3-ad3b-4051-8a08-ac52750ab4a0.tmp\n",
      "25/05/05 11:22:08 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/51 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.51.bdc9e18a-cf7f-4c0f-a58e-05e444e04ce1.tmp\n",
      "25/05/05 11:22:08 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.51.902984f3-ad3b-4051-8a08-ac52750ab4a0.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/51\n",
      "25/05/05 11:22:08 INFO MicroBatchExecution: Committed offsets for batch 51. Metadata OffsetSeqMetadata(0,1746458528488,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:22:08 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.51.bf90c231-75cd-4661-95bf-73470aac9312.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/51\n",
      "25/05/05 11:22:08 INFO MicroBatchExecution: Committed offsets for batch 51. Metadata OffsetSeqMetadata(0,1746458528478,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:22:08 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.51.bdc9e18a-cf7f-4c0f-a58e-05e444e04ce1.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/51\n",
      "25/05/05 11:22:08 INFO MicroBatchExecution: Committed offsets for batch 51. Metadata OffsetSeqMetadata(0,1746458528488,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:22:08 INFO IncrementalExecution: Current batch timestamp = 1746458528488\n",
      "25/05/05 11:22:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:08 INFO IncrementalExecution: Current batch timestamp = 1746458528478\n",
      "25/05/05 11:22:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:08 INFO IncrementalExecution: Current batch timestamp = 1746458528488\n",
      "25/05/05 11:22:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:08 INFO IncrementalExecution: Current batch timestamp = 1746458528488\n",
      "25/05/05 11:22:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:08 INFO IncrementalExecution: Current batch timestamp = 1746458528478\n",
      "25/05/05 11:22:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:08 INFO IncrementalExecution: Current batch timestamp = 1746458528488\n",
      "25/05/05 11:22:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:08 INFO IncrementalExecution: Current batch timestamp = 1746458528478\n",
      "25/05/05 11:22:08 INFO IncrementalExecution: Current batch timestamp = 1746458528488\n",
      "25/05/05 11:22:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:08 INFO CodeGenerator: Code generated in 4.727959 ms\n",
      "25/05/05 11:22:08 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 51, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@5ea544cf]. The input RDD has 1 partitions.\n",
      "25/05/05 11:22:08 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:08 INFO DAGScheduler: Got job 153 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:22:08 INFO DAGScheduler: Final stage: ResultStage 152 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:22:08 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:22:08 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:22:08 INFO DAGScheduler: Submitting ResultStage 152 (MapPartitionsRDD[824] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:22:08 INFO MemoryStore: Block broadcast_203 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:22:08 INFO MemoryStore: Block broadcast_203_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:22:08 INFO BlockManagerInfo: Added broadcast_203_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:22:08 INFO SparkContext: Created broadcast 203 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:22:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 152 (MapPartitionsRDD[824] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:22:08 INFO TaskSchedulerImpl: Adding task set 152.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:22:08 INFO TaskSetManager: Starting task 0.0 in stage 152.0 (TID 152) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:22:08 INFO Executor: Running task 0.0 in stage 152.0 (TID 152)\n",
      "25/05/05 11:22:08 INFO CodeGenerator: Code generated in 14.540042 ms\n",
      "25/05/05 11:22:08 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 51, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:22:08 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:08 INFO DAGScheduler: Got job 154 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:22:08 INFO DAGScheduler: Final stage: ResultStage 153 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:22:08 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:22:08 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:22:08 INFO DAGScheduler: Submitting ResultStage 153 (MapPartitionsRDD[827] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:22:08 INFO MemoryStore: Block broadcast_204 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:22:08 INFO MemoryStore: Block broadcast_204_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:22:08 INFO BlockManagerInfo: Added broadcast_204_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:22:08 INFO SparkContext: Created broadcast 204 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:22:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 153 (MapPartitionsRDD[827] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:22:08 INFO TaskSchedulerImpl: Adding task set 153.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:22:08 INFO TaskSetManager: Starting task 0.0 in stage 153.0 (TID 153) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:22:08 INFO CodeGenerator: Code generated in 13.918417 ms\n",
      "25/05/05 11:22:08 INFO Executor: Running task 0.0 in stage 153.0 (TID 153)\n",
      "25/05/05 11:22:08 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3282 untilOffset=3283, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=51 taskId=152 partitionId=0\n",
      "25/05/05 11:22:08 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3282 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:08 INFO CodeGenerator: Code generated in 4.58975 ms\n",
      "25/05/05 11:22:08 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3282 untilOffset=3283, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=51 taskId=153 partitionId=0\n",
      "25/05/05 11:22:08 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3282 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:08 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:08 INFO DAGScheduler: Got job 155 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:22:08 INFO DAGScheduler: Final stage: ResultStage 154 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:22:08 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:22:08 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:22:08 INFO DAGScheduler: Submitting ResultStage 154 (MapPartitionsRDD[832] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:22:08 INFO MemoryStore: Block broadcast_205 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:22:08 INFO MemoryStore: Block broadcast_205_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.1 MiB)\n",
      "25/05/05 11:22:08 INFO BlockManagerInfo: Added broadcast_205_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:08 INFO SparkContext: Created broadcast 205 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:22:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 154 (MapPartitionsRDD[832] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:22:08 INFO TaskSchedulerImpl: Adding task set 154.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:22:08 INFO TaskSetManager: Starting task 0.0 in stage 154.0 (TID 154) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:22:08 INFO Executor: Running task 0.0 in stage 154.0 (TID 154)\n",
      "25/05/05 11:22:08 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3282 untilOffset=3283, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=51 taskId=154 partitionId=0\n",
      "25/05/05 11:22:08 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3282 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3283, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3283, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:09 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:22:09 INFO DataWritingSparkTask: Committed partition 0 (task 153, attempt 0, stage 153.0)\n",
      "25/05/05 11:22:09 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 506056000 nanos, during time span of 506396584 nanos.\n",
      "25/05/05 11:22:09 INFO Executor: Finished task 0.0 in stage 153.0 (TID 153). 2145 bytes result sent to driver\n",
      "25/05/05 11:22:09 INFO TaskSetManager: Finished task 0.0 in stage 153.0 (TID 153) in 517 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:22:09 INFO TaskSchedulerImpl: Removed TaskSet 153.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:22:09 INFO DAGScheduler: ResultStage 153 (start at NativeMethodAccessorImpl.java:0) finished in 0.519 s\n",
      "25/05/05 11:22:09 INFO DAGScheduler: Job 154 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:22:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 153: Stage finished\n",
      "25/05/05 11:22:09 INFO DAGScheduler: Job 154 finished: start at NativeMethodAccessorImpl.java:0, took 0.519789 s\n",
      "25/05/05 11:22:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 51, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:22:09 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:22:09 INFO DataWritingSparkTask: Committed partition 0 (task 152, attempt 0, stage 152.0)\n",
      "25/05/05 11:22:09 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 509983500 nanos, during time span of 512277959 nanos.\n",
      "25/05/05 11:22:09 INFO Executor: Finished task 0.0 in stage 152.0 (TID 152). 3510 bytes result sent to driver\n",
      "25/05/05 11:22:09 INFO TaskSetManager: Finished task 0.0 in stage 152.0 (TID 152) in 533 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:22:09 INFO TaskSchedulerImpl: Removed TaskSet 152.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:22:09 INFO DAGScheduler: ResultStage 152 (start at NativeMethodAccessorImpl.java:0) finished in 0.536 s\n",
      "25/05/05 11:22:09 INFO DAGScheduler: Job 153 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:22:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 152: Stage finished\n",
      "25/05/05 11:22:09 INFO DAGScheduler: Job 153 finished: start at NativeMethodAccessorImpl.java:0, took 0.536123 s\n",
      "25/05/05 11:22:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 51, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@5ea544cf] is committing.\n",
      "25/05/05 11:22:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 51, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@5ea544cf] committed.\n",
      "25/05/05 11:22:09 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/51 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.51.c56abf37-76cd-43a1-ba84-5a4fe728f2bf.tmp\n",
      "25/05/05 11:22:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 51, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:22:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3283, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:09 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:22:09 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:22:09 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:22:09 INFO connection: Opened connection [connectionId{localValue:101, serverValue:4471}] to localhost:27017\n",
      "25/05/05 11:22:09 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=391209}\n",
      "25/05/05 11:22:09 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/51 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.51.a8dd68c2-ac8f-4af2-849f-58a52b480892.tmp\n",
      "25/05/05 11:22:09 INFO connection: Opened connection [connectionId{localValue:102, serverValue:4472}] to localhost:27017\n",
      "25/05/05 11:22:09 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:22:09 INFO connection: Closed connection [connectionId{localValue:102, serverValue:4472}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:22:09 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 504189041 nanos, during time span of 510790292 nanos.\n",
      "25/05/05 11:22:09 INFO Executor: Finished task 0.0 in stage 154.0 (TID 154). 1645 bytes result sent to driver\n",
      "25/05/05 11:22:09 INFO TaskSetManager: Finished task 0.0 in stage 154.0 (TID 154) in 518 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:22:09 INFO TaskSchedulerImpl: Removed TaskSet 154.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:22:09 INFO DAGScheduler: ResultStage 154 (start at NativeMethodAccessorImpl.java:0) finished in 0.525 s\n",
      "25/05/05 11:22:09 INFO DAGScheduler: Job 155 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:22:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 154: Stage finished\n",
      "25/05/05 11:22:09 INFO DAGScheduler: Job 155 finished: start at NativeMethodAccessorImpl.java:0, took 0.528023 s\n",
      "25/05/05 11:22:09 INFO MemoryStore: Block broadcast_206 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:22:09 INFO MemoryStore: Block broadcast_206_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:22:09 INFO BlockManagerInfo: Added broadcast_206_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:22:09 INFO SparkContext: Created broadcast 206 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:09 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/51 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.51.dd559210-6ded-407e-a038-8cf2de31f7bf.tmp\n",
      "25/05/05 11:22:09 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.51.c56abf37-76cd-43a1-ba84-5a4fe728f2bf.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/51\n",
      "25/05/05 11:22:09 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:22:08.484Z\",\n",
      "  \"batchId\" : 51,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 62.5,\n",
      "  \"processedRowsPerSecond\" : 1.5797788309636651,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 550,\n",
      "    \"commitOffsets\" : 32,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 4,\n",
      "    \"queryPlanning\" : 7,\n",
      "    \"triggerExecution\" : 633,\n",
      "    \"walCommit\" : 40\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3282\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3283\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3283\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 62.5,\n",
      "    \"processedRowsPerSecond\" : 1.5797788309636651,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:22:09 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.51.a8dd68c2-ac8f-4af2-849f-58a52b480892.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/51\n",
      "25/05/05 11:22:09 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:22:08.477Z\",\n",
      "  \"batchId\" : 51,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.5455950540958268,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 559,\n",
      "    \"commitOffsets\" : 31,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 6,\n",
      "    \"triggerExecution\" : 647,\n",
      "    \"walCommit\" : 50\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3282\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3283\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3283\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.5455950540958268,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:22:09 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.51.dd559210-6ded-407e-a038-8cf2de31f7bf.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/51\n",
      "25/05/05 11:22:09 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:22:08.484Z\",\n",
      "  \"batchId\" : 51,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.5313935681470137,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 574,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 4,\n",
      "    \"queryPlanning\" : 6,\n",
      "    \"triggerExecution\" : 653,\n",
      "    \"walCommit\" : 38\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3282\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3283\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3283\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.5313935681470137,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 51\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION  |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A002|R170|00-00-00|FULTON ST|05/05/2025|11:22:07|REGULAR|8          |5         |2025-05-05 11:22:08.478|\n",
      "+----+----+--------+---------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:22:14 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/52 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.52.45b75d17-a095-4672-848e-d3b25dbfbbe7.tmp\n",
      "25/05/05 11:22:14 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/52 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.52.67581756-4c6d-45c1-b335-53423f905250.tmp\n",
      "25/05/05 11:22:14 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/52 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.52.173314ff-8bb7-43f1-98ba-f263709e3faf.tmp\n",
      "25/05/05 11:22:14 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.52.173314ff-8bb7-43f1-98ba-f263709e3faf.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/52\n",
      "25/05/05 11:22:14 INFO MicroBatchExecution: Committed offsets for batch 52. Metadata OffsetSeqMetadata(0,1746458534016,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:22:14 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.52.67581756-4c6d-45c1-b335-53423f905250.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/52\n",
      "25/05/05 11:22:14 INFO MicroBatchExecution: Committed offsets for batch 52. Metadata OffsetSeqMetadata(0,1746458534014,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:22:14 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.52.45b75d17-a095-4672-848e-d3b25dbfbbe7.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/52\n",
      "25/05/05 11:22:14 INFO MicroBatchExecution: Committed offsets for batch 52. Metadata OffsetSeqMetadata(0,1746458534014,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:22:14 INFO IncrementalExecution: Current batch timestamp = 1746458534014\n",
      "25/05/05 11:22:14 INFO IncrementalExecution: Current batch timestamp = 1746458534016\n",
      "25/05/05 11:22:14 INFO IncrementalExecution: Current batch timestamp = 1746458534014\n",
      "25/05/05 11:22:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:14 INFO IncrementalExecution: Current batch timestamp = 1746458534014\n",
      "25/05/05 11:22:14 INFO IncrementalExecution: Current batch timestamp = 1746458534014\n",
      "25/05/05 11:22:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:14 INFO IncrementalExecution: Current batch timestamp = 1746458534016\n",
      "25/05/05 11:22:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:14 INFO IncrementalExecution: Current batch timestamp = 1746458534016\n",
      "25/05/05 11:22:14 INFO IncrementalExecution: Current batch timestamp = 1746458534014\n",
      "25/05/05 11:22:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:14 INFO CodeGenerator: Code generated in 4.820708 ms\n",
      "25/05/05 11:22:14 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 52, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:22:14 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:14 INFO DAGScheduler: Got job 156 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:22:14 INFO DAGScheduler: Final stage: ResultStage 155 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:22:14 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:22:14 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:22:14 INFO DAGScheduler: Submitting ResultStage 155 (MapPartitionsRDD[840] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:22:14 INFO CodeGenerator: Code generated in 4.90025 ms\n",
      "25/05/05 11:22:14 INFO MemoryStore: Block broadcast_207 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:22:14 INFO MemoryStore: Block broadcast_207_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:22:14 INFO BlockManagerInfo: Added broadcast_207_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:14 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 52, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@752f19fd]. The input RDD has 1 partitions.\n",
      "25/05/05 11:22:14 INFO SparkContext: Created broadcast 207 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:22:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 155 (MapPartitionsRDD[840] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:22:14 INFO TaskSchedulerImpl: Adding task set 155.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:22:14 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:14 INFO DAGScheduler: Got job 157 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:22:14 INFO DAGScheduler: Final stage: ResultStage 156 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:22:14 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:22:14 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:22:14 INFO DAGScheduler: Submitting ResultStage 156 (MapPartitionsRDD[843] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:22:14 INFO MemoryStore: Block broadcast_208 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:22:14 INFO TaskSetManager: Starting task 0.0 in stage 155.0 (TID 155) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:22:14 INFO Executor: Running task 0.0 in stage 155.0 (TID 155)\n",
      "25/05/05 11:22:14 INFO MemoryStore: Block broadcast_208_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)\n",
      "25/05/05 11:22:14 INFO BlockManagerInfo: Added broadcast_208_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:14 INFO SparkContext: Created broadcast 208 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:22:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 156 (MapPartitionsRDD[843] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:22:14 INFO TaskSchedulerImpl: Adding task set 156.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:22:14 INFO TaskSetManager: Starting task 0.0 in stage 156.0 (TID 156) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:22:14 INFO Executor: Running task 0.0 in stage 156.0 (TID 156)\n",
      "25/05/05 11:22:14 INFO CodeGenerator: Code generated in 5.609333 ms\n",
      "25/05/05 11:22:14 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3283 untilOffset=3284, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=52 taskId=155 partitionId=0\n",
      "25/05/05 11:22:14 INFO CodeGenerator: Code generated in 6.74275 ms\n",
      "25/05/05 11:22:14 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3283 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:14 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3283 untilOffset=3284, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=52 taskId=156 partitionId=0\n",
      "25/05/05 11:22:14 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3283 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:14 INFO BlockManagerInfo: Removed broadcast_206_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:22:14 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:14 INFO DAGScheduler: Got job 158 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:22:14 INFO DAGScheduler: Final stage: ResultStage 157 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:22:14 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:22:14 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:22:14 INFO DAGScheduler: Submitting ResultStage 157 (MapPartitionsRDD[848] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:22:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:14 INFO BlockManagerInfo: Removed broadcast_204_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:14 INFO MemoryStore: Block broadcast_209 stored as values in memory (estimated size 47.1 KiB, free 366.0 MiB)\n",
      "25/05/05 11:22:14 INFO BlockManagerInfo: Removed broadcast_205_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:22:14 INFO MemoryStore: Block broadcast_209_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.1 MiB)\n",
      "25/05/05 11:22:14 INFO BlockManagerInfo: Added broadcast_209_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:14 INFO SparkContext: Created broadcast 209 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:22:14 INFO BlockManagerInfo: Removed broadcast_203_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 157 (MapPartitionsRDD[848] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:22:14 INFO TaskSchedulerImpl: Adding task set 157.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:22:14 INFO TaskSetManager: Starting task 0.0 in stage 157.0 (TID 157) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:22:14 INFO Executor: Running task 0.0 in stage 157.0 (TID 157)\n",
      "25/05/05 11:22:14 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3283 untilOffset=3284, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=52 taskId=157 partitionId=0\n",
      "25/05/05 11:22:14 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3283 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3284, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:14 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:22:14 INFO DataWritingSparkTask: Committed partition 0 (task 155, attempt 0, stage 155.0)\n",
      "25/05/05 11:22:14 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 505376709 nanos, during time span of 505973333 nanos.\n",
      "25/05/05 11:22:14 INFO Executor: Finished task 0.0 in stage 155.0 (TID 155). 2231 bytes result sent to driver\n",
      "25/05/05 11:22:14 INFO TaskSetManager: Finished task 0.0 in stage 155.0 (TID 155) in 518 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:22:14 INFO TaskSchedulerImpl: Removed TaskSet 155.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:22:14 INFO DAGScheduler: ResultStage 155 (start at NativeMethodAccessorImpl.java:0) finished in 0.523 s\n",
      "25/05/05 11:22:14 INFO DAGScheduler: Job 156 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:22:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 155: Stage finished\n",
      "25/05/05 11:22:14 INFO DAGScheduler: Job 156 finished: start at NativeMethodAccessorImpl.java:0, took 0.526275 s\n",
      "25/05/05 11:22:14 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 52, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:22:14 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 52, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:22:14 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/52 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.52.5634d472-17a6-4af5-a9c9-81e5dd7ea14f.tmp\n",
      "25/05/05 11:22:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3284, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:14 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:22:14 INFO DataWritingSparkTask: Committed partition 0 (task 156, attempt 0, stage 156.0)\n",
      "25/05/05 11:22:14 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 514363708 nanos, during time span of 516162791 nanos.\n",
      "25/05/05 11:22:14 INFO Executor: Finished task 0.0 in stage 156.0 (TID 156). 3559 bytes result sent to driver\n",
      "25/05/05 11:22:14 INFO TaskSetManager: Finished task 0.0 in stage 156.0 (TID 156) in 548 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:22:14 INFO TaskSchedulerImpl: Removed TaskSet 156.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:22:14 INFO DAGScheduler: ResultStage 156 (start at NativeMethodAccessorImpl.java:0) finished in 0.551 s\n",
      "25/05/05 11:22:14 INFO DAGScheduler: Job 157 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:22:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 156: Stage finished\n",
      "25/05/05 11:22:14 INFO DAGScheduler: Job 157 finished: start at NativeMethodAccessorImpl.java:0, took 0.552291 s\n",
      "25/05/05 11:22:14 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 52, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@752f19fd] is committing.\n",
      "25/05/05 11:22:14 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 52, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@752f19fd] committed.\n",
      "25/05/05 11:22:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:14 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/52 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.52.06f49609-876e-449b-8fc2-d1ae61c88dd0.tmp\n",
      "25/05/05 11:22:14 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.52.5634d472-17a6-4af5-a9c9-81e5dd7ea14f.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/52\n",
      "25/05/05 11:22:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3284, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:14 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:22:14.013Z\",\n",
      "  \"batchId\" : 52,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.5923566878980893,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 555,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 7,\n",
      "    \"triggerExecution\" : 628,\n",
      "    \"walCommit\" : 37\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3283\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3284\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3284\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.5923566878980893,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:22:14 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:22:14 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:22:14 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:22:14 INFO connection: Opened connection [connectionId{localValue:103, serverValue:4473}] to localhost:27017\n",
      "25/05/05 11:22:14 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=268542}\n",
      "25/05/05 11:22:14 INFO connection: Opened connection [connectionId{localValue:104, serverValue:4474}] to localhost:27017\n",
      "25/05/05 11:22:14 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:22:14 INFO connection: Closed connection [connectionId{localValue:104, serverValue:4474}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:22:14 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 504544459 nanos, during time span of 511825666 nanos.\n",
      "25/05/05 11:22:14 INFO Executor: Finished task 0.0 in stage 157.0 (TID 157). 1645 bytes result sent to driver\n",
      "25/05/05 11:22:14 INFO TaskSetManager: Finished task 0.0 in stage 157.0 (TID 157) in 518 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:22:14 INFO TaskSchedulerImpl: Removed TaskSet 157.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:22:14 INFO DAGScheduler: ResultStage 157 (start at NativeMethodAccessorImpl.java:0) finished in 0.524 s\n",
      "25/05/05 11:22:14 INFO DAGScheduler: Job 158 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:22:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 157: Stage finished\n",
      "25/05/05 11:22:14 INFO DAGScheduler: Job 158 finished: start at NativeMethodAccessorImpl.java:0, took 0.525584 s\n",
      "25/05/05 11:22:14 INFO MemoryStore: Block broadcast_210 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:22:14 INFO MemoryStore: Block broadcast_210_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:22:14 INFO BlockManagerInfo: Added broadcast_210_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:22:14 INFO SparkContext: Created broadcast 210 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:14 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/52 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.52.31eec751-5db5-45b9-a539-92674fd15e98.tmp\n",
      "25/05/05 11:22:14 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.52.06f49609-876e-449b-8fc2-d1ae61c88dd0.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/52\n",
      "25/05/05 11:22:14 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:22:14.015Z\",\n",
      "  \"batchId\" : 52,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 62.5,\n",
      "  \"processedRowsPerSecond\" : 1.5432098765432098,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 577,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 8,\n",
      "    \"triggerExecution\" : 648,\n",
      "    \"walCommit\" : 34\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3283\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3284\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3284\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 62.5,\n",
      "    \"processedRowsPerSecond\" : 1.5432098765432098,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:22:14 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.52.31eec751-5db5-45b9-a539-92674fd15e98.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/52\n",
      "25/05/05 11:22:14 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:22:14.013Z\",\n",
      "  \"batchId\" : 52,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.5037593984962405,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 594,\n",
      "    \"commitOffsets\" : 26,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 8,\n",
      "    \"triggerExecution\" : 665,\n",
      "    \"walCommit\" : 36\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3283\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3284\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3284\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.5037593984962405,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 52\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|B013|R081|02-00-00|161/YANKEE STAD|05/05/2025|11:22:12|REGULAR|9          |6         |2025-05-05 11:22:14.014|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:22:14 INFO NetworkClient: [AdminClient clientId=adminclient-1] Node -1 disconnected.\n",
      "25/05/05 11:22:14 INFO NetworkClient: [AdminClient clientId=adminclient-2] Node -1 disconnected.\n",
      "25/05/05 11:22:14 INFO NetworkClient: [AdminClient clientId=adminclient-3] Node -1 disconnected.\n",
      "25/05/05 11:22:18 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/53 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.53.b8e47a03-df09-426e-a878-77d2356a3b56.tmp\n",
      "25/05/05 11:22:18 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/53 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.53.8c1933f2-1eaa-4ab8-8802-f568bbaa6bb2.tmp\n",
      "25/05/05 11:22:18 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/53 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.53.1d452c2e-6dcb-4fa7-915e-7fd331d2db00.tmp\n",
      "25/05/05 11:22:18 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.53.b8e47a03-df09-426e-a878-77d2356a3b56.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/53\n",
      "25/05/05 11:22:18 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.53.8c1933f2-1eaa-4ab8-8802-f568bbaa6bb2.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/53\n",
      "25/05/05 11:22:18 INFO MicroBatchExecution: Committed offsets for batch 53. Metadata OffsetSeqMetadata(0,1746458538511,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:22:18 INFO MicroBatchExecution: Committed offsets for batch 53. Metadata OffsetSeqMetadata(0,1746458538511,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:22:18 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.53.1d452c2e-6dcb-4fa7-915e-7fd331d2db00.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/53\n",
      "25/05/05 11:22:18 INFO MicroBatchExecution: Committed offsets for batch 53. Metadata OffsetSeqMetadata(0,1746458538511,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:22:18 INFO IncrementalExecution: Current batch timestamp = 1746458538511\n",
      "25/05/05 11:22:18 INFO IncrementalExecution: Current batch timestamp = 1746458538511\n",
      "25/05/05 11:22:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:18 INFO IncrementalExecution: Current batch timestamp = 1746458538511\n",
      "25/05/05 11:22:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:18 INFO IncrementalExecution: Current batch timestamp = 1746458538511\n",
      "25/05/05 11:22:18 INFO IncrementalExecution: Current batch timestamp = 1746458538511\n",
      "25/05/05 11:22:18 INFO IncrementalExecution: Current batch timestamp = 1746458538511\n",
      "25/05/05 11:22:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:18 INFO IncrementalExecution: Current batch timestamp = 1746458538511\n",
      "25/05/05 11:22:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:18 INFO IncrementalExecution: Current batch timestamp = 1746458538511\n",
      "25/05/05 11:22:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:18 INFO CodeGenerator: Code generated in 5.100459 ms\n",
      "25/05/05 11:22:18 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 53, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4b5b7af4]. The input RDD has 1 partitions.\n",
      "25/05/05 11:22:18 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:18 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 53, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:22:18 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:18 INFO DAGScheduler: Got job 159 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:22:18 INFO DAGScheduler: Final stage: ResultStage 158 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:22:18 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:22:18 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:22:18 INFO DAGScheduler: Submitting ResultStage 158 (MapPartitionsRDD[857] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:22:18 INFO MemoryStore: Block broadcast_211 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:22:18 INFO MemoryStore: Block broadcast_211_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.1 MiB)\n",
      "25/05/05 11:22:18 INFO BlockManagerInfo: Added broadcast_211_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:18 INFO SparkContext: Created broadcast 211 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:22:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 158 (MapPartitionsRDD[857] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:22:18 INFO TaskSchedulerImpl: Adding task set 158.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:22:18 INFO DAGScheduler: Got job 160 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:22:18 INFO DAGScheduler: Final stage: ResultStage 159 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:22:18 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:22:18 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:22:18 INFO DAGScheduler: Submitting ResultStage 159 (MapPartitionsRDD[859] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:22:18 INFO TaskSetManager: Starting task 0.0 in stage 158.0 (TID 158) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:22:18 INFO MemoryStore: Block broadcast_212 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:22:18 INFO Executor: Running task 0.0 in stage 158.0 (TID 158)\n",
      "25/05/05 11:22:18 INFO MemoryStore: Block broadcast_212_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:22:18 INFO BlockManagerInfo: Added broadcast_212_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:18 INFO SparkContext: Created broadcast 212 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:22:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 159 (MapPartitionsRDD[859] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:22:18 INFO TaskSchedulerImpl: Adding task set 159.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:22:18 INFO TaskSetManager: Starting task 0.0 in stage 159.0 (TID 159) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:22:18 INFO Executor: Running task 0.0 in stage 159.0 (TID 159)\n",
      "25/05/05 11:22:18 INFO CodeGenerator: Code generated in 4.341709 ms\n",
      "25/05/05 11:22:18 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3284 untilOffset=3285, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=53 taskId=158 partitionId=0\n",
      "25/05/05 11:22:18 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3284 untilOffset=3285, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=53 taskId=159 partitionId=0\n",
      "25/05/05 11:22:18 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3284 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:18 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3284 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:18 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:18 INFO DAGScheduler: Got job 161 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:22:18 INFO DAGScheduler: Final stage: ResultStage 160 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:22:18 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:22:18 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:22:18 INFO DAGScheduler: Submitting ResultStage 160 (MapPartitionsRDD[864] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:22:18 INFO MemoryStore: Block broadcast_213 stored as values in memory (estimated size 47.1 KiB, free 366.0 MiB)\n",
      "25/05/05 11:22:18 INFO MemoryStore: Block broadcast_213_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.0 MiB)\n",
      "25/05/05 11:22:18 INFO BlockManagerInfo: Added broadcast_213_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:18 INFO SparkContext: Created broadcast 213 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:22:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 160 (MapPartitionsRDD[864] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:22:18 INFO TaskSchedulerImpl: Adding task set 160.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:22:18 INFO TaskSetManager: Starting task 0.0 in stage 160.0 (TID 160) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:22:18 INFO Executor: Running task 0.0 in stage 160.0 (TID 160)\n",
      "25/05/05 11:22:18 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3284 untilOffset=3285, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=53 taskId=160 partitionId=0\n",
      "25/05/05 11:22:18 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3284 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:19 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:19 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:19 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:19 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:19 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3285, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:19 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3285, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:19 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:22:19 INFO DataWritingSparkTask: Committed partition 0 (task 159, attempt 0, stage 159.0)\n",
      "25/05/05 11:22:19 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 508447625 nanos, during time span of 508854917 nanos.\n",
      "25/05/05 11:22:19 INFO Executor: Finished task 0.0 in stage 159.0 (TID 159). 2145 bytes result sent to driver\n",
      "25/05/05 11:22:19 INFO TaskSetManager: Finished task 0.0 in stage 159.0 (TID 159) in 519 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:22:19 INFO TaskSchedulerImpl: Removed TaskSet 159.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:22:19 INFO DAGScheduler: ResultStage 159 (start at NativeMethodAccessorImpl.java:0) finished in 0.521 s\n",
      "25/05/05 11:22:19 INFO DAGScheduler: Job 160 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:22:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 159: Stage finished\n",
      "25/05/05 11:22:19 INFO DAGScheduler: Job 160 finished: start at NativeMethodAccessorImpl.java:0, took 0.524521 s\n",
      "25/05/05 11:22:19 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 53, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:22:19 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:22:19 INFO DataWritingSparkTask: Committed partition 0 (task 158, attempt 0, stage 158.0)\n",
      "25/05/05 11:22:19 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 507617042 nanos, during time span of 509678042 nanos.\n",
      "25/05/05 11:22:19 INFO Executor: Finished task 0.0 in stage 158.0 (TID 158). 3516 bytes result sent to driver\n",
      "25/05/05 11:22:19 INFO TaskSetManager: Finished task 0.0 in stage 158.0 (TID 158) in 522 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:22:19 INFO TaskSchedulerImpl: Removed TaskSet 158.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:22:19 INFO DAGScheduler: ResultStage 158 (start at NativeMethodAccessorImpl.java:0) finished in 0.525 s\n",
      "25/05/05 11:22:19 INFO DAGScheduler: Job 159 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:22:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 158: Stage finished\n",
      "25/05/05 11:22:19 INFO DAGScheduler: Job 159 finished: start at NativeMethodAccessorImpl.java:0, took 0.526705 s\n",
      "25/05/05 11:22:19 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 53, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4b5b7af4] is committing.\n",
      "25/05/05 11:22:19 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 53, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4b5b7af4] committed.\n",
      "25/05/05 11:22:19 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 53, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:22:19 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/53 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.53.45d1e358-3044-451c-b053-46d7d32b4c1b.tmp\n",
      "25/05/05 11:22:19 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/53 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.53.51382a68-bc48-493c-8d58-84494b06b4ad.tmp\n",
      "25/05/05 11:22:19 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.53.45d1e358-3044-451c-b053-46d7d32b4c1b.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/53\n",
      "25/05/05 11:22:19 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:22:18.510Z\",\n",
      "  \"batchId\" : 53,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 62.5,\n",
      "  \"processedRowsPerSecond\" : 1.6025641025641026,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 545,\n",
      "    \"commitOffsets\" : 40,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 624,\n",
      "    \"walCommit\" : 33\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3284\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3285\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3285\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 62.5,\n",
      "    \"processedRowsPerSecond\" : 1.6025641025641026,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:22:19 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.53.51382a68-bc48-493c-8d58-84494b06b4ad.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/53\n",
      "25/05/05 11:22:19 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:22:18.509Z\",\n",
      "  \"batchId\" : 53,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.5873015873015872,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 554,\n",
      "    \"commitOffsets\" : 38,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 630,\n",
      "    \"walCommit\" : 31\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3284\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3285\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3285\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.5873015873015872,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:22:19 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:19 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:19 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3285, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:19 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:22:19 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:22:19 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:22:19 INFO connection: Opened connection [connectionId{localValue:105, serverValue:4475}] to localhost:27017\n",
      "25/05/05 11:22:19 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=344041}\n",
      "25/05/05 11:22:19 INFO connection: Opened connection [connectionId{localValue:106, serverValue:4476}] to localhost:27017\n",
      "25/05/05 11:22:19 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:22:19 INFO connection: Closed connection [connectionId{localValue:106, serverValue:4476}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:22:19 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 503478542 nanos, during time span of 512674916 nanos.\n",
      "25/05/05 11:22:19 INFO Executor: Finished task 0.0 in stage 160.0 (TID 160). 1645 bytes result sent to driver\n",
      "25/05/05 11:22:19 INFO TaskSetManager: Finished task 0.0 in stage 160.0 (TID 160) in 519 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:22:19 INFO TaskSchedulerImpl: Removed TaskSet 160.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:22:19 INFO DAGScheduler: ResultStage 160 (start at NativeMethodAccessorImpl.java:0) finished in 0.575 s\n",
      "25/05/05 11:22:19 INFO DAGScheduler: Job 161 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:22:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 160: Stage finished\n",
      "25/05/05 11:22:19 INFO DAGScheduler: Job 161 finished: start at NativeMethodAccessorImpl.java:0, took 0.575830 s\n",
      "25/05/05 11:22:19 INFO MemoryStore: Block broadcast_214 stored as values in memory (estimated size 248.0 B, free 366.0 MiB)\n",
      "25/05/05 11:22:19 INFO MemoryStore: Block broadcast_214_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.0 MiB)\n",
      "25/05/05 11:22:19 INFO BlockManagerInfo: Added broadcast_214_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:22:19 INFO SparkContext: Created broadcast 214 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:19 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/53 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.53.e3974c86-1af9-44ff-8cfb-8bad3d334aed.tmp\n",
      "25/05/05 11:22:19 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.53.e3974c86-1af9-44ff-8cfb-8bad3d334aed.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/53\n",
      "25/05/05 11:22:19 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:22:18.510Z\",\n",
      "  \"batchId\" : 53,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.443001443001443,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 623,\n",
      "    \"commitOffsets\" : 33,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 693,\n",
      "    \"walCommit\" : 31\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3284\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3285\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3285\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.443001443001443,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 53\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R022|R033|01-00-00|42 ST-PORT AUTH|05/05/2025|11:22:17|REGULAR|10         |4         |2025-05-05 11:22:18.511|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:22:25 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/54 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.54.af55b1f4-a0cb-4b03-9f66-4334c1478012.tmp\n",
      "25/05/05 11:22:25 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/54 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.54.099b09d1-625f-486b-8b2a-e40bce431b43.tmp\n",
      "25/05/05 11:22:25 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/54 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.54.c3402018-b623-4461-832a-e3b72b1d8c06.tmp\n",
      "25/05/05 11:22:25 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.54.af55b1f4-a0cb-4b03-9f66-4334c1478012.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/54\n",
      "25/05/05 11:22:25 INFO MicroBatchExecution: Committed offsets for batch 54. Metadata OffsetSeqMetadata(0,1746458544980,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:22:25 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.54.099b09d1-625f-486b-8b2a-e40bce431b43.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/54\n",
      "25/05/05 11:22:25 INFO MicroBatchExecution: Committed offsets for batch 54. Metadata OffsetSeqMetadata(0,1746458544998,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:22:25 INFO IncrementalExecution: Current batch timestamp = 1746458544980\n",
      "25/05/05 11:22:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:25 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.54.c3402018-b623-4461-832a-e3b72b1d8c06.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/54\n",
      "25/05/05 11:22:25 INFO MicroBatchExecution: Committed offsets for batch 54. Metadata OffsetSeqMetadata(0,1746458544998,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:22:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:25 INFO IncrementalExecution: Current batch timestamp = 1746458544998\n",
      "25/05/05 11:22:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:25 INFO IncrementalExecution: Current batch timestamp = 1746458544980\n",
      "25/05/05 11:22:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:25 INFO IncrementalExecution: Current batch timestamp = 1746458544998\n",
      "25/05/05 11:22:25 INFO IncrementalExecution: Current batch timestamp = 1746458544998\n",
      "25/05/05 11:22:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:25 INFO IncrementalExecution: Current batch timestamp = 1746458544980\n",
      "25/05/05 11:22:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:25 INFO IncrementalExecution: Current batch timestamp = 1746458544998\n",
      "25/05/05 11:22:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:25 INFO IncrementalExecution: Current batch timestamp = 1746458544998\n",
      "25/05/05 11:22:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:25 INFO CodeGenerator: Code generated in 8.354084 ms\n",
      "25/05/05 11:22:25 INFO CodeGenerator: Code generated in 8.007292 ms\n",
      "25/05/05 11:22:25 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 54, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4b0a95e6]. The input RDD has 1 partitions.\n",
      "25/05/05 11:22:25 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:25 INFO DAGScheduler: Got job 162 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:22:25 INFO DAGScheduler: Final stage: ResultStage 161 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:22:25 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:22:25 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:22:25 INFO DAGScheduler: Submitting ResultStage 161 (MapPartitionsRDD[869] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:22:25 INFO MemoryStore: Block broadcast_215 stored as values in memory (estimated size 16.2 KiB, free 366.0 MiB)\n",
      "25/05/05 11:22:25 INFO MemoryStore: Block broadcast_215_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)\n",
      "25/05/05 11:22:25 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 54, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:22:25 INFO BlockManagerInfo: Added broadcast_215_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:25 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:25 INFO SparkContext: Created broadcast 215 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:22:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 161 (MapPartitionsRDD[869] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:22:25 INFO TaskSchedulerImpl: Adding task set 161.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:22:25 INFO TaskSetManager: Starting task 0.0 in stage 161.0 (TID 161) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:22:25 INFO DAGScheduler: Got job 163 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:22:25 INFO DAGScheduler: Final stage: ResultStage 162 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:22:25 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:22:25 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:22:25 INFO DAGScheduler: Submitting ResultStage 162 (MapPartitionsRDD[875] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:22:25 INFO Executor: Running task 0.0 in stage 161.0 (TID 161)\n",
      "25/05/05 11:22:25 INFO MemoryStore: Block broadcast_216 stored as values in memory (estimated size 15.2 KiB, free 365.9 MiB)\n",
      "25/05/05 11:22:25 INFO MemoryStore: Block broadcast_216_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 365.9 MiB)\n",
      "25/05/05 11:22:25 INFO BlockManagerInfo: Added broadcast_216_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:25 INFO SparkContext: Created broadcast 216 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:22:25 INFO BlockManagerInfo: Removed broadcast_214_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:22:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 162 (MapPartitionsRDD[875] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:22:25 INFO TaskSchedulerImpl: Adding task set 162.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:22:25 INFO TaskSetManager: Starting task 0.0 in stage 162.0 (TID 162) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:22:25 INFO Executor: Running task 0.0 in stage 162.0 (TID 162)\n",
      "25/05/05 11:22:25 INFO BlockManagerInfo: Removed broadcast_210_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:22:25 INFO CodeGenerator: Code generated in 4.63825 ms\n",
      "25/05/05 11:22:25 INFO BlockManagerInfo: Removed broadcast_209_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:25 INFO CodeGenerator: Code generated in 4.674833 ms\n",
      "25/05/05 11:22:25 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3285 untilOffset=3286, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=54 taskId=161 partitionId=0\n",
      "25/05/05 11:22:25 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3285 untilOffset=3286, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=54 taskId=162 partitionId=0\n",
      "25/05/05 11:22:25 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3285 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:25 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3285 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:25 INFO BlockManagerInfo: Removed broadcast_211_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:25 INFO BlockManagerInfo: Removed broadcast_207_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:25 INFO BlockManagerInfo: Removed broadcast_208_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:25 INFO BlockManagerInfo: Removed broadcast_213_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:22:25 INFO BlockManagerInfo: Removed broadcast_212_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:22:25 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:25 INFO DAGScheduler: Got job 164 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:22:25 INFO DAGScheduler: Final stage: ResultStage 163 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:22:25 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:22:25 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:22:25 INFO DAGScheduler: Submitting ResultStage 163 (MapPartitionsRDD[880] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:22:25 INFO MemoryStore: Block broadcast_217 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:22:25 INFO MemoryStore: Block broadcast_217_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.1 MiB)\n",
      "25/05/05 11:22:25 INFO BlockManagerInfo: Added broadcast_217_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:25 INFO SparkContext: Created broadcast 217 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:22:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 163 (MapPartitionsRDD[880] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:22:25 INFO TaskSchedulerImpl: Adding task set 163.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:22:25 INFO TaskSetManager: Starting task 0.0 in stage 163.0 (TID 163) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:22:25 INFO Executor: Running task 0.0 in stage 163.0 (TID 163)\n",
      "25/05/05 11:22:25 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3285 untilOffset=3286, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=54 taskId=163 partitionId=0\n",
      "25/05/05 11:22:25 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3285 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3286, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3286, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:25 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:22:25 INFO DataWritingSparkTask: Committed partition 0 (task 162, attempt 0, stage 162.0)\n",
      "25/05/05 11:22:25 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 513189167 nanos, during time span of 513675375 nanos.\n",
      "25/05/05 11:22:25 INFO Executor: Finished task 0.0 in stage 162.0 (TID 162). 2145 bytes result sent to driver\n",
      "25/05/05 11:22:25 INFO TaskSetManager: Finished task 0.0 in stage 162.0 (TID 162) in 523 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:22:25 INFO TaskSchedulerImpl: Removed TaskSet 162.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:22:25 INFO DAGScheduler: ResultStage 162 (start at NativeMethodAccessorImpl.java:0) finished in 0.534 s\n",
      "25/05/05 11:22:25 INFO DAGScheduler: Job 163 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:22:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 162: Stage finished\n",
      "25/05/05 11:22:25 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:22:25 INFO DataWritingSparkTask: Committed partition 0 (task 161, attempt 0, stage 161.0)\n",
      "25/05/05 11:22:25 INFO DAGScheduler: Job 163 finished: start at NativeMethodAccessorImpl.java:0, took 0.535961 s\n",
      "25/05/05 11:22:25 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 510837583 nanos, during time span of 512657667 nanos.\n",
      "25/05/05 11:22:25 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 54, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:22:25 INFO Executor: Finished task 0.0 in stage 161.0 (TID 161). 3602 bytes result sent to driver\n",
      "25/05/05 11:22:25 INFO TaskSetManager: Finished task 0.0 in stage 161.0 (TID 161) in 536 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:22:25 INFO TaskSchedulerImpl: Removed TaskSet 161.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:22:25 INFO DAGScheduler: ResultStage 161 (start at NativeMethodAccessorImpl.java:0) finished in 0.539 s\n",
      "25/05/05 11:22:25 INFO DAGScheduler: Job 162 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:22:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 161: Stage finished\n",
      "25/05/05 11:22:25 INFO DAGScheduler: Job 162 finished: start at NativeMethodAccessorImpl.java:0, took 0.540239 s\n",
      "25/05/05 11:22:25 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 54, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4b0a95e6] is committing.\n",
      "25/05/05 11:22:25 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 54, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4b0a95e6] committed.\n",
      "25/05/05 11:22:25 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/54 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.54.2f00b194-8c2a-4c0d-8078-a48856424fb5.tmp\n",
      "25/05/05 11:22:25 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 54, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:22:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3286, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:25 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:22:25 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:22:25 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:22:25 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/54 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.54.8cefc83f-8a91-482e-a2d4-128e6f69a52f.tmp\n",
      "25/05/05 11:22:25 INFO connection: Opened connection [connectionId{localValue:107, serverValue:4477}] to localhost:27017\n",
      "25/05/05 11:22:25 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=677417}\n",
      "25/05/05 11:22:25 INFO connection: Opened connection [connectionId{localValue:108, serverValue:4478}] to localhost:27017\n",
      "25/05/05 11:22:25 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:22:25 INFO connection: Closed connection [connectionId{localValue:108, serverValue:4478}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:22:25 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 506469000 nanos, during time span of 516356291 nanos.\n",
      "25/05/05 11:22:25 INFO Executor: Finished task 0.0 in stage 163.0 (TID 163). 1645 bytes result sent to driver\n",
      "25/05/05 11:22:25 INFO TaskSetManager: Finished task 0.0 in stage 163.0 (TID 163) in 524 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:22:25 INFO TaskSchedulerImpl: Removed TaskSet 163.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:22:25 INFO DAGScheduler: ResultStage 163 (start at NativeMethodAccessorImpl.java:0) finished in 0.529 s\n",
      "25/05/05 11:22:25 INFO DAGScheduler: Job 164 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:22:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 163: Stage finished\n",
      "25/05/05 11:22:25 INFO DAGScheduler: Job 164 finished: start at NativeMethodAccessorImpl.java:0, took 0.530119 s\n",
      "25/05/05 11:22:25 INFO MemoryStore: Block broadcast_218 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:22:25 INFO MemoryStore: Block broadcast_218_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:22:25 INFO BlockManagerInfo: Added broadcast_218_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:22:25 INFO SparkContext: Created broadcast 218 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:25 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/54 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.54.22c578f0-097a-4f52-842b-195a957b8c95.tmp\n",
      "25/05/05 11:22:25 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.54.2f00b194-8c2a-4c0d-8078-a48856424fb5.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/54\n",
      "25/05/05 11:22:25 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:22:24.977Z\",\n",
      "  \"batchId\" : 54,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.5037593984962405,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 564,\n",
      "    \"commitOffsets\" : 40,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 665,\n",
      "    \"walCommit\" : 53\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3285\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3286\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3286\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.5037593984962405,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:22:25 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.54.8cefc83f-8a91-482e-a2d4-128e6f69a52f.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/54\n",
      "25/05/05 11:22:25 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:22:24.988Z\",\n",
      "  \"batchId\" : 54,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.510574018126888,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 570,\n",
      "    \"commitOffsets\" : 40,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 10,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 662,\n",
      "    \"walCommit\" : 38\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3285\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3286\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3286\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.510574018126888,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:22:25 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.54.22c578f0-097a-4f52-842b-195a957b8c95.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/54\n",
      "25/05/05 11:22:25 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:22:24.992Z\",\n",
      "  \"batchId\" : 54,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 52.631578947368425,\n",
      "  \"processedRowsPerSecond\" : 1.4903129657228018,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 583,\n",
      "    \"commitOffsets\" : 33,\n",
      "    \"getBatch\" : 1,\n",
      "    \"latestOffset\" : 6,\n",
      "    \"queryPlanning\" : 8,\n",
      "    \"triggerExecution\" : 671,\n",
      "    \"walCommit\" : 39\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3285\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3286\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3286\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 52.631578947368425,\n",
      "    \"processedRowsPerSecond\" : 1.4903129657228018,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 54\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A001|R079|01-00-00|WORLD TRADE CTR|05/05/2025|11:22:23|REGULAR|14         |9         |2025-05-05 11:22:24.998|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:22:30 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/55 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.55.ccad5abb-bd3c-4411-882d-dedf69032ac3.tmp\n",
      "25/05/05 11:22:30 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/55 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.55.5bec0ccb-5cce-4915-81bb-e7a1d8e4cace.tmp\n",
      "25/05/05 11:22:30 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/55 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.55.447cdd43-1bfb-4928-b67c-5eefd4a48b29.tmp\n",
      "25/05/05 11:22:30 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.55.ccad5abb-bd3c-4411-882d-dedf69032ac3.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/55\n",
      "25/05/05 11:22:30 INFO MicroBatchExecution: Committed offsets for batch 55. Metadata OffsetSeqMetadata(0,1746458550483,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:22:30 INFO IncrementalExecution: Current batch timestamp = 1746458550483\n",
      "25/05/05 11:22:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:30 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.55.5bec0ccb-5cce-4915-81bb-e7a1d8e4cace.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/55\n",
      "25/05/05 11:22:30 INFO MicroBatchExecution: Committed offsets for batch 55. Metadata OffsetSeqMetadata(0,1746458550499,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:22:30 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.55.447cdd43-1bfb-4928-b67c-5eefd4a48b29.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/55\n",
      "25/05/05 11:22:30 INFO MicroBatchExecution: Committed offsets for batch 55. Metadata OffsetSeqMetadata(0,1746458550499,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:22:30 INFO IncrementalExecution: Current batch timestamp = 1746458550483\n",
      "25/05/05 11:22:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:30 INFO IncrementalExecution: Current batch timestamp = 1746458550499\n",
      "25/05/05 11:22:30 INFO IncrementalExecution: Current batch timestamp = 1746458550499\n",
      "25/05/05 11:22:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:30 INFO IncrementalExecution: Current batch timestamp = 1746458550499\n",
      "25/05/05 11:22:30 INFO IncrementalExecution: Current batch timestamp = 1746458550483\n",
      "25/05/05 11:22:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:30 INFO IncrementalExecution: Current batch timestamp = 1746458550499\n",
      "25/05/05 11:22:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:30 INFO IncrementalExecution: Current batch timestamp = 1746458550499\n",
      "25/05/05 11:22:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:30 INFO CodeGenerator: Code generated in 7.025083 ms\n",
      "25/05/05 11:22:30 INFO CodeGenerator: Code generated in 5.831167 ms\n",
      "25/05/05 11:22:30 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 55, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:22:30 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:30 INFO DAGScheduler: Got job 165 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:22:30 INFO DAGScheduler: Final stage: ResultStage 164 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:22:30 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:22:30 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 55, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@761e14dc]. The input RDD has 1 partitions.\n",
      "25/05/05 11:22:30 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:22:30 INFO DAGScheduler: Submitting ResultStage 164 (MapPartitionsRDD[886] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:22:30 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:30 INFO MemoryStore: Block broadcast_219 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:22:30 INFO MemoryStore: Block broadcast_219_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:22:30 INFO BlockManagerInfo: Added broadcast_219_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:30 INFO SparkContext: Created broadcast 219 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:22:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 164 (MapPartitionsRDD[886] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:22:30 INFO TaskSchedulerImpl: Adding task set 164.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:22:30 INFO DAGScheduler: Got job 166 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:22:30 INFO DAGScheduler: Final stage: ResultStage 165 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:22:30 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:22:30 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:22:30 INFO DAGScheduler: Submitting ResultStage 165 (MapPartitionsRDD[891] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:22:30 INFO TaskSetManager: Starting task 0.0 in stage 164.0 (TID 164) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:22:30 INFO MemoryStore: Block broadcast_220 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:22:30 INFO Executor: Running task 0.0 in stage 164.0 (TID 164)\n",
      "25/05/05 11:22:30 INFO MemoryStore: Block broadcast_220_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)\n",
      "25/05/05 11:22:30 INFO BlockManagerInfo: Added broadcast_220_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:30 INFO SparkContext: Created broadcast 220 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:22:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 165 (MapPartitionsRDD[891] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:22:30 INFO TaskSchedulerImpl: Adding task set 165.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:22:30 INFO TaskSetManager: Starting task 0.0 in stage 165.0 (TID 165) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:22:30 INFO Executor: Running task 0.0 in stage 165.0 (TID 165)\n",
      "25/05/05 11:22:30 INFO CodeGenerator: Code generated in 12.331458 ms\n",
      "25/05/05 11:22:30 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3286 untilOffset=3287, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=55 taskId=164 partitionId=0\n",
      "25/05/05 11:22:30 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3286 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:30 INFO CodeGenerator: Code generated in 13.8685 ms\n",
      "25/05/05 11:22:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:30 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3286 untilOffset=3287, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=55 taskId=165 partitionId=0\n",
      "25/05/05 11:22:30 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3286 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:30 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:30 INFO DAGScheduler: Got job 167 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:22:30 INFO DAGScheduler: Final stage: ResultStage 166 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:22:30 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:22:30 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:22:30 INFO DAGScheduler: Submitting ResultStage 166 (MapPartitionsRDD[896] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:22:30 INFO MemoryStore: Block broadcast_221 stored as values in memory (estimated size 47.1 KiB, free 366.0 MiB)\n",
      "25/05/05 11:22:30 INFO MemoryStore: Block broadcast_221_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.0 MiB)\n",
      "25/05/05 11:22:30 INFO BlockManagerInfo: Added broadcast_221_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:30 INFO SparkContext: Created broadcast 221 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:22:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 166 (MapPartitionsRDD[896] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:22:30 INFO TaskSchedulerImpl: Adding task set 166.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:22:30 INFO TaskSetManager: Starting task 0.0 in stage 166.0 (TID 166) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:22:30 INFO Executor: Running task 0.0 in stage 166.0 (TID 166)\n",
      "25/05/05 11:22:30 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3286 untilOffset=3287, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=55 taskId=166 partitionId=0\n",
      "25/05/05 11:22:30 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3286 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3287, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:31 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:22:31 INFO DataWritingSparkTask: Committed partition 0 (task 164, attempt 0, stage 164.0)\n",
      "25/05/05 11:22:31 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 503122042 nanos, during time span of 503648625 nanos.\n",
      "25/05/05 11:22:31 INFO Executor: Finished task 0.0 in stage 164.0 (TID 164). 2137 bytes result sent to driver\n",
      "25/05/05 11:22:31 INFO TaskSetManager: Finished task 0.0 in stage 164.0 (TID 164) in 523 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:22:31 INFO TaskSchedulerImpl: Removed TaskSet 164.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:22:31 INFO DAGScheduler: ResultStage 164 (start at NativeMethodAccessorImpl.java:0) finished in 0.525 s\n",
      "25/05/05 11:22:31 INFO DAGScheduler: Job 165 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:22:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 164: Stage finished\n",
      "25/05/05 11:22:31 INFO DAGScheduler: Job 165 finished: start at NativeMethodAccessorImpl.java:0, took 0.525782 s\n",
      "25/05/05 11:22:31 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 55, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:22:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3287, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:31 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:22:31 INFO DataWritingSparkTask: Committed partition 0 (task 165, attempt 0, stage 165.0)\n",
      "25/05/05 11:22:31 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 503193833 nanos, during time span of 504843416 nanos.\n",
      "25/05/05 11:22:31 INFO Executor: Finished task 0.0 in stage 165.0 (TID 165). 3507 bytes result sent to driver\n",
      "25/05/05 11:22:31 INFO TaskSetManager: Finished task 0.0 in stage 165.0 (TID 165) in 525 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:22:31 INFO TaskSchedulerImpl: Removed TaskSet 165.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:22:31 INFO DAGScheduler: ResultStage 165 (start at NativeMethodAccessorImpl.java:0) finished in 0.529 s\n",
      "25/05/05 11:22:31 INFO DAGScheduler: Job 166 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:22:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 165: Stage finished\n",
      "25/05/05 11:22:31 INFO DAGScheduler: Job 166 finished: start at NativeMethodAccessorImpl.java:0, took 0.530422 s\n",
      "25/05/05 11:22:31 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 55, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@761e14dc] is committing.\n",
      "25/05/05 11:22:31 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 55, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@761e14dc] committed.\n",
      "25/05/05 11:22:31 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 55, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:22:31 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/55 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.55.250ac7bb-3caa-4206-9a8e-d6a66a392dad.tmp\n",
      "25/05/05 11:22:31 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/55 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.55.c02695b8-b139-44c3-b7e2-4b6a23e06c19.tmp\n",
      "25/05/05 11:22:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3287, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:31 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:22:31 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:22:31 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:22:31 INFO connection: Opened connection [connectionId{localValue:109, serverValue:4479}] to localhost:27017\n",
      "25/05/05 11:22:31 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=829250}\n",
      "25/05/05 11:22:31 INFO connection: Opened connection [connectionId{localValue:110, serverValue:4480}] to localhost:27017\n",
      "25/05/05 11:22:31 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:22:31 INFO connection: Closed connection [connectionId{localValue:110, serverValue:4480}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:22:31 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 505349333 nanos, during time span of 511395667 nanos.\n",
      "25/05/05 11:22:31 INFO Executor: Finished task 0.0 in stage 166.0 (TID 166). 1645 bytes result sent to driver\n",
      "25/05/05 11:22:31 INFO TaskSetManager: Finished task 0.0 in stage 166.0 (TID 166) in 519 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:22:31 INFO TaskSchedulerImpl: Removed TaskSet 166.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:22:31 INFO DAGScheduler: ResultStage 166 (start at NativeMethodAccessorImpl.java:0) finished in 0.524 s\n",
      "25/05/05 11:22:31 INFO DAGScheduler: Job 167 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:22:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 166: Stage finished\n",
      "25/05/05 11:22:31 INFO DAGScheduler: Job 167 finished: start at NativeMethodAccessorImpl.java:0, took 0.525068 s\n",
      "25/05/05 11:22:31 INFO MemoryStore: Block broadcast_222 stored as values in memory (estimated size 248.0 B, free 366.0 MiB)\n",
      "25/05/05 11:22:31 INFO MemoryStore: Block broadcast_222_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.0 MiB)\n",
      "25/05/05 11:22:31 INFO BlockManagerInfo: Added broadcast_222_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:22:31 INFO SparkContext: Created broadcast 222 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:31 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/55 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.55.2681e1e6-9d16-4701-abab-bdd0e0dd134a.tmp\n",
      "25/05/05 11:22:31 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.55.250ac7bb-3caa-4206-9a8e-d6a66a392dad.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/55\n",
      "25/05/05 11:22:31 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:22:30.497Z\",\n",
      "  \"batchId\" : 55,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 47.61904761904761,\n",
      "  \"processedRowsPerSecond\" : 1.6103059581320451,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 546,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 621,\n",
      "    \"walCommit\" : 39\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3286\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3287\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3287\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 47.61904761904761,\n",
      "    \"processedRowsPerSecond\" : 1.6103059581320451,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:22:31 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.55.c02695b8-b139-44c3-b7e2-4b6a23e06c19.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/55\n",
      "25/05/05 11:22:31 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:22:30.480Z\",\n",
      "  \"batchId\" : 55,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.5649452269170578,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 553,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 639,\n",
      "    \"walCommit\" : 48\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3286\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3287\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3287\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.5649452269170578,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:22:31 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.55.2681e1e6-9d16-4701-abab-bdd0e0dd134a.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/55\n",
      "25/05/05 11:22:31 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:22:30.497Z\",\n",
      "  \"batchId\" : 55,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 50.0,\n",
      "  \"processedRowsPerSecond\" : 1.5625,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 569,\n",
      "    \"commitOffsets\" : 25,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 640,\n",
      "    \"walCommit\" : 39\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3286\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3287\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3287\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 50.0,\n",
      "    \"processedRowsPerSecond\" : 1.5625,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 55\n",
      "-------------------------------------------\n",
      "+----+----+--------+-------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION|DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+-------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A004|R033|00-00-00|125 ST |05/05/2025|11:22:29|REGULAR|10         |14        |2025-05-05 11:22:30.483|\n",
      "+----+----+--------+-------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:22:35 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/56 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.56.44fb6514-ebad-46d3-b8c2-c1548d25ee6e.tmp\n",
      "25/05/05 11:22:35 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/56 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.56.78e21b39-3262-4c05-bd31-87bae71257e9.tmp\n",
      "25/05/05 11:22:35 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/56 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.56.0f573657-42b3-4161-9775-a3543b2e718f.tmp\n",
      "25/05/05 11:22:35 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.56.44fb6514-ebad-46d3-b8c2-c1548d25ee6e.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/56\n",
      "25/05/05 11:22:35 INFO MicroBatchExecution: Committed offsets for batch 56. Metadata OffsetSeqMetadata(0,1746458555040,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:22:35 INFO IncrementalExecution: Current batch timestamp = 1746458555040\n",
      "25/05/05 11:22:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:35 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.56.0f573657-42b3-4161-9775-a3543b2e718f.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/56\n",
      "25/05/05 11:22:35 INFO MicroBatchExecution: Committed offsets for batch 56. Metadata OffsetSeqMetadata(0,1746458555056,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:22:35 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.56.78e21b39-3262-4c05-bd31-87bae71257e9.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/56\n",
      "25/05/05 11:22:35 INFO MicroBatchExecution: Committed offsets for batch 56. Metadata OffsetSeqMetadata(0,1746458555056,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:22:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:35 INFO IncrementalExecution: Current batch timestamp = 1746458555040\n",
      "25/05/05 11:22:35 INFO IncrementalExecution: Current batch timestamp = 1746458555056\n",
      "25/05/05 11:22:35 INFO IncrementalExecution: Current batch timestamp = 1746458555056\n",
      "25/05/05 11:22:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:35 INFO IncrementalExecution: Current batch timestamp = 1746458555056\n",
      "25/05/05 11:22:35 INFO IncrementalExecution: Current batch timestamp = 1746458555056\n",
      "25/05/05 11:22:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:35 INFO IncrementalExecution: Current batch timestamp = 1746458555040\n",
      "25/05/05 11:22:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:35 INFO IncrementalExecution: Current batch timestamp = 1746458555056\n",
      "25/05/05 11:22:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:35 INFO BlockManagerInfo: Removed broadcast_215_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:35 INFO CodeGenerator: Code generated in 16.015958 ms\n",
      "25/05/05 11:22:35 INFO CodeGenerator: Code generated in 14.949916 ms\n",
      "25/05/05 11:22:35 INFO BlockManagerInfo: Removed broadcast_221_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:35 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 56, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@1322c544]. The input RDD has 1 partitions.\n",
      "25/05/05 11:22:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:35 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 56, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:22:35 INFO DAGScheduler: Got job 168 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:22:35 INFO DAGScheduler: Final stage: ResultStage 167 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:22:35 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:22:35 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:22:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:35 INFO DAGScheduler: Submitting ResultStage 167 (MapPartitionsRDD[904] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:22:35 INFO BlockManagerInfo: Removed broadcast_220_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:35 INFO MemoryStore: Block broadcast_223 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:22:35 INFO MemoryStore: Block broadcast_223_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.1 MiB)\n",
      "25/05/05 11:22:35 INFO BlockManagerInfo: Added broadcast_223_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:35 INFO BlockManagerInfo: Removed broadcast_218_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:22:35 INFO SparkContext: Created broadcast 223 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:22:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 167 (MapPartitionsRDD[904] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:22:35 INFO TaskSchedulerImpl: Adding task set 167.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:22:35 INFO DAGScheduler: Got job 169 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:22:35 INFO DAGScheduler: Final stage: ResultStage 168 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:22:35 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:22:35 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:22:35 INFO DAGScheduler: Submitting ResultStage 168 (MapPartitionsRDD[907] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:22:35 INFO TaskSetManager: Starting task 0.0 in stage 167.0 (TID 167) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:22:35 INFO BlockManagerInfo: Removed broadcast_222_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:22:35 INFO Executor: Running task 0.0 in stage 167.0 (TID 167)\n",
      "25/05/05 11:22:35 INFO MemoryStore: Block broadcast_224 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:22:35 INFO BlockManagerInfo: Removed broadcast_219_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:35 INFO MemoryStore: Block broadcast_224_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:22:35 INFO BlockManagerInfo: Added broadcast_224_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:35 INFO SparkContext: Created broadcast 224 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:22:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 168 (MapPartitionsRDD[907] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:22:35 INFO TaskSchedulerImpl: Adding task set 168.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:22:35 INFO BlockManagerInfo: Removed broadcast_216_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:35 INFO TaskSetManager: Starting task 0.0 in stage 168.0 (TID 168) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:22:35 INFO Executor: Running task 0.0 in stage 168.0 (TID 168)\n",
      "25/05/05 11:22:35 INFO BlockManagerInfo: Removed broadcast_217_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:22:35 INFO CodeGenerator: Code generated in 4.121792 ms\n",
      "25/05/05 11:22:35 INFO CodeGenerator: Code generated in 2.855167 ms\n",
      "25/05/05 11:22:35 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3287 untilOffset=3288, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=56 taskId=168 partitionId=0\n",
      "25/05/05 11:22:35 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3287 untilOffset=3288, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=56 taskId=167 partitionId=0\n",
      "25/05/05 11:22:35 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3287 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:35 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3287 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:35 INFO DAGScheduler: Got job 170 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:22:35 INFO DAGScheduler: Final stage: ResultStage 169 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:22:35 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:22:35 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:22:35 INFO DAGScheduler: Submitting ResultStage 169 (MapPartitionsRDD[912] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:22:35 INFO MemoryStore: Block broadcast_225 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:22:35 INFO MemoryStore: Block broadcast_225_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.1 MiB)\n",
      "25/05/05 11:22:35 INFO BlockManagerInfo: Added broadcast_225_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:35 INFO SparkContext: Created broadcast 225 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:22:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 169 (MapPartitionsRDD[912] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:22:35 INFO TaskSchedulerImpl: Adding task set 169.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:22:35 INFO TaskSetManager: Starting task 0.0 in stage 169.0 (TID 169) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:22:35 INFO Executor: Running task 0.0 in stage 169.0 (TID 169)\n",
      "25/05/05 11:22:35 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3287 untilOffset=3288, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=56 taskId=169 partitionId=0\n",
      "25/05/05 11:22:35 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3287 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3288, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3288, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:35 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:22:35 INFO DataWritingSparkTask: Committed partition 0 (task 168, attempt 0, stage 168.0)\n",
      "25/05/05 11:22:35 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 506636875 nanos, during time span of 507115416 nanos.\n",
      "25/05/05 11:22:35 INFO Executor: Finished task 0.0 in stage 168.0 (TID 168). 2145 bytes result sent to driver\n",
      "25/05/05 11:22:35 INFO TaskSetManager: Finished task 0.0 in stage 168.0 (TID 168) in 515 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:22:35 INFO TaskSchedulerImpl: Removed TaskSet 168.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:22:35 INFO DAGScheduler: ResultStage 168 (start at NativeMethodAccessorImpl.java:0) finished in 0.516 s\n",
      "25/05/05 11:22:35 INFO DAGScheduler: Job 169 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:22:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 168: Stage finished\n",
      "25/05/05 11:22:35 INFO DAGScheduler: Job 169 finished: start at NativeMethodAccessorImpl.java:0, took 0.518936 s\n",
      "25/05/05 11:22:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 56, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:22:35 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:22:35 INFO DataWritingSparkTask: Committed partition 0 (task 167, attempt 0, stage 167.0)\n",
      "25/05/05 11:22:35 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 506131000 nanos, during time span of 507909333 nanos.\n",
      "25/05/05 11:22:35 INFO Executor: Finished task 0.0 in stage 167.0 (TID 167). 3510 bytes result sent to driver\n",
      "25/05/05 11:22:35 INFO TaskSetManager: Finished task 0.0 in stage 167.0 (TID 167) in 518 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:22:35 INFO TaskSchedulerImpl: Removed TaskSet 167.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:22:35 INFO DAGScheduler: ResultStage 167 (start at NativeMethodAccessorImpl.java:0) finished in 0.519 s\n",
      "25/05/05 11:22:35 INFO DAGScheduler: Job 168 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:22:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 167: Stage finished\n",
      "25/05/05 11:22:35 INFO DAGScheduler: Job 168 finished: start at NativeMethodAccessorImpl.java:0, took 0.520689 s\n",
      "25/05/05 11:22:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 56, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@1322c544] is committing.\n",
      "25/05/05 11:22:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 56, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@1322c544] committed.\n",
      "25/05/05 11:22:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 56, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:22:35 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/56 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.56.d0e67754-3a92-4c9c-8c89-1edf5baf69e8.tmp\n",
      "25/05/05 11:22:35 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/56 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.56.7a38348c-cee6-4b65-8522-b2687fc6c15e.tmp\n",
      "25/05/05 11:22:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3288, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:35 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:22:35 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:22:35 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:22:35 INFO connection: Opened connection [connectionId{localValue:111, serverValue:4481}] to localhost:27017\n",
      "25/05/05 11:22:35 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=258583}\n",
      "25/05/05 11:22:35 INFO connection: Opened connection [connectionId{localValue:112, serverValue:4482}] to localhost:27017\n",
      "25/05/05 11:22:35 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:22:35 INFO connection: Closed connection [connectionId{localValue:112, serverValue:4482}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:22:35 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 509081750 nanos, during time span of 513903541 nanos.\n",
      "25/05/05 11:22:35 INFO Executor: Finished task 0.0 in stage 169.0 (TID 169). 1645 bytes result sent to driver\n",
      "25/05/05 11:22:35 INFO TaskSetManager: Finished task 0.0 in stage 169.0 (TID 169) in 519 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:22:35 INFO TaskSchedulerImpl: Removed TaskSet 169.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:22:35 INFO DAGScheduler: ResultStage 169 (start at NativeMethodAccessorImpl.java:0) finished in 0.524 s\n",
      "25/05/05 11:22:35 INFO DAGScheduler: Job 170 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:22:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 169: Stage finished\n",
      "25/05/05 11:22:35 INFO DAGScheduler: Job 170 finished: start at NativeMethodAccessorImpl.java:0, took 0.525126 s\n",
      "25/05/05 11:22:35 INFO MemoryStore: Block broadcast_226 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:22:35 INFO MemoryStore: Block broadcast_226_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:22:35 INFO BlockManagerInfo: Added broadcast_226_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:22:35 INFO SparkContext: Created broadcast 226 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:35 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.56.d0e67754-3a92-4c9c-8c89-1edf5baf69e8.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/56\n",
      "25/05/05 11:22:35 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:22:35.053Z\",\n",
      "  \"batchId\" : 56,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.6025641025641026,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 546,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 624,\n",
      "    \"walCommit\" : 43\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3287\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3288\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3288\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.6025641025641026,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:22:35 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/56 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.56.6eec7959-0f6a-4e63-bffd-27e736aedc13.tmp\n",
      "25/05/05 11:22:35 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.56.7a38348c-cee6-4b65-8522-b2687fc6c15e.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/56\n",
      "25/05/05 11:22:35 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:22:35.039Z\",\n",
      "  \"batchId\" : 56,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.5527950310559007,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 554,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 1,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 644,\n",
      "    \"walCommit\" : 54\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3287\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3288\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3288\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.5527950310559007,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:22:35 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.56.6eec7959-0f6a-4e63-bffd-27e736aedc13.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/56\n",
      "25/05/05 11:22:35 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:22:35.053Z\",\n",
      "  \"batchId\" : 56,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.5455950540958268,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 571,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 647,\n",
      "    \"walCommit\" : 43\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3287\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3288\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3288\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.5455950540958268,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 56\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------+----------+--------+-------+-----------+----------+----------------------+\n",
      "|C/A |UNIT|SCP     |STATION  |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time           |\n",
      "+----+----+--------+---------+----------+--------+-------+-----------+----------+----------------------+\n",
      "|A002|R170|00-00-00|FULTON ST|05/05/2025|11:22:33|REGULAR|8          |5         |2025-05-05 11:22:35.04|\n",
      "+----+----+--------+---------+----------+--------+-------+-----------+----------+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:22:39 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/57 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.57.69d60b7f-066b-435a-8f01-90d22eb1503f.tmp\n",
      "25/05/05 11:22:39 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/57 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.57.4627b4da-1f35-46dc-b527-ff8cba63d5f2.tmp\n",
      "25/05/05 11:22:39 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/57 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.57.69aaad39-2bd6-4cb8-9d94-a9b010ed0374.tmp\n",
      "25/05/05 11:22:39 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.57.69d60b7f-066b-435a-8f01-90d22eb1503f.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/57\n",
      "25/05/05 11:22:39 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.57.69aaad39-2bd6-4cb8-9d94-a9b010ed0374.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/57\n",
      "25/05/05 11:22:39 INFO MicroBatchExecution: Committed offsets for batch 57. Metadata OffsetSeqMetadata(0,1746458559528,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:22:39 INFO MicroBatchExecution: Committed offsets for batch 57. Metadata OffsetSeqMetadata(0,1746458559527,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:22:39 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.57.4627b4da-1f35-46dc-b527-ff8cba63d5f2.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/57\n",
      "25/05/05 11:22:39 INFO MicroBatchExecution: Committed offsets for batch 57. Metadata OffsetSeqMetadata(0,1746458559529,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:22:39 INFO IncrementalExecution: Current batch timestamp = 1746458559527\n",
      "25/05/05 11:22:39 INFO IncrementalExecution: Current batch timestamp = 1746458559528\n",
      "25/05/05 11:22:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:39 INFO IncrementalExecution: Current batch timestamp = 1746458559529\n",
      "25/05/05 11:22:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:39 INFO IncrementalExecution: Current batch timestamp = 1746458559528\n",
      "25/05/05 11:22:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:39 INFO IncrementalExecution: Current batch timestamp = 1746458559527\n",
      "25/05/05 11:22:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:39 INFO IncrementalExecution: Current batch timestamp = 1746458559529\n",
      "25/05/05 11:22:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:39 INFO IncrementalExecution: Current batch timestamp = 1746458559527\n",
      "25/05/05 11:22:39 INFO IncrementalExecution: Current batch timestamp = 1746458559528\n",
      "25/05/05 11:22:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:39 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:39 INFO CodeGenerator: Code generated in 7.392125 ms\n",
      "25/05/05 11:22:39 INFO CodeGenerator: Code generated in 4.097375 ms\n",
      "25/05/05 11:22:39 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 57, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:22:39 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:39 INFO DAGScheduler: Got job 171 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:22:39 INFO DAGScheduler: Final stage: ResultStage 170 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:22:39 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:22:39 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:22:39 INFO DAGScheduler: Submitting ResultStage 170 (MapPartitionsRDD[920] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:22:39 INFO MemoryStore: Block broadcast_227 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:22:39 INFO MemoryStore: Block broadcast_227_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:22:39 INFO CodeGenerator: Code generated in 4.147 ms\n",
      "25/05/05 11:22:39 INFO BlockManagerInfo: Added broadcast_227_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:39 INFO SparkContext: Created broadcast 227 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:22:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 170 (MapPartitionsRDD[920] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:22:39 INFO TaskSchedulerImpl: Adding task set 170.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:22:39 INFO TaskSetManager: Starting task 0.0 in stage 170.0 (TID 170) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:22:39 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 57, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@80ca48f]. The input RDD has 1 partitions.\n",
      "25/05/05 11:22:39 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:39 INFO Executor: Running task 0.0 in stage 170.0 (TID 170)\n",
      "25/05/05 11:22:39 INFO DAGScheduler: Got job 172 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:22:39 INFO DAGScheduler: Final stage: ResultStage 171 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:22:39 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:22:39 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:22:39 INFO DAGScheduler: Submitting ResultStage 171 (MapPartitionsRDD[923] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:22:39 INFO MemoryStore: Block broadcast_228 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:22:39 INFO MemoryStore: Block broadcast_228_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)\n",
      "25/05/05 11:22:39 INFO BlockManagerInfo: Added broadcast_228_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:39 INFO SparkContext: Created broadcast 228 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:22:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 171 (MapPartitionsRDD[923] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:22:39 INFO TaskSchedulerImpl: Adding task set 171.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:22:39 INFO TaskSetManager: Starting task 0.0 in stage 171.0 (TID 171) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:22:39 INFO Executor: Running task 0.0 in stage 171.0 (TID 171)\n",
      "25/05/05 11:22:39 INFO CodeGenerator: Code generated in 6.036959 ms\n",
      "25/05/05 11:22:39 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3288 untilOffset=3289, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=57 taskId=170 partitionId=0\n",
      "25/05/05 11:22:39 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3288 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:39 INFO CodeGenerator: Code generated in 4.072708 ms\n",
      "25/05/05 11:22:39 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3288 untilOffset=3289, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=57 taskId=171 partitionId=0\n",
      "25/05/05 11:22:39 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3288 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:39 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:39 INFO DAGScheduler: Got job 173 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:22:39 INFO DAGScheduler: Final stage: ResultStage 172 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:22:39 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:22:39 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:22:39 INFO DAGScheduler: Submitting ResultStage 172 (MapPartitionsRDD[928] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:22:39 INFO MemoryStore: Block broadcast_229 stored as values in memory (estimated size 47.1 KiB, free 366.0 MiB)\n",
      "25/05/05 11:22:39 INFO MemoryStore: Block broadcast_229_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.0 MiB)\n",
      "25/05/05 11:22:39 INFO BlockManagerInfo: Added broadcast_229_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:39 INFO SparkContext: Created broadcast 229 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:22:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 172 (MapPartitionsRDD[928] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:22:39 INFO TaskSchedulerImpl: Adding task set 172.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:22:39 INFO TaskSetManager: Starting task 0.0 in stage 172.0 (TID 172) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:22:39 INFO Executor: Running task 0.0 in stage 172.0 (TID 172)\n",
      "25/05/05 11:22:39 INFO CodeGenerator: Code generated in 4.280042 ms\n",
      "25/05/05 11:22:39 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3288 untilOffset=3289, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=57 taskId=172 partitionId=0\n",
      "25/05/05 11:22:39 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3288 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3289, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:40 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:22:40 INFO DataWritingSparkTask: Committed partition 0 (task 170, attempt 0, stage 170.0)\n",
      "25/05/05 11:22:40 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 504853917 nanos, during time span of 505227833 nanos.\n",
      "25/05/05 11:22:40 INFO Executor: Finished task 0.0 in stage 170.0 (TID 170). 2188 bytes result sent to driver\n",
      "25/05/05 11:22:40 INFO TaskSetManager: Finished task 0.0 in stage 170.0 (TID 170) in 517 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:22:40 INFO TaskSchedulerImpl: Removed TaskSet 170.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:22:40 INFO DAGScheduler: ResultStage 170 (start at NativeMethodAccessorImpl.java:0) finished in 0.519 s\n",
      "25/05/05 11:22:40 INFO DAGScheduler: Job 171 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:22:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 170: Stage finished\n",
      "25/05/05 11:22:40 INFO DAGScheduler: Job 171 finished: start at NativeMethodAccessorImpl.java:0, took 0.519712 s\n",
      "25/05/05 11:22:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 57, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:22:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3289, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:40 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:22:40 INFO DataWritingSparkTask: Committed partition 0 (task 171, attempt 0, stage 171.0)\n",
      "25/05/05 11:22:40 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 502647458 nanos, during time span of 504402875 nanos.\n",
      "25/05/05 11:22:40 INFO Executor: Finished task 0.0 in stage 171.0 (TID 171). 3516 bytes result sent to driver\n",
      "25/05/05 11:22:40 INFO TaskSetManager: Finished task 0.0 in stage 171.0 (TID 171) in 515 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:22:40 INFO TaskSchedulerImpl: Removed TaskSet 171.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:22:40 INFO DAGScheduler: ResultStage 171 (start at NativeMethodAccessorImpl.java:0) finished in 0.521 s\n",
      "25/05/05 11:22:40 INFO DAGScheduler: Job 172 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:22:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 171: Stage finished\n",
      "25/05/05 11:22:40 INFO DAGScheduler: Job 172 finished: start at NativeMethodAccessorImpl.java:0, took 0.521465 s\n",
      "25/05/05 11:22:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 57, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@80ca48f] is committing.\n",
      "25/05/05 11:22:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 57, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@80ca48f] committed.\n",
      "25/05/05 11:22:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 57, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:22:40 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/57 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.57.51040617-7ff2-4bd9-895f-8fdb0db7f81e.tmp\n",
      "25/05/05 11:22:40 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/57 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.57.b6fc8007-53e6-46fe-9b44-67ec0e0bbf00.tmp\n",
      "25/05/05 11:22:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3289, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:40 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:22:40 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:22:40 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:22:40 INFO connection: Opened connection [connectionId{localValue:113, serverValue:4483}] to localhost:27017\n",
      "25/05/05 11:22:40 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=285291}\n",
      "25/05/05 11:22:40 INFO connection: Opened connection [connectionId{localValue:114, serverValue:4484}] to localhost:27017\n",
      "25/05/05 11:22:40 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:22:40 INFO connection: Closed connection [connectionId{localValue:114, serverValue:4484}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:22:40 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 506602917 nanos, during time span of 511818500 nanos.\n",
      "25/05/05 11:22:40 INFO Executor: Finished task 0.0 in stage 172.0 (TID 172). 1645 bytes result sent to driver\n",
      "25/05/05 11:22:40 INFO TaskSetManager: Finished task 0.0 in stage 172.0 (TID 172) in 523 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:22:40 INFO TaskSchedulerImpl: Removed TaskSet 172.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:22:40 INFO DAGScheduler: ResultStage 172 (start at NativeMethodAccessorImpl.java:0) finished in 0.528 s\n",
      "25/05/05 11:22:40 INFO DAGScheduler: Job 173 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:22:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 172: Stage finished\n",
      "25/05/05 11:22:40 INFO DAGScheduler: Job 173 finished: start at NativeMethodAccessorImpl.java:0, took 0.528269 s\n",
      "25/05/05 11:22:40 INFO MemoryStore: Block broadcast_230 stored as values in memory (estimated size 248.0 B, free 366.0 MiB)\n",
      "25/05/05 11:22:40 INFO MemoryStore: Block broadcast_230_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.0 MiB)\n",
      "25/05/05 11:22:40 INFO BlockManagerInfo: Added broadcast_230_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:22:40 INFO SparkContext: Created broadcast 230 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:40 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.57.51040617-7ff2-4bd9-895f-8fdb0db7f81e.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/57\n",
      "25/05/05 11:22:40 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:22:39.523Z\",\n",
      "  \"batchId\" : 57,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.547987616099071,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 543,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 1,\n",
      "    \"latestOffset\" : 4,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 646,\n",
      "    \"walCommit\" : 63\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3288\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3289\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3289\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.547987616099071,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:22:40 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/57 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.57.7287b2d8-18eb-4789-bba5-60aadc17fd32.tmp\n",
      "25/05/05 11:22:40 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.57.b6fc8007-53e6-46fe-9b44-67ec0e0bbf00.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/57\n",
      "25/05/05 11:22:40 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:22:39.523Z\",\n",
      "  \"batchId\" : 57,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.5432098765432098,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 547,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 5,\n",
      "    \"queryPlanning\" : 6,\n",
      "    \"triggerExecution\" : 648,\n",
      "    \"walCommit\" : 62\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3288\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3289\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3289\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.5432098765432098,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:22:40 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.57.7287b2d8-18eb-4789-bba5-60aadc17fd32.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/57\n",
      "25/05/05 11:22:40 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:22:39.524Z\",\n",
      "  \"batchId\" : 57,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.4947683109118086,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 567,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 5,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 669,\n",
      "    \"walCommit\" : 64\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3288\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3289\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3289\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.4947683109118086,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 57\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R022|R033|01-00-00|42 ST-PORT AUTH|05/05/2025|11:22:38|REGULAR|10         |11        |2025-05-05 11:22:39.528|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:22:45 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/58 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.58.dbeb3cf2-f920-4ad2-bb9b-3b22f04ea994.tmp\n",
      "25/05/05 11:22:45 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/58 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.58.afd2426e-41cc-4745-803c-9953966cf76b.tmp\n",
      "25/05/05 11:22:45 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/58 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.58.146c67aa-6e2d-443d-ba44-505b71ac918b.tmp\n",
      "25/05/05 11:22:45 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.58.afd2426e-41cc-4745-803c-9953966cf76b.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/58\n",
      "25/05/05 11:22:45 INFO MicroBatchExecution: Committed offsets for batch 58. Metadata OffsetSeqMetadata(0,1746458565014,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:22:45 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.58.146c67aa-6e2d-443d-ba44-505b71ac918b.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/58\n",
      "25/05/05 11:22:45 INFO MicroBatchExecution: Committed offsets for batch 58. Metadata OffsetSeqMetadata(0,1746458565025,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:22:45 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.58.dbeb3cf2-f920-4ad2-bb9b-3b22f04ea994.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/58\n",
      "25/05/05 11:22:45 INFO MicroBatchExecution: Committed offsets for batch 58. Metadata OffsetSeqMetadata(0,1746458565014,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:22:45 INFO IncrementalExecution: Current batch timestamp = 1746458565014\n",
      "25/05/05 11:22:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:45 INFO IncrementalExecution: Current batch timestamp = 1746458565014\n",
      "25/05/05 11:22:45 INFO IncrementalExecution: Current batch timestamp = 1746458565025\n",
      "25/05/05 11:22:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:45 INFO IncrementalExecution: Current batch timestamp = 1746458565014\n",
      "25/05/05 11:22:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:45 INFO IncrementalExecution: Current batch timestamp = 1746458565014\n",
      "25/05/05 11:22:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:45 INFO IncrementalExecution: Current batch timestamp = 1746458565025\n",
      "25/05/05 11:22:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:45 INFO IncrementalExecution: Current batch timestamp = 1746458565014\n",
      "25/05/05 11:22:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:45 INFO IncrementalExecution: Current batch timestamp = 1746458565014\n",
      "25/05/05 11:22:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:45 INFO BlockManagerInfo: Removed broadcast_227_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:45 INFO BlockManagerInfo: Removed broadcast_225_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:45 INFO BlockManagerInfo: Removed broadcast_224_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:45 INFO CodeGenerator: Code generated in 10.321959 ms\n",
      "25/05/05 11:22:45 INFO BlockManagerInfo: Removed broadcast_226_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:22:45 INFO BlockManagerInfo: Removed broadcast_228_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:45 INFO BlockManagerInfo: Removed broadcast_223_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:22:45 INFO BlockManagerInfo: Removed broadcast_230_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:22:45 INFO CodeGenerator: Code generated in 3.191166 ms\n",
      "25/05/05 11:22:45 INFO BlockManagerInfo: Removed broadcast_229_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:22:45 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 58, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@43f3a64]. The input RDD has 1 partitions.\n",
      "25/05/05 11:22:45 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 58, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:22:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:45 INFO DAGScheduler: Got job 174 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:22:45 INFO DAGScheduler: Final stage: ResultStage 173 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:22:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:22:45 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:22:45 INFO DAGScheduler: Submitting ResultStage 173 (MapPartitionsRDD[938] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:22:45 INFO MemoryStore: Block broadcast_231 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:22:45 INFO MemoryStore: Block broadcast_231_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:22:45 INFO BlockManagerInfo: Added broadcast_231_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:22:45 INFO SparkContext: Created broadcast 231 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:22:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 173 (MapPartitionsRDD[938] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:22:45 INFO TaskSchedulerImpl: Adding task set 173.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:22:45 INFO DAGScheduler: Got job 175 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:22:45 INFO DAGScheduler: Final stage: ResultStage 174 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:22:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:22:45 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:22:45 INFO DAGScheduler: Submitting ResultStage 174 (MapPartitionsRDD[939] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:22:45 INFO TaskSetManager: Starting task 0.0 in stage 173.0 (TID 173) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:22:45 INFO Executor: Running task 0.0 in stage 173.0 (TID 173)\n",
      "25/05/05 11:22:45 INFO MemoryStore: Block broadcast_232 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:22:45 INFO MemoryStore: Block broadcast_232_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:22:45 INFO BlockManagerInfo: Added broadcast_232_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:22:45 INFO SparkContext: Created broadcast 232 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:22:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 174 (MapPartitionsRDD[939] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:22:45 INFO TaskSchedulerImpl: Adding task set 174.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:22:45 INFO TaskSetManager: Starting task 0.0 in stage 174.0 (TID 174) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:22:45 INFO Executor: Running task 0.0 in stage 174.0 (TID 174)\n",
      "25/05/05 11:22:45 INFO CodeGenerator: Code generated in 3.937417 ms\n",
      "25/05/05 11:22:45 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3289 untilOffset=3290, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=58 taskId=174 partitionId=0\n",
      "25/05/05 11:22:45 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3289 untilOffset=3290, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=58 taskId=173 partitionId=0\n",
      "25/05/05 11:22:45 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3289 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:45 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3289 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:45 INFO DAGScheduler: Got job 176 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:22:45 INFO DAGScheduler: Final stage: ResultStage 175 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:22:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:22:45 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:22:45 INFO DAGScheduler: Submitting ResultStage 175 (MapPartitionsRDD[944] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:22:45 INFO MemoryStore: Block broadcast_233 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:22:45 INFO MemoryStore: Block broadcast_233_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:22:45 INFO BlockManagerInfo: Added broadcast_233_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:45 INFO SparkContext: Created broadcast 233 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:22:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 175 (MapPartitionsRDD[944] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:22:45 INFO TaskSchedulerImpl: Adding task set 175.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:22:45 INFO TaskSetManager: Starting task 0.0 in stage 175.0 (TID 175) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:22:45 INFO Executor: Running task 0.0 in stage 175.0 (TID 175)\n",
      "25/05/05 11:22:45 INFO CodeGenerator: Code generated in 3.876667 ms\n",
      "25/05/05 11:22:45 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3289 untilOffset=3290, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=58 taskId=175 partitionId=0\n",
      "25/05/05 11:22:45 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3289 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3290, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3290, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:45 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:22:45 INFO DataWritingSparkTask: Committed partition 0 (task 174, attempt 0, stage 174.0)\n",
      "25/05/05 11:22:45 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 504597583 nanos, during time span of 504954875 nanos.\n",
      "25/05/05 11:22:45 INFO Executor: Finished task 0.0 in stage 174.0 (TID 174). 2145 bytes result sent to driver\n",
      "25/05/05 11:22:45 INFO TaskSetManager: Finished task 0.0 in stage 174.0 (TID 174) in 513 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:22:45 INFO TaskSchedulerImpl: Removed TaskSet 174.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:22:45 INFO DAGScheduler: ResultStage 174 (start at NativeMethodAccessorImpl.java:0) finished in 0.514 s\n",
      "25/05/05 11:22:45 INFO DAGScheduler: Job 175 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:22:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 174: Stage finished\n",
      "25/05/05 11:22:45 INFO DAGScheduler: Job 175 finished: start at NativeMethodAccessorImpl.java:0, took 0.515350 s\n",
      "25/05/05 11:22:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 58, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:22:45 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:22:45 INFO DataWritingSparkTask: Committed partition 0 (task 173, attempt 0, stage 173.0)\n",
      "25/05/05 11:22:45 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 503935542 nanos, during time span of 505535250 nanos.\n",
      "25/05/05 11:22:45 INFO Executor: Finished task 0.0 in stage 173.0 (TID 173). 3516 bytes result sent to driver\n",
      "25/05/05 11:22:45 INFO TaskSetManager: Finished task 0.0 in stage 173.0 (TID 173) in 515 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:22:45 INFO TaskSchedulerImpl: Removed TaskSet 173.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:22:45 INFO DAGScheduler: ResultStage 173 (start at NativeMethodAccessorImpl.java:0) finished in 0.516 s\n",
      "25/05/05 11:22:45 INFO DAGScheduler: Job 174 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:22:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 173: Stage finished\n",
      "25/05/05 11:22:45 INFO DAGScheduler: Job 174 finished: start at NativeMethodAccessorImpl.java:0, took 0.516871 s\n",
      "25/05/05 11:22:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 58, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@43f3a64] is committing.\n",
      "25/05/05 11:22:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 58, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@43f3a64] committed.\n",
      "25/05/05 11:22:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 58, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:22:45 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/58 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.58.73c51b4a-ec9b-417d-ab75-09ccb7205328.tmp\n",
      "25/05/05 11:22:45 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/58 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.58.f32b5214-e0f2-455f-b547-511898f430d3.tmp\n",
      "25/05/05 11:22:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3290, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:45 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:22:45 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:22:45 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:22:45 INFO connection: Opened connection [connectionId{localValue:115, serverValue:4485}] to localhost:27017\n",
      "25/05/05 11:22:45 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1748958}\n",
      "25/05/05 11:22:45 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.58.73c51b4a-ec9b-417d-ab75-09ccb7205328.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/58\n",
      "25/05/05 11:22:45 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:22:45.012Z\",\n",
      "  \"batchId\" : 58,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.6025641025641026,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 536,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 624,\n",
      "    \"walCommit\" : 54\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3289\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3290\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3290\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.6025641025641026,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:22:45 INFO connection: Opened connection [connectionId{localValue:116, serverValue:4486}] to localhost:27017\n",
      "25/05/05 11:22:45 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:22:45 INFO connection: Closed connection [connectionId{localValue:116, serverValue:4486}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:22:45 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 506404583 nanos, during time span of 515569083 nanos.\n",
      "25/05/05 11:22:45 INFO Executor: Finished task 0.0 in stage 175.0 (TID 175). 1645 bytes result sent to driver\n",
      "25/05/05 11:22:45 INFO TaskSetManager: Finished task 0.0 in stage 175.0 (TID 175) in 526 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:22:45 INFO TaskSchedulerImpl: Removed TaskSet 175.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:22:45 INFO DAGScheduler: ResultStage 175 (start at NativeMethodAccessorImpl.java:0) finished in 0.529 s\n",
      "25/05/05 11:22:45 INFO DAGScheduler: Job 176 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:22:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 175: Stage finished\n",
      "25/05/05 11:22:45 INFO DAGScheduler: Job 176 finished: start at NativeMethodAccessorImpl.java:0, took 0.530667 s\n",
      "25/05/05 11:22:45 INFO MemoryStore: Block broadcast_234 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:22:45 INFO MemoryStore: Block broadcast_234_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:22:45 INFO BlockManagerInfo: Added broadcast_234_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:22:45 INFO SparkContext: Created broadcast 234 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:45 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.58.f32b5214-e0f2-455f-b547-511898f430d3.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/58\n",
      "25/05/05 11:22:45 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:22:45.012Z\",\n",
      "  \"batchId\" : 58,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.5847860538827259,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 541,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 631,\n",
      "    \"walCommit\" : 57\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3289\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3290\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3290\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.5847860538827259,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:22:45 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/58 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.58.e3e4be3e-991c-4636-9cd7-e57876329818.tmp\n",
      "25/05/05 11:22:45 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.58.e3e4be3e-991c-4636-9cd7-e57876329818.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/58\n",
      "25/05/05 11:22:45 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:22:45.015Z\",\n",
      "  \"batchId\" : 58,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.5384615384615383,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 567,\n",
      "    \"commitOffsets\" : 24,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 10,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 650,\n",
      "    \"walCommit\" : 45\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3289\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3290\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3290\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.5384615384615383,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 58\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|B013|R081|02-00-00|161/YANKEE STAD|05/05/2025|11:22:43|REGULAR|4          |7         |2025-05-05 11:22:45.014|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:22:51 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/59 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.59.d6fa80c1-8ece-430f-a629-cf02e57a3702.tmp\n",
      "25/05/05 11:22:51 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/59 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.59.783946c5-9b61-4a0d-9742-052765fb0923.tmp\n",
      "25/05/05 11:22:51 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/59 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.59.d60407e0-6735-4be7-802c-39363c514fa3.tmp\n",
      "25/05/05 11:22:51 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.59.d6fa80c1-8ece-430f-a629-cf02e57a3702.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/59\n",
      "25/05/05 11:22:51 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.59.783946c5-9b61-4a0d-9742-052765fb0923.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/59\n",
      "25/05/05 11:22:51 INFO MicroBatchExecution: Committed offsets for batch 59. Metadata OffsetSeqMetadata(0,1746458571487,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:22:51 INFO MicroBatchExecution: Committed offsets for batch 59. Metadata OffsetSeqMetadata(0,1746458571494,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:22:51 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.59.d60407e0-6735-4be7-802c-39363c514fa3.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/59\n",
      "25/05/05 11:22:51 INFO MicroBatchExecution: Committed offsets for batch 59. Metadata OffsetSeqMetadata(0,1746458571495,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:22:51 INFO IncrementalExecution: Current batch timestamp = 1746458571494\n",
      "25/05/05 11:22:51 INFO IncrementalExecution: Current batch timestamp = 1746458571487\n",
      "25/05/05 11:22:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:51 INFO IncrementalExecution: Current batch timestamp = 1746458571495\n",
      "25/05/05 11:22:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:51 INFO IncrementalExecution: Current batch timestamp = 1746458571487\n",
      "25/05/05 11:22:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:51 INFO IncrementalExecution: Current batch timestamp = 1746458571494\n",
      "25/05/05 11:22:51 INFO IncrementalExecution: Current batch timestamp = 1746458571495\n",
      "25/05/05 11:22:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:51 INFO IncrementalExecution: Current batch timestamp = 1746458571487\n",
      "25/05/05 11:22:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:51 INFO IncrementalExecution: Current batch timestamp = 1746458571495\n",
      "25/05/05 11:22:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:51 INFO CodeGenerator: Code generated in 25.102458 ms\n",
      "25/05/05 11:22:51 INFO CodeGenerator: Code generated in 22.888833 ms\n",
      "25/05/05 11:22:51 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 59, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:22:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:51 INFO CodeGenerator: Code generated in 3.757375 ms\n",
      "25/05/05 11:22:51 INFO DAGScheduler: Got job 177 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:22:51 INFO DAGScheduler: Final stage: ResultStage 176 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:22:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:22:51 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:22:51 INFO DAGScheduler: Submitting ResultStage 176 (MapPartitionsRDD[952] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:22:51 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 59, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@2a91b453]. The input RDD has 1 partitions.\n",
      "25/05/05 11:22:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:51 INFO MemoryStore: Block broadcast_235 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:22:51 INFO MemoryStore: Block broadcast_235_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:22:51 INFO BlockManagerInfo: Added broadcast_235_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:51 INFO SparkContext: Created broadcast 235 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:22:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 176 (MapPartitionsRDD[952] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:22:51 INFO TaskSchedulerImpl: Adding task set 176.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:22:51 INFO DAGScheduler: Got job 178 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:22:51 INFO DAGScheduler: Final stage: ResultStage 177 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:22:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:22:51 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:22:51 INFO DAGScheduler: Submitting ResultStage 177 (MapPartitionsRDD[955] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:22:51 INFO TaskSetManager: Starting task 0.0 in stage 176.0 (TID 176) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:22:51 INFO MemoryStore: Block broadcast_236 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:22:51 INFO Executor: Running task 0.0 in stage 176.0 (TID 176)\n",
      "25/05/05 11:22:51 INFO MemoryStore: Block broadcast_236_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)\n",
      "25/05/05 11:22:51 INFO BlockManagerInfo: Added broadcast_236_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:51 INFO SparkContext: Created broadcast 236 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:22:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 177 (MapPartitionsRDD[955] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:22:51 INFO TaskSchedulerImpl: Adding task set 177.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:22:51 INFO TaskSetManager: Starting task 0.0 in stage 177.0 (TID 177) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:22:51 INFO Executor: Running task 0.0 in stage 177.0 (TID 177)\n",
      "25/05/05 11:22:51 INFO CodeGenerator: Code generated in 4.789542 ms\n",
      "25/05/05 11:22:51 INFO CodeGenerator: Code generated in 4.5635 ms\n",
      "25/05/05 11:22:51 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3290 untilOffset=3291, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=59 taskId=176 partitionId=0\n",
      "25/05/05 11:22:51 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3290 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:51 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3290 untilOffset=3291, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=59 taskId=177 partitionId=0\n",
      "25/05/05 11:22:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:51 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3290 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:51 INFO DAGScheduler: Got job 179 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:22:51 INFO DAGScheduler: Final stage: ResultStage 178 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:22:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:22:51 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:22:51 INFO DAGScheduler: Submitting ResultStage 178 (MapPartitionsRDD[960] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:22:51 INFO MemoryStore: Block broadcast_237 stored as values in memory (estimated size 47.1 KiB, free 366.0 MiB)\n",
      "25/05/05 11:22:51 INFO MemoryStore: Block broadcast_237_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.0 MiB)\n",
      "25/05/05 11:22:51 INFO BlockManagerInfo: Added broadcast_237_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:51 INFO SparkContext: Created broadcast 237 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:22:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 178 (MapPartitionsRDD[960] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:22:51 INFO TaskSchedulerImpl: Adding task set 178.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:22:51 INFO TaskSetManager: Starting task 0.0 in stage 178.0 (TID 178) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:22:51 INFO Executor: Running task 0.0 in stage 178.0 (TID 178)\n",
      "25/05/05 11:22:51 INFO CodeGenerator: Code generated in 4.194958 ms\n",
      "25/05/05 11:22:51 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3290 untilOffset=3291, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=59 taskId=178 partitionId=0\n",
      "25/05/05 11:22:51 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3290 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:52 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:52 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:52 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:52 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:52 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3291, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:52 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3291, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:52 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:22:52 INFO DataWritingSparkTask: Committed partition 0 (task 176, attempt 0, stage 176.0)\n",
      "25/05/05 11:22:52 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 505905500 nanos, during time span of 506263417 nanos.\n",
      "25/05/05 11:22:52 INFO Executor: Finished task 0.0 in stage 176.0 (TID 176). 2145 bytes result sent to driver\n",
      "25/05/05 11:22:52 INFO TaskSetManager: Finished task 0.0 in stage 176.0 (TID 176) in 516 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:22:52 INFO TaskSchedulerImpl: Removed TaskSet 176.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:22:52 INFO DAGScheduler: ResultStage 176 (start at NativeMethodAccessorImpl.java:0) finished in 0.517 s\n",
      "25/05/05 11:22:52 INFO DAGScheduler: Job 177 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:22:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 176: Stage finished\n",
      "25/05/05 11:22:52 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:22:52 INFO DataWritingSparkTask: Committed partition 0 (task 177, attempt 0, stage 177.0)\n",
      "25/05/05 11:22:52 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 502080083 nanos, during time span of 503876125 nanos.\n",
      "25/05/05 11:22:52 INFO DAGScheduler: Job 177 finished: start at NativeMethodAccessorImpl.java:0, took 0.519407 s\n",
      "25/05/05 11:22:52 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 59, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:22:52 INFO Executor: Finished task 0.0 in stage 177.0 (TID 177). 3515 bytes result sent to driver\n",
      "25/05/05 11:22:52 INFO TaskSetManager: Finished task 0.0 in stage 177.0 (TID 177) in 517 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:22:52 INFO TaskSchedulerImpl: Removed TaskSet 177.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:22:52 INFO DAGScheduler: ResultStage 177 (start at NativeMethodAccessorImpl.java:0) finished in 0.520 s\n",
      "25/05/05 11:22:52 INFO DAGScheduler: Job 178 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:22:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 177: Stage finished\n",
      "25/05/05 11:22:52 INFO DAGScheduler: Job 178 finished: start at NativeMethodAccessorImpl.java:0, took 0.521378 s\n",
      "25/05/05 11:22:52 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 59, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@2a91b453] is committing.\n",
      "25/05/05 11:22:52 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 59, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@2a91b453] committed.\n",
      "25/05/05 11:22:52 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 59, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:22:52 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/59 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.59.68a9eaa7-10eb-4941-8232-6332ed1a8414.tmp\n",
      "25/05/05 11:22:52 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/59 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.59.a497c40d-8a8a-475a-b661-3971975c0db7.tmp\n",
      "25/05/05 11:22:52 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:52 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:52 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3291, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:52 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:22:52 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:22:52 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:22:52 INFO connection: Opened connection [connectionId{localValue:117, serverValue:4487}] to localhost:27017\n",
      "25/05/05 11:22:52 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=176875}\n",
      "25/05/05 11:22:52 INFO connection: Opened connection [connectionId{localValue:118, serverValue:4488}] to localhost:27017\n",
      "25/05/05 11:22:52 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:22:52 INFO connection: Closed connection [connectionId{localValue:118, serverValue:4488}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:22:52 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 506172333 nanos, during time span of 513416125 nanos.\n",
      "25/05/05 11:22:52 INFO Executor: Finished task 0.0 in stage 178.0 (TID 178). 1645 bytes result sent to driver\n",
      "25/05/05 11:22:52 INFO TaskSetManager: Finished task 0.0 in stage 178.0 (TID 178) in 524 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:22:52 INFO TaskSchedulerImpl: Removed TaskSet 178.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:22:52 INFO DAGScheduler: ResultStage 178 (start at NativeMethodAccessorImpl.java:0) finished in 0.528 s\n",
      "25/05/05 11:22:52 INFO DAGScheduler: Job 179 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:22:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 178: Stage finished\n",
      "25/05/05 11:22:52 INFO DAGScheduler: Job 179 finished: start at NativeMethodAccessorImpl.java:0, took 0.529698 s\n",
      "25/05/05 11:22:52 INFO MemoryStore: Block broadcast_238 stored as values in memory (estimated size 248.0 B, free 366.0 MiB)\n",
      "25/05/05 11:22:52 INFO MemoryStore: Block broadcast_238_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.0 MiB)\n",
      "25/05/05 11:22:52 INFO BlockManagerInfo: Added broadcast_238_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:22:52 INFO SparkContext: Created broadcast 238 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:52 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.59.68a9eaa7-10eb-4941-8232-6332ed1a8414.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/59\n",
      "25/05/05 11:22:52 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:22:51.489Z\",\n",
      "  \"batchId\" : 59,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.5313935681470137,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 557,\n",
      "    \"commitOffsets\" : 32,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 6,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 653,\n",
      "    \"walCommit\" : 53\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3290\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3291\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3291\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.5313935681470137,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:22:52 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.59.a497c40d-8a8a-475a-b661-3971975c0db7.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/59\n",
      "25/05/05 11:22:52 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:22:51.486Z\",\n",
      "  \"batchId\" : 59,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.5174506828528072,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 562,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 0,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 659,\n",
      "    \"walCommit\" : 60\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3290\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3291\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3291\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.5174506828528072,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:22:52 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/59 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.59.d69a11f7-6e2c-475b-9139-fff137aed4bf.tmp\n",
      "25/05/05 11:22:52 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.59.d69a11f7-6e2c-475b-9139-fff137aed4bf.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/59\n",
      "25/05/05 11:22:52 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:22:51.489Z\",\n",
      "  \"batchId\" : 59,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.4684287812041115,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 589,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 5,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 681,\n",
      "    \"walCommit\" : 53\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3290\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3291\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3291\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.4684287812041115,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 59\n",
      "-------------------------------------------\n",
      "+----+----+--------+--------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION       |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+--------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A002|R051|02-00-00|34 ST-PENN STA|05/05/2025|11:22:50|REGULAR|5          |12        |2025-05-05 11:22:51.487|\n",
      "+----+----+--------+--------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:22:56 INFO BlockManagerInfo: Removed broadcast_232_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:56 INFO BlockManagerInfo: Removed broadcast_237_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:56 INFO BlockManagerInfo: Removed broadcast_233_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:22:56 INFO BlockManagerInfo: Removed broadcast_231_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:22:56 INFO BlockManagerInfo: Removed broadcast_238_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:22:56 INFO BlockManagerInfo: Removed broadcast_236_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:22:56 INFO BlockManagerInfo: Removed broadcast_234_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:22:56 INFO BlockManagerInfo: Removed broadcast_235_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:22:57 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/60 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.60.c5dbba7f-987f-4004-9068-41a3812b6dec.tmp\n",
      "25/05/05 11:22:57 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/60 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.60.073394de-e720-4194-9998-87680d25226a.tmp\n",
      "25/05/05 11:22:57 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/60 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.60.7d4a1575-740e-4e42-a828-02bd5624d207.tmp\n",
      "25/05/05 11:22:57 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.60.c5dbba7f-987f-4004-9068-41a3812b6dec.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/60\n",
      "25/05/05 11:22:57 INFO MicroBatchExecution: Committed offsets for batch 60. Metadata OffsetSeqMetadata(0,1746458577041,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:22:57 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.60.073394de-e720-4194-9998-87680d25226a.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/60\n",
      "25/05/05 11:22:57 INFO MicroBatchExecution: Committed offsets for batch 60. Metadata OffsetSeqMetadata(0,1746458577045,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:22:57 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.60.7d4a1575-740e-4e42-a828-02bd5624d207.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/60\n",
      "25/05/05 11:22:57 INFO MicroBatchExecution: Committed offsets for batch 60. Metadata OffsetSeqMetadata(0,1746458577047,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:22:57 INFO IncrementalExecution: Current batch timestamp = 1746458577041\n",
      "25/05/05 11:22:57 INFO IncrementalExecution: Current batch timestamp = 1746458577045\n",
      "25/05/05 11:22:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:57 INFO IncrementalExecution: Current batch timestamp = 1746458577047\n",
      "25/05/05 11:22:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:57 INFO IncrementalExecution: Current batch timestamp = 1746458577041\n",
      "25/05/05 11:22:57 INFO IncrementalExecution: Current batch timestamp = 1746458577045\n",
      "25/05/05 11:22:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:57 INFO IncrementalExecution: Current batch timestamp = 1746458577047\n",
      "25/05/05 11:22:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:57 INFO IncrementalExecution: Current batch timestamp = 1746458577041\n",
      "25/05/05 11:22:57 INFO IncrementalExecution: Current batch timestamp = 1746458577045\n",
      "25/05/05 11:22:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:22:57 INFO CodeGenerator: Code generated in 4.046166 ms\n",
      "25/05/05 11:22:57 INFO CodeGenerator: Code generated in 4.201833 ms\n",
      "25/05/05 11:22:57 INFO CodeGenerator: Code generated in 4.29075 ms\n",
      "25/05/05 11:22:57 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 60, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4439b0db]. The input RDD has 1 partitions.\n",
      "25/05/05 11:22:57 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 60, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:22:57 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:57 INFO DAGScheduler: Got job 180 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:22:57 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:57 INFO DAGScheduler: Final stage: ResultStage 179 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:22:57 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:22:57 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:22:57 INFO DAGScheduler: Submitting ResultStage 179 (MapPartitionsRDD[970] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:22:57 INFO MemoryStore: Block broadcast_239 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:22:57 INFO MemoryStore: Block broadcast_239_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:22:57 INFO BlockManagerInfo: Added broadcast_239_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:22:57 INFO SparkContext: Created broadcast 239 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:22:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 179 (MapPartitionsRDD[970] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:22:57 INFO TaskSchedulerImpl: Adding task set 179.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:22:57 INFO DAGScheduler: Got job 181 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:22:57 INFO DAGScheduler: Final stage: ResultStage 180 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:22:57 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:22:57 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:22:57 INFO DAGScheduler: Submitting ResultStage 180 (MapPartitionsRDD[971] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:22:57 INFO TaskSetManager: Starting task 0.0 in stage 179.0 (TID 179) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:22:57 INFO MemoryStore: Block broadcast_240 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:22:57 INFO Executor: Running task 0.0 in stage 179.0 (TID 179)\n",
      "25/05/05 11:22:57 INFO MemoryStore: Block broadcast_240_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:22:57 INFO BlockManagerInfo: Added broadcast_240_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:22:57 INFO SparkContext: Created broadcast 240 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:22:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 180 (MapPartitionsRDD[971] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:22:57 INFO TaskSchedulerImpl: Adding task set 180.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:22:57 INFO TaskSetManager: Starting task 0.0 in stage 180.0 (TID 180) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:22:57 INFO Executor: Running task 0.0 in stage 180.0 (TID 180)\n",
      "25/05/05 11:22:57 INFO CodeGenerator: Code generated in 4.930709 ms\n",
      "25/05/05 11:22:57 INFO CodeGenerator: Code generated in 6.547875 ms\n",
      "25/05/05 11:22:57 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3291 untilOffset=3292, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=60 taskId=179 partitionId=0\n",
      "25/05/05 11:22:57 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3291 untilOffset=3292, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=60 taskId=180 partitionId=0\n",
      "25/05/05 11:22:57 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3291 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:57 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3291 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:57 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:57 INFO DAGScheduler: Got job 182 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:22:57 INFO DAGScheduler: Final stage: ResultStage 181 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:22:57 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:22:57 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:22:57 INFO DAGScheduler: Submitting ResultStage 181 (MapPartitionsRDD[976] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:22:57 INFO MemoryStore: Block broadcast_241 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:22:57 INFO MemoryStore: Block broadcast_241_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.1 MiB)\n",
      "25/05/05 11:22:57 INFO BlockManagerInfo: Added broadcast_241_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:22:57 INFO SparkContext: Created broadcast 241 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:22:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 181 (MapPartitionsRDD[976] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:22:57 INFO TaskSchedulerImpl: Adding task set 181.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:22:57 INFO TaskSetManager: Starting task 0.0 in stage 181.0 (TID 181) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:22:57 INFO Executor: Running task 0.0 in stage 181.0 (TID 181)\n",
      "25/05/05 11:22:57 INFO CodeGenerator: Code generated in 4.266583 ms\n",
      "25/05/05 11:22:57 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3291 untilOffset=3292, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=60 taskId=181 partitionId=0\n",
      "25/05/05 11:22:57 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3291 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3292, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3292, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:57 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:22:57 INFO DataWritingSparkTask: Committed partition 0 (task 180, attempt 0, stage 180.0)\n",
      "25/05/05 11:22:57 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 502713250 nanos, during time span of 503142250 nanos.\n",
      "25/05/05 11:22:57 INFO Executor: Finished task 0.0 in stage 180.0 (TID 180). 2145 bytes result sent to driver\n",
      "25/05/05 11:22:57 INFO TaskSetManager: Finished task 0.0 in stage 180.0 (TID 180) in 516 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:22:57 INFO TaskSchedulerImpl: Removed TaskSet 180.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:22:57 INFO DAGScheduler: ResultStage 180 (start at NativeMethodAccessorImpl.java:0) finished in 0.518 s\n",
      "25/05/05 11:22:57 INFO DAGScheduler: Job 181 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:22:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 180: Stage finished\n",
      "25/05/05 11:22:57 INFO DAGScheduler: Job 181 finished: start at NativeMethodAccessorImpl.java:0, took 0.520397 s\n",
      "25/05/05 11:22:57 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:22:57 INFO DataWritingSparkTask: Committed partition 0 (task 179, attempt 0, stage 179.0)\n",
      "25/05/05 11:22:57 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 60, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:22:57 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 504100625 nanos, during time span of 506011208 nanos.\n",
      "25/05/05 11:22:57 INFO Executor: Finished task 0.0 in stage 179.0 (TID 179). 3559 bytes result sent to driver\n",
      "25/05/05 11:22:57 INFO TaskSetManager: Finished task 0.0 in stage 179.0 (TID 179) in 520 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:22:57 INFO TaskSchedulerImpl: Removed TaskSet 179.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:22:57 INFO DAGScheduler: ResultStage 179 (start at NativeMethodAccessorImpl.java:0) finished in 0.521 s\n",
      "25/05/05 11:22:57 INFO DAGScheduler: Job 180 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:22:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 179: Stage finished\n",
      "25/05/05 11:22:57 INFO DAGScheduler: Job 180 finished: start at NativeMethodAccessorImpl.java:0, took 0.522073 s\n",
      "25/05/05 11:22:57 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 60, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4439b0db] is committing.\n",
      "25/05/05 11:22:57 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 60, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4439b0db] committed.\n",
      "25/05/05 11:22:57 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 60, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:22:57 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/60 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.60.12f626f6-0c84-44b5-9803-61d5ef3cd588.tmp\n",
      "25/05/05 11:22:57 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/60 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.60.5f994d40-f01a-44c7-8f82-dec90d8c0b4c.tmp\n",
      "25/05/05 11:22:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:22:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3292, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:22:57 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:22:57 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:22:57 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:22:57 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.60.12f626f6-0c84-44b5-9803-61d5ef3cd588.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/60\n",
      "25/05/05 11:22:57 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:22:57.041Z\",\n",
      "  \"batchId\" : 60,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.5649452269170578,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 537,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 4,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 639,\n",
      "    \"walCommit\" : 63\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3291\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3292\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3292\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.5649452269170578,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:22:57 INFO connection: Opened connection [connectionId{localValue:119, serverValue:4489}] to localhost:27017\n",
      "25/05/05 11:22:57 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=317333}\n",
      "25/05/05 11:22:57 INFO connection: Opened connection [connectionId{localValue:120, serverValue:4490}] to localhost:27017\n",
      "25/05/05 11:22:57 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.60.5f994d40-f01a-44c7-8f82-dec90d8c0b4c.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/60\n",
      "25/05/05 11:22:57 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:22:57.040Z\",\n",
      "  \"batchId\" : 60,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.5503875968992247,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 543,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 6,\n",
      "    \"triggerExecution\" : 645,\n",
      "    \"walCommit\" : 66\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3291\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3292\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3292\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.5503875968992247,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:22:57 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:22:57 INFO connection: Closed connection [connectionId{localValue:120, serverValue:4490}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:22:57 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 504034583 nanos, during time span of 518499542 nanos.\n",
      "25/05/05 11:22:57 INFO Executor: Finished task 0.0 in stage 181.0 (TID 181). 1645 bytes result sent to driver\n",
      "25/05/05 11:22:57 INFO TaskSetManager: Finished task 0.0 in stage 181.0 (TID 181) in 532 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:22:57 INFO TaskSchedulerImpl: Removed TaskSet 181.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:22:57 INFO DAGScheduler: ResultStage 181 (start at NativeMethodAccessorImpl.java:0) finished in 0.537 s\n",
      "25/05/05 11:22:57 INFO DAGScheduler: Job 182 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:22:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 181: Stage finished\n",
      "25/05/05 11:22:57 INFO DAGScheduler: Job 182 finished: start at NativeMethodAccessorImpl.java:0, took 0.537953 s\n",
      "25/05/05 11:22:57 INFO MemoryStore: Block broadcast_242 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:22:57 INFO MemoryStore: Block broadcast_242_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:22:57 INFO BlockManagerInfo: Added broadcast_242_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:22:57 INFO SparkContext: Created broadcast 242 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:22:57 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/60 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.60.925aca28-449f-436c-a836-e759cbf3ec4e.tmp\n",
      "25/05/05 11:22:57 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.60.925aca28-449f-436c-a836-e759cbf3ec4e.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/60\n",
      "25/05/05 11:22:57 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:22:57.044Z\",\n",
      "  \"batchId\" : 60,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "  \"processedRowsPerSecond\" : 1.4858841010401187,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 575,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 673,\n",
      "    \"walCommit\" : 62\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3291\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3292\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3292\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "    \"processedRowsPerSecond\" : 1.4858841010401187,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 60\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A001|R079|01-00-00|WORLD TRADE CTR|05/05/2025|11:22:55|REGULAR|6          |12        |2025-05-05 11:22:57.041|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:23:03 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/61 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.61.a2d1ad9e-b324-44f4-a775-0bc12386da43.tmp\n",
      "25/05/05 11:23:03 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/61 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.61.9892f0a6-4c24-416e-9c91-88d937b329e5.tmp\n",
      "25/05/05 11:23:03 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/61 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.61.77b6ce46-f142-4557-b23e-1cd6e3238675.tmp\n",
      "25/05/05 11:23:03 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.61.9892f0a6-4c24-416e-9c91-88d937b329e5.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/61\n",
      "25/05/05 11:23:03 INFO MicroBatchExecution: Committed offsets for batch 61. Metadata OffsetSeqMetadata(0,1746458583548,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:23:03 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.61.a2d1ad9e-b324-44f4-a775-0bc12386da43.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/61\n",
      "25/05/05 11:23:03 INFO MicroBatchExecution: Committed offsets for batch 61. Metadata OffsetSeqMetadata(0,1746458583540,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:23:03 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.61.77b6ce46-f142-4557-b23e-1cd6e3238675.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/61\n",
      "25/05/05 11:23:03 INFO MicroBatchExecution: Committed offsets for batch 61. Metadata OffsetSeqMetadata(0,1746458583547,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:23:03 INFO IncrementalExecution: Current batch timestamp = 1746458583540\n",
      "25/05/05 11:23:03 INFO IncrementalExecution: Current batch timestamp = 1746458583548\n",
      "25/05/05 11:23:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:03 INFO IncrementalExecution: Current batch timestamp = 1746458583547\n",
      "25/05/05 11:23:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:03 INFO IncrementalExecution: Current batch timestamp = 1746458583540\n",
      "25/05/05 11:23:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:03 INFO IncrementalExecution: Current batch timestamp = 1746458583548\n",
      "25/05/05 11:23:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:03 INFO IncrementalExecution: Current batch timestamp = 1746458583547\n",
      "25/05/05 11:23:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:03 INFO IncrementalExecution: Current batch timestamp = 1746458583540\n",
      "25/05/05 11:23:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:03 INFO IncrementalExecution: Current batch timestamp = 1746458583547\n",
      "25/05/05 11:23:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:03 INFO CodeGenerator: Code generated in 5.646125 ms\n",
      "25/05/05 11:23:03 INFO CodeGenerator: Code generated in 3.461666 ms\n",
      "25/05/05 11:23:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 61, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:23:03 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:03 INFO DAGScheduler: Got job 183 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:23:03 INFO DAGScheduler: Final stage: ResultStage 182 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:23:03 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:23:03 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:23:03 INFO DAGScheduler: Submitting ResultStage 182 (MapPartitionsRDD[984] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:23:03 INFO MemoryStore: Block broadcast_243 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:23:03 INFO MemoryStore: Block broadcast_243_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:23:03 INFO BlockManagerInfo: Added broadcast_243_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:03 INFO CodeGenerator: Code generated in 4.109792 ms\n",
      "25/05/05 11:23:03 INFO SparkContext: Created broadcast 243 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:23:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 182 (MapPartitionsRDD[984] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:23:03 INFO TaskSchedulerImpl: Adding task set 182.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:23:03 INFO TaskSetManager: Starting task 0.0 in stage 182.0 (TID 182) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:23:03 INFO Executor: Running task 0.0 in stage 182.0 (TID 182)\n",
      "25/05/05 11:23:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 61, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@214b777d]. The input RDD has 1 partitions.\n",
      "25/05/05 11:23:03 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:03 INFO DAGScheduler: Got job 184 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:23:03 INFO DAGScheduler: Final stage: ResultStage 183 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:23:03 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:23:03 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:23:03 INFO DAGScheduler: Submitting ResultStage 183 (MapPartitionsRDD[987] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:23:03 INFO MemoryStore: Block broadcast_244 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:23:03 INFO MemoryStore: Block broadcast_244_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)\n",
      "25/05/05 11:23:03 INFO BlockManagerInfo: Added broadcast_244_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:03 INFO SparkContext: Created broadcast 244 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:23:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 183 (MapPartitionsRDD[987] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:23:03 INFO TaskSchedulerImpl: Adding task set 183.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:23:03 INFO TaskSetManager: Starting task 0.0 in stage 183.0 (TID 183) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:23:03 INFO Executor: Running task 0.0 in stage 183.0 (TID 183)\n",
      "25/05/05 11:23:03 INFO CodeGenerator: Code generated in 4.109542 ms\n",
      "25/05/05 11:23:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3292 untilOffset=3293, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=61 taskId=182 partitionId=0\n",
      "25/05/05 11:23:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3292 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:03 INFO CodeGenerator: Code generated in 4.232334 ms\n",
      "25/05/05 11:23:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3292 untilOffset=3293, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=61 taskId=183 partitionId=0\n",
      "25/05/05 11:23:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3292 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:03 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:03 INFO DAGScheduler: Got job 185 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:23:03 INFO DAGScheduler: Final stage: ResultStage 184 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:23:03 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:23:03 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:23:03 INFO DAGScheduler: Submitting ResultStage 184 (MapPartitionsRDD[992] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:23:03 INFO MemoryStore: Block broadcast_245 stored as values in memory (estimated size 47.1 KiB, free 366.0 MiB)\n",
      "25/05/05 11:23:03 INFO MemoryStore: Block broadcast_245_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:23:03 INFO BlockManagerInfo: Added broadcast_245_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:03 INFO SparkContext: Created broadcast 245 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:23:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 184 (MapPartitionsRDD[992] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:23:03 INFO TaskSchedulerImpl: Adding task set 184.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:23:03 INFO TaskSetManager: Starting task 0.0 in stage 184.0 (TID 184) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:23:03 INFO Executor: Running task 0.0 in stage 184.0 (TID 184)\n",
      "25/05/05 11:23:03 INFO CodeGenerator: Code generated in 3.808708 ms\n",
      "25/05/05 11:23:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3292 untilOffset=3293, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=61 taskId=184 partitionId=0\n",
      "25/05/05 11:23:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3292 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3293, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:04 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:23:04 INFO DataWritingSparkTask: Committed partition 0 (task 182, attempt 0, stage 182.0)\n",
      "25/05/05 11:23:04 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 503472500 nanos, during time span of 503832500 nanos.\n",
      "25/05/05 11:23:04 INFO Executor: Finished task 0.0 in stage 182.0 (TID 182). 2145 bytes result sent to driver\n",
      "25/05/05 11:23:04 INFO TaskSetManager: Finished task 0.0 in stage 182.0 (TID 182) in 513 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:23:04 INFO TaskSchedulerImpl: Removed TaskSet 182.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:23:04 INFO DAGScheduler: ResultStage 182 (start at NativeMethodAccessorImpl.java:0) finished in 0.514 s\n",
      "25/05/05 11:23:04 INFO DAGScheduler: Job 183 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:23:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 182: Stage finished\n",
      "25/05/05 11:23:04 INFO DAGScheduler: Job 183 finished: start at NativeMethodAccessorImpl.java:0, took 0.514900 s\n",
      "25/05/05 11:23:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 61, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:23:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3293, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:04 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:23:04 INFO DataWritingSparkTask: Committed partition 0 (task 183, attempt 0, stage 183.0)\n",
      "25/05/05 11:23:04 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 504912416 nanos, during time span of 508186250 nanos.\n",
      "25/05/05 11:23:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 61, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:23:04 INFO Executor: Finished task 0.0 in stage 183.0 (TID 183). 3516 bytes result sent to driver\n",
      "25/05/05 11:23:04 INFO TaskSetManager: Finished task 0.0 in stage 183.0 (TID 183) in 521 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:23:04 INFO TaskSchedulerImpl: Removed TaskSet 183.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:23:04 INFO DAGScheduler: ResultStage 183 (start at NativeMethodAccessorImpl.java:0) finished in 0.523 s\n",
      "25/05/05 11:23:04 INFO DAGScheduler: Job 184 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:23:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 183: Stage finished\n",
      "25/05/05 11:23:04 INFO DAGScheduler: Job 184 finished: start at NativeMethodAccessorImpl.java:0, took 0.524361 s\n",
      "25/05/05 11:23:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 61, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@214b777d] is committing.\n",
      "25/05/05 11:23:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 61, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@214b777d] committed.\n",
      "25/05/05 11:23:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:04 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/61 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.61.29246190-df41-4779-aa79-9dfe5d5e6329.tmp\n",
      "25/05/05 11:23:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3293, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:04 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:23:04 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/61 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.61.06928e0f-e122-4986-8d8d-f14f3654ffb7.tmp\n",
      "25/05/05 11:23:04 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:23:04 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:23:04 INFO connection: Opened connection [connectionId{localValue:121, serverValue:4491}] to localhost:27017\n",
      "25/05/05 11:23:04 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1106833}\n",
      "25/05/05 11:23:04 INFO connection: Opened connection [connectionId{localValue:122, serverValue:4492}] to localhost:27017\n",
      "25/05/05 11:23:04 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:23:04 INFO connection: Closed connection [connectionId{localValue:122, serverValue:4492}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:23:04 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 507559708 nanos, during time span of 528074416 nanos.\n",
      "25/05/05 11:23:04 INFO Executor: Finished task 0.0 in stage 184.0 (TID 184). 1645 bytes result sent to driver\n",
      "25/05/05 11:23:04 INFO TaskSetManager: Finished task 0.0 in stage 184.0 (TID 184) in 540 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:23:04 INFO TaskSchedulerImpl: Removed TaskSet 184.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:23:04 INFO DAGScheduler: ResultStage 184 (start at NativeMethodAccessorImpl.java:0) finished in 0.544 s\n",
      "25/05/05 11:23:04 INFO DAGScheduler: Job 185 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:23:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 184: Stage finished\n",
      "25/05/05 11:23:04 INFO DAGScheduler: Job 185 finished: start at NativeMethodAccessorImpl.java:0, took 0.544918 s\n",
      "25/05/05 11:23:04 INFO MemoryStore: Block broadcast_246 stored as values in memory (estimated size 248.0 B, free 366.0 MiB)\n",
      "25/05/05 11:23:04 INFO MemoryStore: Block broadcast_246_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.0 MiB)\n",
      "25/05/05 11:23:04 INFO BlockManagerInfo: Added broadcast_246_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:23:04 INFO SparkContext: Created broadcast 246 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:04 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.61.29246190-df41-4779-aa79-9dfe5d5e6329.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/61\n",
      "25/05/05 11:23:04 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:23:03.538Z\",\n",
      "  \"batchId\" : 61,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.5267175572519083,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 540,\n",
      "    \"commitOffsets\" : 43,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 655,\n",
      "    \"walCommit\" : 66\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3292\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3293\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3293\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.5267175572519083,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:23:04 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.61.06928e0f-e122-4986-8d8d-f14f3654ffb7.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/61\n",
      "25/05/05 11:23:04 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:23:03.545Z\",\n",
      "  \"batchId\" : 61,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.5408320493066254,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 541,\n",
      "    \"commitOffsets\" : 40,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 649,\n",
      "    \"walCommit\" : 61\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3292\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3293\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3293\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.5408320493066254,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:23:04 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/61 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.61.8dad060f-d08b-407a-9e6c-d8562d0b81cc.tmp\n",
      "25/05/05 11:23:04 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.61.8dad060f-d08b-407a-9e6c-d8562d0b81cc.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/61\n",
      "25/05/05 11:23:04 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:23:03.545Z\",\n",
      "  \"batchId\" : 61,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.4749262536873156,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 581,\n",
      "    \"commitOffsets\" : 31,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 678,\n",
      "    \"walCommit\" : 57\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3292\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3293\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3293\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.4749262536873156,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 61\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time           |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+----------------------+\n",
      "|A001|R079|01-00-00|WORLD TRADE CTR|05/05/2025|11:23:02|REGULAR|5          |4         |2025-05-05 11:23:03.54|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:23:07 INFO BlockManagerInfo: Removed broadcast_244_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:07 INFO BlockManagerInfo: Removed broadcast_240_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:07 INFO BlockManagerInfo: Removed broadcast_239_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:07 INFO BlockManagerInfo: Removed broadcast_241_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:07 INFO BlockManagerInfo: Removed broadcast_242_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:23:07 INFO BlockManagerInfo: Removed broadcast_246_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:23:07 INFO BlockManagerInfo: Removed broadcast_243_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:23:07 INFO BlockManagerInfo: Removed broadcast_245_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:23:09 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/62 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.62.f13decb2-25b7-4b3a-a465-7808893ab26c.tmp\n",
      "25/05/05 11:23:09 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/62 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.62.30e928e0-b50d-4d11-8390-1819ef3e68c2.tmp\n",
      "25/05/05 11:23:09 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/62 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.62.eba81b23-c5f6-4407-b70d-ae7dfd98713c.tmp\n",
      "25/05/05 11:23:09 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.62.f13decb2-25b7-4b3a-a465-7808893ab26c.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/62\n",
      "25/05/05 11:23:09 INFO MicroBatchExecution: Committed offsets for batch 62. Metadata OffsetSeqMetadata(0,1746458589035,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:23:09 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.62.30e928e0-b50d-4d11-8390-1819ef3e68c2.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/62\n",
      "25/05/05 11:23:09 INFO MicroBatchExecution: Committed offsets for batch 62. Metadata OffsetSeqMetadata(0,1746458589035,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:23:09 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.62.eba81b23-c5f6-4407-b70d-ae7dfd98713c.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/62\n",
      "25/05/05 11:23:09 INFO MicroBatchExecution: Committed offsets for batch 62. Metadata OffsetSeqMetadata(0,1746458589048,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:23:09 INFO IncrementalExecution: Current batch timestamp = 1746458589035\n",
      "25/05/05 11:23:09 INFO IncrementalExecution: Current batch timestamp = 1746458589035\n",
      "25/05/05 11:23:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:09 INFO IncrementalExecution: Current batch timestamp = 1746458589048\n",
      "25/05/05 11:23:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:09 INFO IncrementalExecution: Current batch timestamp = 1746458589035\n",
      "25/05/05 11:23:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:09 INFO IncrementalExecution: Current batch timestamp = 1746458589035\n",
      "25/05/05 11:23:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:09 INFO IncrementalExecution: Current batch timestamp = 1746458589048\n",
      "25/05/05 11:23:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:09 INFO IncrementalExecution: Current batch timestamp = 1746458589035\n",
      "25/05/05 11:23:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:09 INFO IncrementalExecution: Current batch timestamp = 1746458589048\n",
      "25/05/05 11:23:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:09 INFO CodeGenerator: Code generated in 4.087666 ms\n",
      "25/05/05 11:23:09 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 62, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@74900547]. The input RDD has 1 partitions.\n",
      "25/05/05 11:23:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:09 INFO DAGScheduler: Got job 186 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:23:09 INFO DAGScheduler: Final stage: ResultStage 185 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:23:09 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:23:09 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:23:09 INFO DAGScheduler: Submitting ResultStage 185 (MapPartitionsRDD[1000] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:23:09 INFO MemoryStore: Block broadcast_247 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:23:09 INFO MemoryStore: Block broadcast_247_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:23:09 INFO CodeGenerator: Code generated in 3.90875 ms\n",
      "25/05/05 11:23:09 INFO BlockManagerInfo: Added broadcast_247_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:23:09 INFO SparkContext: Created broadcast 247 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:23:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 185 (MapPartitionsRDD[1000] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:23:09 INFO TaskSchedulerImpl: Adding task set 185.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:23:09 INFO TaskSetManager: Starting task 0.0 in stage 185.0 (TID 185) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:23:09 INFO Executor: Running task 0.0 in stage 185.0 (TID 185)\n",
      "25/05/05 11:23:09 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 62, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:23:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:09 INFO DAGScheduler: Got job 187 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:23:09 INFO DAGScheduler: Final stage: ResultStage 186 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:23:09 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:23:09 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:23:09 INFO DAGScheduler: Submitting ResultStage 186 (MapPartitionsRDD[1003] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:23:09 INFO MemoryStore: Block broadcast_248 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:23:09 INFO MemoryStore: Block broadcast_248_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:23:09 INFO BlockManagerInfo: Added broadcast_248_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:23:09 INFO SparkContext: Created broadcast 248 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:23:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 186 (MapPartitionsRDD[1003] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:23:09 INFO TaskSchedulerImpl: Adding task set 186.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:23:09 INFO TaskSetManager: Starting task 0.0 in stage 186.0 (TID 186) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:23:09 INFO Executor: Running task 0.0 in stage 186.0 (TID 186)\n",
      "25/05/05 11:23:09 INFO CodeGenerator: Code generated in 4.439917 ms\n",
      "25/05/05 11:23:09 INFO CodeGenerator: Code generated in 3.464167 ms\n",
      "25/05/05 11:23:09 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3293 untilOffset=3294, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=62 taskId=185 partitionId=0\n",
      "25/05/05 11:23:09 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3293 untilOffset=3294, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=62 taskId=186 partitionId=0\n",
      "25/05/05 11:23:09 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3293 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:09 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3293 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:09 INFO DAGScheduler: Got job 188 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:23:09 INFO DAGScheduler: Final stage: ResultStage 187 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:23:09 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:23:09 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:23:09 INFO DAGScheduler: Submitting ResultStage 187 (MapPartitionsRDD[1008] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:23:09 INFO MemoryStore: Block broadcast_249 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:23:09 INFO MemoryStore: Block broadcast_249_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:23:09 INFO BlockManagerInfo: Added broadcast_249_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:09 INFO SparkContext: Created broadcast 249 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:23:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 187 (MapPartitionsRDD[1008] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:23:09 INFO TaskSchedulerImpl: Adding task set 187.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:23:09 INFO TaskSetManager: Starting task 0.0 in stage 187.0 (TID 187) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:23:09 INFO Executor: Running task 0.0 in stage 187.0 (TID 187)\n",
      "25/05/05 11:23:09 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3293 untilOffset=3294, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=62 taskId=187 partitionId=0\n",
      "25/05/05 11:23:09 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3293 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3294, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3294, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:09 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:23:09 INFO DataWritingSparkTask: Committed partition 0 (task 186, attempt 0, stage 186.0)\n",
      "25/05/05 11:23:09 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 505001959 nanos, during time span of 505391833 nanos.\n",
      "25/05/05 11:23:09 INFO Executor: Finished task 0.0 in stage 186.0 (TID 186). 2145 bytes result sent to driver\n",
      "25/05/05 11:23:09 INFO TaskSetManager: Finished task 0.0 in stage 186.0 (TID 186) in 513 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:23:09 INFO TaskSchedulerImpl: Removed TaskSet 186.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:23:09 INFO DAGScheduler: ResultStage 186 (start at NativeMethodAccessorImpl.java:0) finished in 0.515 s\n",
      "25/05/05 11:23:09 INFO DAGScheduler: Job 187 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:23:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 186: Stage finished\n",
      "25/05/05 11:23:09 INFO DAGScheduler: Job 187 finished: start at NativeMethodAccessorImpl.java:0, took 0.515200 s\n",
      "25/05/05 11:23:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 62, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:23:09 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:23:09 INFO DataWritingSparkTask: Committed partition 0 (task 185, attempt 0, stage 185.0)\n",
      "25/05/05 11:23:09 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 505293167 nanos, during time span of 507133000 nanos.\n",
      "25/05/05 11:23:09 INFO Executor: Finished task 0.0 in stage 185.0 (TID 185). 3515 bytes result sent to driver\n",
      "25/05/05 11:23:09 INFO TaskSetManager: Finished task 0.0 in stage 185.0 (TID 185) in 518 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:23:09 INFO TaskSchedulerImpl: Removed TaskSet 185.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:23:09 INFO DAGScheduler: ResultStage 185 (start at NativeMethodAccessorImpl.java:0) finished in 0.519 s\n",
      "25/05/05 11:23:09 INFO DAGScheduler: Job 186 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:23:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 185: Stage finished\n",
      "25/05/05 11:23:09 INFO DAGScheduler: Job 186 finished: start at NativeMethodAccessorImpl.java:0, took 0.519595 s\n",
      "25/05/05 11:23:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 62, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@74900547] is committing.\n",
      "25/05/05 11:23:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 62, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@74900547] committed.\n",
      "25/05/05 11:23:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 62, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:23:09 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/62 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.62.fe37e59a-18cb-4b29-b291-9a188df59a54.tmp\n",
      "25/05/05 11:23:09 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/62 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.62.9fac4da4-76c0-4d3c-a985-9ede9856c684.tmp\n",
      "25/05/05 11:23:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3294, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:09 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:23:09 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:23:09 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:23:09 INFO connection: Opened connection [connectionId{localValue:123, serverValue:4493}] to localhost:27017\n",
      "25/05/05 11:23:09 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=270417}\n",
      "25/05/05 11:23:09 INFO connection: Opened connection [connectionId{localValue:124, serverValue:4494}] to localhost:27017\n",
      "25/05/05 11:23:09 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:23:09 INFO connection: Closed connection [connectionId{localValue:124, serverValue:4494}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:23:09 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 508021666 nanos, during time span of 512190875 nanos.\n",
      "25/05/05 11:23:09 INFO Executor: Finished task 0.0 in stage 187.0 (TID 187). 1645 bytes result sent to driver\n",
      "25/05/05 11:23:09 INFO TaskSetManager: Finished task 0.0 in stage 187.0 (TID 187) in 518 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:23:09 INFO TaskSchedulerImpl: Removed TaskSet 187.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:23:09 INFO DAGScheduler: ResultStage 187 (start at NativeMethodAccessorImpl.java:0) finished in 0.523 s\n",
      "25/05/05 11:23:09 INFO DAGScheduler: Job 188 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:23:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 187: Stage finished\n",
      "25/05/05 11:23:09 INFO DAGScheduler: Job 188 finished: start at NativeMethodAccessorImpl.java:0, took 0.523082 s\n",
      "25/05/05 11:23:09 INFO MemoryStore: Block broadcast_250 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:23:09 INFO MemoryStore: Block broadcast_250_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:23:09 INFO BlockManagerInfo: Added broadcast_250_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:23:09 INFO SparkContext: Created broadcast 250 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:09 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/62 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.62.be62dd34-5554-4bf5-a552-3a10e4edd752.tmp\n",
      "25/05/05 11:23:09 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.62.fe37e59a-18cb-4b29-b291-9a188df59a54.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/62\n",
      "25/05/05 11:23:09 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:23:09.034Z\",\n",
      "  \"batchId\" : 62,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.5923566878980893,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 531,\n",
      "    \"commitOffsets\" : 32,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 628,\n",
      "    \"walCommit\" : 59\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3293\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3294\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3294\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.5923566878980893,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:23:09 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.62.9fac4da4-76c0-4d3c-a985-9ede9856c684.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/62\n",
      "25/05/05 11:23:09 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:23:09.044Z\",\n",
      "  \"batchId\" : 62,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 55.55555555555556,\n",
      "  \"processedRowsPerSecond\" : 1.6051364365971108,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 534,\n",
      "    \"commitOffsets\" : 32,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 4,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 623,\n",
      "    \"walCommit\" : 49\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3293\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3294\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3294\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 55.55555555555556,\n",
      "    \"processedRowsPerSecond\" : 1.6051364365971108,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:23:09 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.62.be62dd34-5554-4bf5-a552-3a10e4edd752.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/62\n",
      "25/05/05 11:23:09 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:23:09.034Z\",\n",
      "  \"batchId\" : 62,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.5408320493066254,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 554,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 649,\n",
      "    \"walCommit\" : 59\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3293\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3294\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3294\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.5408320493066254,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 62\n",
      "-------------------------------------------\n",
      "+----+----+--------+--------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION       |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+--------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A002|R051|02-00-00|34 ST-PENN STA|05/05/2025|11:23:07|REGULAR|3          |8         |2025-05-05 11:23:09.048|\n",
      "+----+----+--------+--------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:23:13 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/63 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.63.3f277e80-a01f-4c11-8b70-f9e70ceef45f.tmp\n",
      "25/05/05 11:23:13 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/63 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.63.b4dae309-df59-4da4-9a87-f4727541111a.tmp\n",
      "25/05/05 11:23:13 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/63 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.63.888db4aa-349d-4f48-9a2f-e7b6df2ca674.tmp\n",
      "25/05/05 11:23:13 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.63.3f277e80-a01f-4c11-8b70-f9e70ceef45f.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/63\n",
      "25/05/05 11:23:13 INFO MicroBatchExecution: Committed offsets for batch 63. Metadata OffsetSeqMetadata(0,1746458593499,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:23:13 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.63.888db4aa-349d-4f48-9a2f-e7b6df2ca674.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/63\n",
      "25/05/05 11:23:13 INFO MicroBatchExecution: Committed offsets for batch 63. Metadata OffsetSeqMetadata(0,1746458593502,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:23:13 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.63.b4dae309-df59-4da4-9a87-f4727541111a.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/63\n",
      "25/05/05 11:23:13 INFO MicroBatchExecution: Committed offsets for batch 63. Metadata OffsetSeqMetadata(0,1746458593501,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:23:13 INFO IncrementalExecution: Current batch timestamp = 1746458593499\n",
      "25/05/05 11:23:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:13 INFO IncrementalExecution: Current batch timestamp = 1746458593502\n",
      "25/05/05 11:23:13 INFO IncrementalExecution: Current batch timestamp = 1746458593501\n",
      "25/05/05 11:23:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:13 INFO IncrementalExecution: Current batch timestamp = 1746458593499\n",
      "25/05/05 11:23:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:13 INFO IncrementalExecution: Current batch timestamp = 1746458593501\n",
      "25/05/05 11:23:13 INFO IncrementalExecution: Current batch timestamp = 1746458593502\n",
      "25/05/05 11:23:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:13 INFO IncrementalExecution: Current batch timestamp = 1746458593501\n",
      "25/05/05 11:23:13 INFO IncrementalExecution: Current batch timestamp = 1746458593499\n",
      "25/05/05 11:23:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:13 INFO CodeGenerator: Code generated in 3.908625 ms\n",
      "25/05/05 11:23:13 INFO CodeGenerator: Code generated in 3.532917 ms\n",
      "25/05/05 11:23:13 INFO CodeGenerator: Code generated in 3.7085 ms\n",
      "25/05/05 11:23:13 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 63, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@473cb9a2]. The input RDD has 1 partitions.\n",
      "25/05/05 11:23:13 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 63, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:23:13 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:13 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:13 INFO DAGScheduler: Got job 189 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:23:13 INFO DAGScheduler: Final stage: ResultStage 188 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:23:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:23:13 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:23:13 INFO DAGScheduler: Submitting ResultStage 188 (MapPartitionsRDD[1018] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:23:13 INFO MemoryStore: Block broadcast_251 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:23:13 INFO MemoryStore: Block broadcast_251_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.1 MiB)\n",
      "25/05/05 11:23:13 INFO BlockManagerInfo: Added broadcast_251_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:13 INFO SparkContext: Created broadcast 251 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:23:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 188 (MapPartitionsRDD[1018] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:23:13 INFO TaskSchedulerImpl: Adding task set 188.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:23:13 INFO DAGScheduler: Got job 190 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:23:13 INFO DAGScheduler: Final stage: ResultStage 189 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:23:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:23:13 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:23:13 INFO DAGScheduler: Submitting ResultStage 189 (MapPartitionsRDD[1019] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:23:13 INFO TaskSetManager: Starting task 0.0 in stage 188.0 (TID 188) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:23:13 INFO Executor: Running task 0.0 in stage 188.0 (TID 188)\n",
      "25/05/05 11:23:13 INFO MemoryStore: Block broadcast_252 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:23:13 INFO MemoryStore: Block broadcast_252_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:23:13 INFO BlockManagerInfo: Added broadcast_252_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:13 INFO SparkContext: Created broadcast 252 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:23:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 189 (MapPartitionsRDD[1019] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:23:13 INFO TaskSchedulerImpl: Adding task set 189.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:23:13 INFO TaskSetManager: Starting task 0.0 in stage 189.0 (TID 189) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:23:13 INFO Executor: Running task 0.0 in stage 189.0 (TID 189)\n",
      "25/05/05 11:23:13 INFO CodeGenerator: Code generated in 3.999 ms\n",
      "25/05/05 11:23:13 INFO CodeGenerator: Code generated in 4.811 ms\n",
      "25/05/05 11:23:13 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3294 untilOffset=3295, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=63 taskId=188 partitionId=0\n",
      "25/05/05 11:23:13 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3294 untilOffset=3295, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=63 taskId=189 partitionId=0\n",
      "25/05/05 11:23:13 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3294 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:13 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3294 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:13 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:13 INFO DAGScheduler: Got job 191 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:23:13 INFO DAGScheduler: Final stage: ResultStage 190 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:23:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:23:13 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:23:13 INFO DAGScheduler: Submitting ResultStage 190 (MapPartitionsRDD[1024] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:23:13 INFO MemoryStore: Block broadcast_253 stored as values in memory (estimated size 47.1 KiB, free 366.0 MiB)\n",
      "25/05/05 11:23:13 INFO MemoryStore: Block broadcast_253_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:23:13 INFO BlockManagerInfo: Added broadcast_253_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:13 INFO SparkContext: Created broadcast 253 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:23:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 190 (MapPartitionsRDD[1024] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:23:13 INFO TaskSchedulerImpl: Adding task set 190.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:23:13 INFO TaskSetManager: Starting task 0.0 in stage 190.0 (TID 190) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:23:13 INFO Executor: Running task 0.0 in stage 190.0 (TID 190)\n",
      "25/05/05 11:23:13 INFO CodeGenerator: Code generated in 3.611542 ms\n",
      "25/05/05 11:23:13 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3294 untilOffset=3295, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=63 taskId=190 partitionId=0\n",
      "25/05/05 11:23:13 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3294 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3295, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3295, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:14 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:23:14 INFO DataWritingSparkTask: Committed partition 0 (task 189, attempt 0, stage 189.0)\n",
      "25/05/05 11:23:14 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 503959917 nanos, during time span of 504318417 nanos.\n",
      "25/05/05 11:23:14 INFO Executor: Finished task 0.0 in stage 189.0 (TID 189). 2188 bytes result sent to driver\n",
      "25/05/05 11:23:14 INFO TaskSetManager: Finished task 0.0 in stage 189.0 (TID 189) in 514 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:23:14 INFO TaskSchedulerImpl: Removed TaskSet 189.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:23:14 INFO DAGScheduler: ResultStage 189 (start at NativeMethodAccessorImpl.java:0) finished in 0.515 s\n",
      "25/05/05 11:23:14 INFO DAGScheduler: Job 190 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:23:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 189: Stage finished\n",
      "25/05/05 11:23:14 INFO DAGScheduler: Job 190 finished: start at NativeMethodAccessorImpl.java:0, took 0.517040 s\n",
      "25/05/05 11:23:14 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 63, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:23:14 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:23:14 INFO DataWritingSparkTask: Committed partition 0 (task 188, attempt 0, stage 188.0)\n",
      "25/05/05 11:23:14 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 504344750 nanos, during time span of 506156083 nanos.\n",
      "25/05/05 11:23:14 INFO Executor: Finished task 0.0 in stage 188.0 (TID 188). 3559 bytes result sent to driver\n",
      "25/05/05 11:23:14 INFO TaskSetManager: Finished task 0.0 in stage 188.0 (TID 188) in 516 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:23:14 INFO TaskSchedulerImpl: Removed TaskSet 188.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:23:14 INFO DAGScheduler: ResultStage 188 (start at NativeMethodAccessorImpl.java:0) finished in 0.518 s\n",
      "25/05/05 11:23:14 INFO DAGScheduler: Job 189 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:23:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 188: Stage finished\n",
      "25/05/05 11:23:14 INFO DAGScheduler: Job 189 finished: start at NativeMethodAccessorImpl.java:0, took 0.518499 s\n",
      "25/05/05 11:23:14 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 63, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@473cb9a2] is committing.\n",
      "25/05/05 11:23:14 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 63, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@473cb9a2] committed.\n",
      "25/05/05 11:23:14 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 63, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:23:14 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/63 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.63.9781c534-8082-4c1e-acac-fbac0d5a6c82.tmp\n",
      "25/05/05 11:23:14 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/63 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.63.937f8316-4489-489a-b9dc-fc72467a09ad.tmp\n",
      "25/05/05 11:23:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3295, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:14 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:23:14 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:23:14 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:23:14 INFO connection: Opened connection [connectionId{localValue:125, serverValue:4495}] to localhost:27017\n",
      "25/05/05 11:23:14 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=336042}\n",
      "25/05/05 11:23:14 INFO connection: Opened connection [connectionId{localValue:126, serverValue:4496}] to localhost:27017\n",
      "25/05/05 11:23:14 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:23:14 INFO connection: Closed connection [connectionId{localValue:126, serverValue:4496}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:23:14 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 502829000 nanos, during time span of 507397167 nanos.\n",
      "25/05/05 11:23:14 INFO Executor: Finished task 0.0 in stage 190.0 (TID 190). 1645 bytes result sent to driver\n",
      "25/05/05 11:23:14 INFO TaskSetManager: Finished task 0.0 in stage 190.0 (TID 190) in 518 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:23:14 INFO TaskSchedulerImpl: Removed TaskSet 190.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:23:14 INFO DAGScheduler: ResultStage 190 (start at NativeMethodAccessorImpl.java:0) finished in 0.521 s\n",
      "25/05/05 11:23:14 INFO DAGScheduler: Job 191 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:23:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 190: Stage finished\n",
      "25/05/05 11:23:14 INFO DAGScheduler: Job 191 finished: start at NativeMethodAccessorImpl.java:0, took 0.521805 s\n",
      "25/05/05 11:23:14 INFO MemoryStore: Block broadcast_254 stored as values in memory (estimated size 248.0 B, free 366.0 MiB)\n",
      "25/05/05 11:23:14 INFO MemoryStore: Block broadcast_254_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.0 MiB)\n",
      "25/05/05 11:23:14 INFO BlockManagerInfo: Added broadcast_254_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:23:14 INFO SparkContext: Created broadcast 254 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:14 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/63 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.63.672f490b-2146-41c4-b014-7911a4fd1a1e.tmp\n",
      "25/05/05 11:23:14 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.63.9781c534-8082-4c1e-acac-fbac0d5a6c82.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/63\n",
      "25/05/05 11:23:14 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:23:13.496Z\",\n",
      "  \"batchId\" : 63,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.6051364365971108,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 532,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 623,\n",
      "    \"walCommit\" : 53\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3294\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3295\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3295\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.6051364365971108,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:23:14 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.63.937f8316-4489-489a-b9dc-fc72467a09ad.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/63\n",
      "25/05/05 11:23:14 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:23:13.497Z\",\n",
      "  \"batchId\" : 63,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.597444089456869,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 537,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 4,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 626,\n",
      "    \"walCommit\" : 53\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3294\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3295\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3295\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.597444089456869,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:23:14 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.63.672f490b-2146-41c4-b014-7911a4fd1a1e.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/63\n",
      "25/05/05 11:23:14 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:23:13.497Z\",\n",
      "  \"batchId\" : 63,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.567398119122257,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 552,\n",
      "    \"commitOffsets\" : 25,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 5,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 638,\n",
      "    \"walCommit\" : 52\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3294\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3295\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3295\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.567398119122257,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 63\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R023|R045|00-00-00|34 ST-HERALD SQ|05/05/2025|11:23:12|REGULAR|4          |3         |2025-05-05 11:23:13.501|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:23:18 INFO BlockManagerInfo: Removed broadcast_251_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:18 INFO BlockManagerInfo: Removed broadcast_249_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:18 INFO BlockManagerInfo: Removed broadcast_248_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:18 INFO BlockManagerInfo: Removed broadcast_253_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:23:18 INFO BlockManagerInfo: Removed broadcast_252_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:23:18 INFO BlockManagerInfo: Removed broadcast_254_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:23:18 INFO BlockManagerInfo: Removed broadcast_250_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:23:18 INFO BlockManagerInfo: Removed broadcast_247_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:23:20 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/64 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.64.970f7323-cc9a-4101-8df0-5c6232ef552a.tmp\n",
      "25/05/05 11:23:20 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/64 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.64.ef33fac9-1ced-47c4-b78c-ca640c887533.tmp\n",
      "25/05/05 11:23:20 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/64 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.64.fd7b59ca-85ec-40d1-96c4-049e5753e597.tmp\n",
      "25/05/05 11:23:20 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.64.970f7323-cc9a-4101-8df0-5c6232ef552a.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/64\n",
      "25/05/05 11:23:20 INFO MicroBatchExecution: Committed offsets for batch 64. Metadata OffsetSeqMetadata(0,1746458599972,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:23:20 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.64.ef33fac9-1ced-47c4-b78c-ca640c887533.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/64\n",
      "25/05/05 11:23:20 INFO MicroBatchExecution: Committed offsets for batch 64. Metadata OffsetSeqMetadata(0,1746458599972,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:23:20 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.64.fd7b59ca-85ec-40d1-96c4-049e5753e597.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/64\n",
      "25/05/05 11:23:20 INFO MicroBatchExecution: Committed offsets for batch 64. Metadata OffsetSeqMetadata(0,1746458599979,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:23:20 INFO IncrementalExecution: Current batch timestamp = 1746458599972\n",
      "25/05/05 11:23:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:20 INFO IncrementalExecution: Current batch timestamp = 1746458599972\n",
      "25/05/05 11:23:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:20 INFO IncrementalExecution: Current batch timestamp = 1746458599979\n",
      "25/05/05 11:23:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:20 INFO IncrementalExecution: Current batch timestamp = 1746458599972\n",
      "25/05/05 11:23:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:20 INFO IncrementalExecution: Current batch timestamp = 1746458599972\n",
      "25/05/05 11:23:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:20 INFO IncrementalExecution: Current batch timestamp = 1746458599979\n",
      "25/05/05 11:23:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:20 INFO IncrementalExecution: Current batch timestamp = 1746458599972\n",
      "25/05/05 11:23:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:20 INFO IncrementalExecution: Current batch timestamp = 1746458599979\n",
      "25/05/05 11:23:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:20 INFO CodeGenerator: Code generated in 4.397375 ms\n",
      "25/05/05 11:23:20 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 64, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:23:20 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:20 INFO DAGScheduler: Got job 192 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:23:20 INFO DAGScheduler: Final stage: ResultStage 191 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:23:20 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:23:20 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:23:20 INFO DAGScheduler: Submitting ResultStage 191 (MapPartitionsRDD[1032] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:23:20 INFO MemoryStore: Block broadcast_255 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:23:20 INFO MemoryStore: Block broadcast_255_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:23:20 INFO BlockManagerInfo: Added broadcast_255_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:23:20 INFO SparkContext: Created broadcast 255 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:23:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 191 (MapPartitionsRDD[1032] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:23:20 INFO TaskSchedulerImpl: Adding task set 191.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:23:20 INFO TaskSetManager: Starting task 0.0 in stage 191.0 (TID 191) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:23:20 INFO Executor: Running task 0.0 in stage 191.0 (TID 191)\n",
      "25/05/05 11:23:20 INFO CodeGenerator: Code generated in 4.681542 ms\n",
      "25/05/05 11:23:20 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 64, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@70f75ab1]. The input RDD has 1 partitions.\n",
      "25/05/05 11:23:20 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:20 INFO DAGScheduler: Got job 193 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:23:20 INFO DAGScheduler: Final stage: ResultStage 192 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:23:20 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:23:20 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:23:20 INFO DAGScheduler: Submitting ResultStage 192 (MapPartitionsRDD[1035] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:23:20 INFO CodeGenerator: Code generated in 3.982334 ms\n",
      "25/05/05 11:23:20 INFO MemoryStore: Block broadcast_256 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:23:20 INFO MemoryStore: Block broadcast_256_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:23:20 INFO BlockManagerInfo: Added broadcast_256_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:23:20 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3295 untilOffset=3296, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=64 taskId=191 partitionId=0\n",
      "25/05/05 11:23:20 INFO SparkContext: Created broadcast 256 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:23:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 192 (MapPartitionsRDD[1035] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:23:20 INFO TaskSchedulerImpl: Adding task set 192.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:23:20 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3295 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:20 INFO TaskSetManager: Starting task 0.0 in stage 192.0 (TID 192) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:23:20 INFO Executor: Running task 0.0 in stage 192.0 (TID 192)\n",
      "25/05/05 11:23:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:20 INFO CodeGenerator: Code generated in 4.077667 ms\n",
      "25/05/05 11:23:20 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:20 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3295 untilOffset=3296, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=64 taskId=192 partitionId=0\n",
      "25/05/05 11:23:20 INFO DAGScheduler: Got job 194 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:23:20 INFO DAGScheduler: Final stage: ResultStage 193 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:23:20 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:23:20 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:23:20 INFO DAGScheduler: Submitting ResultStage 193 (MapPartitionsRDD[1040] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:23:20 INFO MemoryStore: Block broadcast_257 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:23:20 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3295 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:20 INFO MemoryStore: Block broadcast_257_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.1 MiB)\n",
      "25/05/05 11:23:20 INFO BlockManagerInfo: Added broadcast_257_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:20 INFO SparkContext: Created broadcast 257 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:23:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 193 (MapPartitionsRDD[1040] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:23:20 INFO TaskSchedulerImpl: Adding task set 193.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:23:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:20 INFO TaskSetManager: Starting task 0.0 in stage 193.0 (TID 193) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:23:20 INFO Executor: Running task 0.0 in stage 193.0 (TID 193)\n",
      "25/05/05 11:23:20 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3295 untilOffset=3296, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=64 taskId=193 partitionId=0\n",
      "25/05/05 11:23:20 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3295 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3296, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:20 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:23:20 INFO DataWritingSparkTask: Committed partition 0 (task 191, attempt 0, stage 191.0)\n",
      "25/05/05 11:23:20 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 504463916 nanos, during time span of 504818166 nanos.\n",
      "25/05/05 11:23:20 INFO Executor: Finished task 0.0 in stage 191.0 (TID 191). 2188 bytes result sent to driver\n",
      "25/05/05 11:23:20 INFO TaskSetManager: Finished task 0.0 in stage 191.0 (TID 191) in 515 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:23:20 INFO TaskSchedulerImpl: Removed TaskSet 191.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:23:20 INFO DAGScheduler: ResultStage 191 (start at NativeMethodAccessorImpl.java:0) finished in 0.517 s\n",
      "25/05/05 11:23:20 INFO DAGScheduler: Job 192 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:23:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 191: Stage finished\n",
      "25/05/05 11:23:20 INFO DAGScheduler: Job 192 finished: start at NativeMethodAccessorImpl.java:0, took 0.518205 s\n",
      "25/05/05 11:23:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 64, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:23:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 64, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:23:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3296, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:20 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/64 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.64.1486ff6b-919b-47ac-9b31-fde0fb84ffc3.tmp\n",
      "25/05/05 11:23:20 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:23:20 INFO DataWritingSparkTask: Committed partition 0 (task 192, attempt 0, stage 192.0)\n",
      "25/05/05 11:23:20 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 506251958 nanos, during time span of 508016458 nanos.\n",
      "25/05/05 11:23:20 INFO Executor: Finished task 0.0 in stage 192.0 (TID 192). 3558 bytes result sent to driver\n",
      "25/05/05 11:23:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:20 INFO TaskSetManager: Finished task 0.0 in stage 192.0 (TID 192) in 525 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:23:20 INFO TaskSchedulerImpl: Removed TaskSet 192.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:23:20 INFO DAGScheduler: ResultStage 192 (start at NativeMethodAccessorImpl.java:0) finished in 0.529 s\n",
      "25/05/05 11:23:20 INFO DAGScheduler: Job 193 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:23:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 192: Stage finished\n",
      "25/05/05 11:23:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3296, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:20 INFO DAGScheduler: Job 193 finished: start at NativeMethodAccessorImpl.java:0, took 0.530614 s\n",
      "25/05/05 11:23:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 64, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@70f75ab1] is committing.\n",
      "25/05/05 11:23:20 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:23:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 64, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@70f75ab1] committed.\n",
      "25/05/05 11:23:20 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:23:20 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:23:20 INFO connection: Opened connection [connectionId{localValue:127, serverValue:4497}] to localhost:27017\n",
      "25/05/05 11:23:20 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=291792}\n",
      "25/05/05 11:23:20 INFO connection: Opened connection [connectionId{localValue:128, serverValue:4498}] to localhost:27017\n",
      "25/05/05 11:23:20 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:23:20 INFO connection: Closed connection [connectionId{localValue:128, serverValue:4498}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:23:20 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 502928333 nanos, during time span of 507965834 nanos.\n",
      "25/05/05 11:23:20 INFO Executor: Finished task 0.0 in stage 193.0 (TID 193). 1645 bytes result sent to driver\n",
      "25/05/05 11:23:20 INFO TaskSetManager: Finished task 0.0 in stage 193.0 (TID 193) in 514 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:23:20 INFO TaskSchedulerImpl: Removed TaskSet 193.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:23:20 INFO DAGScheduler: ResultStage 193 (start at NativeMethodAccessorImpl.java:0) finished in 0.519 s\n",
      "25/05/05 11:23:20 INFO DAGScheduler: Job 194 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:23:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 193: Stage finished\n",
      "25/05/05 11:23:20 INFO DAGScheduler: Job 194 finished: start at NativeMethodAccessorImpl.java:0, took 0.521105 s\n",
      "25/05/05 11:23:20 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/64 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.64.26f724e5-5882-4145-b52a-804a36adcefd.tmp\n",
      "25/05/05 11:23:20 INFO MemoryStore: Block broadcast_258 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:23:20 INFO MemoryStore: Block broadcast_258_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:23:20 INFO BlockManagerInfo: Added broadcast_258_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:23:20 INFO SparkContext: Created broadcast 258 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:20 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/64 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.64.ff3e9453-f33a-45a3-b642-0249bf46ecff.tmp\n",
      "25/05/05 11:23:20 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.64.1486ff6b-919b-47ac-9b31-fde0fb84ffc3.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/64\n",
      "25/05/05 11:23:20 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:23:19.970Z\",\n",
      "  \"batchId\" : 64,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.5408320493066254,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 538,\n",
      "    \"commitOffsets\" : 34,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 9,\n",
      "    \"triggerExecution\" : 649,\n",
      "    \"walCommit\" : 65\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3295\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3296\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3296\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.5408320493066254,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:23:20 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.64.26f724e5-5882-4145-b52a-804a36adcefd.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/64\n",
      "25/05/05 11:23:20 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:23:19.973Z\",\n",
      "  \"batchId\" : 64,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "  \"processedRowsPerSecond\" : 1.524390243902439,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 546,\n",
      "    \"commitOffsets\" : 34,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 6,\n",
      "    \"queryPlanning\" : 8,\n",
      "    \"triggerExecution\" : 656,\n",
      "    \"walCommit\" : 61\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3295\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3296\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3296\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "    \"processedRowsPerSecond\" : 1.524390243902439,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:23:20 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.64.ff3e9453-f33a-45a3-b642-0249bf46ecff.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/64\n",
      "25/05/05 11:23:20 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:23:19.970Z\",\n",
      "  \"batchId\" : 64,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.5037593984962405,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 556,\n",
      "    \"commitOffsets\" : 33,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 6,\n",
      "    \"triggerExecution\" : 665,\n",
      "    \"walCommit\" : 68\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3295\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3296\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3296\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.5037593984962405,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 64\n",
      "-------------------------------------------\n",
      "+----+----+--------+--------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION       |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+--------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A002|R051|02-00-00|34 ST-PENN STA|05/05/2025|11:23:18|REGULAR|11         |13        |2025-05-05 11:23:19.972|\n",
      "+----+----+--------+--------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:23:26 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/65 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.65.4a357f4b-2ef1-4420-aa1d-bb168e0b1bf0.tmp\n",
      "25/05/05 11:23:26 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/65 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.65.26119ddd-3d4b-465c-ae42-bbf3bc9d45f2.tmp\n",
      "25/05/05 11:23:26 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/65 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.65.32e9c16e-ab9c-4549-aba3-1a6a803a08d8.tmp\n",
      "25/05/05 11:23:26 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.65.4a357f4b-2ef1-4420-aa1d-bb168e0b1bf0.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/65\n",
      "25/05/05 11:23:26 INFO MicroBatchExecution: Committed offsets for batch 65. Metadata OffsetSeqMetadata(0,1746458606482,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:23:26 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.65.32e9c16e-ab9c-4549-aba3-1a6a803a08d8.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/65\n",
      "25/05/05 11:23:26 INFO MicroBatchExecution: Committed offsets for batch 65. Metadata OffsetSeqMetadata(0,1746458606483,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:23:26 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.65.26119ddd-3d4b-465c-ae42-bbf3bc9d45f2.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/65\n",
      "25/05/05 11:23:26 INFO MicroBatchExecution: Committed offsets for batch 65. Metadata OffsetSeqMetadata(0,1746458606482,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:23:26 INFO IncrementalExecution: Current batch timestamp = 1746458606482\n",
      "25/05/05 11:23:26 INFO IncrementalExecution: Current batch timestamp = 1746458606483\n",
      "25/05/05 11:23:26 INFO IncrementalExecution: Current batch timestamp = 1746458606482\n",
      "25/05/05 11:23:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:26 INFO IncrementalExecution: Current batch timestamp = 1746458606483\n",
      "25/05/05 11:23:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:26 INFO IncrementalExecution: Current batch timestamp = 1746458606482\n",
      "25/05/05 11:23:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:26 INFO IncrementalExecution: Current batch timestamp = 1746458606482\n",
      "25/05/05 11:23:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:26 INFO IncrementalExecution: Current batch timestamp = 1746458606483\n",
      "25/05/05 11:23:26 INFO IncrementalExecution: Current batch timestamp = 1746458606482\n",
      "25/05/05 11:23:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:26 INFO CodeGenerator: Code generated in 4.487208 ms\n",
      "25/05/05 11:23:26 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 65, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:23:26 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:26 INFO CodeGenerator: Code generated in 3.790792 ms\n",
      "25/05/05 11:23:26 INFO DAGScheduler: Got job 195 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:23:26 INFO DAGScheduler: Final stage: ResultStage 194 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:23:26 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:23:26 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:23:26 INFO DAGScheduler: Submitting ResultStage 194 (MapPartitionsRDD[1048] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:23:26 INFO MemoryStore: Block broadcast_259 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:23:26 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 65, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@29130a43]. The input RDD has 1 partitions.\n",
      "25/05/05 11:23:26 INFO MemoryStore: Block broadcast_259_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:23:26 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:26 INFO BlockManagerInfo: Added broadcast_259_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:26 INFO SparkContext: Created broadcast 259 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:23:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 194 (MapPartitionsRDD[1048] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:23:26 INFO TaskSchedulerImpl: Adding task set 194.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:23:26 INFO TaskSetManager: Starting task 0.0 in stage 194.0 (TID 194) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:23:26 INFO DAGScheduler: Got job 196 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:23:26 INFO DAGScheduler: Final stage: ResultStage 195 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:23:26 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:23:26 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:23:26 INFO DAGScheduler: Submitting ResultStage 195 (MapPartitionsRDD[1051] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:23:26 INFO Executor: Running task 0.0 in stage 194.0 (TID 194)\n",
      "25/05/05 11:23:26 INFO MemoryStore: Block broadcast_260 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:23:26 INFO MemoryStore: Block broadcast_260_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)\n",
      "25/05/05 11:23:26 INFO BlockManagerInfo: Added broadcast_260_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:26 INFO SparkContext: Created broadcast 260 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:23:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 195 (MapPartitionsRDD[1051] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:23:26 INFO TaskSchedulerImpl: Adding task set 195.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:23:26 INFO TaskSetManager: Starting task 0.0 in stage 195.0 (TID 195) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:23:26 INFO Executor: Running task 0.0 in stage 195.0 (TID 195)\n",
      "25/05/05 11:23:26 INFO CodeGenerator: Code generated in 5.991625 ms\n",
      "25/05/05 11:23:26 INFO CodeGenerator: Code generated in 5.238583 ms\n",
      "25/05/05 11:23:26 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3296 untilOffset=3297, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=65 taskId=194 partitionId=0\n",
      "25/05/05 11:23:26 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3296 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:26 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3296 untilOffset=3297, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=65 taskId=195 partitionId=0\n",
      "25/05/05 11:23:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:26 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3296 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:26 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:26 INFO DAGScheduler: Got job 197 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:23:26 INFO DAGScheduler: Final stage: ResultStage 196 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:23:26 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:23:26 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:23:26 INFO DAGScheduler: Submitting ResultStage 196 (MapPartitionsRDD[1056] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:23:26 INFO MemoryStore: Block broadcast_261 stored as values in memory (estimated size 47.1 KiB, free 366.0 MiB)\n",
      "25/05/05 11:23:26 INFO MemoryStore: Block broadcast_261_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:23:26 INFO BlockManagerInfo: Added broadcast_261_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:26 INFO SparkContext: Created broadcast 261 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:23:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 196 (MapPartitionsRDD[1056] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:23:26 INFO TaskSchedulerImpl: Adding task set 196.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:23:26 INFO TaskSetManager: Starting task 0.0 in stage 196.0 (TID 196) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:23:26 INFO Executor: Running task 0.0 in stage 196.0 (TID 196)\n",
      "25/05/05 11:23:26 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3296 untilOffset=3297, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=65 taskId=196 partitionId=0\n",
      "25/05/05 11:23:26 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3296 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3297, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:27 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:23:27 INFO DataWritingSparkTask: Committed partition 0 (task 194, attempt 0, stage 194.0)\n",
      "25/05/05 11:23:27 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 501781000 nanos, during time span of 502174833 nanos.\n",
      "25/05/05 11:23:27 INFO Executor: Finished task 0.0 in stage 194.0 (TID 194). 2145 bytes result sent to driver\n",
      "25/05/05 11:23:27 INFO TaskSetManager: Finished task 0.0 in stage 194.0 (TID 194) in 515 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:23:27 INFO TaskSchedulerImpl: Removed TaskSet 194.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:23:27 INFO DAGScheduler: ResultStage 194 (start at NativeMethodAccessorImpl.java:0) finished in 0.517 s\n",
      "25/05/05 11:23:27 INFO DAGScheduler: Job 195 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:23:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 194: Stage finished\n",
      "25/05/05 11:23:27 INFO DAGScheduler: Job 195 finished: start at NativeMethodAccessorImpl.java:0, took 0.517653 s\n",
      "25/05/05 11:23:27 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 65, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:23:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3297, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:27 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:23:27 INFO DataWritingSparkTask: Committed partition 0 (task 195, attempt 0, stage 195.0)\n",
      "25/05/05 11:23:27 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 502200000 nanos, during time span of 505393333 nanos.\n",
      "25/05/05 11:23:27 INFO Executor: Finished task 0.0 in stage 195.0 (TID 195). 3514 bytes result sent to driver\n",
      "25/05/05 11:23:27 INFO TaskSetManager: Finished task 0.0 in stage 195.0 (TID 195) in 517 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:23:27 INFO TaskSchedulerImpl: Removed TaskSet 195.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:23:27 INFO DAGScheduler: ResultStage 195 (start at NativeMethodAccessorImpl.java:0) finished in 0.519 s\n",
      "25/05/05 11:23:27 INFO DAGScheduler: Job 196 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:23:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 195: Stage finished\n",
      "25/05/05 11:23:27 INFO DAGScheduler: Job 196 finished: start at NativeMethodAccessorImpl.java:0, took 0.520843 s\n",
      "25/05/05 11:23:27 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 65, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@29130a43] is committing.\n",
      "25/05/05 11:23:27 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 65, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@29130a43] committed.\n",
      "25/05/05 11:23:27 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 65, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:23:27 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/65 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.65.6f91ab38-519a-4472-b5f2-1df009f1051c.tmp\n",
      "25/05/05 11:23:27 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/65 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.65.683581a6-f4d7-47da-867e-057d9b2df4d4.tmp\n",
      "25/05/05 11:23:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3297, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:27 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:23:27 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:23:27 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:23:27 INFO connection: Opened connection [connectionId{localValue:129, serverValue:4499}] to localhost:27017\n",
      "25/05/05 11:23:27 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=216250}\n",
      "25/05/05 11:23:27 INFO connection: Opened connection [connectionId{localValue:130, serverValue:4500}] to localhost:27017\n",
      "25/05/05 11:23:27 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:23:27 INFO connection: Closed connection [connectionId{localValue:130, serverValue:4500}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:23:27 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 505116583 nanos, during time span of 509597583 nanos.\n",
      "25/05/05 11:23:27 INFO Executor: Finished task 0.0 in stage 196.0 (TID 196). 1645 bytes result sent to driver\n",
      "25/05/05 11:23:27 INFO TaskSetManager: Finished task 0.0 in stage 196.0 (TID 196) in 516 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:23:27 INFO TaskSchedulerImpl: Removed TaskSet 196.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:23:27 INFO DAGScheduler: ResultStage 196 (start at NativeMethodAccessorImpl.java:0) finished in 0.519 s\n",
      "25/05/05 11:23:27 INFO DAGScheduler: Job 197 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:23:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 196: Stage finished\n",
      "25/05/05 11:23:27 INFO DAGScheduler: Job 197 finished: start at NativeMethodAccessorImpl.java:0, took 0.520740 s\n",
      "25/05/05 11:23:27 INFO MemoryStore: Block broadcast_262 stored as values in memory (estimated size 248.0 B, free 366.0 MiB)\n",
      "25/05/05 11:23:27 INFO MemoryStore: Block broadcast_262_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.0 MiB)\n",
      "25/05/05 11:23:27 INFO BlockManagerInfo: Added broadcast_262_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:23:27 INFO SparkContext: Created broadcast 262 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:27 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/65 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.65.74e74442-b8bd-4847-ba6c-b146565a44d9.tmp\n",
      "25/05/05 11:23:27 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.65.6f91ab38-519a-4472-b5f2-1df009f1051c.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/65\n",
      "25/05/05 11:23:27 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:23:26.479Z\",\n",
      "  \"batchId\" : 65,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.6366612111292962,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 538,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 4,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 611,\n",
      "    \"walCommit\" : 35\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3296\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3297\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3297\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.6366612111292962,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:23:27 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.65.683581a6-f4d7-47da-867e-057d9b2df4d4.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/65\n",
      "25/05/05 11:23:27 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:23:26.479Z\",\n",
      "  \"batchId\" : 65,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.6339869281045751,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 539,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 612,\n",
      "    \"walCommit\" : 35\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3296\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3297\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3297\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.6339869281045751,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:23:27 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.65.74e74442-b8bd-4847-ba6c-b146565a44d9.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/65\n",
      "25/05/05 11:23:27 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:23:26.478Z\",\n",
      "  \"batchId\" : 65,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.5873015873015872,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 557,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 4,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 630,\n",
      "    \"walCommit\" : 36\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3296\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3297\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3297\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.5873015873015872,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 65\n",
      "-------------------------------------------\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION      |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R016|R032|00-00-01|TIME SQ-42 ST|05/05/2025|11:23:25|REGULAR|9          |4         |2025-05-05 11:23:26.482|\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:23:29 INFO BlockManagerInfo: Removed broadcast_257_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:29 INFO BlockManagerInfo: Removed broadcast_256_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:29 INFO BlockManagerInfo: Removed broadcast_260_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:29 INFO BlockManagerInfo: Removed broadcast_258_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:23:29 INFO BlockManagerInfo: Removed broadcast_259_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:29 INFO BlockManagerInfo: Removed broadcast_262_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:23:29 INFO BlockManagerInfo: Removed broadcast_255_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:23:29 INFO BlockManagerInfo: Removed broadcast_261_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:23:33 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/66 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.66.6b72fa6d-cce8-408f-a814-fd22790eaaa4.tmp\n",
      "25/05/05 11:23:33 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/66 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.66.496a648a-3be8-466e-a5f6-f01e905b0f08.tmp\n",
      "25/05/05 11:23:33 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/66 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.66.9ff1be49-832f-4dab-b761-0028f4628d36.tmp\n",
      "25/05/05 11:23:33 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.66.6b72fa6d-cce8-408f-a814-fd22790eaaa4.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/66\n",
      "25/05/05 11:23:33 INFO MicroBatchExecution: Committed offsets for batch 66. Metadata OffsetSeqMetadata(0,1746458612965,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:23:33 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.66.9ff1be49-832f-4dab-b761-0028f4628d36.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/66\n",
      "25/05/05 11:23:33 INFO MicroBatchExecution: Committed offsets for batch 66. Metadata OffsetSeqMetadata(0,1746458612970,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:23:33 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.66.496a648a-3be8-466e-a5f6-f01e905b0f08.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/66\n",
      "25/05/05 11:23:33 INFO MicroBatchExecution: Committed offsets for batch 66. Metadata OffsetSeqMetadata(0,1746458612969,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:23:33 INFO IncrementalExecution: Current batch timestamp = 1746458612965\n",
      "25/05/05 11:23:33 INFO IncrementalExecution: Current batch timestamp = 1746458612969\n",
      "25/05/05 11:23:33 INFO IncrementalExecution: Current batch timestamp = 1746458612970\n",
      "25/05/05 11:23:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:33 INFO IncrementalExecution: Current batch timestamp = 1746458612965\n",
      "25/05/05 11:23:33 INFO IncrementalExecution: Current batch timestamp = 1746458612969\n",
      "25/05/05 11:23:33 INFO IncrementalExecution: Current batch timestamp = 1746458612970\n",
      "25/05/05 11:23:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:33 INFO IncrementalExecution: Current batch timestamp = 1746458612970\n",
      "25/05/05 11:23:33 INFO IncrementalExecution: Current batch timestamp = 1746458612965\n",
      "25/05/05 11:23:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:33 INFO CodeGenerator: Code generated in 4.136167 ms\n",
      "25/05/05 11:23:33 INFO CodeGenerator: Code generated in 3.553583 ms\n",
      "25/05/05 11:23:33 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 66, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@689327b5]. The input RDD has 1 partitions.\n",
      "25/05/05 11:23:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:33 INFO DAGScheduler: Got job 198 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:23:33 INFO DAGScheduler: Final stage: ResultStage 197 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:23:33 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:23:33 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:23:33 INFO DAGScheduler: Submitting ResultStage 197 (MapPartitionsRDD[1064] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:23:33 INFO MemoryStore: Block broadcast_263 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:23:33 INFO MemoryStore: Block broadcast_263_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:23:33 INFO BlockManagerInfo: Added broadcast_263_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:23:33 INFO CodeGenerator: Code generated in 4.915083 ms\n",
      "25/05/05 11:23:33 INFO SparkContext: Created broadcast 263 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:23:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 197 (MapPartitionsRDD[1064] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:23:33 INFO TaskSchedulerImpl: Adding task set 197.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:23:33 INFO TaskSetManager: Starting task 0.0 in stage 197.0 (TID 197) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:23:33 INFO Executor: Running task 0.0 in stage 197.0 (TID 197)\n",
      "25/05/05 11:23:33 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 66, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:23:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:33 INFO DAGScheduler: Got job 199 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:23:33 INFO DAGScheduler: Final stage: ResultStage 198 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:23:33 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:23:33 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:23:33 INFO DAGScheduler: Submitting ResultStage 198 (MapPartitionsRDD[1067] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:23:33 INFO MemoryStore: Block broadcast_264 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:23:33 INFO MemoryStore: Block broadcast_264_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:23:33 INFO BlockManagerInfo: Added broadcast_264_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:23:33 INFO SparkContext: Created broadcast 264 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:23:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 198 (MapPartitionsRDD[1067] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:23:33 INFO TaskSchedulerImpl: Adding task set 198.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:23:33 INFO TaskSetManager: Starting task 0.0 in stage 198.0 (TID 198) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:23:33 INFO Executor: Running task 0.0 in stage 198.0 (TID 198)\n",
      "25/05/05 11:23:33 INFO CodeGenerator: Code generated in 5.2625 ms\n",
      "25/05/05 11:23:33 INFO CodeGenerator: Code generated in 3.921792 ms\n",
      "25/05/05 11:23:33 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3297 untilOffset=3298, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=66 taskId=197 partitionId=0\n",
      "25/05/05 11:23:33 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3297 untilOffset=3298, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=66 taskId=198 partitionId=0\n",
      "25/05/05 11:23:33 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3297 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:33 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3297 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:33 INFO DAGScheduler: Got job 200 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:23:33 INFO DAGScheduler: Final stage: ResultStage 199 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:23:33 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:23:33 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:23:33 INFO DAGScheduler: Submitting ResultStage 199 (MapPartitionsRDD[1072] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:23:33 INFO MemoryStore: Block broadcast_265 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:23:33 INFO MemoryStore: Block broadcast_265_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:23:33 INFO BlockManagerInfo: Added broadcast_265_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:33 INFO SparkContext: Created broadcast 265 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:23:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 199 (MapPartitionsRDD[1072] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:23:33 INFO TaskSchedulerImpl: Adding task set 199.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:23:33 INFO TaskSetManager: Starting task 0.0 in stage 199.0 (TID 199) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:23:33 INFO Executor: Running task 0.0 in stage 199.0 (TID 199)\n",
      "25/05/05 11:23:33 INFO CodeGenerator: Code generated in 4.146583 ms\n",
      "25/05/05 11:23:33 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3297 untilOffset=3298, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=66 taskId=199 partitionId=0\n",
      "25/05/05 11:23:33 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3297 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3298, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3298, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:33 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:23:33 INFO DataWritingSparkTask: Committed partition 0 (task 198, attempt 0, stage 198.0)\n",
      "25/05/05 11:23:33 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 503298417 nanos, during time span of 503895125 nanos.\n",
      "25/05/05 11:23:33 INFO Executor: Finished task 0.0 in stage 198.0 (TID 198). 2145 bytes result sent to driver\n",
      "25/05/05 11:23:33 INFO TaskSetManager: Finished task 0.0 in stage 198.0 (TID 198) in 516 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:23:33 INFO TaskSchedulerImpl: Removed TaskSet 198.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:23:33 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:23:33 INFO DataWritingSparkTask: Committed partition 0 (task 197, attempt 0, stage 197.0)\n",
      "25/05/05 11:23:33 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 504822208 nanos, during time span of 506893541 nanos.\n",
      "25/05/05 11:23:33 INFO DAGScheduler: ResultStage 198 (start at NativeMethodAccessorImpl.java:0) finished in 0.519 s\n",
      "25/05/05 11:23:33 INFO DAGScheduler: Job 199 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:23:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 198: Stage finished\n",
      "25/05/05 11:23:33 INFO DAGScheduler: Job 199 finished: start at NativeMethodAccessorImpl.java:0, took 0.519387 s\n",
      "25/05/05 11:23:33 INFO Executor: Finished task 0.0 in stage 197.0 (TID 197). 3553 bytes result sent to driver\n",
      "25/05/05 11:23:33 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 66, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:23:33 INFO TaskSetManager: Finished task 0.0 in stage 197.0 (TID 197) in 522 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:23:33 INFO TaskSchedulerImpl: Removed TaskSet 197.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:23:33 INFO DAGScheduler: ResultStage 197 (start at NativeMethodAccessorImpl.java:0) finished in 0.524 s\n",
      "25/05/05 11:23:33 INFO DAGScheduler: Job 198 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:23:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 197: Stage finished\n",
      "25/05/05 11:23:33 INFO DAGScheduler: Job 198 finished: start at NativeMethodAccessorImpl.java:0, took 0.524936 s\n",
      "25/05/05 11:23:33 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 66, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@689327b5] is committing.\n",
      "25/05/05 11:23:33 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 66, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@689327b5] committed.\n",
      "25/05/05 11:23:33 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/66 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.66.8ef6922f-1695-48e1-99aa-c9fc39f8f4ed.tmp\n",
      "25/05/05 11:23:33 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 66, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:23:33 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/66 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.66.7d5245d0-925e-4491-82f7-e39171448bab.tmp\n",
      "25/05/05 11:23:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3298, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:33 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:23:33 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:23:33 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:23:33 INFO connection: Opened connection [connectionId{localValue:131, serverValue:4501}] to localhost:27017\n",
      "25/05/05 11:23:33 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=254709}\n",
      "25/05/05 11:23:33 INFO connection: Opened connection [connectionId{localValue:132, serverValue:4502}] to localhost:27017\n",
      "25/05/05 11:23:33 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:23:33 INFO connection: Closed connection [connectionId{localValue:132, serverValue:4502}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:23:33 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 504439875 nanos, during time span of 509069625 nanos.\n",
      "25/05/05 11:23:33 INFO Executor: Finished task 0.0 in stage 199.0 (TID 199). 1645 bytes result sent to driver\n",
      "25/05/05 11:23:33 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.66.8ef6922f-1695-48e1-99aa-c9fc39f8f4ed.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/66\n",
      "25/05/05 11:23:33 INFO TaskSetManager: Finished task 0.0 in stage 199.0 (TID 199) in 520 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:23:33 INFO TaskSchedulerImpl: Removed TaskSet 199.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:23:33 INFO DAGScheduler: ResultStage 199 (start at NativeMethodAccessorImpl.java:0) finished in 0.525 s\n",
      "25/05/05 11:23:33 INFO DAGScheduler: Job 200 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:23:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 199: Stage finished\n",
      "25/05/05 11:23:33 INFO DAGScheduler: Job 200 finished: start at NativeMethodAccessorImpl.java:0, took 0.525910 s\n",
      "25/05/05 11:23:33 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:23:32.964Z\",\n",
      "  \"batchId\" : 66,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.574803149606299,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 539,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 1,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 635,\n",
      "    \"walCommit\" : 61\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3297\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3298\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3298\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.574803149606299,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:23:33 INFO MemoryStore: Block broadcast_266 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:23:33 INFO MemoryStore: Block broadcast_266_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:23:33 INFO BlockManagerInfo: Added broadcast_266_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:23:33 INFO SparkContext: Created broadcast 266 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:33 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.66.7d5245d0-925e-4491-82f7-e39171448bab.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/66\n",
      "25/05/05 11:23:33 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:23:32.967Z\",\n",
      "  \"batchId\" : 66,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.567398119122257,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 546,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 638,\n",
      "    \"walCommit\" : 57\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3297\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3298\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3298\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.567398119122257,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:23:33 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/66 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.66.d51d0d03-15db-4690-9028-716818dc612d.tmp\n",
      "25/05/05 11:23:33 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.66.d51d0d03-15db-4690-9028-716818dc612d.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/66\n",
      "25/05/05 11:23:33 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:23:32.967Z\",\n",
      "  \"batchId\" : 66,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.510574018126888,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 570,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 662,\n",
      "    \"walCommit\" : 59\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3297\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3298\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3298\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.510574018126888,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 66\n",
      "-------------------------------------------\n",
      "+----+----+--------+--------------+----------+--------+-------+-----------+----------+----------------------+\n",
      "|C/A |UNIT|SCP     |STATION       |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time           |\n",
      "+----+----+--------+--------------+----------+--------+-------+-----------+----------+----------------------+\n",
      "|A002|R051|02-00-00|34 ST-PENN STA|05/05/2025|11:23:31|REGULAR|7          |7         |2025-05-05 11:23:32.97|\n",
      "+----+----+--------+--------------+----------+--------+-------+-----------+----------+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:23:37 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/67 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.67.254590c7-3399-4b7c-b4a6-0c1ab42cd272.tmp\n",
      "25/05/05 11:23:37 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/67 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.67.aa362d59-8247-4ea2-8715-dfe803ff3d77.tmp\n",
      "25/05/05 11:23:37 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/67 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.67.7517a724-120b-4131-8a60-dd9fe5e7cdcf.tmp\n",
      "25/05/05 11:23:37 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.67.aa362d59-8247-4ea2-8715-dfe803ff3d77.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/67\n",
      "25/05/05 11:23:37 INFO MicroBatchExecution: Committed offsets for batch 67. Metadata OffsetSeqMetadata(0,1746458617442,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:23:37 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.67.254590c7-3399-4b7c-b4a6-0c1ab42cd272.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/67\n",
      "25/05/05 11:23:37 INFO MicroBatchExecution: Committed offsets for batch 67. Metadata OffsetSeqMetadata(0,1746458617442,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:23:37 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.67.7517a724-120b-4131-8a60-dd9fe5e7cdcf.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/67\n",
      "25/05/05 11:23:37 INFO MicroBatchExecution: Committed offsets for batch 67. Metadata OffsetSeqMetadata(0,1746458617444,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:23:37 INFO IncrementalExecution: Current batch timestamp = 1746458617442\n",
      "25/05/05 11:23:37 INFO IncrementalExecution: Current batch timestamp = 1746458617442\n",
      "25/05/05 11:23:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:37 INFO IncrementalExecution: Current batch timestamp = 1746458617444\n",
      "25/05/05 11:23:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:37 INFO IncrementalExecution: Current batch timestamp = 1746458617442\n",
      "25/05/05 11:23:37 INFO IncrementalExecution: Current batch timestamp = 1746458617442\n",
      "25/05/05 11:23:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:37 INFO IncrementalExecution: Current batch timestamp = 1746458617444\n",
      "25/05/05 11:23:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:37 INFO IncrementalExecution: Current batch timestamp = 1746458617442\n",
      "25/05/05 11:23:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:37 INFO IncrementalExecution: Current batch timestamp = 1746458617444\n",
      "25/05/05 11:23:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:37 INFO CodeGenerator: Code generated in 4.1465 ms\n",
      "25/05/05 11:23:37 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 67, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:23:37 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:37 INFO DAGScheduler: Got job 201 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:23:37 INFO DAGScheduler: Final stage: ResultStage 200 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:23:37 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:23:37 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:23:37 INFO DAGScheduler: Submitting ResultStage 200 (MapPartitionsRDD[1080] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:23:37 INFO MemoryStore: Block broadcast_267 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:23:37 INFO MemoryStore: Block broadcast_267_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:23:37 INFO BlockManagerInfo: Added broadcast_267_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:37 INFO CodeGenerator: Code generated in 3.470542 ms\n",
      "25/05/05 11:23:37 INFO SparkContext: Created broadcast 267 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:23:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 200 (MapPartitionsRDD[1080] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:23:37 INFO TaskSchedulerImpl: Adding task set 200.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:23:37 INFO TaskSetManager: Starting task 0.0 in stage 200.0 (TID 200) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:23:37 INFO Executor: Running task 0.0 in stage 200.0 (TID 200)\n",
      "25/05/05 11:23:37 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 67, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@66cdd31c]. The input RDD has 1 partitions.\n",
      "25/05/05 11:23:37 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:37 INFO DAGScheduler: Got job 202 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:23:37 INFO DAGScheduler: Final stage: ResultStage 201 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:23:37 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:23:37 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:23:37 INFO DAGScheduler: Submitting ResultStage 201 (MapPartitionsRDD[1083] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:23:37 INFO MemoryStore: Block broadcast_268 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:23:37 INFO MemoryStore: Block broadcast_268_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)\n",
      "25/05/05 11:23:37 INFO BlockManagerInfo: Added broadcast_268_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:37 INFO SparkContext: Created broadcast 268 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:23:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 201 (MapPartitionsRDD[1083] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:23:37 INFO TaskSchedulerImpl: Adding task set 201.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:23:37 INFO TaskSetManager: Starting task 0.0 in stage 201.0 (TID 201) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:23:37 INFO Executor: Running task 0.0 in stage 201.0 (TID 201)\n",
      "25/05/05 11:23:37 INFO CodeGenerator: Code generated in 3.7285 ms\n",
      "25/05/05 11:23:37 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3298 untilOffset=3299, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=67 taskId=200 partitionId=0\n",
      "25/05/05 11:23:37 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3298 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:37 INFO CodeGenerator: Code generated in 3.830167 ms\n",
      "25/05/05 11:23:37 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:37 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3298 untilOffset=3299, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=67 taskId=201 partitionId=0\n",
      "25/05/05 11:23:37 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3298 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:37 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:37 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:37 INFO DAGScheduler: Got job 203 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:23:37 INFO DAGScheduler: Final stage: ResultStage 202 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:23:37 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:23:37 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:23:37 INFO DAGScheduler: Submitting ResultStage 202 (MapPartitionsRDD[1088] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:23:37 INFO MemoryStore: Block broadcast_269 stored as values in memory (estimated size 47.1 KiB, free 366.0 MiB)\n",
      "25/05/05 11:23:37 INFO MemoryStore: Block broadcast_269_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.0 MiB)\n",
      "25/05/05 11:23:37 INFO BlockManagerInfo: Added broadcast_269_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:37 INFO SparkContext: Created broadcast 269 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:23:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 202 (MapPartitionsRDD[1088] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:23:37 INFO TaskSchedulerImpl: Adding task set 202.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:23:37 INFO TaskSetManager: Starting task 0.0 in stage 202.0 (TID 202) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:23:37 INFO Executor: Running task 0.0 in stage 202.0 (TID 202)\n",
      "25/05/05 11:23:37 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3298 untilOffset=3299, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=67 taskId=202 partitionId=0\n",
      "25/05/05 11:23:37 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3298 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:37 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3299, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:38 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:23:38 INFO DataWritingSparkTask: Committed partition 0 (task 200, attempt 0, stage 200.0)\n",
      "25/05/05 11:23:38 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 503245167 nanos, during time span of 503608250 nanos.\n",
      "25/05/05 11:23:38 INFO Executor: Finished task 0.0 in stage 200.0 (TID 200). 2145 bytes result sent to driver\n",
      "25/05/05 11:23:38 INFO TaskSetManager: Finished task 0.0 in stage 200.0 (TID 200) in 512 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:23:38 INFO TaskSchedulerImpl: Removed TaskSet 200.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:23:38 INFO DAGScheduler: ResultStage 200 (start at NativeMethodAccessorImpl.java:0) finished in 0.513 s\n",
      "25/05/05 11:23:38 INFO DAGScheduler: Job 201 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:23:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 200: Stage finished\n",
      "25/05/05 11:23:38 INFO DAGScheduler: Job 201 finished: start at NativeMethodAccessorImpl.java:0, took 0.513784 s\n",
      "25/05/05 11:23:38 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 67, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:23:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3299, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:38 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:23:38 INFO DataWritingSparkTask: Committed partition 0 (task 201, attempt 0, stage 201.0)\n",
      "25/05/05 11:23:38 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 501462500 nanos, during time span of 503054583 nanos.\n",
      "25/05/05 11:23:38 INFO Executor: Finished task 0.0 in stage 201.0 (TID 201). 3516 bytes result sent to driver\n",
      "25/05/05 11:23:38 INFO TaskSetManager: Finished task 0.0 in stage 201.0 (TID 201) in 512 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:23:38 INFO TaskSchedulerImpl: Removed TaskSet 201.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:23:38 INFO DAGScheduler: ResultStage 201 (start at NativeMethodAccessorImpl.java:0) finished in 0.513 s\n",
      "25/05/05 11:23:38 INFO DAGScheduler: Job 202 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:23:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 201: Stage finished\n",
      "25/05/05 11:23:38 INFO DAGScheduler: Job 202 finished: start at NativeMethodAccessorImpl.java:0, took 0.514122 s\n",
      "25/05/05 11:23:38 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 67, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@66cdd31c] is committing.\n",
      "25/05/05 11:23:38 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 67, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@66cdd31c] committed.\n",
      "25/05/05 11:23:38 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 67, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:23:38 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/67 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.67.93c51c76-e7a9-4c0e-a180-efda7eb00655.tmp\n",
      "25/05/05 11:23:38 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/67 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.67.962722cf-635a-4b1e-8b7d-17719a6284fe.tmp\n",
      "25/05/05 11:23:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3299, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:38 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:23:38 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:23:38 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:23:38 INFO connection: Opened connection [connectionId{localValue:133, serverValue:4503}] to localhost:27017\n",
      "25/05/05 11:23:38 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=276083}\n",
      "25/05/05 11:23:38 INFO connection: Opened connection [connectionId{localValue:134, serverValue:4504}] to localhost:27017\n",
      "25/05/05 11:23:38 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:23:38 INFO connection: Closed connection [connectionId{localValue:134, serverValue:4504}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:23:38 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 503458375 nanos, during time span of 509589458 nanos.\n",
      "25/05/05 11:23:38 INFO Executor: Finished task 0.0 in stage 202.0 (TID 202). 1645 bytes result sent to driver\n",
      "25/05/05 11:23:38 INFO TaskSetManager: Finished task 0.0 in stage 202.0 (TID 202) in 515 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:23:38 INFO TaskSchedulerImpl: Removed TaskSet 202.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:23:38 INFO DAGScheduler: ResultStage 202 (start at NativeMethodAccessorImpl.java:0) finished in 0.519 s\n",
      "25/05/05 11:23:38 INFO DAGScheduler: Job 203 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:23:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 202: Stage finished\n",
      "25/05/05 11:23:38 INFO DAGScheduler: Job 203 finished: start at NativeMethodAccessorImpl.java:0, took 0.520383 s\n",
      "25/05/05 11:23:38 INFO MemoryStore: Block broadcast_270 stored as values in memory (estimated size 248.0 B, free 366.0 MiB)\n",
      "25/05/05 11:23:38 INFO MemoryStore: Block broadcast_270_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.0 MiB)\n",
      "25/05/05 11:23:38 INFO BlockManagerInfo: Added broadcast_270_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:23:38 INFO SparkContext: Created broadcast 270 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:38 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/67 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.67.4a273bff-24a9-4d42-9032-678657bcc521.tmp\n",
      "25/05/05 11:23:38 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.67.93c51c76-e7a9-4c0e-a180-efda7eb00655.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/67\n",
      "25/05/05 11:23:38 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:23:37.441Z\",\n",
      "  \"batchId\" : 67,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.5923566878980893,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 527,\n",
      "    \"commitOffsets\" : 31,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 628,\n",
      "    \"walCommit\" : 64\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3298\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3299\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3299\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.5923566878980893,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:23:38 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.67.962722cf-635a-4b1e-8b7d-17719a6284fe.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/67\n",
      "25/05/05 11:23:38 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:23:37.441Z\",\n",
      "  \"batchId\" : 67,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.5847860538827259,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 533,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 631,\n",
      "    \"walCommit\" : 65\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3298\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3299\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3299\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.5847860538827259,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:23:38 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.67.4a273bff-24a9-4d42-9032-678657bcc521.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/67\n",
      "25/05/05 11:23:38 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:23:37.441Z\",\n",
      "  \"batchId\" : 67,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.5455950540958268,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 550,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 647,\n",
      "    \"walCommit\" : 64\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3298\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3299\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3299\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.5455950540958268,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 67\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A001|R079|01-00-00|WORLD TRADE CTR|05/05/2025|11:23:36|REGULAR|14         |5         |2025-05-05 11:23:37.442|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:23:41 INFO BlockManagerInfo: Removed broadcast_264_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:41 INFO BlockManagerInfo: Removed broadcast_269_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:41 INFO BlockManagerInfo: Removed broadcast_265_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:23:41 INFO BlockManagerInfo: Removed broadcast_267_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:23:41 INFO BlockManagerInfo: Removed broadcast_263_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:23:41 INFO BlockManagerInfo: Removed broadcast_266_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:23:41 INFO BlockManagerInfo: Removed broadcast_268_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:23:41 INFO BlockManagerInfo: Removed broadcast_270_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:23:43 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/68 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.68.e85b7899-570b-43d5-a279-68977aa374d5.tmp\n",
      "25/05/05 11:23:43 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/68 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.68.942b4761-fbaf-4290-b409-afaf7213a2e1.tmp\n",
      "25/05/05 11:23:43 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/68 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.68.c31d04e2-cf5f-4a9d-8caa-39a9c3000007.tmp\n",
      "25/05/05 11:23:43 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.68.942b4761-fbaf-4290-b409-afaf7213a2e1.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/68\n",
      "25/05/05 11:23:43 INFO MicroBatchExecution: Committed offsets for batch 68. Metadata OffsetSeqMetadata(0,1746458623904,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:23:43 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.68.c31d04e2-cf5f-4a9d-8caa-39a9c3000007.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/68\n",
      "25/05/05 11:23:43 INFO MicroBatchExecution: Committed offsets for batch 68. Metadata OffsetSeqMetadata(0,1746458623910,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:23:43 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.68.e85b7899-570b-43d5-a279-68977aa374d5.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/68\n",
      "25/05/05 11:23:43 INFO MicroBatchExecution: Committed offsets for batch 68. Metadata OffsetSeqMetadata(0,1746458623915,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:23:43 INFO IncrementalExecution: Current batch timestamp = 1746458623904\n",
      "25/05/05 11:23:43 INFO IncrementalExecution: Current batch timestamp = 1746458623910\n",
      "25/05/05 11:23:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:43 INFO IncrementalExecution: Current batch timestamp = 1746458623910\n",
      "25/05/05 11:23:43 INFO IncrementalExecution: Current batch timestamp = 1746458623904\n",
      "25/05/05 11:23:43 INFO IncrementalExecution: Current batch timestamp = 1746458623915\n",
      "25/05/05 11:23:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:43 INFO IncrementalExecution: Current batch timestamp = 1746458623915\n",
      "25/05/05 11:23:43 INFO IncrementalExecution: Current batch timestamp = 1746458623904\n",
      "25/05/05 11:23:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:43 INFO CodeGenerator: Code generated in 3.756584 ms\n",
      "25/05/05 11:23:43 INFO IncrementalExecution: Current batch timestamp = 1746458623915\n",
      "25/05/05 11:23:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:43 INFO CodeGenerator: Code generated in 5.260875 ms\n",
      "25/05/05 11:23:43 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 68, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:23:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:43 INFO CodeGenerator: Code generated in 3.08875 ms\n",
      "25/05/05 11:23:43 INFO DAGScheduler: Got job 204 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:23:43 INFO DAGScheduler: Final stage: ResultStage 203 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:23:43 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:23:43 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:23:43 INFO DAGScheduler: Submitting ResultStage 203 (MapPartitionsRDD[1096] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:23:43 INFO MemoryStore: Block broadcast_271 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:23:43 INFO MemoryStore: Block broadcast_271_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:23:43 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 68, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@7f30f7e7]. The input RDD has 1 partitions.\n",
      "25/05/05 11:23:43 INFO BlockManagerInfo: Added broadcast_271_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:23:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:43 INFO SparkContext: Created broadcast 271 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:23:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 203 (MapPartitionsRDD[1096] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:23:43 INFO TaskSchedulerImpl: Adding task set 203.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:23:43 INFO DAGScheduler: Got job 205 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:23:43 INFO DAGScheduler: Final stage: ResultStage 204 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:23:43 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:23:43 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:23:43 INFO DAGScheduler: Submitting ResultStage 204 (MapPartitionsRDD[1099] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:23:43 INFO TaskSetManager: Starting task 0.0 in stage 203.0 (TID 203) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:23:43 INFO MemoryStore: Block broadcast_272 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:23:43 INFO Executor: Running task 0.0 in stage 203.0 (TID 203)\n",
      "25/05/05 11:23:43 INFO MemoryStore: Block broadcast_272_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:23:43 INFO BlockManagerInfo: Added broadcast_272_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:23:43 INFO SparkContext: Created broadcast 272 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:23:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 204 (MapPartitionsRDD[1099] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:23:43 INFO TaskSchedulerImpl: Adding task set 204.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:23:43 INFO TaskSetManager: Starting task 0.0 in stage 204.0 (TID 204) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:23:43 INFO Executor: Running task 0.0 in stage 204.0 (TID 204)\n",
      "25/05/05 11:23:43 INFO CodeGenerator: Code generated in 3.826792 ms\n",
      "25/05/05 11:23:43 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3299 untilOffset=3300, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=68 taskId=203 partitionId=0\n",
      "25/05/05 11:23:43 INFO CodeGenerator: Code generated in 4.640334 ms\n",
      "25/05/05 11:23:43 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3299 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:43 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3299 untilOffset=3300, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=68 taskId=204 partitionId=0\n",
      "25/05/05 11:23:44 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3299 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:44 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:44 INFO DAGScheduler: Got job 206 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:23:44 INFO DAGScheduler: Final stage: ResultStage 205 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:23:44 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:23:44 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:23:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:44 INFO DAGScheduler: Submitting ResultStage 205 (MapPartitionsRDD[1104] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:23:44 INFO MemoryStore: Block broadcast_273 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:23:44 INFO MemoryStore: Block broadcast_273_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.1 MiB)\n",
      "25/05/05 11:23:44 INFO BlockManagerInfo: Added broadcast_273_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:44 INFO SparkContext: Created broadcast 273 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:23:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 205 (MapPartitionsRDD[1104] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:23:44 INFO TaskSchedulerImpl: Adding task set 205.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:23:44 INFO TaskSetManager: Starting task 0.0 in stage 205.0 (TID 205) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:23:44 INFO Executor: Running task 0.0 in stage 205.0 (TID 205)\n",
      "25/05/05 11:23:44 INFO CodeGenerator: Code generated in 3.9855 ms\n",
      "25/05/05 11:23:44 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3299 untilOffset=3300, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=68 taskId=205 partitionId=0\n",
      "25/05/05 11:23:44 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3299 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3300, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:44 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:23:44 INFO DataWritingSparkTask: Committed partition 0 (task 203, attempt 0, stage 203.0)\n",
      "25/05/05 11:23:44 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 506271250 nanos, during time span of 506774959 nanos.\n",
      "25/05/05 11:23:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3300, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:44 INFO Executor: Finished task 0.0 in stage 203.0 (TID 203). 2188 bytes result sent to driver\n",
      "25/05/05 11:23:44 INFO TaskSetManager: Finished task 0.0 in stage 203.0 (TID 203) in 519 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:23:44 INFO TaskSchedulerImpl: Removed TaskSet 203.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:23:44 INFO DAGScheduler: ResultStage 203 (start at NativeMethodAccessorImpl.java:0) finished in 0.522 s\n",
      "25/05/05 11:23:44 INFO DAGScheduler: Job 204 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:23:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 203: Stage finished\n",
      "25/05/05 11:23:44 INFO DAGScheduler: Job 204 finished: start at NativeMethodAccessorImpl.java:0, took 0.522504 s\n",
      "25/05/05 11:23:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 68, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:23:44 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:23:44 INFO DataWritingSparkTask: Committed partition 0 (task 204, attempt 0, stage 204.0)\n",
      "25/05/05 11:23:44 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 501471208 nanos, during time span of 503039958 nanos.\n",
      "25/05/05 11:23:44 INFO Executor: Finished task 0.0 in stage 204.0 (TID 204). 3559 bytes result sent to driver\n",
      "25/05/05 11:23:44 INFO TaskSetManager: Finished task 0.0 in stage 204.0 (TID 204) in 518 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:23:44 INFO TaskSchedulerImpl: Removed TaskSet 204.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:23:44 INFO DAGScheduler: ResultStage 204 (start at NativeMethodAccessorImpl.java:0) finished in 0.522 s\n",
      "25/05/05 11:23:44 INFO DAGScheduler: Job 205 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:23:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 204: Stage finished\n",
      "25/05/05 11:23:44 INFO DAGScheduler: Job 205 finished: start at NativeMethodAccessorImpl.java:0, took 0.523448 s\n",
      "25/05/05 11:23:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 68, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@7f30f7e7] is committing.\n",
      "25/05/05 11:23:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 68, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@7f30f7e7] committed.\n",
      "25/05/05 11:23:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 68, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:23:44 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/68 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.68.e0a7795f-fcd7-4072-8012-53fb0e21cf91.tmp\n",
      "25/05/05 11:23:44 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/68 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.68.8bdd0ad3-c879-47aa-9458-eb275149e417.tmp\n",
      "25/05/05 11:23:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3300, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:44 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:23:44 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:23:44 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:23:44 INFO connection: Opened connection [connectionId{localValue:135, serverValue:4505}] to localhost:27017\n",
      "25/05/05 11:23:44 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=228541}\n",
      "25/05/05 11:23:44 INFO connection: Opened connection [connectionId{localValue:136, serverValue:4506}] to localhost:27017\n",
      "25/05/05 11:23:44 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:23:44 INFO connection: Closed connection [connectionId{localValue:136, serverValue:4506}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:23:44 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 505616208 nanos, during time span of 514240542 nanos.\n",
      "25/05/05 11:23:44 INFO Executor: Finished task 0.0 in stage 205.0 (TID 205). 1645 bytes result sent to driver\n",
      "25/05/05 11:23:44 INFO TaskSetManager: Finished task 0.0 in stage 205.0 (TID 205) in 525 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:23:44 INFO TaskSchedulerImpl: Removed TaskSet 205.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:23:44 INFO DAGScheduler: ResultStage 205 (start at NativeMethodAccessorImpl.java:0) finished in 0.531 s\n",
      "25/05/05 11:23:44 INFO DAGScheduler: Job 206 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:23:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 205: Stage finished\n",
      "25/05/05 11:23:44 INFO DAGScheduler: Job 206 finished: start at NativeMethodAccessorImpl.java:0, took 0.532094 s\n",
      "25/05/05 11:23:44 INFO MemoryStore: Block broadcast_274 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:23:44 INFO MemoryStore: Block broadcast_274_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:23:44 INFO BlockManagerInfo: Added broadcast_274_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:23:44 INFO SparkContext: Created broadcast 274 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:44 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.68.e0a7795f-fcd7-4072-8012-53fb0e21cf91.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/68\n",
      "25/05/05 11:23:44 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:23:43.902Z\",\n",
      "  \"batchId\" : 68,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.567398119122257,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 540,\n",
      "    \"commitOffsets\" : 35,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 13,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 638,\n",
      "    \"walCommit\" : 45\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3299\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3300\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3300\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.567398119122257,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:23:44 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/68 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.68.d5e7c5b1-1703-416a-a368-74d07297ca93.tmp\n",
      "25/05/05 11:23:44 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.68.8bdd0ad3-c879-47aa-9458-eb275149e417.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/68\n",
      "25/05/05 11:23:44 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:23:43.901Z\",\n",
      "  \"batchId\" : 68,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.557632398753894,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 547,\n",
      "    \"commitOffsets\" : 34,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 642,\n",
      "    \"walCommit\" : 54\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3299\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3300\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3300\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.557632398753894,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:23:44 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.68.d5e7c5b1-1703-416a-a368-74d07297ca93.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/68\n",
      "25/05/05 11:23:44 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:23:43.901Z\",\n",
      "  \"batchId\" : 68,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.5151515151515151,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 572,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 9,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 660,\n",
      "    \"walCommit\" : 49\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3299\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3300\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3300\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.5151515151515151,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 68\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A001|R079|01-00-00|WORLD TRADE CTR|05/05/2025|11:23:42|REGULAR|6          |11        |2025-05-05 11:23:43.904|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:23:50 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/69 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.69.5fb7e7e2-7046-4c8a-959f-8aef99a52517.tmp\n",
      "25/05/05 11:23:50 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/69 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.69.cb568e11-d054-42f0-bee3-5b5a7876b26e.tmp\n",
      "25/05/05 11:23:50 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/69 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.69.a9720aad-2e07-4c36-ae4d-985ff801ee27.tmp\n",
      "25/05/05 11:23:50 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.69.a9720aad-2e07-4c36-ae4d-985ff801ee27.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/69\n",
      "25/05/05 11:23:50 INFO MicroBatchExecution: Committed offsets for batch 69. Metadata OffsetSeqMetadata(0,1746458630378,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:23:50 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.69.5fb7e7e2-7046-4c8a-959f-8aef99a52517.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/69\n",
      "25/05/05 11:23:50 INFO MicroBatchExecution: Committed offsets for batch 69. Metadata OffsetSeqMetadata(0,1746458630378,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:23:50 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.69.cb568e11-d054-42f0-bee3-5b5a7876b26e.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/69\n",
      "25/05/05 11:23:50 INFO MicroBatchExecution: Committed offsets for batch 69. Metadata OffsetSeqMetadata(0,1746458630378,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:23:50 INFO IncrementalExecution: Current batch timestamp = 1746458630378\n",
      "25/05/05 11:23:50 INFO IncrementalExecution: Current batch timestamp = 1746458630378\n",
      "25/05/05 11:23:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:50 INFO IncrementalExecution: Current batch timestamp = 1746458630378\n",
      "25/05/05 11:23:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:50 INFO IncrementalExecution: Current batch timestamp = 1746458630378\n",
      "25/05/05 11:23:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:50 INFO IncrementalExecution: Current batch timestamp = 1746458630378\n",
      "25/05/05 11:23:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:50 INFO IncrementalExecution: Current batch timestamp = 1746458630378\n",
      "25/05/05 11:23:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:50 INFO IncrementalExecution: Current batch timestamp = 1746458630378\n",
      "25/05/05 11:23:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:50 INFO IncrementalExecution: Current batch timestamp = 1746458630378\n",
      "25/05/05 11:23:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:50 INFO CodeGenerator: Code generated in 5.760375 ms\n",
      "25/05/05 11:23:50 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 69, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@46aeb8ef]. The input RDD has 1 partitions.\n",
      "25/05/05 11:23:50 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:50 INFO DAGScheduler: Got job 207 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:23:50 INFO DAGScheduler: Final stage: ResultStage 206 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:23:50 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:23:50 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:23:50 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 69, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:23:50 INFO DAGScheduler: Submitting ResultStage 206 (MapPartitionsRDD[1113] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:23:50 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:50 INFO MemoryStore: Block broadcast_275 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:23:50 INFO MemoryStore: Block broadcast_275_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.1 MiB)\n",
      "25/05/05 11:23:50 INFO BlockManagerInfo: Added broadcast_275_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:50 INFO SparkContext: Created broadcast 275 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:23:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 206 (MapPartitionsRDD[1113] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:23:50 INFO TaskSchedulerImpl: Adding task set 206.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:23:50 INFO DAGScheduler: Got job 208 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:23:50 INFO DAGScheduler: Final stage: ResultStage 207 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:23:50 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:23:50 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:23:50 INFO DAGScheduler: Submitting ResultStage 207 (MapPartitionsRDD[1115] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:23:50 INFO MemoryStore: Block broadcast_276 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:23:50 INFO MemoryStore: Block broadcast_276_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:23:50 INFO TaskSetManager: Starting task 0.0 in stage 206.0 (TID 206) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:23:50 INFO BlockManagerInfo: Added broadcast_276_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:50 INFO Executor: Running task 0.0 in stage 206.0 (TID 206)\n",
      "25/05/05 11:23:50 INFO SparkContext: Created broadcast 276 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:23:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 207 (MapPartitionsRDD[1115] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:23:50 INFO TaskSchedulerImpl: Adding task set 207.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:23:50 INFO TaskSetManager: Starting task 0.0 in stage 207.0 (TID 207) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:23:50 INFO Executor: Running task 0.0 in stage 207.0 (TID 207)\n",
      "25/05/05 11:23:50 INFO CodeGenerator: Code generated in 4.185 ms\n",
      "25/05/05 11:23:50 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3300 untilOffset=3301, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=69 taskId=207 partitionId=0\n",
      "25/05/05 11:23:50 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3300 untilOffset=3301, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=69 taskId=206 partitionId=0\n",
      "25/05/05 11:23:50 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3300 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:50 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3300 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:50 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:50 INFO DAGScheduler: Got job 209 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:23:50 INFO DAGScheduler: Final stage: ResultStage 208 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:23:50 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:23:50 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:23:50 INFO DAGScheduler: Submitting ResultStage 208 (MapPartitionsRDD[1120] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:23:50 INFO MemoryStore: Block broadcast_277 stored as values in memory (estimated size 47.1 KiB, free 366.0 MiB)\n",
      "25/05/05 11:23:50 INFO MemoryStore: Block broadcast_277_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:23:50 INFO BlockManagerInfo: Added broadcast_277_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:50 INFO SparkContext: Created broadcast 277 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:23:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 208 (MapPartitionsRDD[1120] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:23:50 INFO TaskSchedulerImpl: Adding task set 208.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:23:50 INFO TaskSetManager: Starting task 0.0 in stage 208.0 (TID 208) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:23:50 INFO Executor: Running task 0.0 in stage 208.0 (TID 208)\n",
      "25/05/05 11:23:50 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3300 untilOffset=3301, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=69 taskId=208 partitionId=0\n",
      "25/05/05 11:23:50 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3300 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3301, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3301, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:50 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:23:50 INFO DataWritingSparkTask: Committed partition 0 (task 207, attempt 0, stage 207.0)\n",
      "25/05/05 11:23:50 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 509314958 nanos, during time span of 509692333 nanos.\n",
      "25/05/05 11:23:50 INFO Executor: Finished task 0.0 in stage 207.0 (TID 207). 2145 bytes result sent to driver\n",
      "25/05/05 11:23:50 INFO TaskSetManager: Finished task 0.0 in stage 207.0 (TID 207) in 517 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:23:50 INFO TaskSchedulerImpl: Removed TaskSet 207.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:23:50 INFO DAGScheduler: ResultStage 207 (start at NativeMethodAccessorImpl.java:0) finished in 0.520 s\n",
      "25/05/05 11:23:50 INFO DAGScheduler: Job 208 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:23:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 207: Stage finished\n",
      "25/05/05 11:23:50 INFO DAGScheduler: Job 208 finished: start at NativeMethodAccessorImpl.java:0, took 0.521520 s\n",
      "25/05/05 11:23:50 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 69, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:23:50 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:23:50 INFO DataWritingSparkTask: Committed partition 0 (task 206, attempt 0, stage 206.0)\n",
      "25/05/05 11:23:50 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 508519667 nanos, during time span of 510135916 nanos.\n",
      "25/05/05 11:23:50 INFO Executor: Finished task 0.0 in stage 206.0 (TID 206). 3516 bytes result sent to driver\n",
      "25/05/05 11:23:50 INFO TaskSetManager: Finished task 0.0 in stage 206.0 (TID 206) in 520 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:23:50 INFO TaskSchedulerImpl: Removed TaskSet 206.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:23:50 INFO DAGScheduler: ResultStage 206 (start at NativeMethodAccessorImpl.java:0) finished in 0.523 s\n",
      "25/05/05 11:23:50 INFO DAGScheduler: Job 207 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:23:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 206: Stage finished\n",
      "25/05/05 11:23:50 INFO DAGScheduler: Job 207 finished: start at NativeMethodAccessorImpl.java:0, took 0.523740 s\n",
      "25/05/05 11:23:50 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 69, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@46aeb8ef] is committing.\n",
      "25/05/05 11:23:50 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 69, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@46aeb8ef] committed.\n",
      "25/05/05 11:23:50 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 69, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:23:50 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/69 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.69.66a16965-03b8-41f7-819c-43128c52d164.tmp\n",
      "25/05/05 11:23:50 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/69 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.69.a47e574a-b7cd-4d58-aa08-fbb929f9135e.tmp\n",
      "25/05/05 11:23:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:50 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3301, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:50 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:23:50 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:23:50 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:23:50 INFO connection: Opened connection [connectionId{localValue:137, serverValue:4507}] to localhost:27017\n",
      "25/05/05 11:23:50 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=284209}\n",
      "25/05/05 11:23:50 INFO connection: Opened connection [connectionId{localValue:138, serverValue:4508}] to localhost:27017\n",
      "25/05/05 11:23:50 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:23:50 INFO connection: Closed connection [connectionId{localValue:138, serverValue:4508}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:23:50 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 503562917 nanos, during time span of 510286042 nanos.\n",
      "25/05/05 11:23:50 INFO Executor: Finished task 0.0 in stage 208.0 (TID 208). 1645 bytes result sent to driver\n",
      "25/05/05 11:23:50 INFO TaskSetManager: Finished task 0.0 in stage 208.0 (TID 208) in 516 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:23:50 INFO TaskSchedulerImpl: Removed TaskSet 208.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:23:50 INFO DAGScheduler: ResultStage 208 (start at NativeMethodAccessorImpl.java:0) finished in 0.521 s\n",
      "25/05/05 11:23:50 INFO DAGScheduler: Job 209 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:23:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 208: Stage finished\n",
      "25/05/05 11:23:50 INFO DAGScheduler: Job 209 finished: start at NativeMethodAccessorImpl.java:0, took 0.521746 s\n",
      "25/05/05 11:23:50 INFO MemoryStore: Block broadcast_278 stored as values in memory (estimated size 248.0 B, free 366.0 MiB)\n",
      "25/05/05 11:23:50 INFO MemoryStore: Block broadcast_278_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.0 MiB)\n",
      "25/05/05 11:23:50 INFO BlockManagerInfo: Added broadcast_278_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:23:50 INFO SparkContext: Created broadcast 278 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:50 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/69 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.69.25faf852-41b3-4e06-b37a-105eb801c262.tmp\n",
      "25/05/05 11:23:50 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.69.66a16965-03b8-41f7-819c-43128c52d164.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/69\n",
      "25/05/05 11:23:50 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:23:50.374Z\",\n",
      "  \"batchId\" : 69,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "  \"processedRowsPerSecond\" : 1.639344262295082,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 539,\n",
      "    \"commitOffsets\" : 32,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 4,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 610,\n",
      "    \"walCommit\" : 31\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3300\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3301\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3301\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "    \"processedRowsPerSecond\" : 1.639344262295082,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:23:50 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.69.a47e574a-b7cd-4d58-aa08-fbb929f9135e.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/69\n",
      "25/05/05 11:23:50 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:23:50.376Z\",\n",
      "  \"batchId\" : 69,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.6366612111292962,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 542,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 6,\n",
      "    \"triggerExecution\" : 611,\n",
      "    \"walCommit\" : 32\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3300\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3301\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3301\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.6366612111292962,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:23:51 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.69.25faf852-41b3-4e06-b37a-105eb801c262.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/69\n",
      "25/05/05 11:23:51 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:23:50.377Z\",\n",
      "  \"batchId\" : 69,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.5923566878980893,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 561,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 628,\n",
      "    \"walCommit\" : 32\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3300\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3301\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3301\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.5923566878980893,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 69\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|B013|R081|02-00-00|161/YANKEE STAD|05/05/2025|11:23:49|REGULAR|14         |5         |2025-05-05 11:23:50.378|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:23:53 INFO BlockManagerInfo: Removed broadcast_274_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:23:53 INFO BlockManagerInfo: Removed broadcast_278_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:23:53 INFO BlockManagerInfo: Removed broadcast_272_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:53 INFO BlockManagerInfo: Removed broadcast_271_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:53 INFO BlockManagerInfo: Removed broadcast_276_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:53 INFO BlockManagerInfo: Removed broadcast_273_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:53 INFO BlockManagerInfo: Removed broadcast_275_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:23:53 INFO BlockManagerInfo: Removed broadcast_277_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:23:55 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/70 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.70.80d2febe-f721-438a-83d8-05451f0a6ad8.tmp\n",
      "25/05/05 11:23:55 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/70 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.70.686e7d02-2099-45eb-9527-dea60b1de6fb.tmp\n",
      "25/05/05 11:23:55 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/70 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.70.fe0d9171-8534-4dc0-ac5f-5343bdd30dde.tmp\n",
      "25/05/05 11:23:55 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.70.80d2febe-f721-438a-83d8-05451f0a6ad8.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/70\n",
      "25/05/05 11:23:55 INFO MicroBatchExecution: Committed offsets for batch 70. Metadata OffsetSeqMetadata(0,1746458635855,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:23:55 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.70.fe0d9171-8534-4dc0-ac5f-5343bdd30dde.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/70\n",
      "25/05/05 11:23:55 INFO MicroBatchExecution: Committed offsets for batch 70. Metadata OffsetSeqMetadata(0,1746458635859,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:23:55 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.70.686e7d02-2099-45eb-9527-dea60b1de6fb.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/70\n",
      "25/05/05 11:23:55 INFO MicroBatchExecution: Committed offsets for batch 70. Metadata OffsetSeqMetadata(0,1746458635859,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:23:55 INFO IncrementalExecution: Current batch timestamp = 1746458635859\n",
      "25/05/05 11:23:55 INFO IncrementalExecution: Current batch timestamp = 1746458635855\n",
      "25/05/05 11:23:55 INFO IncrementalExecution: Current batch timestamp = 1746458635859\n",
      "25/05/05 11:23:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:55 INFO IncrementalExecution: Current batch timestamp = 1746458635855\n",
      "25/05/05 11:23:55 INFO IncrementalExecution: Current batch timestamp = 1746458635859\n",
      "25/05/05 11:23:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:55 INFO IncrementalExecution: Current batch timestamp = 1746458635859\n",
      "25/05/05 11:23:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:55 INFO IncrementalExecution: Current batch timestamp = 1746458635859\n",
      "25/05/05 11:23:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:55 INFO IncrementalExecution: Current batch timestamp = 1746458635859\n",
      "25/05/05 11:23:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:23:55 INFO CodeGenerator: Code generated in 4.53675 ms\n",
      "25/05/05 11:23:55 INFO CodeGenerator: Code generated in 3.877833 ms\n",
      "25/05/05 11:23:55 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 70, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6abd3a09]. The input RDD has 1 partitions.\n",
      "25/05/05 11:23:55 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 70, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:23:55 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:55 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:55 INFO DAGScheduler: Got job 210 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:23:55 INFO DAGScheduler: Final stage: ResultStage 209 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:23:55 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:23:55 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:23:55 INFO DAGScheduler: Submitting ResultStage 209 (MapPartitionsRDD[1130] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:23:55 INFO MemoryStore: Block broadcast_279 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:23:55 INFO MemoryStore: Block broadcast_279_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:23:55 INFO BlockManagerInfo: Added broadcast_279_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:23:55 INFO SparkContext: Created broadcast 279 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:23:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 209 (MapPartitionsRDD[1130] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:23:55 INFO TaskSchedulerImpl: Adding task set 209.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:23:55 INFO DAGScheduler: Got job 211 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:23:55 INFO DAGScheduler: Final stage: ResultStage 210 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:23:55 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:23:55 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:23:55 INFO DAGScheduler: Submitting ResultStage 210 (MapPartitionsRDD[1131] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:23:55 INFO TaskSetManager: Starting task 0.0 in stage 209.0 (TID 209) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:23:55 INFO MemoryStore: Block broadcast_280 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:23:55 INFO Executor: Running task 0.0 in stage 209.0 (TID 209)\n",
      "25/05/05 11:23:55 INFO MemoryStore: Block broadcast_280_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:23:55 INFO BlockManagerInfo: Added broadcast_280_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:23:55 INFO SparkContext: Created broadcast 280 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:23:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 210 (MapPartitionsRDD[1131] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:23:55 INFO TaskSchedulerImpl: Adding task set 210.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:23:55 INFO TaskSetManager: Starting task 0.0 in stage 210.0 (TID 210) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:23:55 INFO Executor: Running task 0.0 in stage 210.0 (TID 210)\n",
      "25/05/05 11:23:55 INFO CodeGenerator: Code generated in 4.247167 ms\n",
      "25/05/05 11:23:55 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3301 untilOffset=3302, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=70 taskId=210 partitionId=0\n",
      "25/05/05 11:23:55 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3301 untilOffset=3302, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=70 taskId=209 partitionId=0\n",
      "25/05/05 11:23:55 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3301 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:55 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3301 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:55 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:55 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:55 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:55 INFO DAGScheduler: Got job 212 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:23:55 INFO DAGScheduler: Final stage: ResultStage 211 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:23:55 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:23:55 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:23:55 INFO DAGScheduler: Submitting ResultStage 211 (MapPartitionsRDD[1136] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:23:55 INFO MemoryStore: Block broadcast_281 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:23:55 INFO MemoryStore: Block broadcast_281_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:23:55 INFO BlockManagerInfo: Added broadcast_281_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:23:55 INFO SparkContext: Created broadcast 281 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:23:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 211 (MapPartitionsRDD[1136] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:23:55 INFO TaskSchedulerImpl: Adding task set 211.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:23:55 INFO TaskSetManager: Starting task 0.0 in stage 211.0 (TID 211) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:23:55 INFO Executor: Running task 0.0 in stage 211.0 (TID 211)\n",
      "25/05/05 11:23:55 INFO CodeGenerator: Code generated in 4.073792 ms\n",
      "25/05/05 11:23:55 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3301 untilOffset=3302, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=70 taskId=211 partitionId=0\n",
      "25/05/05 11:23:55 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3301 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:55 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3302, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3302, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:56 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:23:56 INFO DataWritingSparkTask: Committed partition 0 (task 210, attempt 0, stage 210.0)\n",
      "25/05/05 11:23:56 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 504861541 nanos, during time span of 505087084 nanos.\n",
      "25/05/05 11:23:56 INFO Executor: Finished task 0.0 in stage 210.0 (TID 210). 2145 bytes result sent to driver\n",
      "25/05/05 11:23:56 INFO TaskSetManager: Finished task 0.0 in stage 210.0 (TID 210) in 513 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:23:56 INFO TaskSchedulerImpl: Removed TaskSet 210.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:23:56 INFO DAGScheduler: ResultStage 210 (start at NativeMethodAccessorImpl.java:0) finished in 0.515 s\n",
      "25/05/05 11:23:56 INFO DAGScheduler: Job 211 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:23:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 210: Stage finished\n",
      "25/05/05 11:23:56 INFO DAGScheduler: Job 211 finished: start at NativeMethodAccessorImpl.java:0, took 0.516995 s\n",
      "25/05/05 11:23:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 70, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:23:56 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:23:56 INFO DataWritingSparkTask: Committed partition 0 (task 209, attempt 0, stage 209.0)\n",
      "25/05/05 11:23:56 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 503254542 nanos, during time span of 504991125 nanos.\n",
      "25/05/05 11:23:56 INFO Executor: Finished task 0.0 in stage 209.0 (TID 209). 3557 bytes result sent to driver\n",
      "25/05/05 11:23:56 INFO TaskSetManager: Finished task 0.0 in stage 209.0 (TID 209) in 516 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:23:56 INFO TaskSchedulerImpl: Removed TaskSet 209.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:23:56 INFO DAGScheduler: ResultStage 209 (start at NativeMethodAccessorImpl.java:0) finished in 0.518 s\n",
      "25/05/05 11:23:56 INFO DAGScheduler: Job 210 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:23:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 209: Stage finished\n",
      "25/05/05 11:23:56 INFO DAGScheduler: Job 210 finished: start at NativeMethodAccessorImpl.java:0, took 0.518264 s\n",
      "25/05/05 11:23:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 70, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6abd3a09] is committing.\n",
      "25/05/05 11:23:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 70, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6abd3a09] committed.\n",
      "25/05/05 11:23:56 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/70 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.70.3b381cab-86b8-4c00-8ade-3fa6dd451832.tmp\n",
      "25/05/05 11:23:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 70, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:23:56 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/70 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.70.d2541278-d062-485f-a6b7-9dab87cafc7c.tmp\n",
      "25/05/05 11:23:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:23:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3302, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:23:56 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:23:56 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:23:56 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:23:56 INFO connection: Opened connection [connectionId{localValue:139, serverValue:4509}] to localhost:27017\n",
      "25/05/05 11:23:56 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=267500}\n",
      "25/05/05 11:23:56 INFO connection: Opened connection [connectionId{localValue:140, serverValue:4510}] to localhost:27017\n",
      "25/05/05 11:23:56 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.70.3b381cab-86b8-4c00-8ade-3fa6dd451832.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/70\n",
      "25/05/05 11:23:56 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:23:55.850Z\",\n",
      "  \"batchId\" : 70,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "  \"processedRowsPerSecond\" : 1.5698587127158556,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 532,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 9,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 637,\n",
      "    \"walCommit\" : 63\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3301\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3302\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3302\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "    \"processedRowsPerSecond\" : 1.5698587127158556,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:23:56 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:23:56 INFO connection: Closed connection [connectionId{localValue:140, serverValue:4510}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:23:56 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 503503625 nanos, during time span of 518668125 nanos.\n",
      "25/05/05 11:23:56 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.70.d2541278-d062-485f-a6b7-9dab87cafc7c.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/70\n",
      "25/05/05 11:23:56 INFO Executor: Finished task 0.0 in stage 211.0 (TID 211). 1645 bytes result sent to driver\n",
      "25/05/05 11:23:56 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:23:55.850Z\",\n",
      "  \"batchId\" : 70,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.5552099533437014,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 539,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 9,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 643,\n",
      "    \"walCommit\" : 63\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3301\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3302\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3302\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.5552099533437014,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:23:56 INFO TaskSetManager: Finished task 0.0 in stage 211.0 (TID 211) in 530 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:23:56 INFO TaskSchedulerImpl: Removed TaskSet 211.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:23:56 INFO DAGScheduler: ResultStage 211 (start at NativeMethodAccessorImpl.java:0) finished in 0.534 s\n",
      "25/05/05 11:23:56 INFO DAGScheduler: Job 212 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:23:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 211: Stage finished\n",
      "25/05/05 11:23:56 INFO DAGScheduler: Job 212 finished: start at NativeMethodAccessorImpl.java:0, took 0.534961 s\n",
      "25/05/05 11:23:56 INFO MemoryStore: Block broadcast_282 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:23:56 INFO MemoryStore: Block broadcast_282_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:23:56 INFO BlockManagerInfo: Added broadcast_282_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:23:56 INFO SparkContext: Created broadcast 282 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:23:56 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/70 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.70.7a78733a-3d31-4bab-9a6d-ad838dda3014.tmp\n",
      "25/05/05 11:23:56 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.70.7a78733a-3d31-4bab-9a6d-ad838dda3014.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/70\n",
      "25/05/05 11:23:56 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:23:55.849Z\",\n",
      "  \"batchId\" : 70,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.4858841010401187,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 568,\n",
      "    \"commitOffsets\" : 26,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 6,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 673,\n",
      "    \"walCommit\" : 66\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3301\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3302\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3302\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.4858841010401187,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 70\n",
      "-------------------------------------------\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION      |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R016|R032|00-00-01|TIME SQ-42 ST|05/05/2025|11:23:54|REGULAR|6          |3         |2025-05-05 11:23:55.859|\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:24:01 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/71 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.71.2acf0a0e-ba1f-47bf-8bda-3faf8bd48a33.tmp\n",
      "25/05/05 11:24:01 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/71 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.71.13818bc8-19ef-43c6-bc58-87021168c5a1.tmp\n",
      "25/05/05 11:24:01 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/71 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.71.3ce11c17-c5e2-4dd2-aff2-1acd97247557.tmp\n",
      "25/05/05 11:24:01 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.71.13818bc8-19ef-43c6-bc58-87021168c5a1.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/71\n",
      "25/05/05 11:24:01 INFO MicroBatchExecution: Committed offsets for batch 71. Metadata OffsetSeqMetadata(0,1746458641333,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:24:01 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.71.2acf0a0e-ba1f-47bf-8bda-3faf8bd48a33.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/71\n",
      "25/05/05 11:24:01 INFO MicroBatchExecution: Committed offsets for batch 71. Metadata OffsetSeqMetadata(0,1746458641334,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:24:01 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.71.3ce11c17-c5e2-4dd2-aff2-1acd97247557.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/71\n",
      "25/05/05 11:24:01 INFO MicroBatchExecution: Committed offsets for batch 71. Metadata OffsetSeqMetadata(0,1746458641333,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:24:01 INFO IncrementalExecution: Current batch timestamp = 1746458641333\n",
      "25/05/05 11:24:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:01 INFO IncrementalExecution: Current batch timestamp = 1746458641333\n",
      "25/05/05 11:24:01 INFO IncrementalExecution: Current batch timestamp = 1746458641334\n",
      "25/05/05 11:24:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:01 INFO IncrementalExecution: Current batch timestamp = 1746458641333\n",
      "25/05/05 11:24:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:01 INFO IncrementalExecution: Current batch timestamp = 1746458641334\n",
      "25/05/05 11:24:01 INFO IncrementalExecution: Current batch timestamp = 1746458641333\n",
      "25/05/05 11:24:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:01 INFO IncrementalExecution: Current batch timestamp = 1746458641333\n",
      "25/05/05 11:24:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:01 INFO IncrementalExecution: Current batch timestamp = 1746458641333\n",
      "25/05/05 11:24:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:01 INFO CodeGenerator: Code generated in 4.671584 ms\n",
      "25/05/05 11:24:01 INFO CodeGenerator: Code generated in 3.527417 ms\n",
      "25/05/05 11:24:01 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 71, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4818dc28]. The input RDD has 1 partitions.\n",
      "25/05/05 11:24:01 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 71, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:24:01 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:01 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:01 INFO DAGScheduler: Got job 213 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:24:01 INFO DAGScheduler: Final stage: ResultStage 212 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:24:01 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:24:01 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:24:01 INFO DAGScheduler: Submitting ResultStage 212 (MapPartitionsRDD[1147] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:24:01 INFO MemoryStore: Block broadcast_283 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:24:01 INFO MemoryStore: Block broadcast_283_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.1 MiB)\n",
      "25/05/05 11:24:01 INFO BlockManagerInfo: Added broadcast_283_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:01 INFO SparkContext: Created broadcast 283 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:24:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 212 (MapPartitionsRDD[1147] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:24:01 INFO TaskSchedulerImpl: Adding task set 212.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:24:01 INFO DAGScheduler: Got job 214 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:24:01 INFO DAGScheduler: Final stage: ResultStage 213 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:24:01 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:24:01 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:24:01 INFO DAGScheduler: Submitting ResultStage 213 (MapPartitionsRDD[1146] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:24:01 INFO TaskSetManager: Starting task 0.0 in stage 212.0 (TID 212) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:24:01 INFO Executor: Running task 0.0 in stage 212.0 (TID 212)\n",
      "25/05/05 11:24:01 INFO MemoryStore: Block broadcast_284 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:24:01 INFO MemoryStore: Block broadcast_284_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:24:01 INFO BlockManagerInfo: Added broadcast_284_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:01 INFO SparkContext: Created broadcast 284 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:24:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 213 (MapPartitionsRDD[1146] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:24:01 INFO TaskSchedulerImpl: Adding task set 213.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:24:01 INFO TaskSetManager: Starting task 0.0 in stage 213.0 (TID 213) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:24:01 INFO Executor: Running task 0.0 in stage 213.0 (TID 213)\n",
      "25/05/05 11:24:01 INFO CodeGenerator: Code generated in 3.845916 ms\n",
      "25/05/05 11:24:01 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3302 untilOffset=3303, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=71 taskId=213 partitionId=0\n",
      "25/05/05 11:24:01 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3302 untilOffset=3303, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=71 taskId=212 partitionId=0\n",
      "25/05/05 11:24:01 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3302 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:01 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3302 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:01 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:01 INFO DAGScheduler: Got job 215 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:24:01 INFO DAGScheduler: Final stage: ResultStage 214 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:24:01 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:24:01 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:24:01 INFO DAGScheduler: Submitting ResultStage 214 (MapPartitionsRDD[1152] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:24:01 INFO MemoryStore: Block broadcast_285 stored as values in memory (estimated size 47.1 KiB, free 366.0 MiB)\n",
      "25/05/05 11:24:01 INFO MemoryStore: Block broadcast_285_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:24:01 INFO BlockManagerInfo: Added broadcast_285_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:01 INFO SparkContext: Created broadcast 285 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:24:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 214 (MapPartitionsRDD[1152] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:24:01 INFO TaskSchedulerImpl: Adding task set 214.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:24:01 INFO TaskSetManager: Starting task 0.0 in stage 214.0 (TID 214) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:24:01 INFO Executor: Running task 0.0 in stage 214.0 (TID 214)\n",
      "25/05/05 11:24:01 INFO CodeGenerator: Code generated in 3.631291 ms\n",
      "25/05/05 11:24:01 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3302 untilOffset=3303, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=71 taskId=214 partitionId=0\n",
      "25/05/05 11:24:01 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3302 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3303, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3303, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:01 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:24:01 INFO DataWritingSparkTask: Committed partition 0 (task 213, attempt 0, stage 213.0)\n",
      "25/05/05 11:24:01 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 505504375 nanos, during time span of 505974166 nanos.\n",
      "25/05/05 11:24:01 INFO Executor: Finished task 0.0 in stage 213.0 (TID 213). 2145 bytes result sent to driver\n",
      "25/05/05 11:24:01 INFO TaskSetManager: Finished task 0.0 in stage 213.0 (TID 213) in 513 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:24:01 INFO TaskSchedulerImpl: Removed TaskSet 213.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:24:01 INFO DAGScheduler: ResultStage 213 (start at NativeMethodAccessorImpl.java:0) finished in 0.514 s\n",
      "25/05/05 11:24:01 INFO DAGScheduler: Job 214 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:24:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 213: Stage finished\n",
      "25/05/05 11:24:01 INFO DAGScheduler: Job 214 finished: start at NativeMethodAccessorImpl.java:0, took 0.516889 s\n",
      "25/05/05 11:24:01 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 71, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:24:01 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:24:01 INFO DataWritingSparkTask: Committed partition 0 (task 212, attempt 0, stage 212.0)\n",
      "25/05/05 11:24:01 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 504611750 nanos, during time span of 506243583 nanos.\n",
      "25/05/05 11:24:01 INFO Executor: Finished task 0.0 in stage 212.0 (TID 212). 3516 bytes result sent to driver\n",
      "25/05/05 11:24:01 INFO TaskSetManager: Finished task 0.0 in stage 212.0 (TID 212) in 515 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:24:01 INFO TaskSchedulerImpl: Removed TaskSet 212.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:24:01 INFO DAGScheduler: ResultStage 212 (start at NativeMethodAccessorImpl.java:0) finished in 0.518 s\n",
      "25/05/05 11:24:01 INFO DAGScheduler: Job 213 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:24:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 212: Stage finished\n",
      "25/05/05 11:24:01 INFO DAGScheduler: Job 213 finished: start at NativeMethodAccessorImpl.java:0, took 0.518261 s\n",
      "25/05/05 11:24:01 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 71, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4818dc28] is committing.\n",
      "25/05/05 11:24:01 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 71, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4818dc28] committed.\n",
      "25/05/05 11:24:01 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 71, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:24:01 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/71 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.71.8c07241f-6626-49f7-bcd0-7924ed754195.tmp\n",
      "25/05/05 11:24:01 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/71 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.71.16235767-b515-4106-ad5d-6a589938f338.tmp\n",
      "25/05/05 11:24:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3303, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:01 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:24:01 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:24:01 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:24:01 INFO connection: Opened connection [connectionId{localValue:141, serverValue:4511}] to localhost:27017\n",
      "25/05/05 11:24:01 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=233167}\n",
      "25/05/05 11:24:01 INFO connection: Opened connection [connectionId{localValue:142, serverValue:4512}] to localhost:27017\n",
      "25/05/05 11:24:01 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:24:01 INFO connection: Closed connection [connectionId{localValue:142, serverValue:4512}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:24:01 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 507154084 nanos, during time span of 512591292 nanos.\n",
      "25/05/05 11:24:01 INFO Executor: Finished task 0.0 in stage 214.0 (TID 214). 1645 bytes result sent to driver\n",
      "25/05/05 11:24:01 INFO TaskSetManager: Finished task 0.0 in stage 214.0 (TID 214) in 523 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:24:01 INFO TaskSchedulerImpl: Removed TaskSet 214.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:24:01 INFO DAGScheduler: ResultStage 214 (start at NativeMethodAccessorImpl.java:0) finished in 0.526 s\n",
      "25/05/05 11:24:01 INFO DAGScheduler: Job 215 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:24:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 214: Stage finished\n",
      "25/05/05 11:24:01 INFO DAGScheduler: Job 215 finished: start at NativeMethodAccessorImpl.java:0, took 0.527270 s\n",
      "25/05/05 11:24:01 INFO MemoryStore: Block broadcast_286 stored as values in memory (estimated size 248.0 B, free 366.0 MiB)\n",
      "25/05/05 11:24:01 INFO MemoryStore: Block broadcast_286_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.0 MiB)\n",
      "25/05/05 11:24:01 INFO BlockManagerInfo: Added broadcast_286_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:24:01 INFO SparkContext: Created broadcast 286 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:01 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.71.8c07241f-6626-49f7-bcd0-7924ed754195.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/71\n",
      "25/05/05 11:24:01 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:24:01.332Z\",\n",
      "  \"batchId\" : 71,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "  \"processedRowsPerSecond\" : 1.6420361247947455,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 532,\n",
      "    \"commitOffsets\" : 31,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 609,\n",
      "    \"walCommit\" : 41\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3302\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3303\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3303\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "    \"processedRowsPerSecond\" : 1.6420361247947455,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:24:01 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/71 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.71.aa89530a-4e3d-4c23-bc24-a65e8b37b91a.tmp\n",
      "25/05/05 11:24:01 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.71.16235767-b515-4106-ad5d-6a589938f338.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/71\n",
      "25/05/05 11:24:01 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:24:01.332Z\",\n",
      "  \"batchId\" : 71,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "  \"processedRowsPerSecond\" : 1.6260162601626016,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 538,\n",
      "    \"commitOffsets\" : 32,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 615,\n",
      "    \"walCommit\" : 39\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3302\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3303\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3303\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "    \"processedRowsPerSecond\" : 1.6260162601626016,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:24:01 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.71.aa89530a-4e3d-4c23-bc24-a65e8b37b91a.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/71\n",
      "25/05/05 11:24:01 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:24:01.332Z\",\n",
      "  \"batchId\" : 71,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 40.0,\n",
      "  \"processedRowsPerSecond\" : 1.5822784810126582,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 559,\n",
      "    \"commitOffsets\" : 26,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 632,\n",
      "    \"walCommit\" : 40\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3302\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3303\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3303\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 40.0,\n",
      "    \"processedRowsPerSecond\" : 1.5822784810126582,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 71\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R023|R045|00-00-00|34 ST-HERALD SQ|05/05/2025|11:24:00|REGULAR|3          |7         |2025-05-05 11:24:01.333|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:24:06 INFO BlockManagerInfo: Removed broadcast_280_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:06 INFO BlockManagerInfo: Removed broadcast_286_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:24:06 INFO BlockManagerInfo: Removed broadcast_281_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:06 INFO BlockManagerInfo: Removed broadcast_285_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:24:06 INFO BlockManagerInfo: Removed broadcast_284_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:24:06 INFO BlockManagerInfo: Removed broadcast_283_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:24:06 INFO BlockManagerInfo: Removed broadcast_279_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:24:06 INFO BlockManagerInfo: Removed broadcast_282_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:24:06 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/72 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.72.a31649b0-1bf7-4054-ad60-0d9e28f9e991.tmp\n",
      "25/05/05 11:24:06 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/72 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.72.1e8869ff-678f-40c3-8269-52d7ee127213.tmp\n",
      "25/05/05 11:24:06 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/72 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.72.7756c3d9-5a58-4467-8523-7c62d0ce4794.tmp\n",
      "25/05/05 11:24:06 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.72.1e8869ff-678f-40c3-8269-52d7ee127213.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/72\n",
      "25/05/05 11:24:06 INFO MicroBatchExecution: Committed offsets for batch 72. Metadata OffsetSeqMetadata(0,1746458646809,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:24:06 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.72.a31649b0-1bf7-4054-ad60-0d9e28f9e991.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/72\n",
      "25/05/05 11:24:06 INFO MicroBatchExecution: Committed offsets for batch 72. Metadata OffsetSeqMetadata(0,1746458646808,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:24:06 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.72.7756c3d9-5a58-4467-8523-7c62d0ce4794.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/72\n",
      "25/05/05 11:24:06 INFO MicroBatchExecution: Committed offsets for batch 72. Metadata OffsetSeqMetadata(0,1746458646808,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:24:06 INFO IncrementalExecution: Current batch timestamp = 1746458646809\n",
      "25/05/05 11:24:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:06 INFO IncrementalExecution: Current batch timestamp = 1746458646808\n",
      "25/05/05 11:24:06 INFO IncrementalExecution: Current batch timestamp = 1746458646808\n",
      "25/05/05 11:24:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:06 INFO IncrementalExecution: Current batch timestamp = 1746458646809\n",
      "25/05/05 11:24:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:06 INFO IncrementalExecution: Current batch timestamp = 1746458646808\n",
      "25/05/05 11:24:06 INFO IncrementalExecution: Current batch timestamp = 1746458646808\n",
      "25/05/05 11:24:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:06 INFO IncrementalExecution: Current batch timestamp = 1746458646809\n",
      "25/05/05 11:24:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:06 INFO IncrementalExecution: Current batch timestamp = 1746458646808\n",
      "25/05/05 11:24:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:06 INFO CodeGenerator: Code generated in 4.268167 ms\n",
      "25/05/05 11:24:06 INFO CodeGenerator: Code generated in 4.494583 ms\n",
      "25/05/05 11:24:06 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 72, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:24:06 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 72, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@13e0ea69]. The input RDD has 1 partitions.\n",
      "25/05/05 11:24:06 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:06 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:06 INFO DAGScheduler: Got job 216 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:24:06 INFO DAGScheduler: Final stage: ResultStage 215 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:24:06 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:24:06 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:24:06 INFO DAGScheduler: Submitting ResultStage 215 (MapPartitionsRDD[1162] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:24:06 INFO MemoryStore: Block broadcast_287 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:24:06 INFO MemoryStore: Block broadcast_287_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:24:06 INFO BlockManagerInfo: Added broadcast_287_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:24:06 INFO SparkContext: Created broadcast 287 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:24:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 215 (MapPartitionsRDD[1162] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:24:06 INFO TaskSchedulerImpl: Adding task set 215.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:24:06 INFO DAGScheduler: Got job 217 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:24:06 INFO DAGScheduler: Final stage: ResultStage 216 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:24:06 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:24:06 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:24:06 INFO DAGScheduler: Submitting ResultStage 216 (MapPartitionsRDD[1163] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:24:06 INFO TaskSetManager: Starting task 0.0 in stage 215.0 (TID 215) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:24:06 INFO MemoryStore: Block broadcast_288 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:24:06 INFO Executor: Running task 0.0 in stage 215.0 (TID 215)\n",
      "25/05/05 11:24:06 INFO MemoryStore: Block broadcast_288_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:24:06 INFO BlockManagerInfo: Added broadcast_288_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:24:06 INFO SparkContext: Created broadcast 288 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:24:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 216 (MapPartitionsRDD[1163] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:24:06 INFO TaskSchedulerImpl: Adding task set 216.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:24:06 INFO TaskSetManager: Starting task 0.0 in stage 216.0 (TID 216) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:24:06 INFO Executor: Running task 0.0 in stage 216.0 (TID 216)\n",
      "25/05/05 11:24:06 INFO CodeGenerator: Code generated in 4.450209 ms\n",
      "25/05/05 11:24:06 INFO CodeGenerator: Code generated in 3.79125 ms\n",
      "25/05/05 11:24:06 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3303 untilOffset=3304, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=72 taskId=215 partitionId=0\n",
      "25/05/05 11:24:06 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3303 untilOffset=3304, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=72 taskId=216 partitionId=0\n",
      "25/05/05 11:24:06 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3303 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:06 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3303 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:06 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:06 INFO DAGScheduler: Got job 218 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:24:06 INFO DAGScheduler: Final stage: ResultStage 217 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:24:06 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:24:06 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:24:06 INFO DAGScheduler: Submitting ResultStage 217 (MapPartitionsRDD[1168] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:24:06 INFO MemoryStore: Block broadcast_289 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:24:06 INFO MemoryStore: Block broadcast_289_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:24:06 INFO BlockManagerInfo: Added broadcast_289_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:06 INFO SparkContext: Created broadcast 289 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:24:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 217 (MapPartitionsRDD[1168] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:24:06 INFO TaskSchedulerImpl: Adding task set 217.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:24:06 INFO TaskSetManager: Starting task 0.0 in stage 217.0 (TID 217) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:24:06 INFO Executor: Running task 0.0 in stage 217.0 (TID 217)\n",
      "25/05/05 11:24:06 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3303 untilOffset=3304, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=72 taskId=217 partitionId=0\n",
      "25/05/05 11:24:06 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3303 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3304, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3304, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:07 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:24:07 INFO DataWritingSparkTask: Committed partition 0 (task 215, attempt 0, stage 215.0)\n",
      "25/05/05 11:24:07 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 503248833 nanos, during time span of 503606541 nanos.\n",
      "25/05/05 11:24:07 INFO Executor: Finished task 0.0 in stage 215.0 (TID 215). 2145 bytes result sent to driver\n",
      "25/05/05 11:24:07 INFO TaskSetManager: Finished task 0.0 in stage 215.0 (TID 215) in 514 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:24:07 INFO TaskSchedulerImpl: Removed TaskSet 215.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:24:07 INFO DAGScheduler: ResultStage 215 (start at NativeMethodAccessorImpl.java:0) finished in 0.515 s\n",
      "25/05/05 11:24:07 INFO DAGScheduler: Job 216 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:24:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 215: Stage finished\n",
      "25/05/05 11:24:07 INFO DAGScheduler: Job 216 finished: start at NativeMethodAccessorImpl.java:0, took 0.515947 s\n",
      "25/05/05 11:24:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 72, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:24:07 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:24:07 INFO DataWritingSparkTask: Committed partition 0 (task 216, attempt 0, stage 216.0)\n",
      "25/05/05 11:24:07 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 502191709 nanos, during time span of 504210958 nanos.\n",
      "25/05/05 11:24:07 INFO Executor: Finished task 0.0 in stage 216.0 (TID 216). 3516 bytes result sent to driver\n",
      "25/05/05 11:24:07 INFO TaskSetManager: Finished task 0.0 in stage 216.0 (TID 216) in 515 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:24:07 INFO TaskSchedulerImpl: Removed TaskSet 216.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:24:07 INFO DAGScheduler: ResultStage 216 (start at NativeMethodAccessorImpl.java:0) finished in 0.516 s\n",
      "25/05/05 11:24:07 INFO DAGScheduler: Job 217 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:24:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 216: Stage finished\n",
      "25/05/05 11:24:07 INFO DAGScheduler: Job 217 finished: start at NativeMethodAccessorImpl.java:0, took 0.518149 s\n",
      "25/05/05 11:24:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 72, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@13e0ea69] is committing.\n",
      "25/05/05 11:24:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 72, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@13e0ea69] committed.\n",
      "25/05/05 11:24:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 72, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:24:07 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/72 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.72.41d0fe87-d54c-4054-b470-acf0299b3114.tmp\n",
      "25/05/05 11:24:07 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/72 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.72.b4b1c0cb-c00a-4a5b-9561-5627fba86c1f.tmp\n",
      "25/05/05 11:24:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3304, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:07 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:24:07 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:24:07 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:24:07 INFO connection: Opened connection [connectionId{localValue:143, serverValue:4513}] to localhost:27017\n",
      "25/05/05 11:24:07 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=182375}\n",
      "25/05/05 11:24:07 INFO connection: Opened connection [connectionId{localValue:144, serverValue:4514}] to localhost:27017\n",
      "25/05/05 11:24:07 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:24:07 INFO connection: Closed connection [connectionId{localValue:144, serverValue:4514}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:24:07 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 505595750 nanos, during time span of 509599584 nanos.\n",
      "25/05/05 11:24:07 INFO Executor: Finished task 0.0 in stage 217.0 (TID 217). 1645 bytes result sent to driver\n",
      "25/05/05 11:24:07 INFO TaskSetManager: Finished task 0.0 in stage 217.0 (TID 217) in 516 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:24:07 INFO TaskSchedulerImpl: Removed TaskSet 217.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:24:07 INFO DAGScheduler: ResultStage 217 (start at NativeMethodAccessorImpl.java:0) finished in 0.521 s\n",
      "25/05/05 11:24:07 INFO DAGScheduler: Job 218 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:24:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 217: Stage finished\n",
      "25/05/05 11:24:07 INFO DAGScheduler: Job 218 finished: start at NativeMethodAccessorImpl.java:0, took 0.521372 s\n",
      "25/05/05 11:24:07 INFO MemoryStore: Block broadcast_290 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:24:07 INFO MemoryStore: Block broadcast_290_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:24:07 INFO BlockManagerInfo: Added broadcast_290_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:24:07 INFO SparkContext: Created broadcast 290 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:07 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/72 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.72.0f8526ce-b243-44af-9c6e-f33522a013af.tmp\n",
      "25/05/05 11:24:07 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.72.41d0fe87-d54c-4054-b470-acf0299b3114.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/72\n",
      "25/05/05 11:24:07 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:24:06.807Z\",\n",
      "  \"batchId\" : 72,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.6025641025641026,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 535,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 6,\n",
      "    \"triggerExecution\" : 624,\n",
      "    \"walCommit\" : 53\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3303\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3304\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3304\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.6025641025641026,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:24:07 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.72.b4b1c0cb-c00a-4a5b-9561-5627fba86c1f.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/72\n",
      "25/05/05 11:24:07 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:24:06.807Z\",\n",
      "  \"batchId\" : 72,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.589825119236884,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 543,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 629,\n",
      "    \"walCommit\" : 51\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3303\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3304\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3304\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.589825119236884,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:24:07 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.72.0f8526ce-b243-44af-9c6e-f33522a013af.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/72\n",
      "25/05/05 11:24:07 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:24:06.807Z\",\n",
      "  \"batchId\" : 72,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 62.5,\n",
      "  \"processedRowsPerSecond\" : 1.557632398753894,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 556,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 642,\n",
      "    \"walCommit\" : 54\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3303\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3304\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3304\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 62.5,\n",
      "    \"processedRowsPerSecond\" : 1.557632398753894,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 72\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A001|R079|01-00-00|WORLD TRADE CTR|05/05/2025|11:24:05|REGULAR|6          |11        |2025-05-05 11:24:06.809|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:24:13 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/73 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.73.81b61286-08fe-43e6-ac0a-92f9659ae891.tmp\n",
      "25/05/05 11:24:13 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/73 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.73.b8b9ce25-9870-4da9-97f5-6f586dcbd7ce.tmp\n",
      "25/05/05 11:24:13 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/73 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.73.480ffe47-2031-455b-9e40-6db07a1dbd87.tmp\n",
      "25/05/05 11:24:13 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.73.b8b9ce25-9870-4da9-97f5-6f586dcbd7ce.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/73\n",
      "25/05/05 11:24:13 INFO MicroBatchExecution: Committed offsets for batch 73. Metadata OffsetSeqMetadata(0,1746458653276,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:24:13 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.73.81b61286-08fe-43e6-ac0a-92f9659ae891.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/73\n",
      "25/05/05 11:24:13 INFO MicroBatchExecution: Committed offsets for batch 73. Metadata OffsetSeqMetadata(0,1746458653278,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:24:13 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.73.480ffe47-2031-455b-9e40-6db07a1dbd87.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/73\n",
      "25/05/05 11:24:13 INFO MicroBatchExecution: Committed offsets for batch 73. Metadata OffsetSeqMetadata(0,1746458653277,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:24:13 INFO IncrementalExecution: Current batch timestamp = 1746458653278\n",
      "25/05/05 11:24:13 INFO IncrementalExecution: Current batch timestamp = 1746458653276\n",
      "25/05/05 11:24:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:13 INFO IncrementalExecution: Current batch timestamp = 1746458653277\n",
      "25/05/05 11:24:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:13 INFO IncrementalExecution: Current batch timestamp = 1746458653276\n",
      "25/05/05 11:24:13 INFO IncrementalExecution: Current batch timestamp = 1746458653278\n",
      "25/05/05 11:24:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:13 INFO IncrementalExecution: Current batch timestamp = 1746458653277\n",
      "25/05/05 11:24:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:13 INFO IncrementalExecution: Current batch timestamp = 1746458653276\n",
      "25/05/05 11:24:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:13 INFO IncrementalExecution: Current batch timestamp = 1746458653277\n",
      "25/05/05 11:24:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:13 INFO CodeGenerator: Code generated in 4.418375 ms\n",
      "25/05/05 11:24:13 INFO CodeGenerator: Code generated in 4.077708 ms\n",
      "25/05/05 11:24:13 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 73, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6f829744]. The input RDD has 1 partitions.\n",
      "25/05/05 11:24:13 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:13 INFO DAGScheduler: Got job 219 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:24:13 INFO DAGScheduler: Final stage: ResultStage 218 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:24:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:24:13 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:24:13 INFO DAGScheduler: Submitting ResultStage 218 (MapPartitionsRDD[1176] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:24:13 INFO CodeGenerator: Code generated in 4.489125 ms\n",
      "25/05/05 11:24:13 INFO MemoryStore: Block broadcast_291 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:24:13 INFO MemoryStore: Block broadcast_291_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.1 MiB)\n",
      "25/05/05 11:24:13 INFO BlockManagerInfo: Added broadcast_291_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:13 INFO SparkContext: Created broadcast 291 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:24:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 218 (MapPartitionsRDD[1176] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:24:13 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 73, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:24:13 INFO TaskSchedulerImpl: Adding task set 218.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:24:13 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:13 INFO TaskSetManager: Starting task 0.0 in stage 218.0 (TID 218) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:24:13 INFO DAGScheduler: Got job 220 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:24:13 INFO DAGScheduler: Final stage: ResultStage 219 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:24:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:24:13 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:24:13 INFO DAGScheduler: Submitting ResultStage 219 (MapPartitionsRDD[1179] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:24:13 INFO Executor: Running task 0.0 in stage 218.0 (TID 218)\n",
      "25/05/05 11:24:13 INFO MemoryStore: Block broadcast_292 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:24:13 INFO MemoryStore: Block broadcast_292_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:24:13 INFO BlockManagerInfo: Added broadcast_292_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:13 INFO SparkContext: Created broadcast 292 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:24:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 219 (MapPartitionsRDD[1179] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:24:13 INFO TaskSchedulerImpl: Adding task set 219.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:24:13 INFO TaskSetManager: Starting task 0.0 in stage 219.0 (TID 219) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:24:13 INFO Executor: Running task 0.0 in stage 219.0 (TID 219)\n",
      "25/05/05 11:24:13 INFO CodeGenerator: Code generated in 4.1455 ms\n",
      "25/05/05 11:24:13 INFO CodeGenerator: Code generated in 3.679666 ms\n",
      "25/05/05 11:24:13 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3304 untilOffset=3305, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=73 taskId=219 partitionId=0\n",
      "25/05/05 11:24:13 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3304 untilOffset=3305, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=73 taskId=218 partitionId=0\n",
      "25/05/05 11:24:13 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3304 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:13 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3304 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:13 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:13 INFO DAGScheduler: Got job 221 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:24:13 INFO DAGScheduler: Final stage: ResultStage 220 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:24:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:24:13 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:24:13 INFO DAGScheduler: Submitting ResultStage 220 (MapPartitionsRDD[1184] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:24:13 INFO MemoryStore: Block broadcast_293 stored as values in memory (estimated size 47.1 KiB, free 366.0 MiB)\n",
      "25/05/05 11:24:13 INFO MemoryStore: Block broadcast_293_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:24:13 INFO BlockManagerInfo: Added broadcast_293_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:13 INFO SparkContext: Created broadcast 293 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:24:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 220 (MapPartitionsRDD[1184] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:24:13 INFO TaskSchedulerImpl: Adding task set 220.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:24:13 INFO TaskSetManager: Starting task 0.0 in stage 220.0 (TID 220) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:24:13 INFO Executor: Running task 0.0 in stage 220.0 (TID 220)\n",
      "25/05/05 11:24:13 INFO CodeGenerator: Code generated in 4.382334 ms\n",
      "25/05/05 11:24:13 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3304 untilOffset=3305, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=73 taskId=220 partitionId=0\n",
      "25/05/05 11:24:13 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3304 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3305, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3305, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:13 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:24:13 INFO DataWritingSparkTask: Committed partition 0 (task 219, attempt 0, stage 219.0)\n",
      "25/05/05 11:24:13 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 506592083 nanos, during time span of 506819292 nanos.\n",
      "25/05/05 11:24:13 INFO Executor: Finished task 0.0 in stage 219.0 (TID 219). 2145 bytes result sent to driver\n",
      "25/05/05 11:24:13 INFO TaskSetManager: Finished task 0.0 in stage 219.0 (TID 219) in 515 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:24:13 INFO TaskSchedulerImpl: Removed TaskSet 219.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:24:13 INFO DAGScheduler: ResultStage 219 (start at NativeMethodAccessorImpl.java:0) finished in 0.516 s\n",
      "25/05/05 11:24:13 INFO DAGScheduler: Job 220 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:24:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 219: Stage finished\n",
      "25/05/05 11:24:13 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:24:13 INFO DataWritingSparkTask: Committed partition 0 (task 218, attempt 0, stage 218.0)\n",
      "25/05/05 11:24:13 INFO DAGScheduler: Job 220 finished: start at NativeMethodAccessorImpl.java:0, took 0.516766 s\n",
      "25/05/05 11:24:13 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 505347625 nanos, during time span of 507142250 nanos.\n",
      "25/05/05 11:24:13 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 73, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:24:13 INFO Executor: Finished task 0.0 in stage 218.0 (TID 218). 3516 bytes result sent to driver\n",
      "25/05/05 11:24:13 INFO TaskSetManager: Finished task 0.0 in stage 218.0 (TID 218) in 517 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:24:13 INFO TaskSchedulerImpl: Removed TaskSet 218.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:24:13 INFO DAGScheduler: ResultStage 218 (start at NativeMethodAccessorImpl.java:0) finished in 0.519 s\n",
      "25/05/05 11:24:13 INFO DAGScheduler: Job 219 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:24:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 218: Stage finished\n",
      "25/05/05 11:24:13 INFO DAGScheduler: Job 219 finished: start at NativeMethodAccessorImpl.java:0, took 0.519740 s\n",
      "25/05/05 11:24:13 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 73, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6f829744] is committing.\n",
      "25/05/05 11:24:13 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 73, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6f829744] committed.\n",
      "25/05/05 11:24:13 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 73, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:24:13 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/73 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.73.7aef674d-2204-44db-89ff-05cccbe406c5.tmp\n",
      "25/05/05 11:24:13 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/73 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.73.47155289-f6f4-4098-977f-874bc0b17219.tmp\n",
      "25/05/05 11:24:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3305, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:13 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:24:13 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:24:13 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:24:13 INFO connection: Opened connection [connectionId{localValue:145, serverValue:4515}] to localhost:27017\n",
      "25/05/05 11:24:13 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=416125}\n",
      "25/05/05 11:24:13 INFO connection: Opened connection [connectionId{localValue:146, serverValue:4516}] to localhost:27017\n",
      "25/05/05 11:24:13 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:24:13 INFO connection: Closed connection [connectionId{localValue:146, serverValue:4516}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:24:13 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 502088208 nanos, during time span of 512084709 nanos.\n",
      "25/05/05 11:24:13 INFO Executor: Finished task 0.0 in stage 220.0 (TID 220). 1645 bytes result sent to driver\n",
      "25/05/05 11:24:13 INFO TaskSetManager: Finished task 0.0 in stage 220.0 (TID 220) in 523 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:24:13 INFO TaskSchedulerImpl: Removed TaskSet 220.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:24:13 INFO DAGScheduler: ResultStage 220 (start at NativeMethodAccessorImpl.java:0) finished in 0.527 s\n",
      "25/05/05 11:24:13 INFO DAGScheduler: Job 221 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:24:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 220: Stage finished\n",
      "25/05/05 11:24:13 INFO DAGScheduler: Job 221 finished: start at NativeMethodAccessorImpl.java:0, took 0.528768 s\n",
      "25/05/05 11:24:13 INFO MemoryStore: Block broadcast_294 stored as values in memory (estimated size 248.0 B, free 366.0 MiB)\n",
      "25/05/05 11:24:13 INFO MemoryStore: Block broadcast_294_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.0 MiB)\n",
      "25/05/05 11:24:13 INFO BlockManagerInfo: Added broadcast_294_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:24:13 INFO SparkContext: Created broadcast 294 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:13 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.73.7aef674d-2204-44db-89ff-05cccbe406c5.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/73\n",
      "25/05/05 11:24:13 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:24:13.274Z\",\n",
      "  \"batchId\" : 73,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 62.5,\n",
      "  \"processedRowsPerSecond\" : 1.5873015873015872,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 534,\n",
      "    \"commitOffsets\" : 31,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 630,\n",
      "    \"walCommit\" : 59\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3304\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3305\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3305\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 62.5,\n",
      "    \"processedRowsPerSecond\" : 1.5873015873015872,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:24:13 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/73 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.73.f59d9378-f1ef-43ea-bace-ca994bbb35e6.tmp\n",
      "25/05/05 11:24:13 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.73.47155289-f6f4-4098-977f-874bc0b17219.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/73\n",
      "25/05/05 11:24:13 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:24:13.274Z\",\n",
      "  \"batchId\" : 73,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 62.5,\n",
      "  \"processedRowsPerSecond\" : 1.5772870662460567,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 539,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 634,\n",
      "    \"walCommit\" : 60\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3304\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3305\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3305\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 62.5,\n",
      "    \"processedRowsPerSecond\" : 1.5772870662460567,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:24:13 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.73.f59d9378-f1ef-43ea-bace-ca994bbb35e6.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/73\n",
      "25/05/05 11:24:13 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:24:13.274Z\",\n",
      "  \"batchId\" : 73,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 62.5,\n",
      "  \"processedRowsPerSecond\" : 1.5313935681470137,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 561,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 4,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 653,\n",
      "    \"walCommit\" : 57\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3304\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3305\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3305\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 62.5,\n",
      "    \"processedRowsPerSecond\" : 1.5313935681470137,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 73\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R023|R045|00-00-00|34 ST-HERALD SQ|05/05/2025|11:24:12|REGULAR|3          |6         |2025-05-05 11:24:13.277|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:24:19 INFO BlockManagerInfo: Removed broadcast_291_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:19 INFO BlockManagerInfo: Removed broadcast_288_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:19 INFO BlockManagerInfo: Removed broadcast_287_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:19 INFO BlockManagerInfo: Removed broadcast_294_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:24:19 INFO BlockManagerInfo: Removed broadcast_289_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:19 INFO BlockManagerInfo: Removed broadcast_290_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:24:19 INFO BlockManagerInfo: Removed broadcast_293_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:24:19 INFO BlockManagerInfo: Removed broadcast_292_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:24:19 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/74 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.74.74235178-0ecc-48ab-ad89-5e9554369bfb.tmp\n",
      "25/05/05 11:24:19 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/74 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.74.c9e60cd1-83c5-4def-9987-13fb7b3586fb.tmp\n",
      "25/05/05 11:24:19 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/74 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.74.ad1694c5-bed6-424c-83de-dcfff75ba925.tmp\n",
      "25/05/05 11:24:19 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.74.ad1694c5-bed6-424c-83de-dcfff75ba925.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/74\n",
      "25/05/05 11:24:19 INFO MicroBatchExecution: Committed offsets for batch 74. Metadata OffsetSeqMetadata(0,1746458659764,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:24:19 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.74.74235178-0ecc-48ab-ad89-5e9554369bfb.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/74\n",
      "25/05/05 11:24:19 INFO MicroBatchExecution: Committed offsets for batch 74. Metadata OffsetSeqMetadata(0,1746458659764,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:24:19 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.74.c9e60cd1-83c5-4def-9987-13fb7b3586fb.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/74\n",
      "25/05/05 11:24:19 INFO MicroBatchExecution: Committed offsets for batch 74. Metadata OffsetSeqMetadata(0,1746458659762,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:24:19 INFO IncrementalExecution: Current batch timestamp = 1746458659764\n",
      "25/05/05 11:24:19 INFO IncrementalExecution: Current batch timestamp = 1746458659764\n",
      "25/05/05 11:24:19 INFO IncrementalExecution: Current batch timestamp = 1746458659762\n",
      "25/05/05 11:24:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:19 INFO IncrementalExecution: Current batch timestamp = 1746458659762\n",
      "25/05/05 11:24:19 INFO IncrementalExecution: Current batch timestamp = 1746458659764\n",
      "25/05/05 11:24:19 INFO IncrementalExecution: Current batch timestamp = 1746458659764\n",
      "25/05/05 11:24:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:19 INFO IncrementalExecution: Current batch timestamp = 1746458659764\n",
      "25/05/05 11:24:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:19 INFO IncrementalExecution: Current batch timestamp = 1746458659762\n",
      "25/05/05 11:24:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:19 INFO CodeGenerator: Code generated in 4.674291 ms\n",
      "25/05/05 11:24:19 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:19 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 74, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@640e54d2]. The input RDD has 1 partitions.\n",
      "25/05/05 11:24:19 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:19 INFO DAGScheduler: Got job 222 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:24:19 INFO DAGScheduler: Final stage: ResultStage 221 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:24:19 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:24:19 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:24:19 INFO DAGScheduler: Submitting ResultStage 221 (MapPartitionsRDD[1192] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:24:19 INFO MemoryStore: Block broadcast_295 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:24:19 INFO MemoryStore: Block broadcast_295_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:24:19 INFO BlockManagerInfo: Added broadcast_295_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:24:19 INFO SparkContext: Created broadcast 295 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:24:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 221 (MapPartitionsRDD[1192] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:24:19 INFO TaskSchedulerImpl: Adding task set 221.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:24:19 INFO TaskSetManager: Starting task 0.0 in stage 221.0 (TID 221) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:24:19 INFO Executor: Running task 0.0 in stage 221.0 (TID 221)\n",
      "25/05/05 11:24:19 INFO CodeGenerator: Code generated in 3.498792 ms\n",
      "25/05/05 11:24:19 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 74, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:24:19 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:19 INFO DAGScheduler: Got job 223 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:24:19 INFO DAGScheduler: Final stage: ResultStage 222 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:24:19 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:24:19 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:24:19 INFO DAGScheduler: Submitting ResultStage 222 (MapPartitionsRDD[1195] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:24:19 INFO MemoryStore: Block broadcast_296 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:24:19 INFO MemoryStore: Block broadcast_296_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:24:19 INFO BlockManagerInfo: Added broadcast_296_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:24:19 INFO CodeGenerator: Code generated in 4.290709 ms\n",
      "25/05/05 11:24:19 INFO SparkContext: Created broadcast 296 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:24:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 222 (MapPartitionsRDD[1195] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:24:19 INFO TaskSchedulerImpl: Adding task set 222.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:24:19 INFO TaskSetManager: Starting task 0.0 in stage 222.0 (TID 222) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:24:19 INFO Executor: Running task 0.0 in stage 222.0 (TID 222)\n",
      "25/05/05 11:24:19 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3305 untilOffset=3306, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=74 taskId=221 partitionId=0\n",
      "25/05/05 11:24:19 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3305 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:19 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:19 INFO CodeGenerator: Code generated in 4.426166 ms\n",
      "25/05/05 11:24:19 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3305 untilOffset=3306, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=74 taskId=222 partitionId=0\n",
      "25/05/05 11:24:19 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3305 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:19 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:19 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:19 INFO DAGScheduler: Got job 224 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:24:19 INFO DAGScheduler: Final stage: ResultStage 223 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:24:19 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:24:19 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:24:19 INFO DAGScheduler: Submitting ResultStage 223 (MapPartitionsRDD[1200] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:24:19 INFO MemoryStore: Block broadcast_297 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:24:19 INFO MemoryStore: Block broadcast_297_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:24:19 INFO BlockManagerInfo: Added broadcast_297_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:19 INFO SparkContext: Created broadcast 297 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:24:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 223 (MapPartitionsRDD[1200] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:24:19 INFO TaskSchedulerImpl: Adding task set 223.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:24:19 INFO TaskSetManager: Starting task 0.0 in stage 223.0 (TID 223) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:24:19 INFO Executor: Running task 0.0 in stage 223.0 (TID 223)\n",
      "25/05/05 11:24:19 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3305 untilOffset=3306, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=74 taskId=223 partitionId=0\n",
      "25/05/05 11:24:19 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3305 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:19 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3306, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:20 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:24:20 INFO DataWritingSparkTask: Committed partition 0 (task 221, attempt 0, stage 221.0)\n",
      "25/05/05 11:24:20 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 504512875 nanos, during time span of 506282125 nanos.\n",
      "25/05/05 11:24:20 INFO Executor: Finished task 0.0 in stage 221.0 (TID 221). 3505 bytes result sent to driver\n",
      "25/05/05 11:24:20 INFO TaskSetManager: Finished task 0.0 in stage 221.0 (TID 221) in 519 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:24:20 INFO TaskSchedulerImpl: Removed TaskSet 221.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:24:20 INFO DAGScheduler: ResultStage 221 (start at NativeMethodAccessorImpl.java:0) finished in 0.521 s\n",
      "25/05/05 11:24:20 INFO DAGScheduler: Job 222 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:24:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 221: Stage finished\n",
      "25/05/05 11:24:20 INFO DAGScheduler: Job 222 finished: start at NativeMethodAccessorImpl.java:0, took 0.522348 s\n",
      "25/05/05 11:24:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 74, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@640e54d2] is committing.\n",
      "25/05/05 11:24:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 74, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@640e54d2] committed.\n",
      "25/05/05 11:24:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3306, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:20 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:24:20 INFO DataWritingSparkTask: Committed partition 0 (task 222, attempt 0, stage 222.0)\n",
      "25/05/05 11:24:20 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 503951584 nanos, during time span of 504348458 nanos.\n",
      "25/05/05 11:24:20 INFO Executor: Finished task 0.0 in stage 222.0 (TID 222). 2145 bytes result sent to driver\n",
      "25/05/05 11:24:20 INFO TaskSetManager: Finished task 0.0 in stage 222.0 (TID 222) in 513 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:24:20 INFO TaskSchedulerImpl: Removed TaskSet 222.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:24:20 INFO DAGScheduler: ResultStage 222 (start at NativeMethodAccessorImpl.java:0) finished in 0.515 s\n",
      "25/05/05 11:24:20 INFO DAGScheduler: Job 223 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:24:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 222: Stage finished\n",
      "25/05/05 11:24:20 INFO DAGScheduler: Job 223 finished: start at NativeMethodAccessorImpl.java:0, took 0.517744 s\n",
      "25/05/05 11:24:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 74, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:24:20 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/74 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.74.600c8b0d-c780-43bb-96a8-8ed7a808d1d8.tmp\n",
      "25/05/05 11:24:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 74, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:24:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3306, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:20 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:24:20 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:24:20 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:24:20 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/74 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.74.aac21850-76ac-4cd0-948a-33ade5e49e5d.tmp\n",
      "25/05/05 11:24:20 INFO connection: Opened connection [connectionId{localValue:147, serverValue:4517}] to localhost:27017\n",
      "25/05/05 11:24:20 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=248542}\n",
      "25/05/05 11:24:20 INFO connection: Opened connection [connectionId{localValue:148, serverValue:4518}] to localhost:27017\n",
      "25/05/05 11:24:20 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:24:20 INFO connection: Closed connection [connectionId{localValue:148, serverValue:4518}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:24:20 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 503883875 nanos, during time span of 511400959 nanos.\n",
      "25/05/05 11:24:20 INFO Executor: Finished task 0.0 in stage 223.0 (TID 223). 1645 bytes result sent to driver\n",
      "25/05/05 11:24:20 INFO TaskSetManager: Finished task 0.0 in stage 223.0 (TID 223) in 517 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:24:20 INFO TaskSchedulerImpl: Removed TaskSet 223.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:24:20 INFO DAGScheduler: ResultStage 223 (start at NativeMethodAccessorImpl.java:0) finished in 0.522 s\n",
      "25/05/05 11:24:20 INFO DAGScheduler: Job 224 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:24:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 223: Stage finished\n",
      "25/05/05 11:24:20 INFO DAGScheduler: Job 224 finished: start at NativeMethodAccessorImpl.java:0, took 0.522659 s\n",
      "25/05/05 11:24:20 INFO MemoryStore: Block broadcast_298 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:24:20 INFO MemoryStore: Block broadcast_298_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:24:20 INFO BlockManagerInfo: Added broadcast_298_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:24:20 INFO SparkContext: Created broadcast 298 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:20 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/74 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.74.3e7fa6e3-600c-4370-bccc-3f346f505adc.tmp\n",
      "25/05/05 11:24:20 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.74.600c8b0d-c780-43bb-96a8-8ed7a808d1d8.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/74\n",
      "25/05/05 11:24:20 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:24:19.761Z\",\n",
      "  \"batchId\" : 74,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.567398119122257,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 534,\n",
      "    \"commitOffsets\" : 36,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 638,\n",
      "    \"walCommit\" : 59\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3305\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3306\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3306\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.567398119122257,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:24:20 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.74.aac21850-76ac-4cd0-948a-33ade5e49e5d.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/74\n",
      "25/05/05 11:24:20 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:24:19.761Z\",\n",
      "  \"batchId\" : 74,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.5600624024960998,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 544,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 641,\n",
      "    \"walCommit\" : 62\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3305\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3306\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3306\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.5600624024960998,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:24:20 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.74.3e7fa6e3-600c-4370-bccc-3f346f505adc.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/74\n",
      "25/05/05 11:24:20 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:24:19.760Z\",\n",
      "  \"batchId\" : 74,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.5267175572519083,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 560,\n",
      "    \"commitOffsets\" : 26,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 4,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 655,\n",
      "    \"walCommit\" : 59\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3305\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3306\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3306\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.5267175572519083,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 74\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION  |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A002|R170|00-00-00|FULTON ST|05/05/2025|11:24:18|REGULAR|11         |11        |2025-05-05 11:24:19.762|\n",
      "+----+----+--------+---------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:24:25 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/75 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.75.bfc2ce44-7493-4e88-92e6-1cc6ef89d71e.tmp\n",
      "25/05/05 11:24:25 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/75 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.75.2c31dd92-c3d0-4fcc-8490-4396bf6fa5c7.tmp\n",
      "25/05/05 11:24:25 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/75 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.75.0ba031ae-2203-4931-a242-94e6e648b359.tmp\n",
      "25/05/05 11:24:25 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.75.bfc2ce44-7493-4e88-92e6-1cc6ef89d71e.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/75\n",
      "25/05/05 11:24:25 INFO MicroBatchExecution: Committed offsets for batch 75. Metadata OffsetSeqMetadata(0,1746458665280,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:24:25 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.75.2c31dd92-c3d0-4fcc-8490-4396bf6fa5c7.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/75\n",
      "25/05/05 11:24:25 INFO MicroBatchExecution: Committed offsets for batch 75. Metadata OffsetSeqMetadata(0,1746458665278,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:24:25 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.75.0ba031ae-2203-4931-a242-94e6e648b359.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/75\n",
      "25/05/05 11:24:25 INFO MicroBatchExecution: Committed offsets for batch 75. Metadata OffsetSeqMetadata(0,1746458665278,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:24:25 INFO IncrementalExecution: Current batch timestamp = 1746458665280\n",
      "25/05/05 11:24:25 INFO IncrementalExecution: Current batch timestamp = 1746458665278\n",
      "25/05/05 11:24:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:25 INFO IncrementalExecution: Current batch timestamp = 1746458665278\n",
      "25/05/05 11:24:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:25 INFO IncrementalExecution: Current batch timestamp = 1746458665278\n",
      "25/05/05 11:24:25 INFO IncrementalExecution: Current batch timestamp = 1746458665278\n",
      "25/05/05 11:24:25 INFO IncrementalExecution: Current batch timestamp = 1746458665280\n",
      "25/05/05 11:24:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:25 INFO IncrementalExecution: Current batch timestamp = 1746458665278\n",
      "25/05/05 11:24:25 INFO IncrementalExecution: Current batch timestamp = 1746458665278\n",
      "25/05/05 11:24:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:25 INFO CodeGenerator: Code generated in 4.90575 ms\n",
      "25/05/05 11:24:25 INFO CodeGenerator: Code generated in 3.901417 ms\n",
      "25/05/05 11:24:25 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 75, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4c41b3ec]. The input RDD has 1 partitions.\n",
      "25/05/05 11:24:25 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 75, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:24:25 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:25 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:25 INFO DAGScheduler: Got job 225 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:24:25 INFO DAGScheduler: Final stage: ResultStage 224 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:24:25 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:24:25 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:24:25 INFO DAGScheduler: Submitting ResultStage 224 (MapPartitionsRDD[1210] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:24:25 INFO MemoryStore: Block broadcast_299 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:24:25 INFO MemoryStore: Block broadcast_299_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.1 MiB)\n",
      "25/05/05 11:24:25 INFO BlockManagerInfo: Added broadcast_299_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:25 INFO SparkContext: Created broadcast 299 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:24:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 224 (MapPartitionsRDD[1210] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:24:25 INFO TaskSchedulerImpl: Adding task set 224.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:24:25 INFO DAGScheduler: Got job 226 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:24:25 INFO DAGScheduler: Final stage: ResultStage 225 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:24:25 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:24:25 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:24:25 INFO DAGScheduler: Submitting ResultStage 225 (MapPartitionsRDD[1211] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:24:25 INFO TaskSetManager: Starting task 0.0 in stage 224.0 (TID 224) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:24:25 INFO MemoryStore: Block broadcast_300 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:24:25 INFO Executor: Running task 0.0 in stage 224.0 (TID 224)\n",
      "25/05/05 11:24:25 INFO MemoryStore: Block broadcast_300_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:24:25 INFO BlockManagerInfo: Added broadcast_300_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:25 INFO SparkContext: Created broadcast 300 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:24:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 225 (MapPartitionsRDD[1211] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:24:25 INFO TaskSchedulerImpl: Adding task set 225.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:24:25 INFO TaskSetManager: Starting task 0.0 in stage 225.0 (TID 225) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:24:25 INFO Executor: Running task 0.0 in stage 225.0 (TID 225)\n",
      "25/05/05 11:24:25 INFO CodeGenerator: Code generated in 3.693625 ms\n",
      "25/05/05 11:24:25 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3306 untilOffset=3307, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=75 taskId=225 partitionId=0\n",
      "25/05/05 11:24:25 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3306 untilOffset=3307, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=75 taskId=224 partitionId=0\n",
      "25/05/05 11:24:25 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3306 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:25 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3306 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:25 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:25 INFO DAGScheduler: Got job 227 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:24:25 INFO DAGScheduler: Final stage: ResultStage 226 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:24:25 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:24:25 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:24:25 INFO DAGScheduler: Submitting ResultStage 226 (MapPartitionsRDD[1216] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:24:25 INFO MemoryStore: Block broadcast_301 stored as values in memory (estimated size 47.1 KiB, free 366.0 MiB)\n",
      "25/05/05 11:24:25 INFO MemoryStore: Block broadcast_301_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:24:25 INFO BlockManagerInfo: Added broadcast_301_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:25 INFO SparkContext: Created broadcast 301 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:24:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 226 (MapPartitionsRDD[1216] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:24:25 INFO TaskSchedulerImpl: Adding task set 226.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:24:25 INFO TaskSetManager: Starting task 0.0 in stage 226.0 (TID 226) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:24:25 INFO Executor: Running task 0.0 in stage 226.0 (TID 226)\n",
      "25/05/05 11:24:25 INFO CodeGenerator: Code generated in 3.853541 ms\n",
      "25/05/05 11:24:25 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3306 untilOffset=3307, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=75 taskId=226 partitionId=0\n",
      "25/05/05 11:24:25 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3306 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3307, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3307, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:25 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:24:25 INFO DataWritingSparkTask: Committed partition 0 (task 225, attempt 0, stage 225.0)\n",
      "25/05/05 11:24:25 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 504146084 nanos, during time span of 504544583 nanos.\n",
      "25/05/05 11:24:25 INFO Executor: Finished task 0.0 in stage 225.0 (TID 225). 2145 bytes result sent to driver\n",
      "25/05/05 11:24:25 INFO TaskSetManager: Finished task 0.0 in stage 225.0 (TID 225) in 512 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:24:25 INFO TaskSchedulerImpl: Removed TaskSet 225.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:24:25 INFO DAGScheduler: ResultStage 225 (start at NativeMethodAccessorImpl.java:0) finished in 0.513 s\n",
      "25/05/05 11:24:25 INFO DAGScheduler: Job 226 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:24:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 225: Stage finished\n",
      "25/05/05 11:24:25 INFO DAGScheduler: Job 226 finished: start at NativeMethodAccessorImpl.java:0, took 0.515448 s\n",
      "25/05/05 11:24:25 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 75, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:24:25 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:24:25 INFO DataWritingSparkTask: Committed partition 0 (task 224, attempt 0, stage 224.0)\n",
      "25/05/05 11:24:25 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 503905709 nanos, during time span of 505420167 nanos.\n",
      "25/05/05 11:24:25 INFO Executor: Finished task 0.0 in stage 224.0 (TID 224). 3516 bytes result sent to driver\n",
      "25/05/05 11:24:25 INFO TaskSetManager: Finished task 0.0 in stage 224.0 (TID 224) in 514 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:24:25 INFO TaskSchedulerImpl: Removed TaskSet 224.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:24:25 INFO DAGScheduler: ResultStage 224 (start at NativeMethodAccessorImpl.java:0) finished in 0.516 s\n",
      "25/05/05 11:24:25 INFO DAGScheduler: Job 225 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:24:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 224: Stage finished\n",
      "25/05/05 11:24:25 INFO DAGScheduler: Job 225 finished: start at NativeMethodAccessorImpl.java:0, took 0.516756 s\n",
      "25/05/05 11:24:25 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 75, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4c41b3ec] is committing.\n",
      "25/05/05 11:24:25 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 75, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4c41b3ec] committed.\n",
      "25/05/05 11:24:25 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 75, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:24:25 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/75 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.75.6df921fd-aff9-4054-b56b-a4c017ed935d.tmp\n",
      "25/05/05 11:24:25 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/75 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.75.d23b79b1-6b53-4983-9ba1-9d295d72f66d.tmp\n",
      "25/05/05 11:24:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3307, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:25 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:24:25 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:24:25 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:24:25 INFO connection: Opened connection [connectionId{localValue:149, serverValue:4519}] to localhost:27017\n",
      "25/05/05 11:24:25 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=256666}\n",
      "25/05/05 11:24:25 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.75.6df921fd-aff9-4054-b56b-a4c017ed935d.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/75\n",
      "25/05/05 11:24:25 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:24:25.277Z\",\n",
      "  \"batchId\" : 75,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.5822784810126582,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 531,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 632,\n",
      "    \"walCommit\" : 68\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3306\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3307\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3307\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.5822784810126582,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:24:25 INFO connection: Opened connection [connectionId{localValue:150, serverValue:4520}] to localhost:27017\n",
      "25/05/05 11:24:25 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:24:25 INFO connection: Closed connection [connectionId{localValue:150, serverValue:4520}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:24:25 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 511225250 nanos, during time span of 519214250 nanos.\n",
      "25/05/05 11:24:25 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.75.d23b79b1-6b53-4983-9ba1-9d295d72f66d.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/75\n",
      "25/05/05 11:24:25 INFO Executor: Finished task 0.0 in stage 226.0 (TID 226). 1645 bytes result sent to driver\n",
      "25/05/05 11:24:25 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:24:25.277Z\",\n",
      "  \"batchId\" : 75,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.5698587127158556,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 536,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 637,\n",
      "    \"walCommit\" : 69\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3306\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3307\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3307\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.5698587127158556,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:24:25 INFO TaskSetManager: Finished task 0.0 in stage 226.0 (TID 226) in 530 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:24:25 INFO TaskSchedulerImpl: Removed TaskSet 226.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:24:25 INFO DAGScheduler: ResultStage 226 (start at NativeMethodAccessorImpl.java:0) finished in 0.533 s\n",
      "25/05/05 11:24:25 INFO DAGScheduler: Job 227 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:24:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 226: Stage finished\n",
      "25/05/05 11:24:25 INFO DAGScheduler: Job 227 finished: start at NativeMethodAccessorImpl.java:0, took 0.534147 s\n",
      "25/05/05 11:24:25 INFO MemoryStore: Block broadcast_302 stored as values in memory (estimated size 248.0 B, free 366.0 MiB)\n",
      "25/05/05 11:24:25 INFO MemoryStore: Block broadcast_302_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.0 MiB)\n",
      "25/05/05 11:24:25 INFO BlockManagerInfo: Added broadcast_302_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:24:25 INFO SparkContext: Created broadcast 302 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:25 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/75 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.75.a9184ec0-45df-40b7-a55a-5bcadb881a71.tmp\n",
      "25/05/05 11:24:25 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.75.a9184ec0-45df-40b7-a55a-5bcadb881a71.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/75\n",
      "25/05/05 11:24:25 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:24:25.277Z\",\n",
      "  \"batchId\" : 75,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.5015015015015014,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 566,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 666,\n",
      "    \"walCommit\" : 66\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3306\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3307\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3307\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.5015015015015014,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 75\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|B013|R081|02-00-00|161/YANKEE STAD|05/05/2025|11:24:24|REGULAR|4          |9         |2025-05-05 11:24:25.278|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:24:30 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/76 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.76.6c447c9a-eb6f-4dae-87c1-8868d91c66cd.tmp\n",
      "25/05/05 11:24:30 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/76 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.76.0dfcea23-190e-4ba4-8150-64e0350d47dd.tmp\n",
      "25/05/05 11:24:30 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/76 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.76.86b20bae-72bd-4581-85d2-cc1141bb2d29.tmp\n",
      "25/05/05 11:24:30 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.76.0dfcea23-190e-4ba4-8150-64e0350d47dd.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/76\n",
      "25/05/05 11:24:30 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.76.6c447c9a-eb6f-4dae-87c1-8868d91c66cd.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/76\n",
      "25/05/05 11:24:30 INFO MicroBatchExecution: Committed offsets for batch 76. Metadata OffsetSeqMetadata(0,1746458670759,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:24:30 INFO MicroBatchExecution: Committed offsets for batch 76. Metadata OffsetSeqMetadata(0,1746458670759,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:24:30 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.76.86b20bae-72bd-4581-85d2-cc1141bb2d29.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/76\n",
      "25/05/05 11:24:30 INFO MicroBatchExecution: Committed offsets for batch 76. Metadata OffsetSeqMetadata(0,1746458670759,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:24:30 INFO IncrementalExecution: Current batch timestamp = 1746458670759\n",
      "25/05/05 11:24:30 INFO IncrementalExecution: Current batch timestamp = 1746458670759\n",
      "25/05/05 11:24:30 INFO IncrementalExecution: Current batch timestamp = 1746458670759\n",
      "25/05/05 11:24:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:30 INFO IncrementalExecution: Current batch timestamp = 1746458670759\n",
      "25/05/05 11:24:30 INFO IncrementalExecution: Current batch timestamp = 1746458670759\n",
      "25/05/05 11:24:30 INFO IncrementalExecution: Current batch timestamp = 1746458670759\n",
      "25/05/05 11:24:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:30 INFO IncrementalExecution: Current batch timestamp = 1746458670759\n",
      "25/05/05 11:24:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:30 INFO IncrementalExecution: Current batch timestamp = 1746458670759\n",
      "25/05/05 11:24:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:30 INFO CodeGenerator: Code generated in 10.335166 ms\n",
      "25/05/05 11:24:30 INFO BlockManagerInfo: Removed broadcast_300_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:30 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 76, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:24:30 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:30 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 76, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@7de57217]. The input RDD has 1 partitions.\n",
      "25/05/05 11:24:30 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:30 INFO DAGScheduler: Got job 228 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:24:30 INFO DAGScheduler: Final stage: ResultStage 227 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:24:30 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:24:30 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:24:30 INFO DAGScheduler: Submitting ResultStage 227 (MapPartitionsRDD[1226] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:24:30 INFO MemoryStore: Block broadcast_303 stored as values in memory (estimated size 15.2 KiB, free 366.0 MiB)\n",
      "25/05/05 11:24:30 INFO MemoryStore: Block broadcast_303_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:24:30 INFO BlockManagerInfo: Removed broadcast_299_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:30 INFO BlockManagerInfo: Added broadcast_303_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:30 INFO SparkContext: Created broadcast 303 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:24:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 227 (MapPartitionsRDD[1226] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:24:30 INFO TaskSchedulerImpl: Adding task set 227.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:24:30 INFO DAGScheduler: Got job 229 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:24:30 INFO DAGScheduler: Final stage: ResultStage 228 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:24:30 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:24:30 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:24:30 INFO DAGScheduler: Submitting ResultStage 228 (MapPartitionsRDD[1227] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:24:30 INFO MemoryStore: Block broadcast_304 stored as values in memory (estimated size 16.2 KiB, free 366.0 MiB)\n",
      "25/05/05 11:24:30 INFO TaskSetManager: Starting task 0.0 in stage 227.0 (TID 227) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:24:30 INFO MemoryStore: Block broadcast_304_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)\n",
      "25/05/05 11:24:30 INFO BlockManagerInfo: Added broadcast_304_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:30 INFO SparkContext: Created broadcast 304 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:24:30 INFO Executor: Running task 0.0 in stage 227.0 (TID 227)\n",
      "25/05/05 11:24:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 228 (MapPartitionsRDD[1227] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:24:30 INFO TaskSchedulerImpl: Adding task set 228.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:24:30 INFO TaskSetManager: Starting task 0.0 in stage 228.0 (TID 228) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:24:30 INFO Executor: Running task 0.0 in stage 228.0 (TID 228)\n",
      "25/05/05 11:24:30 INFO BlockManagerInfo: Removed broadcast_295_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:30 INFO BlockManagerInfo: Removed broadcast_296_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:30 INFO CodeGenerator: Code generated in 4.734875 ms\n",
      "25/05/05 11:24:30 INFO BlockManagerInfo: Removed broadcast_298_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:24:30 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3307 untilOffset=3308, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=76 taskId=227 partitionId=0\n",
      "25/05/05 11:24:30 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3307 untilOffset=3308, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=76 taskId=228 partitionId=0\n",
      "25/05/05 11:24:30 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3307 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:30 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3307 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:30 INFO BlockManagerInfo: Removed broadcast_301_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:30 INFO BlockManagerInfo: Removed broadcast_302_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:24:30 INFO BlockManagerInfo: Removed broadcast_297_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:24:30 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:30 INFO DAGScheduler: Got job 230 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:24:30 INFO DAGScheduler: Final stage: ResultStage 229 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:24:30 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:24:30 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:24:30 INFO DAGScheduler: Submitting ResultStage 229 (MapPartitionsRDD[1232] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:24:30 INFO MemoryStore: Block broadcast_305 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:24:30 INFO MemoryStore: Block broadcast_305_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.1 MiB)\n",
      "25/05/05 11:24:30 INFO BlockManagerInfo: Added broadcast_305_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:30 INFO SparkContext: Created broadcast 305 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:24:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 229 (MapPartitionsRDD[1232] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:24:30 INFO TaskSchedulerImpl: Adding task set 229.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:24:30 INFO TaskSetManager: Starting task 0.0 in stage 229.0 (TID 229) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:24:30 INFO Executor: Running task 0.0 in stage 229.0 (TID 229)\n",
      "25/05/05 11:24:30 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3307 untilOffset=3308, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=76 taskId=229 partitionId=0\n",
      "25/05/05 11:24:30 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3307 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3308, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3308, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:31 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:24:31 INFO DataWritingSparkTask: Committed partition 0 (task 227, attempt 0, stage 227.0)\n",
      "25/05/05 11:24:31 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 508073125 nanos, during time span of 508391375 nanos.\n",
      "25/05/05 11:24:31 INFO Executor: Finished task 0.0 in stage 227.0 (TID 227). 2137 bytes result sent to driver\n",
      "25/05/05 11:24:31 INFO TaskSetManager: Finished task 0.0 in stage 227.0 (TID 227) in 519 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:24:31 INFO TaskSchedulerImpl: Removed TaskSet 227.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:24:31 INFO DAGScheduler: ResultStage 227 (start at NativeMethodAccessorImpl.java:0) finished in 0.521 s\n",
      "25/05/05 11:24:31 INFO DAGScheduler: Job 228 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:24:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 227: Stage finished\n",
      "25/05/05 11:24:31 INFO DAGScheduler: Job 228 finished: start at NativeMethodAccessorImpl.java:0, took 0.521687 s\n",
      "25/05/05 11:24:31 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 76, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:24:31 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:24:31 INFO DataWritingSparkTask: Committed partition 0 (task 228, attempt 0, stage 228.0)\n",
      "25/05/05 11:24:31 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 506629917 nanos, during time span of 508336917 nanos.\n",
      "25/05/05 11:24:31 INFO Executor: Finished task 0.0 in stage 228.0 (TID 228). 3507 bytes result sent to driver\n",
      "25/05/05 11:24:31 INFO TaskSetManager: Finished task 0.0 in stage 228.0 (TID 228) in 519 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:24:31 INFO TaskSchedulerImpl: Removed TaskSet 228.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:24:31 INFO DAGScheduler: ResultStage 228 (start at NativeMethodAccessorImpl.java:0) finished in 0.520 s\n",
      "25/05/05 11:24:31 INFO DAGScheduler: Job 229 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:24:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 228: Stage finished\n",
      "25/05/05 11:24:31 INFO DAGScheduler: Job 229 finished: start at NativeMethodAccessorImpl.java:0, took 0.522973 s\n",
      "25/05/05 11:24:31 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 76, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@7de57217] is committing.\n",
      "25/05/05 11:24:31 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 76, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@7de57217] committed.\n",
      "25/05/05 11:24:31 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 76, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:24:31 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/76 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.76.e3360397-5bde-40a2-a0cb-75cc08dee010.tmp\n",
      "25/05/05 11:24:31 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/76 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.76.73405842-42a2-48f4-ba3c-a590e63cc447.tmp\n",
      "25/05/05 11:24:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3308, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:31 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:24:31 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:24:31 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:24:31 INFO connection: Opened connection [connectionId{localValue:151, serverValue:4521}] to localhost:27017\n",
      "25/05/05 11:24:31 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=255084}\n",
      "25/05/05 11:24:31 INFO connection: Opened connection [connectionId{localValue:152, serverValue:4522}] to localhost:27017\n",
      "25/05/05 11:24:31 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:24:31 INFO connection: Closed connection [connectionId{localValue:152, serverValue:4522}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:24:31 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 504140833 nanos, during time span of 510573625 nanos.\n",
      "25/05/05 11:24:31 INFO Executor: Finished task 0.0 in stage 229.0 (TID 229). 1645 bytes result sent to driver\n",
      "25/05/05 11:24:31 INFO TaskSetManager: Finished task 0.0 in stage 229.0 (TID 229) in 517 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:24:31 INFO TaskSchedulerImpl: Removed TaskSet 229.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:24:31 INFO DAGScheduler: ResultStage 229 (start at NativeMethodAccessorImpl.java:0) finished in 0.523 s\n",
      "25/05/05 11:24:31 INFO DAGScheduler: Job 230 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:24:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 229: Stage finished\n",
      "25/05/05 11:24:31 INFO DAGScheduler: Job 230 finished: start at NativeMethodAccessorImpl.java:0, took 0.523204 s\n",
      "25/05/05 11:24:31 INFO MemoryStore: Block broadcast_306 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:24:31 INFO MemoryStore: Block broadcast_306_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:24:31 INFO BlockManagerInfo: Added broadcast_306_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:24:31 INFO SparkContext: Created broadcast 306 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:31 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.76.e3360397-5bde-40a2-a0cb-75cc08dee010.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/76\n",
      "25/05/05 11:24:31 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:24:30.758Z\",\n",
      "  \"batchId\" : 76,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.6474464579901154,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 540,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 607,\n",
      "    \"walCommit\" : 32\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3307\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3308\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3308\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.6474464579901154,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:24:31 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.76.73405842-42a2-48f4-ba3c-a590e63cc447.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/76\n",
      "25/05/05 11:24:31 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:24:30.758Z\",\n",
      "  \"batchId\" : 76,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.6366612111292962,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 545,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 611,\n",
      "    \"walCommit\" : 32\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3307\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3308\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3308\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.6366612111292962,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:24:31 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/76 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.76.2e7412b9-7aeb-4038-a5ed-08657b8de4cc.tmp\n",
      "25/05/05 11:24:31 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.76.2e7412b9-7aeb-4038-a5ed-08657b8de4cc.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/76\n",
      "25/05/05 11:24:31 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:24:30.758Z\",\n",
      "  \"batchId\" : 76,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.574803149606299,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 569,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 635,\n",
      "    \"walCommit\" : 32\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3307\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3308\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3308\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.574803149606299,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 76\n",
      "-------------------------------------------\n",
      "+----+----+--------+-------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION|DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+-------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A004|R033|00-00-00|125 ST |05/05/2025|11:24:29|REGULAR|14         |13        |2025-05-05 11:24:30.759|\n",
      "+----+----+--------+-------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:24:37 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/77 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.77.fbc6ffd1-d558-469e-8b11-fb2740f61119.tmp\n",
      "25/05/05 11:24:37 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/77 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.77.5dd71071-66ca-496e-be6f-d09d448ac9b6.tmp\n",
      "25/05/05 11:24:37 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/77 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.77.0fcebdfe-fb05-4ef5-a956-57af70292186.tmp\n",
      "25/05/05 11:24:37 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.77.fbc6ffd1-d558-469e-8b11-fb2740f61119.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/77\n",
      "25/05/05 11:24:37 INFO MicroBatchExecution: Committed offsets for batch 77. Metadata OffsetSeqMetadata(0,1746458677259,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:24:37 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.77.5dd71071-66ca-496e-be6f-d09d448ac9b6.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/77\n",
      "25/05/05 11:24:37 INFO MicroBatchExecution: Committed offsets for batch 77. Metadata OffsetSeqMetadata(0,1746458677258,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:24:37 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.77.0fcebdfe-fb05-4ef5-a956-57af70292186.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/77\n",
      "25/05/05 11:24:37 INFO MicroBatchExecution: Committed offsets for batch 77. Metadata OffsetSeqMetadata(0,1746458677259,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:24:37 INFO IncrementalExecution: Current batch timestamp = 1746458677259\n",
      "25/05/05 11:24:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:37 INFO IncrementalExecution: Current batch timestamp = 1746458677258\n",
      "25/05/05 11:24:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:37 INFO IncrementalExecution: Current batch timestamp = 1746458677259\n",
      "25/05/05 11:24:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:37 INFO IncrementalExecution: Current batch timestamp = 1746458677258\n",
      "25/05/05 11:24:37 INFO IncrementalExecution: Current batch timestamp = 1746458677259\n",
      "25/05/05 11:24:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:37 INFO IncrementalExecution: Current batch timestamp = 1746458677259\n",
      "25/05/05 11:24:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:37 INFO IncrementalExecution: Current batch timestamp = 1746458677258\n",
      "25/05/05 11:24:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:37 INFO IncrementalExecution: Current batch timestamp = 1746458677259\n",
      "25/05/05 11:24:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:37 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:37 INFO CodeGenerator: Code generated in 5.708833 ms\n",
      "25/05/05 11:24:37 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 77, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:24:37 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:37 INFO DAGScheduler: Got job 231 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:24:37 INFO DAGScheduler: Final stage: ResultStage 230 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:24:37 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:24:37 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:24:37 INFO DAGScheduler: Submitting ResultStage 230 (MapPartitionsRDD[1240] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:24:37 INFO CodeGenerator: Code generated in 3.977375 ms\n",
      "25/05/05 11:24:37 INFO MemoryStore: Block broadcast_307 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:24:37 INFO MemoryStore: Block broadcast_307_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:24:37 INFO BlockManagerInfo: Added broadcast_307_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:37 INFO SparkContext: Created broadcast 307 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:24:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 230 (MapPartitionsRDD[1240] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:24:37 INFO TaskSchedulerImpl: Adding task set 230.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:24:37 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 77, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6afed041]. The input RDD has 1 partitions.\n",
      "25/05/05 11:24:37 INFO TaskSetManager: Starting task 0.0 in stage 230.0 (TID 230) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:24:37 INFO Executor: Running task 0.0 in stage 230.0 (TID 230)\n",
      "25/05/05 11:24:37 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:37 INFO DAGScheduler: Got job 232 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:24:37 INFO DAGScheduler: Final stage: ResultStage 231 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:24:37 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:24:37 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:24:37 INFO DAGScheduler: Submitting ResultStage 231 (MapPartitionsRDD[1243] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:24:37 INFO MemoryStore: Block broadcast_308 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:24:37 INFO MemoryStore: Block broadcast_308_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)\n",
      "25/05/05 11:24:37 INFO BlockManagerInfo: Added broadcast_308_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:37 INFO SparkContext: Created broadcast 308 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:24:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 231 (MapPartitionsRDD[1243] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:24:37 INFO TaskSchedulerImpl: Adding task set 231.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:24:37 INFO TaskSetManager: Starting task 0.0 in stage 231.0 (TID 231) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:24:37 INFO Executor: Running task 0.0 in stage 231.0 (TID 231)\n",
      "25/05/05 11:24:37 INFO CodeGenerator: Code generated in 4.022917 ms\n",
      "25/05/05 11:24:37 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3308 untilOffset=3309, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=77 taskId=230 partitionId=0\n",
      "25/05/05 11:24:37 INFO CodeGenerator: Code generated in 3.537667 ms\n",
      "25/05/05 11:24:37 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3308 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:37 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:37 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3308 untilOffset=3309, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=77 taskId=231 partitionId=0\n",
      "25/05/05 11:24:37 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3308 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:37 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:37 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:37 INFO DAGScheduler: Got job 233 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:24:37 INFO DAGScheduler: Final stage: ResultStage 232 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:24:37 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:24:37 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:24:37 INFO DAGScheduler: Submitting ResultStage 232 (MapPartitionsRDD[1248] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:24:37 INFO MemoryStore: Block broadcast_309 stored as values in memory (estimated size 47.1 KiB, free 366.0 MiB)\n",
      "25/05/05 11:24:37 INFO MemoryStore: Block broadcast_309_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.0 MiB)\n",
      "25/05/05 11:24:37 INFO BlockManagerInfo: Added broadcast_309_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:37 INFO SparkContext: Created broadcast 309 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:24:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 232 (MapPartitionsRDD[1248] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:24:37 INFO TaskSchedulerImpl: Adding task set 232.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:24:37 INFO TaskSetManager: Starting task 0.0 in stage 232.0 (TID 232) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:24:37 INFO Executor: Running task 0.0 in stage 232.0 (TID 232)\n",
      "25/05/05 11:24:37 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3308 untilOffset=3309, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=77 taskId=232 partitionId=0\n",
      "25/05/05 11:24:37 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3308 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:37 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:37 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:37 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:37 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:37 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:37 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3309, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:37 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3309, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:37 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:24:37 INFO DataWritingSparkTask: Committed partition 0 (task 230, attempt 0, stage 230.0)\n",
      "25/05/05 11:24:37 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 505752208 nanos, during time span of 506207042 nanos.\n",
      "25/05/05 11:24:37 INFO Executor: Finished task 0.0 in stage 230.0 (TID 230). 2145 bytes result sent to driver\n",
      "25/05/05 11:24:37 INFO TaskSetManager: Finished task 0.0 in stage 230.0 (TID 230) in 515 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:24:37 INFO TaskSchedulerImpl: Removed TaskSet 230.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:24:37 INFO DAGScheduler: ResultStage 230 (start at NativeMethodAccessorImpl.java:0) finished in 0.516 s\n",
      "25/05/05 11:24:37 INFO DAGScheduler: Job 231 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:24:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 230: Stage finished\n",
      "25/05/05 11:24:37 INFO DAGScheduler: Job 231 finished: start at NativeMethodAccessorImpl.java:0, took 0.517044 s\n",
      "25/05/05 11:24:37 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:24:37 INFO DataWritingSparkTask: Committed partition 0 (task 231, attempt 0, stage 231.0)\n",
      "25/05/05 11:24:37 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 77, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:24:37 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 503227834 nanos, during time span of 505198958 nanos.\n",
      "25/05/05 11:24:37 INFO Executor: Finished task 0.0 in stage 231.0 (TID 231). 3516 bytes result sent to driver\n",
      "25/05/05 11:24:37 INFO TaskSetManager: Finished task 0.0 in stage 231.0 (TID 231) in 514 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:24:37 INFO TaskSchedulerImpl: Removed TaskSet 231.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:24:37 INFO DAGScheduler: ResultStage 231 (start at NativeMethodAccessorImpl.java:0) finished in 0.515 s\n",
      "25/05/05 11:24:37 INFO DAGScheduler: Job 232 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:24:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 231: Stage finished\n",
      "25/05/05 11:24:37 INFO DAGScheduler: Job 232 finished: start at NativeMethodAccessorImpl.java:0, took 0.515876 s\n",
      "25/05/05 11:24:37 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 77, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6afed041] is committing.\n",
      "25/05/05 11:24:37 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 77, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6afed041] committed.\n",
      "25/05/05 11:24:37 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 77, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:24:37 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/77 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.77.cef76170-9f85-4413-9344-bcfd369255e5.tmp\n",
      "25/05/05 11:24:37 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:37 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:37 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3309, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:37 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:24:37 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:24:37 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:24:37 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/77 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.77.00644983-6c52-4cda-9c7c-994580af208f.tmp\n",
      "25/05/05 11:24:37 INFO connection: Opened connection [connectionId{localValue:153, serverValue:4523}] to localhost:27017\n",
      "25/05/05 11:24:37 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=193375}\n",
      "25/05/05 11:24:37 INFO connection: Opened connection [connectionId{localValue:154, serverValue:4524}] to localhost:27017\n",
      "25/05/05 11:24:37 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:24:37 INFO connection: Closed connection [connectionId{localValue:154, serverValue:4524}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:24:37 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 503836500 nanos, during time span of 508003458 nanos.\n",
      "25/05/05 11:24:37 INFO Executor: Finished task 0.0 in stage 232.0 (TID 232). 1645 bytes result sent to driver\n",
      "25/05/05 11:24:37 INFO TaskSetManager: Finished task 0.0 in stage 232.0 (TID 232) in 514 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:24:37 INFO TaskSchedulerImpl: Removed TaskSet 232.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:24:37 INFO DAGScheduler: ResultStage 232 (start at NativeMethodAccessorImpl.java:0) finished in 0.518 s\n",
      "25/05/05 11:24:37 INFO DAGScheduler: Job 233 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:24:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 232: Stage finished\n",
      "25/05/05 11:24:37 INFO DAGScheduler: Job 233 finished: start at NativeMethodAccessorImpl.java:0, took 0.518384 s\n",
      "25/05/05 11:24:37 INFO MemoryStore: Block broadcast_310 stored as values in memory (estimated size 248.0 B, free 366.0 MiB)\n",
      "25/05/05 11:24:37 INFO MemoryStore: Block broadcast_310_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.0 MiB)\n",
      "25/05/05 11:24:37 INFO BlockManagerInfo: Added broadcast_310_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:24:37 INFO SparkContext: Created broadcast 310 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:37 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/77 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.77.82eb2591-65dc-4df0-9cfe-628c78f3c0a8.tmp\n",
      "25/05/05 11:24:37 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.77.cef76170-9f85-4413-9344-bcfd369255e5.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/77\n",
      "25/05/05 11:24:37 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:24:37.256Z\",\n",
      "  \"batchId\" : 77,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.6025641025641026,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 531,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 624,\n",
      "    \"walCommit\" : 58\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3308\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3309\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3309\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.6025641025641026,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:24:37 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.77.00644983-6c52-4cda-9c7c-994580af208f.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/77\n",
      "25/05/05 11:24:37 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:24:37.258Z\",\n",
      "  \"batchId\" : 77,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.594896331738437,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 535,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 627,\n",
      "    \"walCommit\" : 59\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3308\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3309\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3309\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.594896331738437,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:24:37 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.77.82eb2591-65dc-4df0-9cfe-628c78f3c0a8.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/77\n",
      "25/05/05 11:24:37 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:24:37.257Z\",\n",
      "  \"batchId\" : 77,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.567398119122257,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 550,\n",
      "    \"commitOffsets\" : 25,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 638,\n",
      "    \"walCommit\" : 56\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3308\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3309\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3309\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.567398119122257,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 77\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|B013|R081|02-00-00|161/YANKEE STAD|05/05/2025|11:24:36|REGULAR|10         |7         |2025-05-05 11:24:37.259|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:24:41 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/78 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.78.f259402e-30de-4ac2-9794-948e92d39221.tmp\n",
      "25/05/05 11:24:41 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/78 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.78.cdb3365a-ff77-4a8a-9d93-6d76438a0ce1.tmp\n",
      "25/05/05 11:24:41 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/78 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.78.c0df3826-8983-46ab-ba91-d31fcddd204e.tmp\n",
      "25/05/05 11:24:41 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.78.f259402e-30de-4ac2-9794-948e92d39221.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/78\n",
      "25/05/05 11:24:41 INFO MicroBatchExecution: Committed offsets for batch 78. Metadata OffsetSeqMetadata(0,1746458681746,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:24:41 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.78.c0df3826-8983-46ab-ba91-d31fcddd204e.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/78\n",
      "25/05/05 11:24:41 INFO MicroBatchExecution: Committed offsets for batch 78. Metadata OffsetSeqMetadata(0,1746458681745,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:24:41 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.78.cdb3365a-ff77-4a8a-9d93-6d76438a0ce1.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/78\n",
      "25/05/05 11:24:41 INFO MicroBatchExecution: Committed offsets for batch 78. Metadata OffsetSeqMetadata(0,1746458681747,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:24:41 INFO IncrementalExecution: Current batch timestamp = 1746458681746\n",
      "25/05/05 11:24:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:41 INFO IncrementalExecution: Current batch timestamp = 1746458681745\n",
      "25/05/05 11:24:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:41 INFO IncrementalExecution: Current batch timestamp = 1746458681747\n",
      "25/05/05 11:24:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:41 INFO IncrementalExecution: Current batch timestamp = 1746458681746\n",
      "25/05/05 11:24:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:41 INFO IncrementalExecution: Current batch timestamp = 1746458681745\n",
      "25/05/05 11:24:41 INFO IncrementalExecution: Current batch timestamp = 1746458681747\n",
      "25/05/05 11:24:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:41 INFO IncrementalExecution: Current batch timestamp = 1746458681747\n",
      "25/05/05 11:24:41 INFO IncrementalExecution: Current batch timestamp = 1746458681745\n",
      "25/05/05 11:24:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:41 INFO CodeGenerator: Code generated in 3.866166 ms\n",
      "25/05/05 11:24:41 INFO CodeGenerator: Code generated in 3.930375 ms\n",
      "25/05/05 11:24:41 INFO CodeGenerator: Code generated in 4.300041 ms\n",
      "25/05/05 11:24:41 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 78, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@337181f6]. The input RDD has 1 partitions.\n",
      "25/05/05 11:24:41 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:41 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 78, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:24:41 INFO DAGScheduler: Got job 234 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:24:41 INFO DAGScheduler: Final stage: ResultStage 233 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:24:41 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:24:41 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:24:41 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:41 INFO DAGScheduler: Submitting ResultStage 233 (MapPartitionsRDD[1258] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:24:41 INFO MemoryStore: Block broadcast_311 stored as values in memory (estimated size 16.2 KiB, free 366.0 MiB)\n",
      "25/05/05 11:24:41 INFO MemoryStore: Block broadcast_311_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)\n",
      "25/05/05 11:24:41 INFO BlockManagerInfo: Added broadcast_311_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:41 INFO SparkContext: Created broadcast 311 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:24:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 233 (MapPartitionsRDD[1258] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:24:41 INFO TaskSchedulerImpl: Adding task set 233.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:24:41 INFO DAGScheduler: Got job 235 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:24:41 INFO DAGScheduler: Final stage: ResultStage 234 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:24:41 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:24:41 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:24:41 INFO DAGScheduler: Submitting ResultStage 234 (MapPartitionsRDD[1259] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:24:41 INFO TaskSetManager: Starting task 0.0 in stage 233.0 (TID 233) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:24:41 INFO Executor: Running task 0.0 in stage 233.0 (TID 233)\n",
      "25/05/05 11:24:41 INFO MemoryStore: Block broadcast_312 stored as values in memory (estimated size 15.2 KiB, free 365.9 MiB)\n",
      "25/05/05 11:24:41 INFO MemoryStore: Block broadcast_312_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 365.9 MiB)\n",
      "25/05/05 11:24:41 INFO BlockManagerInfo: Added broadcast_312_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:41 INFO SparkContext: Created broadcast 312 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:24:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 234 (MapPartitionsRDD[1259] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:24:41 INFO TaskSchedulerImpl: Adding task set 234.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:24:41 INFO TaskSetManager: Starting task 0.0 in stage 234.0 (TID 234) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:24:41 INFO Executor: Running task 0.0 in stage 234.0 (TID 234)\n",
      "25/05/05 11:24:41 INFO BlockManagerInfo: Removed broadcast_309_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:41 INFO BlockManagerInfo: Removed broadcast_303_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:41 INFO CodeGenerator: Code generated in 9.750292 ms\n",
      "25/05/05 11:24:41 INFO BlockManagerInfo: Removed broadcast_310_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:24:41 INFO BlockManagerInfo: Removed broadcast_304_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:41 INFO CodeGenerator: Code generated in 9.336709 ms\n",
      "25/05/05 11:24:41 INFO BlockManagerInfo: Removed broadcast_307_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:41 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3309 untilOffset=3310, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=78 taskId=234 partitionId=0\n",
      "25/05/05 11:24:41 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3309 untilOffset=3310, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=78 taskId=233 partitionId=0\n",
      "25/05/05 11:24:41 INFO BlockManagerInfo: Removed broadcast_306_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:24:41 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3309 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:41 INFO BlockManagerInfo: Removed broadcast_305_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:24:41 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3309 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:41 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:41 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:41 INFO BlockManagerInfo: Removed broadcast_308_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:24:41 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:41 INFO DAGScheduler: Got job 236 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:24:41 INFO DAGScheduler: Final stage: ResultStage 235 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:24:41 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:24:41 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:24:41 INFO DAGScheduler: Submitting ResultStage 235 (MapPartitionsRDD[1264] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:24:41 INFO MemoryStore: Block broadcast_313 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:24:41 INFO MemoryStore: Block broadcast_313_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:24:41 INFO BlockManagerInfo: Added broadcast_313_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:41 INFO SparkContext: Created broadcast 313 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:24:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 235 (MapPartitionsRDD[1264] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:24:41 INFO TaskSchedulerImpl: Adding task set 235.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:24:41 INFO TaskSetManager: Starting task 0.0 in stage 235.0 (TID 235) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:24:41 INFO Executor: Running task 0.0 in stage 235.0 (TID 235)\n",
      "25/05/05 11:24:41 INFO CodeGenerator: Code generated in 4.103 ms\n",
      "25/05/05 11:24:41 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3309 untilOffset=3310, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=78 taskId=235 partitionId=0\n",
      "25/05/05 11:24:41 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3309 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:41 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:42 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:42 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:42 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:42 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:42 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3310, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:42 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3310, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:42 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:24:42 INFO DataWritingSparkTask: Committed partition 0 (task 234, attempt 0, stage 234.0)\n",
      "25/05/05 11:24:42 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 503051334 nanos, during time span of 503472000 nanos.\n",
      "25/05/05 11:24:42 INFO Executor: Finished task 0.0 in stage 234.0 (TID 234). 2188 bytes result sent to driver\n",
      "25/05/05 11:24:42 INFO TaskSetManager: Finished task 0.0 in stage 234.0 (TID 234) in 517 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:24:42 INFO TaskSchedulerImpl: Removed TaskSet 234.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:24:42 INFO DAGScheduler: ResultStage 234 (start at NativeMethodAccessorImpl.java:0) finished in 0.519 s\n",
      "25/05/05 11:24:42 INFO DAGScheduler: Job 235 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:24:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 234: Stage finished\n",
      "25/05/05 11:24:42 INFO DAGScheduler: Job 235 finished: start at NativeMethodAccessorImpl.java:0, took 0.521027 s\n",
      "25/05/05 11:24:42 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 78, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:24:42 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:24:42 INFO DataWritingSparkTask: Committed partition 0 (task 233, attempt 0, stage 233.0)\n",
      "25/05/05 11:24:42 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 502936042 nanos, during time span of 504665291 nanos.\n",
      "25/05/05 11:24:42 INFO Executor: Finished task 0.0 in stage 233.0 (TID 233). 3558 bytes result sent to driver\n",
      "25/05/05 11:24:42 INFO TaskSetManager: Finished task 0.0 in stage 233.0 (TID 233) in 520 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:24:42 INFO TaskSchedulerImpl: Removed TaskSet 233.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:24:42 INFO DAGScheduler: ResultStage 233 (start at NativeMethodAccessorImpl.java:0) finished in 0.522 s\n",
      "25/05/05 11:24:42 INFO DAGScheduler: Job 234 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:24:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 233: Stage finished\n",
      "25/05/05 11:24:42 INFO DAGScheduler: Job 234 finished: start at NativeMethodAccessorImpl.java:0, took 0.522537 s\n",
      "25/05/05 11:24:42 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 78, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@337181f6] is committing.\n",
      "25/05/05 11:24:42 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 78, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@337181f6] committed.\n",
      "25/05/05 11:24:42 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 78, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:24:42 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/78 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.78.e36c43d4-4b22-460e-b189-a45395f3f124.tmp\n",
      "25/05/05 11:24:42 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/78 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.78.7ff09d88-9151-4a2b-9696-0a2e4d92a7c4.tmp\n",
      "25/05/05 11:24:42 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:42 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:42 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3310, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:42 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:24:42 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:24:42 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:24:42 INFO connection: Opened connection [connectionId{localValue:155, serverValue:4525}] to localhost:27017\n",
      "25/05/05 11:24:42 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=226583}\n",
      "25/05/05 11:24:42 INFO connection: Opened connection [connectionId{localValue:156, serverValue:4526}] to localhost:27017\n",
      "25/05/05 11:24:42 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:24:42 INFO connection: Closed connection [connectionId{localValue:156, serverValue:4526}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:24:42 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 508646667 nanos, during time span of 516071416 nanos.\n",
      "25/05/05 11:24:42 INFO Executor: Finished task 0.0 in stage 235.0 (TID 235). 1645 bytes result sent to driver\n",
      "25/05/05 11:24:42 INFO TaskSetManager: Finished task 0.0 in stage 235.0 (TID 235) in 527 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:24:42 INFO TaskSchedulerImpl: Removed TaskSet 235.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:24:42 INFO DAGScheduler: ResultStage 235 (start at NativeMethodAccessorImpl.java:0) finished in 0.530 s\n",
      "25/05/05 11:24:42 INFO DAGScheduler: Job 236 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:24:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 235: Stage finished\n",
      "25/05/05 11:24:42 INFO DAGScheduler: Job 236 finished: start at NativeMethodAccessorImpl.java:0, took 0.530905 s\n",
      "25/05/05 11:24:42 INFO MemoryStore: Block broadcast_314 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:24:42 INFO MemoryStore: Block broadcast_314_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:24:42 INFO BlockManagerInfo: Added broadcast_314_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:24:42 INFO SparkContext: Created broadcast 314 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:42 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.78.e36c43d4-4b22-460e-b189-a45395f3f124.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/78\n",
      "25/05/05 11:24:42 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:24:41.743Z\",\n",
      "  \"batchId\" : 78,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "  \"processedRowsPerSecond\" : 1.5873015873015872,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 536,\n",
      "    \"commitOffsets\" : 31,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 4,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 630,\n",
      "    \"walCommit\" : 55\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3309\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3310\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3310\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "    \"processedRowsPerSecond\" : 1.5873015873015872,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:24:42 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.78.7ff09d88-9151-4a2b-9696-0a2e4d92a7c4.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/78\n",
      "25/05/05 11:24:42 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:24:41.743Z\",\n",
      "  \"batchId\" : 78,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 55.55555555555556,\n",
      "  \"processedRowsPerSecond\" : 1.5797788309636651,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 542,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 633,\n",
      "    \"walCommit\" : 57\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3309\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3310\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3310\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 55.55555555555556,\n",
      "    \"processedRowsPerSecond\" : 1.5797788309636651,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:24:42 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/78 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.78.08ce8811-683e-49a4-a129-5596ecc155d6.tmp\n",
      "25/05/05 11:24:42 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.78.08ce8811-683e-49a4-a129-5596ecc155d6.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/78\n",
      "25/05/05 11:24:42 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:24:41.742Z\",\n",
      "  \"batchId\" : 78,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 62.5,\n",
      "  \"processedRowsPerSecond\" : 1.5220700152207,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 567,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 657,\n",
      "    \"walCommit\" : 53\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3309\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3310\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3310\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 62.5,\n",
      "    \"processedRowsPerSecond\" : 1.5220700152207,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 78\n",
      "-------------------------------------------\n",
      "+----+----+--------+--------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION       |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+--------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A002|R051|02-00-00|34 ST-PENN STA|05/05/2025|11:24:40|REGULAR|10         |6         |2025-05-05 11:24:41.745|\n",
      "+----+----+--------+--------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:24:46 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/79 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.79.36c652dd-29c7-4168-9db2-88b74508031f.tmp\n",
      "25/05/05 11:24:46 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/79 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.79.6ff0e075-a7d6-4241-a483-2d38dd796ffa.tmp\n",
      "25/05/05 11:24:46 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/79 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.79.8d37168f-4bdf-4759-b34c-92293b928ff4.tmp\n",
      "25/05/05 11:24:46 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.79.8d37168f-4bdf-4759-b34c-92293b928ff4.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/79\n",
      "25/05/05 11:24:46 INFO MicroBatchExecution: Committed offsets for batch 79. Metadata OffsetSeqMetadata(0,1746458686221,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:24:46 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.79.36c652dd-29c7-4168-9db2-88b74508031f.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/79\n",
      "25/05/05 11:24:46 INFO MicroBatchExecution: Committed offsets for batch 79. Metadata OffsetSeqMetadata(0,1746458686233,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:24:46 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.79.6ff0e075-a7d6-4241-a483-2d38dd796ffa.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/79\n",
      "25/05/05 11:24:46 INFO MicroBatchExecution: Committed offsets for batch 79. Metadata OffsetSeqMetadata(0,1746458686221,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:24:46 INFO IncrementalExecution: Current batch timestamp = 1746458686221\n",
      "25/05/05 11:24:46 INFO IncrementalExecution: Current batch timestamp = 1746458686233\n",
      "25/05/05 11:24:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:46 INFO IncrementalExecution: Current batch timestamp = 1746458686221\n",
      "25/05/05 11:24:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:46 INFO IncrementalExecution: Current batch timestamp = 1746458686221\n",
      "25/05/05 11:24:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:46 INFO IncrementalExecution: Current batch timestamp = 1746458686233\n",
      "25/05/05 11:24:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:46 INFO IncrementalExecution: Current batch timestamp = 1746458686221\n",
      "25/05/05 11:24:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:46 INFO IncrementalExecution: Current batch timestamp = 1746458686221\n",
      "25/05/05 11:24:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:46 INFO IncrementalExecution: Current batch timestamp = 1746458686233\n",
      "25/05/05 11:24:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:46 INFO CodeGenerator: Code generated in 3.950208 ms\n",
      "25/05/05 11:24:46 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 79, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@5c44fc28]. The input RDD has 1 partitions.\n",
      "25/05/05 11:24:46 INFO CodeGenerator: Code generated in 3.191583 ms\n",
      "25/05/05 11:24:46 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:46 INFO DAGScheduler: Got job 237 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:24:46 INFO DAGScheduler: Final stage: ResultStage 236 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:24:46 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:24:46 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:24:46 INFO DAGScheduler: Submitting ResultStage 236 (MapPartitionsRDD[1272] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:24:46 INFO MemoryStore: Block broadcast_315 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:24:46 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 79, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:24:46 INFO MemoryStore: Block broadcast_315_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.1 MiB)\n",
      "25/05/05 11:24:46 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:46 INFO BlockManagerInfo: Added broadcast_315_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:46 INFO SparkContext: Created broadcast 315 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:24:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 236 (MapPartitionsRDD[1272] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:24:46 INFO TaskSchedulerImpl: Adding task set 236.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:24:46 INFO DAGScheduler: Got job 238 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:24:46 INFO DAGScheduler: Final stage: ResultStage 237 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:24:46 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:24:46 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:24:46 INFO DAGScheduler: Submitting ResultStage 237 (MapPartitionsRDD[1275] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:24:46 INFO TaskSetManager: Starting task 0.0 in stage 236.0 (TID 236) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:24:46 INFO Executor: Running task 0.0 in stage 236.0 (TID 236)\n",
      "25/05/05 11:24:46 INFO MemoryStore: Block broadcast_316 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:24:46 INFO MemoryStore: Block broadcast_316_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:24:46 INFO BlockManagerInfo: Added broadcast_316_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:46 INFO SparkContext: Created broadcast 316 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:24:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 237 (MapPartitionsRDD[1275] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:24:46 INFO TaskSchedulerImpl: Adding task set 237.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:24:46 INFO TaskSetManager: Starting task 0.0 in stage 237.0 (TID 237) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:24:46 INFO Executor: Running task 0.0 in stage 237.0 (TID 237)\n",
      "25/05/05 11:24:46 INFO CodeGenerator: Code generated in 5.030542 ms\n",
      "25/05/05 11:24:46 INFO CodeGenerator: Code generated in 3.9335 ms\n",
      "25/05/05 11:24:46 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3310 untilOffset=3311, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=79 taskId=236 partitionId=0\n",
      "25/05/05 11:24:46 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3310 untilOffset=3311, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=79 taskId=237 partitionId=0\n",
      "25/05/05 11:24:46 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3310 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:46 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3310 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:46 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:46 INFO DAGScheduler: Got job 239 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:24:46 INFO DAGScheduler: Final stage: ResultStage 238 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:24:46 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:24:46 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:24:46 INFO DAGScheduler: Submitting ResultStage 238 (MapPartitionsRDD[1280] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:24:46 INFO MemoryStore: Block broadcast_317 stored as values in memory (estimated size 47.1 KiB, free 366.0 MiB)\n",
      "25/05/05 11:24:46 INFO MemoryStore: Block broadcast_317_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:24:46 INFO BlockManagerInfo: Added broadcast_317_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:46 INFO SparkContext: Created broadcast 317 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:24:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 238 (MapPartitionsRDD[1280] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:24:46 INFO TaskSchedulerImpl: Adding task set 238.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:24:46 INFO TaskSetManager: Starting task 0.0 in stage 238.0 (TID 238) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:24:46 INFO Executor: Running task 0.0 in stage 238.0 (TID 238)\n",
      "25/05/05 11:24:46 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3310 untilOffset=3311, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=79 taskId=238 partitionId=0\n",
      "25/05/05 11:24:46 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3310 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3311, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3311, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:46 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:24:46 INFO DataWritingSparkTask: Committed partition 0 (task 237, attempt 0, stage 237.0)\n",
      "25/05/05 11:24:46 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 509339292 nanos, during time span of 509566250 nanos.\n",
      "25/05/05 11:24:46 INFO Executor: Finished task 0.0 in stage 237.0 (TID 237). 2145 bytes result sent to driver\n",
      "25/05/05 11:24:46 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:24:46 INFO DataWritingSparkTask: Committed partition 0 (task 236, attempt 0, stage 236.0)\n",
      "25/05/05 11:24:46 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 509462333 nanos, during time span of 511455292 nanos.\n",
      "25/05/05 11:24:46 INFO TaskSetManager: Finished task 0.0 in stage 237.0 (TID 237) in 519 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:24:46 INFO TaskSchedulerImpl: Removed TaskSet 237.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:24:46 INFO Executor: Finished task 0.0 in stage 236.0 (TID 236). 3511 bytes result sent to driver\n",
      "25/05/05 11:24:46 INFO DAGScheduler: ResultStage 237 (start at NativeMethodAccessorImpl.java:0) finished in 0.522 s\n",
      "25/05/05 11:24:46 INFO DAGScheduler: Job 238 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:24:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 237: Stage finished\n",
      "25/05/05 11:24:46 INFO TaskSetManager: Finished task 0.0 in stage 236.0 (TID 236) in 524 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:24:46 INFO DAGScheduler: Job 238 finished: start at NativeMethodAccessorImpl.java:0, took 0.523859 s\n",
      "25/05/05 11:24:46 INFO TaskSchedulerImpl: Removed TaskSet 236.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:24:46 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 79, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:24:46 INFO DAGScheduler: ResultStage 236 (start at NativeMethodAccessorImpl.java:0) finished in 0.525 s\n",
      "25/05/05 11:24:46 INFO DAGScheduler: Job 237 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:24:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 236: Stage finished\n",
      "25/05/05 11:24:46 INFO DAGScheduler: Job 237 finished: start at NativeMethodAccessorImpl.java:0, took 0.526004 s\n",
      "25/05/05 11:24:46 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 79, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@5c44fc28] is committing.\n",
      "25/05/05 11:24:46 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 79, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@5c44fc28] committed.\n",
      "25/05/05 11:24:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3311, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:46 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:24:46 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 79, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:24:46 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:24:46 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:24:46 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/79 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.79.b1e94f85-597b-48af-b274-a8926972a984.tmp\n",
      "25/05/05 11:24:46 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/79 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.79.26a7375c-0346-4824-a611-be20b8b5b27d.tmp\n",
      "25/05/05 11:24:46 INFO connection: Opened connection [connectionId{localValue:157, serverValue:4527}] to localhost:27017\n",
      "25/05/05 11:24:46 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1050500}\n",
      "25/05/05 11:24:46 INFO connection: Opened connection [connectionId{localValue:158, serverValue:4528}] to localhost:27017\n",
      "25/05/05 11:24:46 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:24:46 INFO connection: Closed connection [connectionId{localValue:158, serverValue:4528}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:24:46 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 503970333 nanos, during time span of 517002834 nanos.\n",
      "25/05/05 11:24:46 INFO Executor: Finished task 0.0 in stage 238.0 (TID 238). 1645 bytes result sent to driver\n",
      "25/05/05 11:24:46 INFO TaskSetManager: Finished task 0.0 in stage 238.0 (TID 238) in 523 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:24:46 INFO TaskSchedulerImpl: Removed TaskSet 238.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:24:46 INFO DAGScheduler: ResultStage 238 (start at NativeMethodAccessorImpl.java:0) finished in 0.526 s\n",
      "25/05/05 11:24:46 INFO DAGScheduler: Job 239 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:24:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 238: Stage finished\n",
      "25/05/05 11:24:46 INFO DAGScheduler: Job 239 finished: start at NativeMethodAccessorImpl.java:0, took 0.526848 s\n",
      "25/05/05 11:24:46 INFO MemoryStore: Block broadcast_318 stored as values in memory (estimated size 248.0 B, free 366.0 MiB)\n",
      "25/05/05 11:24:46 INFO MemoryStore: Block broadcast_318_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.0 MiB)\n",
      "25/05/05 11:24:46 INFO BlockManagerInfo: Added broadcast_318_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:24:46 INFO SparkContext: Created broadcast 318 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:46 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/79 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.79.6fec9b06-9e89-47cd-8322-211356f67edd.tmp\n",
      "25/05/05 11:24:46 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.79.b1e94f85-597b-48af-b274-a8926972a984.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/79\n",
      "25/05/05 11:24:46 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:24:46.219Z\",\n",
      "  \"batchId\" : 79,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.597444089456869,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 538,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 0,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 626,\n",
      "    \"walCommit\" : 52\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3310\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3311\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3311\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.597444089456869,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:24:46 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.79.26a7375c-0346-4824-a611-be20b8b5b27d.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/79\n",
      "25/05/05 11:24:46 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:24:46.223Z\",\n",
      "  \"batchId\" : 79,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.594896331738437,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 544,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 10,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 627,\n",
      "    \"walCommit\" : 40\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3310\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3311\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3311\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.594896331738437,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:24:46 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.79.6fec9b06-9e89-47cd-8322-211356f67edd.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/79\n",
      "25/05/05 11:24:46 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:24:46.219Z\",\n",
      "  \"batchId\" : 79,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.5503875968992247,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 559,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 645,\n",
      "    \"walCommit\" : 54\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3310\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3311\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3311\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.5503875968992247,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 79\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|B013|R081|02-00-00|161/YANKEE STAD|05/05/2025|11:24:45|REGULAR|14         |14        |2025-05-05 11:24:46.233|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:24:52 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/80 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.80.d10da22b-b28b-45bc-a9e4-f4659754157b.tmp\n",
      "25/05/05 11:24:52 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/80 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.80.380820cd-ea3b-4d9f-8809-927913800f54.tmp\n",
      "25/05/05 11:24:52 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/80 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.80.15af16e0-8f7e-47ef-812f-752e39ed4791.tmp\n",
      "25/05/05 11:24:52 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.80.380820cd-ea3b-4d9f-8809-927913800f54.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/80\n",
      "25/05/05 11:24:52 INFO MicroBatchExecution: Committed offsets for batch 80. Metadata OffsetSeqMetadata(0,1746458692723,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:24:52 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.80.d10da22b-b28b-45bc-a9e4-f4659754157b.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/80\n",
      "25/05/05 11:24:52 INFO MicroBatchExecution: Committed offsets for batch 80. Metadata OffsetSeqMetadata(0,1746458692723,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:24:52 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.80.15af16e0-8f7e-47ef-812f-752e39ed4791.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/80\n",
      "25/05/05 11:24:52 INFO MicroBatchExecution: Committed offsets for batch 80. Metadata OffsetSeqMetadata(0,1746458692724,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:24:52 INFO IncrementalExecution: Current batch timestamp = 1746458692723\n",
      "25/05/05 11:24:52 INFO IncrementalExecution: Current batch timestamp = 1746458692723\n",
      "25/05/05 11:24:52 INFO IncrementalExecution: Current batch timestamp = 1746458692724\n",
      "25/05/05 11:24:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:52 INFO IncrementalExecution: Current batch timestamp = 1746458692724\n",
      "25/05/05 11:24:52 INFO IncrementalExecution: Current batch timestamp = 1746458692723\n",
      "25/05/05 11:24:52 INFO IncrementalExecution: Current batch timestamp = 1746458692723\n",
      "25/05/05 11:24:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:52 INFO IncrementalExecution: Current batch timestamp = 1746458692723\n",
      "25/05/05 11:24:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:52 INFO IncrementalExecution: Current batch timestamp = 1746458692724\n",
      "25/05/05 11:24:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:52 INFO CodeGenerator: Code generated in 3.852125 ms\n",
      "25/05/05 11:24:52 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 80, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@51d0cf7]. The input RDD has 1 partitions.\n",
      "25/05/05 11:24:52 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:52 INFO DAGScheduler: Got job 240 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:24:52 INFO DAGScheduler: Final stage: ResultStage 239 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:24:52 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:24:52 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:24:52 INFO DAGScheduler: Submitting ResultStage 239 (MapPartitionsRDD[1288] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:24:52 INFO MemoryStore: Block broadcast_319 stored as values in memory (estimated size 16.2 KiB, free 366.0 MiB)\n",
      "25/05/05 11:24:52 INFO MemoryStore: Block broadcast_319_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)\n",
      "25/05/05 11:24:52 INFO BlockManagerInfo: Added broadcast_319_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:52 INFO CodeGenerator: Code generated in 3.855125 ms\n",
      "25/05/05 11:24:52 INFO SparkContext: Created broadcast 319 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:24:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 239 (MapPartitionsRDD[1288] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:24:52 INFO TaskSchedulerImpl: Adding task set 239.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:24:52 INFO TaskSetManager: Starting task 0.0 in stage 239.0 (TID 239) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:24:52 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 80, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:24:52 INFO Executor: Running task 0.0 in stage 239.0 (TID 239)\n",
      "25/05/05 11:24:52 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:52 INFO DAGScheduler: Got job 241 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:24:52 INFO DAGScheduler: Final stage: ResultStage 240 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:24:52 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:24:52 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:24:52 INFO DAGScheduler: Submitting ResultStage 240 (MapPartitionsRDD[1291] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:24:52 INFO MemoryStore: Block broadcast_320 stored as values in memory (estimated size 15.2 KiB, free 365.9 MiB)\n",
      "25/05/05 11:24:52 INFO MemoryStore: Block broadcast_320_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 365.9 MiB)\n",
      "25/05/05 11:24:52 INFO BlockManagerInfo: Added broadcast_320_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:52 INFO SparkContext: Created broadcast 320 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:24:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 240 (MapPartitionsRDD[1291] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:24:52 INFO TaskSchedulerImpl: Adding task set 240.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:24:52 INFO TaskSetManager: Starting task 0.0 in stage 240.0 (TID 240) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:24:52 INFO Executor: Running task 0.0 in stage 240.0 (TID 240)\n",
      "25/05/05 11:24:52 INFO CodeGenerator: Code generated in 4.243459 ms\n",
      "25/05/05 11:24:52 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3311 untilOffset=3312, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=80 taskId=239 partitionId=0\n",
      "25/05/05 11:24:52 INFO CodeGenerator: Code generated in 3.758583 ms\n",
      "25/05/05 11:24:52 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3311 untilOffset=3312, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=80 taskId=240 partitionId=0\n",
      "25/05/05 11:24:52 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3311 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:52 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:52 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3311 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:52 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:52 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:52 INFO DAGScheduler: Got job 242 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:24:52 INFO DAGScheduler: Final stage: ResultStage 241 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:24:52 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:24:52 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:24:52 INFO DAGScheduler: Submitting ResultStage 241 (MapPartitionsRDD[1296] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:24:52 INFO MemoryStore: Block broadcast_321 stored as values in memory (estimated size 47.1 KiB, free 365.9 MiB)\n",
      "25/05/05 11:24:52 INFO MemoryStore: Block broadcast_321_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 365.9 MiB)\n",
      "25/05/05 11:24:52 INFO BlockManagerInfo: Added broadcast_321_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:52 INFO SparkContext: Created broadcast 321 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:24:52 INFO BlockManagerInfo: Removed broadcast_311_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 241 (MapPartitionsRDD[1296] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:24:52 INFO TaskSchedulerImpl: Adding task set 241.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:24:52 INFO TaskSetManager: Starting task 0.0 in stage 241.0 (TID 241) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:24:52 INFO BlockManagerInfo: Removed broadcast_317_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:52 INFO Executor: Running task 0.0 in stage 241.0 (TID 241)\n",
      "25/05/05 11:24:52 INFO BlockManagerInfo: Removed broadcast_318_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:24:52 INFO BlockManagerInfo: Removed broadcast_314_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:24:52 INFO BlockManagerInfo: Removed broadcast_315_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:52 INFO BlockManagerInfo: Removed broadcast_312_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:52 INFO BlockManagerInfo: Removed broadcast_316_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:52 INFO BlockManagerInfo: Removed broadcast_313_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:52 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3311 untilOffset=3312, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=80 taskId=241 partitionId=0\n",
      "25/05/05 11:24:52 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3311 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:52 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3312, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3312, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:53 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:24:53 INFO DataWritingSparkTask: Committed partition 0 (task 240, attempt 0, stage 240.0)\n",
      "25/05/05 11:24:53 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 501454375 nanos, during time span of 501858459 nanos.\n",
      "25/05/05 11:24:53 INFO Executor: Finished task 0.0 in stage 240.0 (TID 240). 2231 bytes result sent to driver\n",
      "25/05/05 11:24:53 INFO TaskSetManager: Finished task 0.0 in stage 240.0 (TID 240) in 511 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:24:53 INFO TaskSchedulerImpl: Removed TaskSet 240.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:24:53 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:24:53 INFO DataWritingSparkTask: Committed partition 0 (task 239, attempt 0, stage 239.0)\n",
      "25/05/05 11:24:53 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 502570917 nanos, during time span of 504268125 nanos.\n",
      "25/05/05 11:24:53 INFO DAGScheduler: ResultStage 240 (start at NativeMethodAccessorImpl.java:0) finished in 0.513 s\n",
      "25/05/05 11:24:53 INFO Executor: Finished task 0.0 in stage 239.0 (TID 239). 3559 bytes result sent to driver\n",
      "25/05/05 11:24:53 INFO DAGScheduler: Job 241 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:24:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 240: Stage finished\n",
      "25/05/05 11:24:53 INFO DAGScheduler: Job 241 finished: start at NativeMethodAccessorImpl.java:0, took 0.513748 s\n",
      "25/05/05 11:24:53 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 80, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:24:53 INFO TaskSetManager: Finished task 0.0 in stage 239.0 (TID 239) in 515 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:24:53 INFO TaskSchedulerImpl: Removed TaskSet 239.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:24:53 INFO DAGScheduler: ResultStage 239 (start at NativeMethodAccessorImpl.java:0) finished in 0.518 s\n",
      "25/05/05 11:24:53 INFO DAGScheduler: Job 240 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:24:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 239: Stage finished\n",
      "25/05/05 11:24:53 INFO DAGScheduler: Job 240 finished: start at NativeMethodAccessorImpl.java:0, took 0.519126 s\n",
      "25/05/05 11:24:53 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 80, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@51d0cf7] is committing.\n",
      "25/05/05 11:24:53 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 80, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@51d0cf7] committed.\n",
      "25/05/05 11:24:53 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 80, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:24:53 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/80 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.80.e58cbf0f-9184-464e-8c35-6c524e08ba03.tmp\n",
      "25/05/05 11:24:53 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/80 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.80.f09d85a8-03bb-4e09-90f0-66ed38f832f0.tmp\n",
      "25/05/05 11:24:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3312, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:53 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:24:53 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:24:53 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:24:53 INFO connection: Opened connection [connectionId{localValue:159, serverValue:4529}] to localhost:27017\n",
      "25/05/05 11:24:53 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=5002042}\n",
      "25/05/05 11:24:53 INFO connection: Opened connection [connectionId{localValue:160, serverValue:4530}] to localhost:27017\n",
      "25/05/05 11:24:53 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:24:53 INFO connection: Closed connection [connectionId{localValue:160, serverValue:4530}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:24:53 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 504873000 nanos, during time span of 520696583 nanos.\n",
      "25/05/05 11:24:53 INFO Executor: Finished task 0.0 in stage 241.0 (TID 241). 1645 bytes result sent to driver\n",
      "25/05/05 11:24:53 INFO TaskSetManager: Finished task 0.0 in stage 241.0 (TID 241) in 527 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:24:53 INFO TaskSchedulerImpl: Removed TaskSet 241.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:24:53 INFO DAGScheduler: ResultStage 241 (start at NativeMethodAccessorImpl.java:0) finished in 0.536 s\n",
      "25/05/05 11:24:53 INFO DAGScheduler: Job 242 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:24:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 241: Stage finished\n",
      "25/05/05 11:24:53 INFO DAGScheduler: Job 242 finished: start at NativeMethodAccessorImpl.java:0, took 0.536457 s\n",
      "25/05/05 11:24:53 INFO MemoryStore: Block broadcast_322 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:24:53 INFO MemoryStore: Block broadcast_322_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:24:53 INFO BlockManagerInfo: Added broadcast_322_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:24:53 INFO SparkContext: Created broadcast 322 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:53 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/80 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.80.4136ea20-afdd-49ea-8414-91a2cc50618e.tmp\n",
      "25/05/05 11:24:53 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.80.f09d85a8-03bb-4e09-90f0-66ed38f832f0.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/80\n",
      "25/05/05 11:24:53 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.80.e58cbf0f-9184-464e-8c35-6c524e08ba03.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/80\n",
      "25/05/05 11:24:53 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:24:52.720Z\",\n",
      "  \"batchId\" : 80,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.6,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 538,\n",
      "    \"commitOffsets\" : 38,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 4,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 625,\n",
      "    \"walCommit\" : 41\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3311\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3312\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3312\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.6,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:24:53 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:24:52.719Z\",\n",
      "  \"batchId\" : 80,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.594896331738437,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 531,\n",
      "    \"commitOffsets\" : 45,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 4,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 627,\n",
      "    \"walCommit\" : 42\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3311\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3312\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3312\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.594896331738437,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:24:53 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.80.4136ea20-afdd-49ea-8414-91a2cc50618e.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/80\n",
      "25/05/05 11:24:53 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:24:52.719Z\",\n",
      "  \"batchId\" : 80,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.547987616099071,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 568,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 4,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 646,\n",
      "    \"walCommit\" : 42\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3311\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3312\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3312\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.547987616099071,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 80\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|B013|R081|02-00-00|161/YANKEE STAD|05/05/2025|11:24:51|REGULAR|13         |10        |2025-05-05 11:24:52.724|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:24:59 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/81 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.81.2b52ecf3-121a-451a-afe0-79325cb06c6b.tmp\n",
      "25/05/05 11:24:59 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/81 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.81.4d2c689a-a47f-4ff2-872c-e1e5b03c423a.tmp\n",
      "25/05/05 11:24:59 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/81 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.81.30a05587-336f-40b6-b9e1-0b22a5ea3ec0.tmp\n",
      "25/05/05 11:24:59 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.81.30a05587-336f-40b6-b9e1-0b22a5ea3ec0.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/81\n",
      "25/05/05 11:24:59 INFO MicroBatchExecution: Committed offsets for batch 81. Metadata OffsetSeqMetadata(0,1746458699177,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:24:59 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.81.2b52ecf3-121a-451a-afe0-79325cb06c6b.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/81\n",
      "25/05/05 11:24:59 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.81.4d2c689a-a47f-4ff2-872c-e1e5b03c423a.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/81\n",
      "25/05/05 11:24:59 INFO MicroBatchExecution: Committed offsets for batch 81. Metadata OffsetSeqMetadata(0,1746458699179,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:24:59 INFO MicroBatchExecution: Committed offsets for batch 81. Metadata OffsetSeqMetadata(0,1746458699180,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:24:59 INFO IncrementalExecution: Current batch timestamp = 1746458699180\n",
      "25/05/05 11:24:59 INFO IncrementalExecution: Current batch timestamp = 1746458699177\n",
      "25/05/05 11:24:59 INFO IncrementalExecution: Current batch timestamp = 1746458699179\n",
      "25/05/05 11:24:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:59 INFO IncrementalExecution: Current batch timestamp = 1746458699179\n",
      "25/05/05 11:24:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:59 INFO IncrementalExecution: Current batch timestamp = 1746458699180\n",
      "25/05/05 11:24:59 INFO IncrementalExecution: Current batch timestamp = 1746458699177\n",
      "25/05/05 11:24:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:59 INFO IncrementalExecution: Current batch timestamp = 1746458699180\n",
      "25/05/05 11:24:59 INFO IncrementalExecution: Current batch timestamp = 1746458699179\n",
      "25/05/05 11:24:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:24:59 INFO CodeGenerator: Code generated in 4.158875 ms\n",
      "25/05/05 11:24:59 INFO CodeGenerator: Code generated in 3.801292 ms\n",
      "25/05/05 11:24:59 INFO CodeGenerator: Code generated in 4.174 ms\n",
      "25/05/05 11:24:59 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 81, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@39f91868]. The input RDD has 1 partitions.\n",
      "25/05/05 11:24:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:59 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 81, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:24:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:59 INFO DAGScheduler: Got job 243 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:24:59 INFO DAGScheduler: Final stage: ResultStage 242 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:24:59 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:24:59 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:24:59 INFO DAGScheduler: Submitting ResultStage 242 (MapPartitionsRDD[1306] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:24:59 INFO MemoryStore: Block broadcast_323 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:24:59 INFO MemoryStore: Block broadcast_323_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.1 MiB)\n",
      "25/05/05 11:24:59 INFO BlockManagerInfo: Added broadcast_323_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:59 INFO SparkContext: Created broadcast 323 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:24:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 242 (MapPartitionsRDD[1306] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:24:59 INFO TaskSchedulerImpl: Adding task set 242.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:24:59 INFO DAGScheduler: Got job 244 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:24:59 INFO DAGScheduler: Final stage: ResultStage 243 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:24:59 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:24:59 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:24:59 INFO DAGScheduler: Submitting ResultStage 243 (MapPartitionsRDD[1307] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:24:59 INFO TaskSetManager: Starting task 0.0 in stage 242.0 (TID 242) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:24:59 INFO MemoryStore: Block broadcast_324 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:24:59 INFO Executor: Running task 0.0 in stage 242.0 (TID 242)\n",
      "25/05/05 11:24:59 INFO MemoryStore: Block broadcast_324_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:24:59 INFO BlockManagerInfo: Added broadcast_324_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:59 INFO SparkContext: Created broadcast 324 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:24:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 243 (MapPartitionsRDD[1307] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:24:59 INFO TaskSchedulerImpl: Adding task set 243.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:24:59 INFO TaskSetManager: Starting task 0.0 in stage 243.0 (TID 243) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:24:59 INFO Executor: Running task 0.0 in stage 243.0 (TID 243)\n",
      "25/05/05 11:24:59 INFO CodeGenerator: Code generated in 3.985041 ms\n",
      "25/05/05 11:24:59 INFO CodeGenerator: Code generated in 3.913459 ms\n",
      "25/05/05 11:24:59 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3312 untilOffset=3313, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=81 taskId=243 partitionId=0\n",
      "25/05/05 11:24:59 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3312 untilOffset=3313, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=81 taskId=242 partitionId=0\n",
      "25/05/05 11:24:59 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3312 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:59 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3312 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:59 INFO DAGScheduler: Got job 245 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:24:59 INFO DAGScheduler: Final stage: ResultStage 244 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:24:59 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:24:59 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:24:59 INFO DAGScheduler: Submitting ResultStage 244 (MapPartitionsRDD[1312] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:24:59 INFO MemoryStore: Block broadcast_325 stored as values in memory (estimated size 47.1 KiB, free 366.0 MiB)\n",
      "25/05/05 11:24:59 INFO MemoryStore: Block broadcast_325_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.0 MiB)\n",
      "25/05/05 11:24:59 INFO BlockManagerInfo: Added broadcast_325_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:24:59 INFO SparkContext: Created broadcast 325 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:24:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 244 (MapPartitionsRDD[1312] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:24:59 INFO TaskSchedulerImpl: Adding task set 244.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:24:59 INFO TaskSetManager: Starting task 0.0 in stage 244.0 (TID 244) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:24:59 INFO Executor: Running task 0.0 in stage 244.0 (TID 244)\n",
      "25/05/05 11:24:59 INFO CodeGenerator: Code generated in 3.983292 ms\n",
      "25/05/05 11:24:59 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3312 untilOffset=3313, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=81 taskId=244 partitionId=0\n",
      "25/05/05 11:24:59 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3312 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3313, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3313, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:59 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:24:59 INFO DataWritingSparkTask: Committed partition 0 (task 243, attempt 0, stage 243.0)\n",
      "25/05/05 11:24:59 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 507225833 nanos, during time span of 507553542 nanos.\n",
      "25/05/05 11:24:59 INFO Executor: Finished task 0.0 in stage 243.0 (TID 243). 2145 bytes result sent to driver\n",
      "25/05/05 11:24:59 INFO TaskSetManager: Finished task 0.0 in stage 243.0 (TID 243) in 516 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:24:59 INFO TaskSchedulerImpl: Removed TaskSet 243.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:24:59 INFO DAGScheduler: ResultStage 243 (start at NativeMethodAccessorImpl.java:0) finished in 0.517 s\n",
      "25/05/05 11:24:59 INFO DAGScheduler: Job 244 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:24:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 243: Stage finished\n",
      "25/05/05 11:24:59 INFO DAGScheduler: Job 244 finished: start at NativeMethodAccessorImpl.java:0, took 0.520075 s\n",
      "25/05/05 11:24:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 81, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:24:59 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:24:59 INFO DataWritingSparkTask: Committed partition 0 (task 242, attempt 0, stage 242.0)\n",
      "25/05/05 11:24:59 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 507065375 nanos, during time span of 508861875 nanos.\n",
      "25/05/05 11:24:59 INFO Executor: Finished task 0.0 in stage 242.0 (TID 242). 3516 bytes result sent to driver\n",
      "25/05/05 11:24:59 INFO TaskSetManager: Finished task 0.0 in stage 242.0 (TID 242) in 518 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:24:59 INFO TaskSchedulerImpl: Removed TaskSet 242.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:24:59 INFO DAGScheduler: ResultStage 242 (start at NativeMethodAccessorImpl.java:0) finished in 0.521 s\n",
      "25/05/05 11:24:59 INFO DAGScheduler: Job 243 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:24:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 242: Stage finished\n",
      "25/05/05 11:24:59 INFO DAGScheduler: Job 243 finished: start at NativeMethodAccessorImpl.java:0, took 0.521755 s\n",
      "25/05/05 11:24:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 81, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@39f91868] is committing.\n",
      "25/05/05 11:24:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 81, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@39f91868] committed.\n",
      "25/05/05 11:24:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 81, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:24:59 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/81 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.81.6e53b87c-5693-4e0d-bfc6-66b7cbf8f6c9.tmp\n",
      "25/05/05 11:24:59 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/81 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.81.00d92e45-0684-47c3-b4f3-dbe96b69d135.tmp\n",
      "25/05/05 11:24:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:24:59 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3313, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:24:59 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:24:59 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:24:59 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:24:59 INFO connection: Opened connection [connectionId{localValue:161, serverValue:4531}] to localhost:27017\n",
      "25/05/05 11:24:59 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=196708}\n",
      "25/05/05 11:24:59 INFO connection: Opened connection [connectionId{localValue:162, serverValue:4532}] to localhost:27017\n",
      "25/05/05 11:24:59 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:24:59 INFO connection: Closed connection [connectionId{localValue:162, serverValue:4532}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:24:59 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 503964292 nanos, during time span of 508659208 nanos.\n",
      "25/05/05 11:24:59 INFO Executor: Finished task 0.0 in stage 244.0 (TID 244). 1645 bytes result sent to driver\n",
      "25/05/05 11:24:59 INFO TaskSetManager: Finished task 0.0 in stage 244.0 (TID 244) in 519 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:24:59 INFO TaskSchedulerImpl: Removed TaskSet 244.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:24:59 INFO DAGScheduler: ResultStage 244 (start at NativeMethodAccessorImpl.java:0) finished in 0.524 s\n",
      "25/05/05 11:24:59 INFO DAGScheduler: Job 245 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:24:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 244: Stage finished\n",
      "25/05/05 11:24:59 INFO DAGScheduler: Job 245 finished: start at NativeMethodAccessorImpl.java:0, took 0.524716 s\n",
      "25/05/05 11:24:59 INFO MemoryStore: Block broadcast_326 stored as values in memory (estimated size 248.0 B, free 366.0 MiB)\n",
      "25/05/05 11:24:59 INFO MemoryStore: Block broadcast_326_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.0 MiB)\n",
      "25/05/05 11:24:59 INFO BlockManagerInfo: Added broadcast_326_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:24:59 INFO SparkContext: Created broadcast 326 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:24:59 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.81.6e53b87c-5693-4e0d-bfc6-66b7cbf8f6c9.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/81\n",
      "25/05/05 11:24:59 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:24:59.173Z\",\n",
      "  \"batchId\" : 81,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.5797788309636651,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 537,\n",
      "    \"commitOffsets\" : 31,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 7,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 633,\n",
      "    \"walCommit\" : 54\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3312\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3313\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3313\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.5797788309636651,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:24:59 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/81 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.81.279a6e12-9a09-4c82-a6bf-634b31da506d.tmp\n",
      "25/05/05 11:24:59 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.81.00d92e45-0684-47c3-b4f3-dbe96b69d135.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/81\n",
      "25/05/05 11:24:59 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:24:59.174Z\",\n",
      "  \"batchId\" : 81,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.572327044025157,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 543,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 5,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 636,\n",
      "    \"walCommit\" : 55\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3312\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3313\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3313\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.572327044025157,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:24:59 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.81.279a6e12-9a09-4c82-a6bf-634b31da506d.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/81\n",
      "25/05/05 11:24:59 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:24:59.174Z\",\n",
      "  \"batchId\" : 81,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 62.5,\n",
      "  \"processedRowsPerSecond\" : 1.5360983102918586,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 561,\n",
      "    \"commitOffsets\" : 25,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 651,\n",
      "    \"walCommit\" : 56\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3312\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3313\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3313\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 62.5,\n",
      "    \"processedRowsPerSecond\" : 1.5360983102918586,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 81\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A001|R079|01-00-00|WORLD TRADE CTR|05/05/2025|11:24:58|REGULAR|3          |5         |2025-05-05 11:24:59.179|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:25:05 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/82 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.82.b0278e97-2ed2-44ec-9d4a-17222db2d00f.tmp\n",
      "25/05/05 11:25:05 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/82 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.82.8f362114-9b58-4306-84e0-5b9c473f38f1.tmp\n",
      "25/05/05 11:25:05 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/82 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.82.216b5e7d-2ab0-4165-a3bd-e377dbdea7b2.tmp\n",
      "25/05/05 11:25:05 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.82.b0278e97-2ed2-44ec-9d4a-17222db2d00f.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/82\n",
      "25/05/05 11:25:05 INFO MicroBatchExecution: Committed offsets for batch 82. Metadata OffsetSeqMetadata(0,1746458705641,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:25:05 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.82.216b5e7d-2ab0-4165-a3bd-e377dbdea7b2.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/82\n",
      "25/05/05 11:25:05 INFO MicroBatchExecution: Committed offsets for batch 82. Metadata OffsetSeqMetadata(0,1746458705641,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:25:05 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.82.8f362114-9b58-4306-84e0-5b9c473f38f1.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/82\n",
      "25/05/05 11:25:05 INFO MicroBatchExecution: Committed offsets for batch 82. Metadata OffsetSeqMetadata(0,1746458705646,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:25:05 INFO IncrementalExecution: Current batch timestamp = 1746458705641\n",
      "25/05/05 11:25:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:05 INFO IncrementalExecution: Current batch timestamp = 1746458705641\n",
      "25/05/05 11:25:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:05 INFO IncrementalExecution: Current batch timestamp = 1746458705646\n",
      "25/05/05 11:25:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:05 INFO IncrementalExecution: Current batch timestamp = 1746458705641\n",
      "25/05/05 11:25:05 INFO IncrementalExecution: Current batch timestamp = 1746458705641\n",
      "25/05/05 11:25:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:05 INFO IncrementalExecution: Current batch timestamp = 1746458705646\n",
      "25/05/05 11:25:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:05 INFO IncrementalExecution: Current batch timestamp = 1746458705641\n",
      "25/05/05 11:25:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:05 INFO IncrementalExecution: Current batch timestamp = 1746458705646\n",
      "25/05/05 11:25:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:05 INFO CodeGenerator: Code generated in 4.321459 ms\n",
      "25/05/05 11:25:05 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 82, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:25:05 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:05 INFO DAGScheduler: Got job 246 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:25:05 INFO DAGScheduler: Final stage: ResultStage 245 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:25:05 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:25:05 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:25:05 INFO DAGScheduler: Submitting ResultStage 245 (MapPartitionsRDD[1320] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:25:05 INFO MemoryStore: Block broadcast_327 stored as values in memory (estimated size 15.2 KiB, free 366.0 MiB)\n",
      "25/05/05 11:25:05 INFO MemoryStore: Block broadcast_327_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:25:05 INFO BlockManagerInfo: Added broadcast_327_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:05 INFO SparkContext: Created broadcast 327 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:25:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 245 (MapPartitionsRDD[1320] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:25:05 INFO TaskSchedulerImpl: Adding task set 245.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:25:05 INFO TaskSetManager: Starting task 0.0 in stage 245.0 (TID 245) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:25:05 INFO Executor: Running task 0.0 in stage 245.0 (TID 245)\n",
      "25/05/05 11:25:05 INFO CodeGenerator: Code generated in 4.185875 ms\n",
      "25/05/05 11:25:05 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 82, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4dd1c5fe]. The input RDD has 1 partitions.\n",
      "25/05/05 11:25:05 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:05 INFO DAGScheduler: Got job 247 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:25:05 INFO DAGScheduler: Final stage: ResultStage 246 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:25:05 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:25:05 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:25:05 INFO DAGScheduler: Submitting ResultStage 246 (MapPartitionsRDD[1323] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:25:05 INFO MemoryStore: Block broadcast_328 stored as values in memory (estimated size 16.2 KiB, free 365.9 MiB)\n",
      "25/05/05 11:25:05 INFO CodeGenerator: Code generated in 3.819542 ms\n",
      "25/05/05 11:25:05 INFO MemoryStore: Block broadcast_328_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 365.9 MiB)\n",
      "25/05/05 11:25:05 INFO BlockManagerInfo: Added broadcast_328_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:05 INFO SparkContext: Created broadcast 328 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:25:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 246 (MapPartitionsRDD[1323] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:25:05 INFO TaskSchedulerImpl: Adding task set 246.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:25:05 INFO TaskSetManager: Starting task 0.0 in stage 246.0 (TID 246) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:25:05 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3313 untilOffset=3314, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=82 taskId=245 partitionId=0\n",
      "25/05/05 11:25:05 INFO Executor: Running task 0.0 in stage 246.0 (TID 246)\n",
      "25/05/05 11:25:05 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3313 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:05 INFO BlockManagerInfo: Removed broadcast_324_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:05 INFO CodeGenerator: Code generated in 12.491375 ms\n",
      "25/05/05 11:25:05 INFO BlockManagerInfo: Removed broadcast_320_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:05 INFO BlockManagerInfo: Removed broadcast_322_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:25:05 INFO BlockManagerInfo: Removed broadcast_321_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:05 INFO BlockManagerInfo: Removed broadcast_325_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:05 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3313 untilOffset=3314, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=82 taskId=246 partitionId=0\n",
      "25/05/05 11:25:05 INFO BlockManagerInfo: Removed broadcast_323_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:25:05 INFO BlockManagerInfo: Removed broadcast_326_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:25:05 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3313 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:05 INFO BlockManagerInfo: Removed broadcast_319_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:25:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:05 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:05 INFO DAGScheduler: Got job 248 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:25:05 INFO DAGScheduler: Final stage: ResultStage 247 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:25:05 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:25:05 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:25:05 INFO DAGScheduler: Submitting ResultStage 247 (MapPartitionsRDD[1328] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:25:05 INFO MemoryStore: Block broadcast_329 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:25:05 INFO MemoryStore: Block broadcast_329_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:25:05 INFO BlockManagerInfo: Added broadcast_329_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:05 INFO SparkContext: Created broadcast 329 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:25:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 247 (MapPartitionsRDD[1328] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:25:05 INFO TaskSchedulerImpl: Adding task set 247.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:25:05 INFO TaskSetManager: Starting task 0.0 in stage 247.0 (TID 247) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:25:05 INFO Executor: Running task 0.0 in stage 247.0 (TID 247)\n",
      "25/05/05 11:25:05 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3313 untilOffset=3314, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=82 taskId=247 partitionId=0\n",
      "25/05/05 11:25:05 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3313 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3314, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:06 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:25:06 INFO DataWritingSparkTask: Committed partition 0 (task 245, attempt 0, stage 245.0)\n",
      "25/05/05 11:25:06 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 508712083 nanos, during time span of 509073583 nanos.\n",
      "25/05/05 11:25:06 INFO Executor: Finished task 0.0 in stage 245.0 (TID 245). 2188 bytes result sent to driver\n",
      "25/05/05 11:25:06 INFO TaskSetManager: Finished task 0.0 in stage 245.0 (TID 245) in 518 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:25:06 INFO TaskSchedulerImpl: Removed TaskSet 245.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:25:06 INFO DAGScheduler: ResultStage 245 (start at NativeMethodAccessorImpl.java:0) finished in 0.520 s\n",
      "25/05/05 11:25:06 INFO DAGScheduler: Job 246 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:25:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 245: Stage finished\n",
      "25/05/05 11:25:06 INFO DAGScheduler: Job 246 finished: start at NativeMethodAccessorImpl.java:0, took 0.520724 s\n",
      "25/05/05 11:25:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 82, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:25:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 82, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:25:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3314, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:06 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:25:06 INFO DataWritingSparkTask: Committed partition 0 (task 246, attempt 0, stage 246.0)\n",
      "25/05/05 11:25:06 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 504881041 nanos, during time span of 506520917 nanos.\n",
      "25/05/05 11:25:06 INFO Executor: Finished task 0.0 in stage 246.0 (TID 246). 3559 bytes result sent to driver\n",
      "25/05/05 11:25:06 INFO TaskSetManager: Finished task 0.0 in stage 246.0 (TID 246) in 526 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:25:06 INFO TaskSchedulerImpl: Removed TaskSet 246.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:25:06 INFO DAGScheduler: ResultStage 246 (start at NativeMethodAccessorImpl.java:0) finished in 0.528 s\n",
      "25/05/05 11:25:06 INFO DAGScheduler: Job 247 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:25:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 246: Stage finished\n",
      "25/05/05 11:25:06 INFO DAGScheduler: Job 247 finished: start at NativeMethodAccessorImpl.java:0, took 0.528313 s\n",
      "25/05/05 11:25:06 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/82 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.82.9cd0a0cf-e7a7-4387-8f9c-bffec5e7daa0.tmp\n",
      "25/05/05 11:25:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 82, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4dd1c5fe] is committing.\n",
      "25/05/05 11:25:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 82, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4dd1c5fe] committed.\n",
      "25/05/05 11:25:06 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/82 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.82.b2d2a415-3095-41e4-b653-405ff9cd7e37.tmp\n",
      "25/05/05 11:25:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3314, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:06 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:25:06 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:25:06 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:25:06 INFO connection: Opened connection [connectionId{localValue:163, serverValue:4533}] to localhost:27017\n",
      "25/05/05 11:25:06 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=749708}\n",
      "25/05/05 11:25:06 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.82.9cd0a0cf-e7a7-4387-8f9c-bffec5e7daa0.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/82\n",
      "25/05/05 11:25:06 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:25:05.637Z\",\n",
      "  \"batchId\" : 82,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "  \"processedRowsPerSecond\" : 1.5797788309636651,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 539,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 633,\n",
      "    \"walCommit\" : 56\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3313\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3314\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3314\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "    \"processedRowsPerSecond\" : 1.5797788309636651,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:25:06 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.82.b2d2a415-3095-41e4-b653-405ff9cd7e37.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/82\n",
      "25/05/05 11:25:06 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:25:05.643Z\",\n",
      "  \"batchId\" : 82,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.5797788309636651,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 545,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 633,\n",
      "    \"walCommit\" : 52\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3313\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3314\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3314\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.5797788309636651,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:25:06 INFO connection: Opened connection [connectionId{localValue:164, serverValue:4534}] to localhost:27017\n",
      "25/05/05 11:25:06 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:25:06 INFO connection: Closed connection [connectionId{localValue:164, serverValue:4534}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:25:06 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 511762583 nanos, during time span of 535418708 nanos.\n",
      "25/05/05 11:25:06 INFO Executor: Finished task 0.0 in stage 247.0 (TID 247). 1645 bytes result sent to driver\n",
      "25/05/05 11:25:06 INFO TaskSetManager: Finished task 0.0 in stage 247.0 (TID 247) in 541 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:25:06 INFO TaskSchedulerImpl: Removed TaskSet 247.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:25:06 INFO DAGScheduler: ResultStage 247 (start at NativeMethodAccessorImpl.java:0) finished in 0.546 s\n",
      "25/05/05 11:25:06 INFO DAGScheduler: Job 248 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:25:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 247: Stage finished\n",
      "25/05/05 11:25:06 INFO DAGScheduler: Job 248 finished: start at NativeMethodAccessorImpl.java:0, took 0.546403 s\n",
      "25/05/05 11:25:06 INFO MemoryStore: Block broadcast_330 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:25:06 INFO MemoryStore: Block broadcast_330_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:25:06 INFO BlockManagerInfo: Added broadcast_330_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:25:06 INFO SparkContext: Created broadcast 330 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:06 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/82 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.82.67c74b2b-c884-4842-9fb5-9bd2fbfa2445.tmp\n",
      "25/05/05 11:25:06 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.82.67c74b2b-c884-4842-9fb5-9bd2fbfa2445.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/82\n",
      "25/05/05 11:25:06 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:25:05.637Z\",\n",
      "  \"batchId\" : 82,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "  \"processedRowsPerSecond\" : 1.4727540500736376,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 588,\n",
      "    \"commitOffsets\" : 26,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 4,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 679,\n",
      "    \"walCommit\" : 55\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3313\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3314\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3314\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "    \"processedRowsPerSecond\" : 1.4727540500736376,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 82\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A001|R079|01-00-00|WORLD TRADE CTR|05/05/2025|11:25:04|REGULAR|5          |11        |2025-05-05 11:25:05.641|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:25:12 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/83 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.83.0554930b-4d12-470a-87bd-2d89da8c6218.tmp\n",
      "25/05/05 11:25:12 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/83 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.83.c8926c5f-ca26-40e8-9cd8-74cea9874ed8.tmp\n",
      "25/05/05 11:25:12 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/83 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.83.da4952bd-9f61-408e-afa1-c4b8147c21de.tmp\n",
      "25/05/05 11:25:12 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.83.c8926c5f-ca26-40e8-9cd8-74cea9874ed8.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/83\n",
      "25/05/05 11:25:12 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.83.da4952bd-9f61-408e-afa1-c4b8147c21de.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/83\n",
      "25/05/05 11:25:12 INFO MicroBatchExecution: Committed offsets for batch 83. Metadata OffsetSeqMetadata(0,1746458712112,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:25:12 INFO MicroBatchExecution: Committed offsets for batch 83. Metadata OffsetSeqMetadata(0,1746458712112,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:25:12 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.83.0554930b-4d12-470a-87bd-2d89da8c6218.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/83\n",
      "25/05/05 11:25:12 INFO MicroBatchExecution: Committed offsets for batch 83. Metadata OffsetSeqMetadata(0,1746458712110,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:25:12 INFO IncrementalExecution: Current batch timestamp = 1746458712112\n",
      "25/05/05 11:25:12 INFO IncrementalExecution: Current batch timestamp = 1746458712112\n",
      "25/05/05 11:25:12 INFO IncrementalExecution: Current batch timestamp = 1746458712110\n",
      "25/05/05 11:25:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:12 INFO IncrementalExecution: Current batch timestamp = 1746458712112\n",
      "25/05/05 11:25:12 INFO IncrementalExecution: Current batch timestamp = 1746458712112\n",
      "25/05/05 11:25:12 INFO IncrementalExecution: Current batch timestamp = 1746458712110\n",
      "25/05/05 11:25:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:12 INFO IncrementalExecution: Current batch timestamp = 1746458712110\n",
      "25/05/05 11:25:12 INFO IncrementalExecution: Current batch timestamp = 1746458712112\n",
      "25/05/05 11:25:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:12 INFO CodeGenerator: Code generated in 4.128125 ms\n",
      "25/05/05 11:25:12 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 83, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:25:12 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:12 INFO DAGScheduler: Got job 249 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:25:12 INFO DAGScheduler: Final stage: ResultStage 248 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:25:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:25:12 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:25:12 INFO DAGScheduler: Submitting ResultStage 248 (MapPartitionsRDD[1336] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:25:12 INFO MemoryStore: Block broadcast_331 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:25:12 INFO CodeGenerator: Code generated in 3.708625 ms\n",
      "25/05/05 11:25:12 INFO MemoryStore: Block broadcast_331_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:25:12 INFO BlockManagerInfo: Added broadcast_331_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:12 INFO SparkContext: Created broadcast 331 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:25:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 248 (MapPartitionsRDD[1336] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:25:12 INFO TaskSchedulerImpl: Adding task set 248.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:25:12 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 83, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4746d311]. The input RDD has 1 partitions.\n",
      "25/05/05 11:25:12 INFO TaskSetManager: Starting task 0.0 in stage 248.0 (TID 248) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:25:12 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:12 INFO Executor: Running task 0.0 in stage 248.0 (TID 248)\n",
      "25/05/05 11:25:12 INFO DAGScheduler: Got job 250 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:25:12 INFO DAGScheduler: Final stage: ResultStage 249 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:25:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:25:12 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:25:12 INFO DAGScheduler: Submitting ResultStage 249 (MapPartitionsRDD[1339] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:25:12 INFO MemoryStore: Block broadcast_332 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:25:12 INFO MemoryStore: Block broadcast_332_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)\n",
      "25/05/05 11:25:12 INFO BlockManagerInfo: Added broadcast_332_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:12 INFO SparkContext: Created broadcast 332 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:25:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 249 (MapPartitionsRDD[1339] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:25:12 INFO TaskSchedulerImpl: Adding task set 249.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:25:12 INFO TaskSetManager: Starting task 0.0 in stage 249.0 (TID 249) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:25:12 INFO Executor: Running task 0.0 in stage 249.0 (TID 249)\n",
      "25/05/05 11:25:12 INFO CodeGenerator: Code generated in 3.954792 ms\n",
      "25/05/05 11:25:12 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3314 untilOffset=3315, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=83 taskId=248 partitionId=0\n",
      "25/05/05 11:25:12 INFO CodeGenerator: Code generated in 3.163708 ms\n",
      "25/05/05 11:25:12 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3314 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:12 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3314 untilOffset=3315, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=83 taskId=249 partitionId=0\n",
      "25/05/05 11:25:12 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:12 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3314 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:12 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:12 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:12 INFO DAGScheduler: Got job 251 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:25:12 INFO DAGScheduler: Final stage: ResultStage 250 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:25:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:25:12 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:25:12 INFO DAGScheduler: Submitting ResultStage 250 (MapPartitionsRDD[1344] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:25:12 INFO MemoryStore: Block broadcast_333 stored as values in memory (estimated size 47.1 KiB, free 366.0 MiB)\n",
      "25/05/05 11:25:12 INFO MemoryStore: Block broadcast_333_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.0 MiB)\n",
      "25/05/05 11:25:12 INFO BlockManagerInfo: Added broadcast_333_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:12 INFO SparkContext: Created broadcast 333 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:25:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 250 (MapPartitionsRDD[1344] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:25:12 INFO TaskSchedulerImpl: Adding task set 250.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:25:12 INFO TaskSetManager: Starting task 0.0 in stage 250.0 (TID 250) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:25:12 INFO Executor: Running task 0.0 in stage 250.0 (TID 250)\n",
      "25/05/05 11:25:12 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3314 untilOffset=3315, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=83 taskId=250 partitionId=0\n",
      "25/05/05 11:25:12 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3314 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:12 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:12 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:12 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:12 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:12 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:12 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3315, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:12 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3315, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:12 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:25:12 INFO DataWritingSparkTask: Committed partition 0 (task 248, attempt 0, stage 248.0)\n",
      "25/05/05 11:25:12 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 506323500 nanos, during time span of 506738333 nanos.\n",
      "25/05/05 11:25:12 INFO Executor: Finished task 0.0 in stage 248.0 (TID 248). 2145 bytes result sent to driver\n",
      "25/05/05 11:25:12 INFO TaskSetManager: Finished task 0.0 in stage 248.0 (TID 248) in 516 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:25:12 INFO TaskSchedulerImpl: Removed TaskSet 248.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:25:12 INFO DAGScheduler: ResultStage 248 (start at NativeMethodAccessorImpl.java:0) finished in 0.518 s\n",
      "25/05/05 11:25:12 INFO DAGScheduler: Job 249 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:25:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 248: Stage finished\n",
      "25/05/05 11:25:12 INFO DAGScheduler: Job 249 finished: start at NativeMethodAccessorImpl.java:0, took 0.518531 s\n",
      "25/05/05 11:25:12 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 83, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:25:12 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:25:12 INFO DataWritingSparkTask: Committed partition 0 (task 249, attempt 0, stage 249.0)\n",
      "25/05/05 11:25:12 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 504402334 nanos, during time span of 506205875 nanos.\n",
      "25/05/05 11:25:12 INFO Executor: Finished task 0.0 in stage 249.0 (TID 249). 3516 bytes result sent to driver\n",
      "25/05/05 11:25:12 INFO TaskSetManager: Finished task 0.0 in stage 249.0 (TID 249) in 516 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:25:12 INFO TaskSchedulerImpl: Removed TaskSet 249.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:25:12 INFO DAGScheduler: ResultStage 249 (start at NativeMethodAccessorImpl.java:0) finished in 0.517 s\n",
      "25/05/05 11:25:12 INFO DAGScheduler: Job 250 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:25:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 249: Stage finished\n",
      "25/05/05 11:25:12 INFO DAGScheduler: Job 250 finished: start at NativeMethodAccessorImpl.java:0, took 0.518015 s\n",
      "25/05/05 11:25:12 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 83, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4746d311] is committing.\n",
      "25/05/05 11:25:12 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 83, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4746d311] committed.\n",
      "25/05/05 11:25:12 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 83, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:25:12 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/83 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.83.4a98dbc8-5d45-41d9-9e45-526dfdae1785.tmp\n",
      "25/05/05 11:25:12 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/83 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.83.6d57c81b-780a-4fcd-9f95-43cada46492e.tmp\n",
      "25/05/05 11:25:12 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:12 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:12 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3315, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:12 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:25:12 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:25:12 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:25:12 INFO connection: Opened connection [connectionId{localValue:165, serverValue:4535}] to localhost:27017\n",
      "25/05/05 11:25:12 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=365375}\n",
      "25/05/05 11:25:12 INFO connection: Opened connection [connectionId{localValue:166, serverValue:4536}] to localhost:27017\n",
      "25/05/05 11:25:12 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:25:12 INFO connection: Closed connection [connectionId{localValue:166, serverValue:4536}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:25:12 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 505830208 nanos, during time span of 511361292 nanos.\n",
      "25/05/05 11:25:12 INFO Executor: Finished task 0.0 in stage 250.0 (TID 250). 1645 bytes result sent to driver\n",
      "25/05/05 11:25:12 INFO TaskSetManager: Finished task 0.0 in stage 250.0 (TID 250) in 518 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:25:12 INFO TaskSchedulerImpl: Removed TaskSet 250.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:25:12 INFO DAGScheduler: ResultStage 250 (start at NativeMethodAccessorImpl.java:0) finished in 0.521 s\n",
      "25/05/05 11:25:12 INFO DAGScheduler: Job 251 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:25:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 250: Stage finished\n",
      "25/05/05 11:25:12 INFO DAGScheduler: Job 251 finished: start at NativeMethodAccessorImpl.java:0, took 0.522072 s\n",
      "25/05/05 11:25:12 INFO MemoryStore: Block broadcast_334 stored as values in memory (estimated size 248.0 B, free 366.0 MiB)\n",
      "25/05/05 11:25:12 INFO MemoryStore: Block broadcast_334_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.0 MiB)\n",
      "25/05/05 11:25:12 INFO BlockManagerInfo: Added broadcast_334_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:25:12 INFO SparkContext: Created broadcast 334 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:12 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/83 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.83.50303fe6-d5a1-41bf-9249-a34912162987.tmp\n",
      "25/05/05 11:25:12 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.83.4a98dbc8-5d45-41d9-9e45-526dfdae1785.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/83\n",
      "25/05/05 11:25:12 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:25:12.106Z\",\n",
      "  \"batchId\" : 83,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.5625,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 532,\n",
      "    \"commitOffsets\" : 32,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 4,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 640,\n",
      "    \"walCommit\" : 69\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3314\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3315\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3315\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.5625,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:25:12 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.83.6d57c81b-780a-4fcd-9f95-43cada46492e.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/83\n",
      "25/05/05 11:25:12 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:25:12.106Z\",\n",
      "  \"batchId\" : 83,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 55.55555555555556,\n",
      "  \"processedRowsPerSecond\" : 1.5503875968992247,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 539,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 6,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 645,\n",
      "    \"walCommit\" : 65\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3314\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3315\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3315\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 55.55555555555556,\n",
      "    \"processedRowsPerSecond\" : 1.5503875968992247,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:25:12 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.83.50303fe6-d5a1-41bf-9249-a34912162987.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/83\n",
      "25/05/05 11:25:12 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:25:12.106Z\",\n",
      "  \"batchId\" : 83,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 55.55555555555556,\n",
      "  \"processedRowsPerSecond\" : 1.519756838905775,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 554,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 6,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 658,\n",
      "    \"walCommit\" : 65\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3314\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3315\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3315\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 55.55555555555556,\n",
      "    \"processedRowsPerSecond\" : 1.519756838905775,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 83\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A001|R079|01-00-00|WORLD TRADE CTR|05/05/2025|11:25:11|REGULAR|3          |12        |2025-05-05 11:25:12.112|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:25:17 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/84 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.84.f3ad02c3-358a-4d38-a982-efb6c85c84f2.tmp\n",
      "25/05/05 11:25:17 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/84 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.84.56b6e242-164c-477b-a404-7f536cd2707a.tmp\n",
      "25/05/05 11:25:17 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/84 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.84.9b842b3d-900e-4ed6-8b2e-f9f7d6954ff7.tmp\n",
      "25/05/05 11:25:17 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.84.f3ad02c3-358a-4d38-a982-efb6c85c84f2.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/84\n",
      "25/05/05 11:25:17 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.84.56b6e242-164c-477b-a404-7f536cd2707a.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/84\n",
      "25/05/05 11:25:17 INFO MicroBatchExecution: Committed offsets for batch 84. Metadata OffsetSeqMetadata(0,1746458717608,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:25:17 INFO MicroBatchExecution: Committed offsets for batch 84. Metadata OffsetSeqMetadata(0,1746458717610,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:25:17 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.84.9b842b3d-900e-4ed6-8b2e-f9f7d6954ff7.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/84\n",
      "25/05/05 11:25:17 INFO MicroBatchExecution: Committed offsets for batch 84. Metadata OffsetSeqMetadata(0,1746458717609,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:25:17 INFO IncrementalExecution: Current batch timestamp = 1746458717609\n",
      "25/05/05 11:25:17 INFO IncrementalExecution: Current batch timestamp = 1746458717610\n",
      "25/05/05 11:25:17 INFO IncrementalExecution: Current batch timestamp = 1746458717608\n",
      "25/05/05 11:25:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:17 INFO IncrementalExecution: Current batch timestamp = 1746458717610\n",
      "25/05/05 11:25:17 INFO IncrementalExecution: Current batch timestamp = 1746458717609\n",
      "25/05/05 11:25:17 INFO IncrementalExecution: Current batch timestamp = 1746458717608\n",
      "25/05/05 11:25:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:17 INFO IncrementalExecution: Current batch timestamp = 1746458717608\n",
      "25/05/05 11:25:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:17 INFO IncrementalExecution: Current batch timestamp = 1746458717610\n",
      "25/05/05 11:25:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:17 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:17 INFO CodeGenerator: Code generated in 4.6325 ms\n",
      "25/05/05 11:25:17 INFO CodeGenerator: Code generated in 3.846875 ms\n",
      "25/05/05 11:25:17 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 84, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:25:17 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:17 INFO DAGScheduler: Got job 252 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:25:17 INFO DAGScheduler: Final stage: ResultStage 251 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:25:17 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:25:17 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:25:17 INFO DAGScheduler: Submitting ResultStage 251 (MapPartitionsRDD[1352] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:25:17 INFO MemoryStore: Block broadcast_335 stored as values in memory (estimated size 15.2 KiB, free 366.0 MiB)\n",
      "25/05/05 11:25:17 INFO CodeGenerator: Code generated in 4.385583 ms\n",
      "25/05/05 11:25:17 INFO MemoryStore: Block broadcast_335_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:25:17 INFO BlockManagerInfo: Added broadcast_335_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:17 INFO SparkContext: Created broadcast 335 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:25:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 251 (MapPartitionsRDD[1352] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:25:17 INFO TaskSchedulerImpl: Adding task set 251.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:25:17 INFO TaskSetManager: Starting task 0.0 in stage 251.0 (TID 251) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:25:17 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 84, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@22a9b240]. The input RDD has 1 partitions.\n",
      "25/05/05 11:25:17 INFO Executor: Running task 0.0 in stage 251.0 (TID 251)\n",
      "25/05/05 11:25:17 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:17 INFO DAGScheduler: Got job 253 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:25:17 INFO DAGScheduler: Final stage: ResultStage 252 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:25:17 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:25:17 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:25:17 INFO DAGScheduler: Submitting ResultStage 252 (MapPartitionsRDD[1355] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:25:17 INFO MemoryStore: Block broadcast_336 stored as values in memory (estimated size 16.2 KiB, free 365.9 MiB)\n",
      "25/05/05 11:25:17 INFO MemoryStore: Block broadcast_336_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 365.9 MiB)\n",
      "25/05/05 11:25:17 INFO BlockManagerInfo: Added broadcast_336_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:17 INFO SparkContext: Created broadcast 336 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:25:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 252 (MapPartitionsRDD[1355] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:25:17 INFO TaskSchedulerImpl: Adding task set 252.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:25:17 INFO TaskSetManager: Starting task 0.0 in stage 252.0 (TID 252) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:25:17 INFO Executor: Running task 0.0 in stage 252.0 (TID 252)\n",
      "25/05/05 11:25:17 INFO CodeGenerator: Code generated in 3.74225 ms\n",
      "25/05/05 11:25:17 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3315 untilOffset=3316, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=84 taskId=251 partitionId=0\n",
      "25/05/05 11:25:17 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3315 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:17 INFO CodeGenerator: Code generated in 3.71125 ms\n",
      "25/05/05 11:25:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:17 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3315 untilOffset=3316, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=84 taskId=252 partitionId=0\n",
      "25/05/05 11:25:17 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3315 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:17 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:17 INFO DAGScheduler: Got job 254 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:25:17 INFO DAGScheduler: Final stage: ResultStage 253 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:25:17 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:25:17 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:25:17 INFO DAGScheduler: Submitting ResultStage 253 (MapPartitionsRDD[1360] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:25:17 INFO MemoryStore: Block broadcast_337 stored as values in memory (estimated size 47.1 KiB, free 365.9 MiB)\n",
      "25/05/05 11:25:17 INFO MemoryStore: Block broadcast_337_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 365.9 MiB)\n",
      "25/05/05 11:25:17 INFO BlockManagerInfo: Added broadcast_337_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:17 INFO SparkContext: Created broadcast 337 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:25:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 253 (MapPartitionsRDD[1360] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:25:17 INFO TaskSchedulerImpl: Adding task set 253.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:25:17 INFO TaskSetManager: Starting task 0.0 in stage 253.0 (TID 253) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:25:17 INFO Executor: Running task 0.0 in stage 253.0 (TID 253)\n",
      "25/05/05 11:25:17 INFO BlockManagerInfo: Removed broadcast_331_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:17 INFO CodeGenerator: Code generated in 11.380291 ms\n",
      "25/05/05 11:25:17 INFO BlockManagerInfo: Removed broadcast_330_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:25:17 INFO BlockManagerInfo: Removed broadcast_332_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:17 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3315 untilOffset=3316, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=84 taskId=253 partitionId=0\n",
      "25/05/05 11:25:17 INFO BlockManagerInfo: Removed broadcast_329_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:17 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3315 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:17 INFO BlockManagerInfo: Removed broadcast_333_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:17 INFO BlockManagerInfo: Removed broadcast_334_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:25:17 INFO BlockManagerInfo: Removed broadcast_328_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:17 INFO BlockManagerInfo: Removed broadcast_327_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3316, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:18 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:25:18 INFO DataWritingSparkTask: Committed partition 0 (task 251, attempt 0, stage 251.0)\n",
      "25/05/05 11:25:18 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 502902166 nanos, during time span of 503292750 nanos.\n",
      "25/05/05 11:25:18 INFO Executor: Finished task 0.0 in stage 251.0 (TID 251). 2188 bytes result sent to driver\n",
      "25/05/05 11:25:18 INFO TaskSetManager: Finished task 0.0 in stage 251.0 (TID 251) in 512 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:25:18 INFO TaskSchedulerImpl: Removed TaskSet 251.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:25:18 INFO DAGScheduler: ResultStage 251 (start at NativeMethodAccessorImpl.java:0) finished in 0.513 s\n",
      "25/05/05 11:25:18 INFO DAGScheduler: Job 252 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:25:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 251: Stage finished\n",
      "25/05/05 11:25:18 INFO DAGScheduler: Job 252 finished: start at NativeMethodAccessorImpl.java:0, took 0.513997 s\n",
      "25/05/05 11:25:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 84, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:25:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3316, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:18 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:25:18 INFO DataWritingSparkTask: Committed partition 0 (task 252, attempt 0, stage 252.0)\n",
      "25/05/05 11:25:18 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 504389250 nanos, during time span of 506068833 nanos.\n",
      "25/05/05 11:25:18 INFO Executor: Finished task 0.0 in stage 252.0 (TID 252). 3559 bytes result sent to driver\n",
      "25/05/05 11:25:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 84, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:25:18 INFO TaskSetManager: Finished task 0.0 in stage 252.0 (TID 252) in 516 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:25:18 INFO TaskSchedulerImpl: Removed TaskSet 252.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:25:18 INFO DAGScheduler: ResultStage 252 (start at NativeMethodAccessorImpl.java:0) finished in 0.519 s\n",
      "25/05/05 11:25:18 INFO DAGScheduler: Job 253 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:25:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 252: Stage finished\n",
      "25/05/05 11:25:18 INFO DAGScheduler: Job 253 finished: start at NativeMethodAccessorImpl.java:0, took 0.519105 s\n",
      "25/05/05 11:25:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 84, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@22a9b240] is committing.\n",
      "25/05/05 11:25:18 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 84, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@22a9b240] committed.\n",
      "25/05/05 11:25:18 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/84 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.84.3125a09f-8503-45e4-b24d-addc162f7aa0.tmp\n",
      "25/05/05 11:25:18 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/84 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.84.35ad2505-ccd3-413e-b432-f44bc588f248.tmp\n",
      "25/05/05 11:25:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3316, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:18 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:25:18 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:25:18 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.84.35ad2505-ccd3-413e-b432-f44bc588f248.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/84\n",
      "25/05/05 11:25:18 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:25:18 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:25:17.609Z\",\n",
      "  \"batchId\" : 84,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.658374792703151,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 538,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 603,\n",
      "    \"walCommit\" : 29\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3315\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3316\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3316\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.658374792703151,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:25:18 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.84.3125a09f-8503-45e4-b24d-addc162f7aa0.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/84\n",
      "25/05/05 11:25:18 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:25:17.606Z\",\n",
      "  \"batchId\" : 84,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "  \"processedRowsPerSecond\" : 1.6474464579901154,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 537,\n",
      "    \"commitOffsets\" : 32,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 607,\n",
      "    \"walCommit\" : 31\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3315\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3316\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3316\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "    \"processedRowsPerSecond\" : 1.6474464579901154,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:25:18 INFO connection: Opened connection [connectionId{localValue:167, serverValue:4537}] to localhost:27017\n",
      "25/05/05 11:25:18 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=308334}\n",
      "25/05/05 11:25:18 INFO connection: Opened connection [connectionId{localValue:168, serverValue:4538}] to localhost:27017\n",
      "25/05/05 11:25:18 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:25:18 INFO connection: Closed connection [connectionId{localValue:168, serverValue:4538}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:25:18 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 508654792 nanos, during time span of 513932125 nanos.\n",
      "25/05/05 11:25:18 INFO Executor: Finished task 0.0 in stage 253.0 (TID 253). 1688 bytes result sent to driver\n",
      "25/05/05 11:25:18 INFO TaskSetManager: Finished task 0.0 in stage 253.0 (TID 253) in 532 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:25:18 INFO TaskSchedulerImpl: Removed TaskSet 253.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:25:18 INFO DAGScheduler: ResultStage 253 (start at NativeMethodAccessorImpl.java:0) finished in 0.541 s\n",
      "25/05/05 11:25:18 INFO DAGScheduler: Job 254 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:25:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 253: Stage finished\n",
      "25/05/05 11:25:18 INFO DAGScheduler: Job 254 finished: start at NativeMethodAccessorImpl.java:0, took 0.541137 s\n",
      "25/05/05 11:25:18 INFO MemoryStore: Block broadcast_338 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:25:18 INFO MemoryStore: Block broadcast_338_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:25:18 INFO BlockManagerInfo: Added broadcast_338_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:25:18 INFO SparkContext: Created broadcast 338 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:18 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/84 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.84.b8db0fca-264f-4991-9f15-b71dfa3c551f.tmp\n",
      "25/05/05 11:25:18 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.84.b8db0fca-264f-4991-9f15-b71dfa3c551f.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/84\n",
      "25/05/05 11:25:18 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:25:17.608Z\",\n",
      "  \"batchId\" : 84,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.567398119122257,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 574,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 638,\n",
      "    \"walCommit\" : 31\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3315\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3316\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3316\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.567398119122257,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 84\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|B013|R081|02-00-00|161/YANKEE STAD|05/05/2025|11:25:16|REGULAR|5          |9         |2025-05-05 11:25:17.608|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:25:22 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/85 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.85.e3a776be-7b4a-406c-9ad5-ba7929972958.tmp\n",
      "25/05/05 11:25:22 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/85 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.85.97d30503-78da-4537-8d22-ffbefdb26255.tmp\n",
      "25/05/05 11:25:22 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/85 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.85.13c29e77-a67d-49d6-8108-143bceb6d00a.tmp\n",
      "25/05/05 11:25:22 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.85.13c29e77-a67d-49d6-8108-143bceb6d00a.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/85\n",
      "25/05/05 11:25:22 INFO MicroBatchExecution: Committed offsets for batch 85. Metadata OffsetSeqMetadata(0,1746458722097,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:25:22 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.85.97d30503-78da-4537-8d22-ffbefdb26255.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/85\n",
      "25/05/05 11:25:22 INFO MicroBatchExecution: Committed offsets for batch 85. Metadata OffsetSeqMetadata(0,1746458722096,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:25:22 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.85.e3a776be-7b4a-406c-9ad5-ba7929972958.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/85\n",
      "25/05/05 11:25:22 INFO MicroBatchExecution: Committed offsets for batch 85. Metadata OffsetSeqMetadata(0,1746458722096,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:25:22 INFO IncrementalExecution: Current batch timestamp = 1746458722096\n",
      "25/05/05 11:25:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:22 INFO IncrementalExecution: Current batch timestamp = 1746458722096\n",
      "25/05/05 11:25:22 INFO IncrementalExecution: Current batch timestamp = 1746458722097\n",
      "25/05/05 11:25:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:22 INFO IncrementalExecution: Current batch timestamp = 1746458722096\n",
      "25/05/05 11:25:22 INFO IncrementalExecution: Current batch timestamp = 1746458722096\n",
      "25/05/05 11:25:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:22 INFO IncrementalExecution: Current batch timestamp = 1746458722097\n",
      "25/05/05 11:25:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:22 INFO IncrementalExecution: Current batch timestamp = 1746458722096\n",
      "25/05/05 11:25:22 INFO IncrementalExecution: Current batch timestamp = 1746458722097\n",
      "25/05/05 11:25:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:22 INFO CodeGenerator: Code generated in 5.334208 ms\n",
      "25/05/05 11:25:22 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 85, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@876ab9a]. The input RDD has 1 partitions.\n",
      "25/05/05 11:25:22 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:22 INFO DAGScheduler: Got job 255 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:25:22 INFO CodeGenerator: Code generated in 4.074042 ms\n",
      "25/05/05 11:25:22 INFO DAGScheduler: Final stage: ResultStage 254 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:25:22 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:25:22 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:25:22 INFO DAGScheduler: Submitting ResultStage 254 (MapPartitionsRDD[1368] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:25:22 INFO MemoryStore: Block broadcast_339 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:25:22 INFO MemoryStore: Block broadcast_339_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.1 MiB)\n",
      "25/05/05 11:25:22 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 85, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:25:22 INFO BlockManagerInfo: Added broadcast_339_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:22 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:22 INFO SparkContext: Created broadcast 339 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:25:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 254 (MapPartitionsRDD[1368] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:25:22 INFO TaskSchedulerImpl: Adding task set 254.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:25:22 INFO DAGScheduler: Got job 256 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:25:22 INFO DAGScheduler: Final stage: ResultStage 255 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:25:22 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:25:22 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:25:22 INFO DAGScheduler: Submitting ResultStage 255 (MapPartitionsRDD[1371] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:25:22 INFO TaskSetManager: Starting task 0.0 in stage 254.0 (TID 254) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:25:22 INFO Executor: Running task 0.0 in stage 254.0 (TID 254)\n",
      "25/05/05 11:25:22 INFO MemoryStore: Block broadcast_340 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:25:22 INFO MemoryStore: Block broadcast_340_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:25:22 INFO BlockManagerInfo: Added broadcast_340_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:22 INFO SparkContext: Created broadcast 340 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:25:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 255 (MapPartitionsRDD[1371] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:25:22 INFO TaskSchedulerImpl: Adding task set 255.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:25:22 INFO TaskSetManager: Starting task 0.0 in stage 255.0 (TID 255) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:25:22 INFO Executor: Running task 0.0 in stage 255.0 (TID 255)\n",
      "25/05/05 11:25:22 INFO CodeGenerator: Code generated in 3.941875 ms\n",
      "25/05/05 11:25:22 INFO CodeGenerator: Code generated in 4.364167 ms\n",
      "25/05/05 11:25:22 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3316 untilOffset=3317, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=85 taskId=255 partitionId=0\n",
      "25/05/05 11:25:22 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3316 untilOffset=3317, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=85 taskId=254 partitionId=0\n",
      "25/05/05 11:25:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3316 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3316 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:22 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:22 INFO DAGScheduler: Got job 257 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:25:22 INFO DAGScheduler: Final stage: ResultStage 256 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:25:22 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:25:22 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:25:22 INFO DAGScheduler: Submitting ResultStage 256 (MapPartitionsRDD[1376] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:25:22 INFO MemoryStore: Block broadcast_341 stored as values in memory (estimated size 47.1 KiB, free 366.0 MiB)\n",
      "25/05/05 11:25:22 INFO MemoryStore: Block broadcast_341_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.0 MiB)\n",
      "25/05/05 11:25:22 INFO BlockManagerInfo: Added broadcast_341_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:22 INFO SparkContext: Created broadcast 341 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:25:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 256 (MapPartitionsRDD[1376] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:25:22 INFO TaskSchedulerImpl: Adding task set 256.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:25:22 INFO TaskSetManager: Starting task 0.0 in stage 256.0 (TID 256) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:25:22 INFO Executor: Running task 0.0 in stage 256.0 (TID 256)\n",
      "25/05/05 11:25:22 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3316 untilOffset=3317, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=85 taskId=256 partitionId=0\n",
      "25/05/05 11:25:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3316 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3317, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3317, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:22 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:25:22 INFO DataWritingSparkTask: Committed partition 0 (task 255, attempt 0, stage 255.0)\n",
      "25/05/05 11:25:22 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 503347083 nanos, during time span of 503965292 nanos.\n",
      "25/05/05 11:25:22 INFO Executor: Finished task 0.0 in stage 255.0 (TID 255). 2145 bytes result sent to driver\n",
      "25/05/05 11:25:22 INFO TaskSetManager: Finished task 0.0 in stage 255.0 (TID 255) in 513 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:25:22 INFO TaskSchedulerImpl: Removed TaskSet 255.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:25:22 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:25:22 INFO DataWritingSparkTask: Committed partition 0 (task 254, attempt 0, stage 254.0)\n",
      "25/05/05 11:25:22 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 502704166 nanos, during time span of 504593125 nanos.\n",
      "25/05/05 11:25:22 INFO DAGScheduler: ResultStage 255 (start at NativeMethodAccessorImpl.java:0) finished in 0.515 s\n",
      "25/05/05 11:25:22 INFO DAGScheduler: Job 256 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:25:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 255: Stage finished\n",
      "25/05/05 11:25:22 INFO DAGScheduler: Job 256 finished: start at NativeMethodAccessorImpl.java:0, took 0.515815 s\n",
      "25/05/05 11:25:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 85, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:25:22 INFO Executor: Finished task 0.0 in stage 254.0 (TID 254). 3554 bytes result sent to driver\n",
      "25/05/05 11:25:22 INFO TaskSetManager: Finished task 0.0 in stage 254.0 (TID 254) in 516 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:25:22 INFO TaskSchedulerImpl: Removed TaskSet 254.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:25:22 INFO DAGScheduler: ResultStage 254 (start at NativeMethodAccessorImpl.java:0) finished in 0.518 s\n",
      "25/05/05 11:25:22 INFO DAGScheduler: Job 255 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:25:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 254: Stage finished\n",
      "25/05/05 11:25:22 INFO DAGScheduler: Job 255 finished: start at NativeMethodAccessorImpl.java:0, took 0.518644 s\n",
      "25/05/05 11:25:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 85, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@876ab9a] is committing.\n",
      "25/05/05 11:25:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 85, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@876ab9a] committed.\n",
      "25/05/05 11:25:22 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/85 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.85.d885cff8-3a4b-464e-8c19-52902199488e.tmp\n",
      "25/05/05 11:25:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 85, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:25:22 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/85 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.85.1fd5eb4b-2f39-48e4-b340-cba28cc6d1ef.tmp\n",
      "25/05/05 11:25:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3317, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:22 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:25:22 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:25:22 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:25:22 INFO connection: Opened connection [connectionId{localValue:169, serverValue:4539}] to localhost:27017\n",
      "25/05/05 11:25:22 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=569583}\n",
      "25/05/05 11:25:22 INFO connection: Opened connection [connectionId{localValue:170, serverValue:4540}] to localhost:27017\n",
      "25/05/05 11:25:22 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:25:22 INFO connection: Closed connection [connectionId{localValue:170, serverValue:4540}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:25:22 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 503688375 nanos, during time span of 509157333 nanos.\n",
      "25/05/05 11:25:22 INFO Executor: Finished task 0.0 in stage 256.0 (TID 256). 1645 bytes result sent to driver\n",
      "25/05/05 11:25:22 INFO TaskSetManager: Finished task 0.0 in stage 256.0 (TID 256) in 516 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:25:22 INFO TaskSchedulerImpl: Removed TaskSet 256.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:25:22 INFO DAGScheduler: ResultStage 256 (start at NativeMethodAccessorImpl.java:0) finished in 0.520 s\n",
      "25/05/05 11:25:22 INFO DAGScheduler: Job 257 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:25:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 256: Stage finished\n",
      "25/05/05 11:25:22 INFO DAGScheduler: Job 257 finished: start at NativeMethodAccessorImpl.java:0, took 0.521061 s\n",
      "25/05/05 11:25:22 INFO MemoryStore: Block broadcast_342 stored as values in memory (estimated size 248.0 B, free 366.0 MiB)\n",
      "25/05/05 11:25:22 INFO MemoryStore: Block broadcast_342_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.0 MiB)\n",
      "25/05/05 11:25:22 INFO BlockManagerInfo: Added broadcast_342_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:25:22 INFO SparkContext: Created broadcast 342 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:22 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.85.d885cff8-3a4b-464e-8c19-52902199488e.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/85\n",
      "25/05/05 11:25:22 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/85 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.85.4a3302ac-b8fc-41f3-8df9-cd51a6a6e4bc.tmp\n",
      "25/05/05 11:25:22 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:25:22.093Z\",\n",
      "  \"batchId\" : 85,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.6339869281045751,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 531,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 612,\n",
      "    \"walCommit\" : 44\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3316\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3317\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3317\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.6339869281045751,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:25:22 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.85.1fd5eb4b-2f39-48e4-b340-cba28cc6d1ef.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/85\n",
      "25/05/05 11:25:22 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:25:22.093Z\",\n",
      "  \"batchId\" : 85,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.6181229773462784,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 537,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 4,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 618,\n",
      "    \"walCommit\" : 43\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3316\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3317\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3317\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.6181229773462784,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:25:22 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.85.4a3302ac-b8fc-41f3-8df9-cd51a6a6e4bc.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/85\n",
      "25/05/05 11:25:22 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:25:22.093Z\",\n",
      "  \"batchId\" : 85,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.5797788309636651,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 555,\n",
      "    \"commitOffsets\" : 26,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 633,\n",
      "    \"walCommit\" : 44\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3316\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3317\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3317\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.5797788309636651,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 85\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|B013|R081|02-00-00|161/YANKEE STAD|05/05/2025|11:25:21|REGULAR|5          |5         |2025-05-05 11:25:22.097|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:25:27 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/86 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.86.ce4cf006-402a-41ed-8444-c719c8bf36eb.tmp\n",
      "25/05/05 11:25:27 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/86 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.86.55570884-a823-46db-9eb9-c174e9529ede.tmp\n",
      "25/05/05 11:25:27 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/86 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.86.d8a1cc0b-bc81-4365-831b-741f3fd5a575.tmp\n",
      "25/05/05 11:25:27 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.86.ce4cf006-402a-41ed-8444-c719c8bf36eb.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/86\n",
      "25/05/05 11:25:27 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.86.d8a1cc0b-bc81-4365-831b-741f3fd5a575.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/86\n",
      "25/05/05 11:25:27 INFO MicroBatchExecution: Committed offsets for batch 86. Metadata OffsetSeqMetadata(0,1746458727540,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:25:27 INFO MicroBatchExecution: Committed offsets for batch 86. Metadata OffsetSeqMetadata(0,1746458727541,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:25:27 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.86.55570884-a823-46db-9eb9-c174e9529ede.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/86\n",
      "25/05/05 11:25:27 INFO MicroBatchExecution: Committed offsets for batch 86. Metadata OffsetSeqMetadata(0,1746458727540,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:25:27 INFO IncrementalExecution: Current batch timestamp = 1746458727541\n",
      "25/05/05 11:25:27 INFO IncrementalExecution: Current batch timestamp = 1746458727540\n",
      "25/05/05 11:25:27 INFO IncrementalExecution: Current batch timestamp = 1746458727540\n",
      "25/05/05 11:25:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:27 INFO IncrementalExecution: Current batch timestamp = 1746458727541\n",
      "25/05/05 11:25:27 INFO IncrementalExecution: Current batch timestamp = 1746458727540\n",
      "25/05/05 11:25:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:27 INFO IncrementalExecution: Current batch timestamp = 1746458727540\n",
      "25/05/05 11:25:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:27 INFO IncrementalExecution: Current batch timestamp = 1746458727540\n",
      "25/05/05 11:25:27 INFO IncrementalExecution: Current batch timestamp = 1746458727541\n",
      "25/05/05 11:25:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:27 INFO CodeGenerator: Code generated in 5.905125 ms\n",
      "25/05/05 11:25:27 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 86, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@18b6c037]. The input RDD has 1 partitions.\n",
      "25/05/05 11:25:27 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:27 INFO DAGScheduler: Got job 258 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:25:27 INFO DAGScheduler: Final stage: ResultStage 257 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:25:27 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:25:27 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:25:27 INFO DAGScheduler: Submitting ResultStage 257 (MapPartitionsRDD[1384] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:25:27 INFO MemoryStore: Block broadcast_343 stored as values in memory (estimated size 16.2 KiB, free 366.0 MiB)\n",
      "25/05/05 11:25:27 INFO MemoryStore: Block broadcast_343_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)\n",
      "25/05/05 11:25:27 INFO BlockManagerInfo: Added broadcast_343_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:27 INFO SparkContext: Created broadcast 343 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:25:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 257 (MapPartitionsRDD[1384] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:25:27 INFO TaskSchedulerImpl: Adding task set 257.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:25:27 INFO TaskSetManager: Starting task 0.0 in stage 257.0 (TID 257) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:25:27 INFO CodeGenerator: Code generated in 4.9095 ms\n",
      "25/05/05 11:25:27 INFO Executor: Running task 0.0 in stage 257.0 (TID 257)\n",
      "25/05/05 11:25:27 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 86, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:25:27 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:27 INFO DAGScheduler: Got job 259 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:25:27 INFO DAGScheduler: Final stage: ResultStage 258 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:25:27 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:25:27 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:25:27 INFO DAGScheduler: Submitting ResultStage 258 (MapPartitionsRDD[1387] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:25:27 INFO MemoryStore: Block broadcast_344 stored as values in memory (estimated size 15.2 KiB, free 365.9 MiB)\n",
      "25/05/05 11:25:27 INFO MemoryStore: Block broadcast_344_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 365.9 MiB)\n",
      "25/05/05 11:25:27 INFO BlockManagerInfo: Added broadcast_344_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:27 INFO SparkContext: Created broadcast 344 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:25:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 258 (MapPartitionsRDD[1387] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:25:27 INFO TaskSchedulerImpl: Adding task set 258.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:25:27 INFO TaskSetManager: Starting task 0.0 in stage 258.0 (TID 258) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:25:27 INFO Executor: Running task 0.0 in stage 258.0 (TID 258)\n",
      "25/05/05 11:25:27 INFO CodeGenerator: Code generated in 4.520958 ms\n",
      "25/05/05 11:25:27 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3317 untilOffset=3318, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=86 taskId=257 partitionId=0\n",
      "25/05/05 11:25:27 INFO CodeGenerator: Code generated in 3.734958 ms\n",
      "25/05/05 11:25:27 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3317 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:27 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3317 untilOffset=3318, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=86 taskId=258 partitionId=0\n",
      "25/05/05 11:25:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:27 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3317 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:27 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:27 INFO DAGScheduler: Got job 260 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:25:27 INFO DAGScheduler: Final stage: ResultStage 259 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:25:27 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:25:27 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:25:27 INFO DAGScheduler: Submitting ResultStage 259 (MapPartitionsRDD[1392] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:25:27 INFO MemoryStore: Block broadcast_345 stored as values in memory (estimated size 47.1 KiB, free 365.9 MiB)\n",
      "25/05/05 11:25:27 INFO MemoryStore: Block broadcast_345_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 365.9 MiB)\n",
      "25/05/05 11:25:27 INFO BlockManagerInfo: Added broadcast_345_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:27 INFO SparkContext: Created broadcast 345 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:25:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 259 (MapPartitionsRDD[1392] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:25:27 INFO TaskSchedulerImpl: Adding task set 259.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:25:27 INFO TaskSetManager: Starting task 0.0 in stage 259.0 (TID 259) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:25:27 INFO Executor: Running task 0.0 in stage 259.0 (TID 259)\n",
      "25/05/05 11:25:27 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3317 untilOffset=3318, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=86 taskId=259 partitionId=0\n",
      "25/05/05 11:25:27 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3317 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3318, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:28 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:25:28 INFO DataWritingSparkTask: Committed partition 0 (task 257, attempt 0, stage 257.0)\n",
      "25/05/05 11:25:28 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 502517250 nanos, during time span of 504327458 nanos.\n",
      "25/05/05 11:25:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3318, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:28 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:25:28 INFO DataWritingSparkTask: Committed partition 0 (task 258, attempt 0, stage 258.0)\n",
      "25/05/05 11:25:28 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 502862666 nanos, during time span of 503108125 nanos.\n",
      "25/05/05 11:25:28 INFO Executor: Finished task 0.0 in stage 257.0 (TID 257). 3516 bytes result sent to driver\n",
      "25/05/05 11:25:28 INFO Executor: Finished task 0.0 in stage 258.0 (TID 258). 2145 bytes result sent to driver\n",
      "25/05/05 11:25:28 INFO TaskSetManager: Finished task 0.0 in stage 258.0 (TID 258) in 511 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:25:28 INFO TaskSchedulerImpl: Removed TaskSet 258.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:25:28 INFO TaskSetManager: Finished task 0.0 in stage 257.0 (TID 257) in 516 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:25:28 INFO TaskSchedulerImpl: Removed TaskSet 257.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:25:28 INFO DAGScheduler: ResultStage 258 (start at NativeMethodAccessorImpl.java:0) finished in 0.513 s\n",
      "25/05/05 11:25:28 INFO DAGScheduler: Job 259 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:25:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 258: Stage finished\n",
      "25/05/05 11:25:28 INFO DAGScheduler: Job 259 finished: start at NativeMethodAccessorImpl.java:0, took 0.513957 s\n",
      "25/05/05 11:25:28 INFO DAGScheduler: ResultStage 257 (start at NativeMethodAccessorImpl.java:0) finished in 0.517 s\n",
      "25/05/05 11:25:28 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 86, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:25:28 INFO DAGScheduler: Job 258 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:25:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 257: Stage finished\n",
      "25/05/05 11:25:28 INFO DAGScheduler: Job 258 finished: start at NativeMethodAccessorImpl.java:0, took 0.518509 s\n",
      "25/05/05 11:25:28 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 86, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@18b6c037] is committing.\n",
      "25/05/05 11:25:28 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 86, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@18b6c037] committed.\n",
      "25/05/05 11:25:28 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/86 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.86.1803f0f4-b664-4e69-84d3-9d20426ec0a2.tmp\n",
      "25/05/05 11:25:28 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 86, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:25:28 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/86 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.86.b6f03e4d-6cba-4ced-bbf7-cfe6d0ba2a7c.tmp\n",
      "25/05/05 11:25:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3318, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:28 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:25:28 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:25:28 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:25:28 INFO connection: Opened connection [connectionId{localValue:171, serverValue:4541}] to localhost:27017\n",
      "25/05/05 11:25:28 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=326541}\n",
      "25/05/05 11:25:28 INFO connection: Opened connection [connectionId{localValue:172, serverValue:4542}] to localhost:27017\n",
      "25/05/05 11:25:28 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:25:28 INFO connection: Closed connection [connectionId{localValue:172, serverValue:4542}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:25:28 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 505457291 nanos, during time span of 510791292 nanos.\n",
      "25/05/05 11:25:28 INFO Executor: Finished task 0.0 in stage 259.0 (TID 259). 1645 bytes result sent to driver\n",
      "25/05/05 11:25:28 INFO TaskSetManager: Finished task 0.0 in stage 259.0 (TID 259) in 516 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:25:28 INFO TaskSchedulerImpl: Removed TaskSet 259.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:25:28 INFO DAGScheduler: ResultStage 259 (start at NativeMethodAccessorImpl.java:0) finished in 0.519 s\n",
      "25/05/05 11:25:28 INFO DAGScheduler: Job 260 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:25:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 259: Stage finished\n",
      "25/05/05 11:25:28 INFO DAGScheduler: Job 260 finished: start at NativeMethodAccessorImpl.java:0, took 0.520119 s\n",
      "25/05/05 11:25:28 INFO MemoryStore: Block broadcast_346 stored as values in memory (estimated size 248.0 B, free 365.9 MiB)\n",
      "25/05/05 11:25:28 INFO MemoryStore: Block broadcast_346_piece0 stored as bytes in memory (estimated size 413.0 B, free 365.9 MiB)\n",
      "25/05/05 11:25:28 INFO BlockManagerInfo: Added broadcast_346_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:25:28 INFO SparkContext: Created broadcast 346 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:28 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.86.1803f0f4-b664-4e69-84d3-9d20426ec0a2.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/86\n",
      "25/05/05 11:25:28 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:25:27.539Z\",\n",
      "  \"batchId\" : 86,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.607717041800643,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 531,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 622,\n",
      "    \"walCommit\" : 59\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3317\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3318\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3318\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.607717041800643,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:25:28 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/86 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.86.10071fd4-930d-4014-8305-a85c10736090.tmp\n",
      "25/05/05 11:25:28 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.86.b6f03e4d-6cba-4ced-bbf7-cfe6d0ba2a7c.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/86\n",
      "25/05/05 11:25:28 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:25:27.539Z\",\n",
      "  \"batchId\" : 86,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.5923566878980893,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 537,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 628,\n",
      "    \"walCommit\" : 57\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3317\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3318\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3318\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.5923566878980893,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:25:28 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.86.10071fd4-930d-4014-8305-a85c10736090.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/86\n",
      "25/05/05 11:25:28 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:25:27.539Z\",\n",
      "  \"batchId\" : 86,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.5527950310559007,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 553,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 644,\n",
      "    \"walCommit\" : 58\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3317\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3318\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3318\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.5527950310559007,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 86\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A001|R079|01-00-00|WORLD TRADE CTR|05/05/2025|11:25:26|REGULAR|4          |5         |2025-05-05 11:25:27.541|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:25:28 INFO BlockManagerInfo: Removed broadcast_335_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:28 INFO BlockManagerInfo: Removed broadcast_342_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:25:28 INFO BlockManagerInfo: Removed broadcast_345_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:28 INFO BlockManagerInfo: Removed broadcast_346_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:25:28 INFO BlockManagerInfo: Removed broadcast_340_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:28 INFO BlockManagerInfo: Removed broadcast_338_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:25:28 INFO BlockManagerInfo: Removed broadcast_336_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:28 INFO BlockManagerInfo: Removed broadcast_337_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:28 INFO BlockManagerInfo: Removed broadcast_341_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:25:28 INFO BlockManagerInfo: Removed broadcast_343_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:25:28 INFO BlockManagerInfo: Removed broadcast_344_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:25:28 INFO BlockManagerInfo: Removed broadcast_339_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:25:32 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/87 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.87.57e264fc-1d1d-47d7-94e6-5e9985c9656e.tmp\n",
      "25/05/05 11:25:32 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/87 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.87.ceb002ae-3a1a-4b44-913b-fce4a637f28c.tmp\n",
      "25/05/05 11:25:32 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/87 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.87.f5ace11f-2d58-40da-a6b9-df5e7cd7fe86.tmp\n",
      "25/05/05 11:25:32 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.87.ceb002ae-3a1a-4b44-913b-fce4a637f28c.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/87\n",
      "25/05/05 11:25:32 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.87.57e264fc-1d1d-47d7-94e6-5e9985c9656e.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/87\n",
      "25/05/05 11:25:32 INFO MicroBatchExecution: Committed offsets for batch 87. Metadata OffsetSeqMetadata(0,1746458732004,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:25:32 INFO MicroBatchExecution: Committed offsets for batch 87. Metadata OffsetSeqMetadata(0,1746458732009,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:25:32 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.87.f5ace11f-2d58-40da-a6b9-df5e7cd7fe86.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/87\n",
      "25/05/05 11:25:32 INFO MicroBatchExecution: Committed offsets for batch 87. Metadata OffsetSeqMetadata(0,1746458732009,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:25:32 INFO IncrementalExecution: Current batch timestamp = 1746458732009\n",
      "25/05/05 11:25:32 INFO IncrementalExecution: Current batch timestamp = 1746458732004\n",
      "25/05/05 11:25:32 INFO IncrementalExecution: Current batch timestamp = 1746458732009\n",
      "25/05/05 11:25:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:32 INFO IncrementalExecution: Current batch timestamp = 1746458732009\n",
      "25/05/05 11:25:32 INFO IncrementalExecution: Current batch timestamp = 1746458732004\n",
      "25/05/05 11:25:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:32 INFO IncrementalExecution: Current batch timestamp = 1746458732009\n",
      "25/05/05 11:25:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:32 INFO IncrementalExecution: Current batch timestamp = 1746458732009\n",
      "25/05/05 11:25:32 INFO IncrementalExecution: Current batch timestamp = 1746458732004\n",
      "25/05/05 11:25:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:32 INFO CodeGenerator: Code generated in 3.804958 ms\n",
      "25/05/05 11:25:32 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 87, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:25:32 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:32 INFO CodeGenerator: Code generated in 3.174583 ms\n",
      "25/05/05 11:25:32 INFO DAGScheduler: Got job 261 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:25:32 INFO DAGScheduler: Final stage: ResultStage 260 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:25:32 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:25:32 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:25:32 INFO DAGScheduler: Submitting ResultStage 260 (MapPartitionsRDD[1400] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:25:32 INFO MemoryStore: Block broadcast_347 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:25:32 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 87, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@405704eb]. The input RDD has 1 partitions.\n",
      "25/05/05 11:25:32 INFO MemoryStore: Block broadcast_347_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:25:32 INFO BlockManagerInfo: Added broadcast_347_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:25:32 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:32 INFO SparkContext: Created broadcast 347 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:25:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 260 (MapPartitionsRDD[1400] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:25:32 INFO TaskSchedulerImpl: Adding task set 260.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:25:32 INFO DAGScheduler: Got job 262 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:25:32 INFO DAGScheduler: Final stage: ResultStage 261 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:25:32 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:25:32 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:25:32 INFO DAGScheduler: Submitting ResultStage 261 (MapPartitionsRDD[1403] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:25:32 INFO TaskSetManager: Starting task 0.0 in stage 260.0 (TID 260) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:25:32 INFO Executor: Running task 0.0 in stage 260.0 (TID 260)\n",
      "25/05/05 11:25:32 INFO MemoryStore: Block broadcast_348 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:25:32 INFO MemoryStore: Block broadcast_348_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:25:32 INFO BlockManagerInfo: Added broadcast_348_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:25:32 INFO SparkContext: Created broadcast 348 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:25:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 261 (MapPartitionsRDD[1403] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:25:32 INFO TaskSchedulerImpl: Adding task set 261.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:25:32 INFO TaskSetManager: Starting task 0.0 in stage 261.0 (TID 261) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:25:32 INFO Executor: Running task 0.0 in stage 261.0 (TID 261)\n",
      "25/05/05 11:25:32 INFO CodeGenerator: Code generated in 3.748667 ms\n",
      "25/05/05 11:25:32 INFO CodeGenerator: Code generated in 3.135042 ms\n",
      "25/05/05 11:25:32 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3318 untilOffset=3319, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=87 taskId=260 partitionId=0\n",
      "25/05/05 11:25:32 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3318 untilOffset=3319, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=87 taskId=261 partitionId=0\n",
      "25/05/05 11:25:32 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3318 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:32 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3318 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:32 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:32 INFO DAGScheduler: Got job 263 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:25:32 INFO DAGScheduler: Final stage: ResultStage 262 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:25:32 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:25:32 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:25:32 INFO DAGScheduler: Submitting ResultStage 262 (MapPartitionsRDD[1408] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:25:32 INFO MemoryStore: Block broadcast_349 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:25:32 INFO MemoryStore: Block broadcast_349_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.1 MiB)\n",
      "25/05/05 11:25:32 INFO BlockManagerInfo: Added broadcast_349_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:32 INFO SparkContext: Created broadcast 349 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:25:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 262 (MapPartitionsRDD[1408] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:25:32 INFO TaskSchedulerImpl: Adding task set 262.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:25:32 INFO TaskSetManager: Starting task 0.0 in stage 262.0 (TID 262) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:25:32 INFO Executor: Running task 0.0 in stage 262.0 (TID 262)\n",
      "25/05/05 11:25:32 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3318 untilOffset=3319, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=87 taskId=262 partitionId=0\n",
      "25/05/05 11:25:32 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3318 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3319, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3319, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:32 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:25:32 INFO DataWritingSparkTask: Committed partition 0 (task 260, attempt 0, stage 260.0)\n",
      "25/05/05 11:25:32 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 505004167 nanos, during time span of 505482792 nanos.\n",
      "25/05/05 11:25:32 INFO Executor: Finished task 0.0 in stage 260.0 (TID 260). 2145 bytes result sent to driver\n",
      "25/05/05 11:25:32 INFO TaskSetManager: Finished task 0.0 in stage 260.0 (TID 260) in 513 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:25:32 INFO TaskSchedulerImpl: Removed TaskSet 260.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:25:32 INFO DAGScheduler: ResultStage 260 (start at NativeMethodAccessorImpl.java:0) finished in 0.515 s\n",
      "25/05/05 11:25:32 INFO DAGScheduler: Job 261 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:25:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 260: Stage finished\n",
      "25/05/05 11:25:32 INFO DAGScheduler: Job 261 finished: start at NativeMethodAccessorImpl.java:0, took 0.515961 s\n",
      "25/05/05 11:25:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 87, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:25:32 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:25:32 INFO DataWritingSparkTask: Committed partition 0 (task 261, attempt 0, stage 261.0)\n",
      "25/05/05 11:25:32 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 504000458 nanos, during time span of 505633625 nanos.\n",
      "25/05/05 11:25:32 INFO Executor: Finished task 0.0 in stage 261.0 (TID 261). 3515 bytes result sent to driver\n",
      "25/05/05 11:25:32 INFO TaskSetManager: Finished task 0.0 in stage 261.0 (TID 261) in 514 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:25:32 INFO TaskSchedulerImpl: Removed TaskSet 261.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:25:32 INFO DAGScheduler: ResultStage 261 (start at NativeMethodAccessorImpl.java:0) finished in 0.515 s\n",
      "25/05/05 11:25:32 INFO DAGScheduler: Job 262 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:25:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 261: Stage finished\n",
      "25/05/05 11:25:32 INFO DAGScheduler: Job 262 finished: start at NativeMethodAccessorImpl.java:0, took 0.515629 s\n",
      "25/05/05 11:25:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 87, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@405704eb] is committing.\n",
      "25/05/05 11:25:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 87, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@405704eb] committed.\n",
      "25/05/05 11:25:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 87, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:25:32 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/87 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.87.e081fe2d-3659-4345-bf26-7b31884b6017.tmp\n",
      "25/05/05 11:25:32 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/87 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.87.e3f1f1dc-388a-4de7-90d9-f5cf62c7ff8d.tmp\n",
      "25/05/05 11:25:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3319, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:32 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:25:32 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:25:32 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:25:32 INFO connection: Opened connection [connectionId{localValue:173, serverValue:4543}] to localhost:27017\n",
      "25/05/05 11:25:32 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1189708}\n",
      "25/05/05 11:25:32 INFO connection: Opened connection [connectionId{localValue:174, serverValue:4544}] to localhost:27017\n",
      "25/05/05 11:25:32 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:25:32 INFO connection: Closed connection [connectionId{localValue:174, serverValue:4544}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:25:32 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 508253875 nanos, during time span of 516356042 nanos.\n",
      "25/05/05 11:25:32 INFO Executor: Finished task 0.0 in stage 262.0 (TID 262). 1645 bytes result sent to driver\n",
      "25/05/05 11:25:32 INFO TaskSetManager: Finished task 0.0 in stage 262.0 (TID 262) in 522 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:25:32 INFO TaskSchedulerImpl: Removed TaskSet 262.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:25:32 INFO DAGScheduler: ResultStage 262 (start at NativeMethodAccessorImpl.java:0) finished in 0.525 s\n",
      "25/05/05 11:25:32 INFO DAGScheduler: Job 263 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:25:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 262: Stage finished\n",
      "25/05/05 11:25:32 INFO DAGScheduler: Job 263 finished: start at NativeMethodAccessorImpl.java:0, took 0.526435 s\n",
      "25/05/05 11:25:32 INFO MemoryStore: Block broadcast_350 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:25:32 INFO MemoryStore: Block broadcast_350_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:25:32 INFO BlockManagerInfo: Added broadcast_350_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:25:32 INFO SparkContext: Created broadcast 350 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:32 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.87.e081fe2d-3659-4345-bf26-7b31884b6017.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/87\n",
      "25/05/05 11:25:32 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:25:32.003Z\",\n",
      "  \"batchId\" : 87,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.594896331738437,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 528,\n",
      "    \"commitOffsets\" : 35,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 627,\n",
      "    \"walCommit\" : 59\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3318\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3319\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3319\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.594896331738437,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:25:32 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/87 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.87.6ced5cf7-41f6-4b21-8431-32c11468dc71.tmp\n",
      "25/05/05 11:25:32 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.87.e3f1f1dc-388a-4de7-90d9-f5cf62c7ff8d.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/87\n",
      "25/05/05 11:25:32 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:25:32.005Z\",\n",
      "  \"batchId\" : 87,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "  \"processedRowsPerSecond\" : 1.5847860538827259,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 534,\n",
      "    \"commitOffsets\" : 35,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 4,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 631,\n",
      "    \"walCommit\" : 54\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3318\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3319\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3319\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "    \"processedRowsPerSecond\" : 1.5847860538827259,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:25:32 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.87.6ced5cf7-41f6-4b21-8431-32c11468dc71.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/87\n",
      "25/05/05 11:25:32 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:25:32.005Z\",\n",
      "  \"batchId\" : 87,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.5384615384615383,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 556,\n",
      "    \"commitOffsets\" : 32,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 4,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 650,\n",
      "    \"walCommit\" : 55\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3318\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3319\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3319\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.5384615384615383,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 87\n",
      "-------------------------------------------\n",
      "+----+----+--------+--------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION       |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+--------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A002|R051|02-00-00|34 ST-PENN STA|05/05/2025|11:25:30|REGULAR|6          |7         |2025-05-05 11:25:32.009|\n",
      "+----+----+--------+--------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:25:36 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/88 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.88.e98ce035-aedf-4eb1-a8f4-9d690429c4af.tmp\n",
      "25/05/05 11:25:36 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/88 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.88.26df6c92-1a92-4da4-9d23-2bc75b0f9982.tmp\n",
      "25/05/05 11:25:36 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/88 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.88.458ca55d-b4dc-4548-804c-c9b22c577e56.tmp\n",
      "25/05/05 11:25:36 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.88.26df6c92-1a92-4da4-9d23-2bc75b0f9982.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/88\n",
      "25/05/05 11:25:36 INFO MicroBatchExecution: Committed offsets for batch 88. Metadata OffsetSeqMetadata(0,1746458736504,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:25:36 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.88.e98ce035-aedf-4eb1-a8f4-9d690429c4af.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/88\n",
      "25/05/05 11:25:36 INFO MicroBatchExecution: Committed offsets for batch 88. Metadata OffsetSeqMetadata(0,1746458736506,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:25:36 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.88.458ca55d-b4dc-4548-804c-c9b22c577e56.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/88\n",
      "25/05/05 11:25:36 INFO MicroBatchExecution: Committed offsets for batch 88. Metadata OffsetSeqMetadata(0,1746458736508,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:25:36 INFO IncrementalExecution: Current batch timestamp = 1746458736504\n",
      "25/05/05 11:25:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:36 INFO IncrementalExecution: Current batch timestamp = 1746458736506\n",
      "25/05/05 11:25:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:36 INFO IncrementalExecution: Current batch timestamp = 1746458736508\n",
      "25/05/05 11:25:36 INFO IncrementalExecution: Current batch timestamp = 1746458736504\n",
      "25/05/05 11:25:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:36 INFO IncrementalExecution: Current batch timestamp = 1746458736506\n",
      "25/05/05 11:25:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:36 INFO IncrementalExecution: Current batch timestamp = 1746458736508\n",
      "25/05/05 11:25:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:36 INFO IncrementalExecution: Current batch timestamp = 1746458736504\n",
      "25/05/05 11:25:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:36 INFO IncrementalExecution: Current batch timestamp = 1746458736508\n",
      "25/05/05 11:25:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:36 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:36 INFO CodeGenerator: Code generated in 3.81775 ms\n",
      "25/05/05 11:25:36 INFO CodeGenerator: Code generated in 2.90025 ms\n",
      "25/05/05 11:25:36 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 88, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:25:36 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:36 INFO CodeGenerator: Code generated in 3.556833 ms\n",
      "25/05/05 11:25:36 INFO DAGScheduler: Got job 264 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:25:36 INFO DAGScheduler: Final stage: ResultStage 263 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:25:36 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:25:36 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:25:36 INFO DAGScheduler: Submitting ResultStage 263 (MapPartitionsRDD[1416] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:25:36 INFO MemoryStore: Block broadcast_351 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:25:36 INFO MemoryStore: Block broadcast_351_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:25:36 INFO BlockManagerInfo: Added broadcast_351_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:36 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 88, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@2a3829dc]. The input RDD has 1 partitions.\n",
      "25/05/05 11:25:36 INFO SparkContext: Created broadcast 351 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:25:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 263 (MapPartitionsRDD[1416] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:25:36 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:36 INFO TaskSchedulerImpl: Adding task set 263.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:25:36 INFO DAGScheduler: Got job 265 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:25:36 INFO DAGScheduler: Final stage: ResultStage 264 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:25:36 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:25:36 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:25:36 INFO DAGScheduler: Submitting ResultStage 264 (MapPartitionsRDD[1419] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:25:36 INFO TaskSetManager: Starting task 0.0 in stage 263.0 (TID 263) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:25:36 INFO Executor: Running task 0.0 in stage 263.0 (TID 263)\n",
      "25/05/05 11:25:36 INFO MemoryStore: Block broadcast_352 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:25:36 INFO MemoryStore: Block broadcast_352_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)\n",
      "25/05/05 11:25:36 INFO BlockManagerInfo: Added broadcast_352_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:36 INFO SparkContext: Created broadcast 352 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:25:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 264 (MapPartitionsRDD[1419] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:25:36 INFO TaskSchedulerImpl: Adding task set 264.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:25:36 INFO TaskSetManager: Starting task 0.0 in stage 264.0 (TID 264) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:25:36 INFO Executor: Running task 0.0 in stage 264.0 (TID 264)\n",
      "25/05/05 11:25:36 INFO CodeGenerator: Code generated in 4.143083 ms\n",
      "25/05/05 11:25:36 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3319 untilOffset=3320, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=88 taskId=263 partitionId=0\n",
      "25/05/05 11:25:36 INFO CodeGenerator: Code generated in 3.70525 ms\n",
      "25/05/05 11:25:36 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3319 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:36 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3319 untilOffset=3320, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=88 taskId=264 partitionId=0\n",
      "25/05/05 11:25:36 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:36 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3319 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:36 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:36 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:36 INFO DAGScheduler: Got job 266 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:25:36 INFO DAGScheduler: Final stage: ResultStage 265 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:25:36 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:25:36 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:25:36 INFO DAGScheduler: Submitting ResultStage 265 (MapPartitionsRDD[1424] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:25:36 INFO MemoryStore: Block broadcast_353 stored as values in memory (estimated size 47.1 KiB, free 366.0 MiB)\n",
      "25/05/05 11:25:36 INFO MemoryStore: Block broadcast_353_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.0 MiB)\n",
      "25/05/05 11:25:36 INFO BlockManagerInfo: Added broadcast_353_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:36 INFO SparkContext: Created broadcast 353 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:25:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 265 (MapPartitionsRDD[1424] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:25:36 INFO TaskSchedulerImpl: Adding task set 265.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:25:36 INFO TaskSetManager: Starting task 0.0 in stage 265.0 (TID 265) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:25:36 INFO Executor: Running task 0.0 in stage 265.0 (TID 265)\n",
      "25/05/05 11:25:36 INFO CodeGenerator: Code generated in 3.516167 ms\n",
      "25/05/05 11:25:36 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3319 untilOffset=3320, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=88 taskId=265 partitionId=0\n",
      "25/05/05 11:25:36 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3319 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:36 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:37 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:37 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:37 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:37 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:37 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3320, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:37 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3320, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:37 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:25:37 INFO DataWritingSparkTask: Committed partition 0 (task 263, attempt 0, stage 263.0)\n",
      "25/05/05 11:25:37 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 504100625 nanos, during time span of 504554958 nanos.\n",
      "25/05/05 11:25:37 INFO Executor: Finished task 0.0 in stage 263.0 (TID 263). 2145 bytes result sent to driver\n",
      "25/05/05 11:25:37 INFO TaskSetManager: Finished task 0.0 in stage 263.0 (TID 263) in 513 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:25:37 INFO TaskSchedulerImpl: Removed TaskSet 263.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:25:37 INFO DAGScheduler: ResultStage 263 (start at NativeMethodAccessorImpl.java:0) finished in 0.515 s\n",
      "25/05/05 11:25:37 INFO DAGScheduler: Job 264 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:25:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 263: Stage finished\n",
      "25/05/05 11:25:37 INFO DAGScheduler: Job 264 finished: start at NativeMethodAccessorImpl.java:0, took 0.515559 s\n",
      "25/05/05 11:25:37 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 88, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:25:37 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:25:37 INFO DataWritingSparkTask: Committed partition 0 (task 264, attempt 0, stage 264.0)\n",
      "25/05/05 11:25:37 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 502084792 nanos, during time span of 503927208 nanos.\n",
      "25/05/05 11:25:37 INFO Executor: Finished task 0.0 in stage 264.0 (TID 264). 3514 bytes result sent to driver\n",
      "25/05/05 11:25:37 INFO TaskSetManager: Finished task 0.0 in stage 264.0 (TID 264) in 513 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:25:37 INFO TaskSchedulerImpl: Removed TaskSet 264.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:25:37 INFO DAGScheduler: ResultStage 264 (start at NativeMethodAccessorImpl.java:0) finished in 0.514 s\n",
      "25/05/05 11:25:37 INFO DAGScheduler: Job 265 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:25:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 264: Stage finished\n",
      "25/05/05 11:25:37 INFO DAGScheduler: Job 265 finished: start at NativeMethodAccessorImpl.java:0, took 0.515015 s\n",
      "25/05/05 11:25:37 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 88, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@2a3829dc] is committing.\n",
      "25/05/05 11:25:37 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 88, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@2a3829dc] committed.\n",
      "25/05/05 11:25:37 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 88, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:25:37 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/88 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.88.2169c04e-ee33-434c-97ad-a2dc067edd4d.tmp\n",
      "25/05/05 11:25:37 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/88 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.88.c1c83191-5c65-4459-97f5-809db4c28781.tmp\n",
      "25/05/05 11:25:37 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:37 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:37 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3320, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:37 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:25:37 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:25:37 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:25:37 INFO connection: Opened connection [connectionId{localValue:175, serverValue:4545}] to localhost:27017\n",
      "25/05/05 11:25:37 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=243250}\n",
      "25/05/05 11:25:37 INFO connection: Opened connection [connectionId{localValue:176, serverValue:4546}] to localhost:27017\n",
      "25/05/05 11:25:37 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.88.2169c04e-ee33-434c-97ad-a2dc067edd4d.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/88\n",
      "25/05/05 11:25:37 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:25:37 INFO connection: Closed connection [connectionId{localValue:176, serverValue:4546}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:25:37 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 507256208 nanos, during time span of 511512084 nanos.\n",
      "25/05/05 11:25:37 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:25:36.503Z\",\n",
      "  \"batchId\" : 88,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.6366612111292962,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 527,\n",
      "    \"commitOffsets\" : 25,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 5,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 611,\n",
      "    \"walCommit\" : 51\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3319\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3320\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3320\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.6366612111292962,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:25:37 INFO Executor: Finished task 0.0 in stage 265.0 (TID 265). 1645 bytes result sent to driver\n",
      "25/05/05 11:25:37 INFO TaskSetManager: Finished task 0.0 in stage 265.0 (TID 265) in 521 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:25:37 INFO TaskSchedulerImpl: Removed TaskSet 265.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:25:37 INFO DAGScheduler: ResultStage 265 (start at NativeMethodAccessorImpl.java:0) finished in 0.525 s\n",
      "25/05/05 11:25:37 INFO DAGScheduler: Job 266 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:25:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 265: Stage finished\n",
      "25/05/05 11:25:37 INFO DAGScheduler: Job 266 finished: start at NativeMethodAccessorImpl.java:0, took 0.525759 s\n",
      "25/05/05 11:25:37 INFO MemoryStore: Block broadcast_354 stored as values in memory (estimated size 248.0 B, free 366.0 MiB)\n",
      "25/05/05 11:25:37 INFO MemoryStore: Block broadcast_354_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.0 MiB)\n",
      "25/05/05 11:25:37 INFO BlockManagerInfo: Added broadcast_354_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:25:37 INFO SparkContext: Created broadcast 354 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:37 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.88.c1c83191-5c65-4459-97f5-809db4c28781.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/88\n",
      "25/05/05 11:25:37 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:25:36.502Z\",\n",
      "  \"batchId\" : 88,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.6181229773462784,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 533,\n",
      "    \"commitOffsets\" : 26,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 618,\n",
      "    \"walCommit\" : 52\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3319\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3320\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3320\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.6181229773462784,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:25:37 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/88 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.88.ab1bd704-0365-4ff3-a5da-41178f97478a.tmp\n",
      "25/05/05 11:25:37 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.88.ab1bd704-0365-4ff3-a5da-41178f97478a.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/88\n",
      "25/05/05 11:25:37 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:25:36.502Z\",\n",
      "  \"batchId\" : 88,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.5625,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 554,\n",
      "    \"commitOffsets\" : 26,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 4,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 640,\n",
      "    \"walCommit\" : 51\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3319\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3320\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3320\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.5625,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 88\n",
      "-------------------------------------------\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION      |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R016|R032|00-00-01|TIME SQ-42 ST|05/05/2025|11:25:35|REGULAR|7          |4         |2025-05-05 11:25:36.504|\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:25:40 INFO BlockManagerInfo: Removed broadcast_354_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:25:40 INFO BlockManagerInfo: Removed broadcast_351_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:40 INFO BlockManagerInfo: Removed broadcast_353_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:40 INFO BlockManagerInfo: Removed broadcast_347_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:40 INFO BlockManagerInfo: Removed broadcast_349_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:25:40 INFO BlockManagerInfo: Removed broadcast_350_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:25:40 INFO BlockManagerInfo: Removed broadcast_348_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:25:40 INFO BlockManagerInfo: Removed broadcast_352_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:25:41 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/89 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.89.c9712c88-e7c6-441a-b112-535afdacbbc8.tmp\n",
      "25/05/05 11:25:41 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/89 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.89.d56b8cee-304d-4523-8993-f8a63df28dc9.tmp\n",
      "25/05/05 11:25:41 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/89 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.89.760e8449-67e8-4d88-a082-dfb5a429d5a1.tmp\n",
      "25/05/05 11:25:41 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.89.c9712c88-e7c6-441a-b112-535afdacbbc8.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/89\n",
      "25/05/05 11:25:41 INFO MicroBatchExecution: Committed offsets for batch 89. Metadata OffsetSeqMetadata(0,1746458740994,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:25:41 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.89.d56b8cee-304d-4523-8993-f8a63df28dc9.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/89\n",
      "25/05/05 11:25:41 INFO MicroBatchExecution: Committed offsets for batch 89. Metadata OffsetSeqMetadata(0,1746458740993,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:25:41 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.89.760e8449-67e8-4d88-a082-dfb5a429d5a1.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/89\n",
      "25/05/05 11:25:41 INFO MicroBatchExecution: Committed offsets for batch 89. Metadata OffsetSeqMetadata(0,1746458740993,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:25:41 INFO IncrementalExecution: Current batch timestamp = 1746458740994\n",
      "25/05/05 11:25:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:41 INFO IncrementalExecution: Current batch timestamp = 1746458740993\n",
      "25/05/05 11:25:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:41 INFO IncrementalExecution: Current batch timestamp = 1746458740993\n",
      "25/05/05 11:25:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:41 INFO IncrementalExecution: Current batch timestamp = 1746458740994\n",
      "25/05/05 11:25:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:41 INFO IncrementalExecution: Current batch timestamp = 1746458740993\n",
      "25/05/05 11:25:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:41 INFO IncrementalExecution: Current batch timestamp = 1746458740993\n",
      "25/05/05 11:25:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:41 INFO IncrementalExecution: Current batch timestamp = 1746458740993\n",
      "25/05/05 11:25:41 INFO IncrementalExecution: Current batch timestamp = 1746458740993\n",
      "25/05/05 11:25:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:41 INFO CodeGenerator: Code generated in 4.028542 ms\n",
      "25/05/05 11:25:41 INFO CodeGenerator: Code generated in 3.37075 ms\n",
      "25/05/05 11:25:41 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 89, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@7618cc0a]. The input RDD has 1 partitions.\n",
      "25/05/05 11:25:41 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 89, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:25:41 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:41 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:41 INFO DAGScheduler: Got job 267 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:25:41 INFO DAGScheduler: Final stage: ResultStage 266 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:25:41 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:25:41 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:25:41 INFO DAGScheduler: Submitting ResultStage 266 (MapPartitionsRDD[1434] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:25:41 INFO MemoryStore: Block broadcast_355 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:25:41 INFO MemoryStore: Block broadcast_355_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:25:41 INFO BlockManagerInfo: Added broadcast_355_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:25:41 INFO SparkContext: Created broadcast 355 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:25:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 266 (MapPartitionsRDD[1434] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:25:41 INFO TaskSchedulerImpl: Adding task set 266.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:25:41 INFO DAGScheduler: Got job 268 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:25:41 INFO DAGScheduler: Final stage: ResultStage 267 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:25:41 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:25:41 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:25:41 INFO DAGScheduler: Submitting ResultStage 267 (MapPartitionsRDD[1435] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:25:41 INFO TaskSetManager: Starting task 0.0 in stage 266.0 (TID 266) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:25:41 INFO Executor: Running task 0.0 in stage 266.0 (TID 266)\n",
      "25/05/05 11:25:41 INFO MemoryStore: Block broadcast_356 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:25:41 INFO MemoryStore: Block broadcast_356_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:25:41 INFO BlockManagerInfo: Added broadcast_356_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:25:41 INFO SparkContext: Created broadcast 356 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:25:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 267 (MapPartitionsRDD[1435] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:25:41 INFO TaskSchedulerImpl: Adding task set 267.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:25:41 INFO TaskSetManager: Starting task 0.0 in stage 267.0 (TID 267) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:25:41 INFO Executor: Running task 0.0 in stage 267.0 (TID 267)\n",
      "25/05/05 11:25:41 INFO CodeGenerator: Code generated in 3.5875 ms\n",
      "25/05/05 11:25:41 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3320 untilOffset=3321, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=89 taskId=267 partitionId=0\n",
      "25/05/05 11:25:41 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3320 untilOffset=3321, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=89 taskId=266 partitionId=0\n",
      "25/05/05 11:25:41 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3320 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:41 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:41 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3320 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:41 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:41 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:41 INFO DAGScheduler: Got job 269 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:25:41 INFO DAGScheduler: Final stage: ResultStage 268 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:25:41 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:25:41 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:25:41 INFO DAGScheduler: Submitting ResultStage 268 (MapPartitionsRDD[1440] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:25:41 INFO MemoryStore: Block broadcast_357 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:25:41 INFO MemoryStore: Block broadcast_357_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.1 MiB)\n",
      "25/05/05 11:25:41 INFO BlockManagerInfo: Added broadcast_357_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:41 INFO SparkContext: Created broadcast 357 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:25:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 268 (MapPartitionsRDD[1440] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:25:41 INFO TaskSchedulerImpl: Adding task set 268.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:25:41 INFO TaskSetManager: Starting task 0.0 in stage 268.0 (TID 268) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:25:41 INFO Executor: Running task 0.0 in stage 268.0 (TID 268)\n",
      "25/05/05 11:25:41 INFO CodeGenerator: Code generated in 3.938292 ms\n",
      "25/05/05 11:25:41 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3320 untilOffset=3321, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=89 taskId=268 partitionId=0\n",
      "25/05/05 11:25:41 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3320 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:41 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:41 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:41 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:41 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:41 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:41 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3321, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:41 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3321, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:41 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:25:41 INFO DataWritingSparkTask: Committed partition 0 (task 267, attempt 0, stage 267.0)\n",
      "25/05/05 11:25:41 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 506051459 nanos, during time span of 506250750 nanos.\n",
      "25/05/05 11:25:41 INFO Executor: Finished task 0.0 in stage 267.0 (TID 267). 2145 bytes result sent to driver\n",
      "25/05/05 11:25:41 INFO TaskSetManager: Finished task 0.0 in stage 267.0 (TID 267) in 513 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:25:41 INFO TaskSchedulerImpl: Removed TaskSet 267.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:25:41 INFO DAGScheduler: ResultStage 267 (start at NativeMethodAccessorImpl.java:0) finished in 0.516 s\n",
      "25/05/05 11:25:41 INFO DAGScheduler: Job 268 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:25:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 267: Stage finished\n",
      "25/05/05 11:25:41 INFO DAGScheduler: Job 268 finished: start at NativeMethodAccessorImpl.java:0, took 0.517997 s\n",
      "25/05/05 11:25:41 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 89, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:25:41 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:25:41 INFO DataWritingSparkTask: Committed partition 0 (task 266, attempt 0, stage 266.0)\n",
      "25/05/05 11:25:41 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 504922792 nanos, during time span of 506658166 nanos.\n",
      "25/05/05 11:25:41 INFO Executor: Finished task 0.0 in stage 266.0 (TID 266). 3554 bytes result sent to driver\n",
      "25/05/05 11:25:41 INFO TaskSetManager: Finished task 0.0 in stage 266.0 (TID 266) in 517 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:25:41 INFO TaskSchedulerImpl: Removed TaskSet 266.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:25:41 INFO DAGScheduler: ResultStage 266 (start at NativeMethodAccessorImpl.java:0) finished in 0.518 s\n",
      "25/05/05 11:25:41 INFO DAGScheduler: Job 267 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:25:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 266: Stage finished\n",
      "25/05/05 11:25:41 INFO DAGScheduler: Job 267 finished: start at NativeMethodAccessorImpl.java:0, took 0.519136 s\n",
      "25/05/05 11:25:41 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 89, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@7618cc0a] is committing.\n",
      "25/05/05 11:25:41 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 89, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@7618cc0a] committed.\n",
      "25/05/05 11:25:41 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 89, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:25:41 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/89 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.89.53208be8-86ad-42f0-8c7f-4ebea5d14fe4.tmp\n",
      "25/05/05 11:25:41 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/89 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.89.48be6f81-d105-403a-8937-9ac7b3d066e7.tmp\n",
      "25/05/05 11:25:41 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:41 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:41 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3321, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:41 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:25:41 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:25:41 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:25:41 INFO connection: Opened connection [connectionId{localValue:177, serverValue:4547}] to localhost:27017\n",
      "25/05/05 11:25:41 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=223625}\n",
      "25/05/05 11:25:41 INFO connection: Opened connection [connectionId{localValue:178, serverValue:4548}] to localhost:27017\n",
      "25/05/05 11:25:41 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:25:41 INFO connection: Closed connection [connectionId{localValue:178, serverValue:4548}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:25:41 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 502186417 nanos, during time span of 509356709 nanos.\n",
      "25/05/05 11:25:41 INFO Executor: Finished task 0.0 in stage 268.0 (TID 268). 1645 bytes result sent to driver\n",
      "25/05/05 11:25:41 INFO TaskSetManager: Finished task 0.0 in stage 268.0 (TID 268) in 519 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:25:41 INFO TaskSchedulerImpl: Removed TaskSet 268.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:25:41 INFO DAGScheduler: ResultStage 268 (start at NativeMethodAccessorImpl.java:0) finished in 0.523 s\n",
      "25/05/05 11:25:41 INFO DAGScheduler: Job 269 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:25:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 268: Stage finished\n",
      "25/05/05 11:25:41 INFO DAGScheduler: Job 269 finished: start at NativeMethodAccessorImpl.java:0, took 0.523993 s\n",
      "25/05/05 11:25:41 INFO MemoryStore: Block broadcast_358 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:25:41 INFO MemoryStore: Block broadcast_358_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:25:41 INFO BlockManagerInfo: Added broadcast_358_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:25:41 INFO SparkContext: Created broadcast 358 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:41 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.89.53208be8-86ad-42f0-8c7f-4ebea5d14fe4.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/89\n",
      "25/05/05 11:25:41 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:25:40.990Z\",\n",
      "  \"batchId\" : 89,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.594896331738437,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 531,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 627,\n",
      "    \"walCommit\" : 61\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3320\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3321\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3321\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.594896331738437,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:25:41 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/89 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.89.7cc1d9d1-135f-4013-962b-c6f482a35ffb.tmp\n",
      "25/05/05 11:25:41 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.89.48be6f81-d105-403a-8937-9ac7b3d066e7.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/89\n",
      "25/05/05 11:25:41 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:25:40.990Z\",\n",
      "  \"batchId\" : 89,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.5847860538827259,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 537,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 631,\n",
      "    \"walCommit\" : 61\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3320\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3321\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3321\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.5847860538827259,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:25:41 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.89.7cc1d9d1-135f-4013-962b-c6f482a35ffb.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/89\n",
      "25/05/05 11:25:41 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:25:40.991Z\",\n",
      "  \"batchId\" : 89,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "  \"processedRowsPerSecond\" : 1.5503875968992247,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 554,\n",
      "    \"commitOffsets\" : 25,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 645,\n",
      "    \"walCommit\" : 58\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3320\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3321\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3321\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "    \"processedRowsPerSecond\" : 1.5503875968992247,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 89\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|B013|R081|02-00-00|161/YANKEE STAD|05/05/2025|11:25:39|REGULAR|10         |10        |2025-05-05 11:25:40.993|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:25:45 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/90 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.90.b6a76afa-28a6-4119-bfe2-e4f9efb35dfc.tmp\n",
      "25/05/05 11:25:45 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/90 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.90.ba46ef43-8b72-4d37-8505-e44cc42fdad2.tmp\n",
      "25/05/05 11:25:45 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/90 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.90.cc92bd30-694c-4c6d-9d6d-2e6e6962150c.tmp\n",
      "25/05/05 11:25:45 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.90.ba46ef43-8b72-4d37-8505-e44cc42fdad2.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/90\n",
      "25/05/05 11:25:45 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.90.b6a76afa-28a6-4119-bfe2-e4f9efb35dfc.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/90\n",
      "25/05/05 11:25:45 INFO MicroBatchExecution: Committed offsets for batch 90. Metadata OffsetSeqMetadata(0,1746458745458,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:25:45 INFO MicroBatchExecution: Committed offsets for batch 90. Metadata OffsetSeqMetadata(0,1746458745459,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:25:45 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.90.cc92bd30-694c-4c6d-9d6d-2e6e6962150c.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/90\n",
      "25/05/05 11:25:45 INFO MicroBatchExecution: Committed offsets for batch 90. Metadata OffsetSeqMetadata(0,1746458745459,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:25:45 INFO IncrementalExecution: Current batch timestamp = 1746458745459\n",
      "25/05/05 11:25:45 INFO IncrementalExecution: Current batch timestamp = 1746458745458\n",
      "25/05/05 11:25:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:45 INFO IncrementalExecution: Current batch timestamp = 1746458745459\n",
      "25/05/05 11:25:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:45 INFO IncrementalExecution: Current batch timestamp = 1746458745458\n",
      "25/05/05 11:25:45 INFO IncrementalExecution: Current batch timestamp = 1746458745459\n",
      "25/05/05 11:25:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:45 INFO IncrementalExecution: Current batch timestamp = 1746458745459\n",
      "25/05/05 11:25:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:45 INFO IncrementalExecution: Current batch timestamp = 1746458745459\n",
      "25/05/05 11:25:45 INFO IncrementalExecution: Current batch timestamp = 1746458745458\n",
      "25/05/05 11:25:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:45 INFO CodeGenerator: Code generated in 3.820833 ms\n",
      "25/05/05 11:25:45 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 90, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:25:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:45 INFO DAGScheduler: Got job 270 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:25:45 INFO DAGScheduler: Final stage: ResultStage 269 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:25:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:25:45 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:25:45 INFO DAGScheduler: Submitting ResultStage 269 (MapPartitionsRDD[1446] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:25:45 INFO MemoryStore: Block broadcast_359 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:25:45 INFO MemoryStore: Block broadcast_359_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:25:45 INFO BlockManagerInfo: Added broadcast_359_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:45 INFO CodeGenerator: Code generated in 3.551375 ms\n",
      "25/05/05 11:25:45 INFO SparkContext: Created broadcast 359 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:25:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 269 (MapPartitionsRDD[1446] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:25:45 INFO TaskSchedulerImpl: Adding task set 269.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:25:45 INFO TaskSetManager: Starting task 0.0 in stage 269.0 (TID 269) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:25:45 INFO Executor: Running task 0.0 in stage 269.0 (TID 269)\n",
      "25/05/05 11:25:45 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 90, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@54efeb45]. The input RDD has 1 partitions.\n",
      "25/05/05 11:25:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:45 INFO DAGScheduler: Got job 271 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:25:45 INFO DAGScheduler: Final stage: ResultStage 270 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:25:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:25:45 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:25:45 INFO DAGScheduler: Submitting ResultStage 270 (MapPartitionsRDD[1451] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:25:45 INFO MemoryStore: Block broadcast_360 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:25:45 INFO MemoryStore: Block broadcast_360_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)\n",
      "25/05/05 11:25:45 INFO BlockManagerInfo: Added broadcast_360_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:45 INFO SparkContext: Created broadcast 360 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:25:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 270 (MapPartitionsRDD[1451] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:25:45 INFO TaskSchedulerImpl: Adding task set 270.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:25:45 INFO TaskSetManager: Starting task 0.0 in stage 270.0 (TID 270) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:25:45 INFO Executor: Running task 0.0 in stage 270.0 (TID 270)\n",
      "25/05/05 11:25:45 INFO CodeGenerator: Code generated in 3.66475 ms\n",
      "25/05/05 11:25:45 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3321 untilOffset=3322, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=90 taskId=269 partitionId=0\n",
      "25/05/05 11:25:45 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3321 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:45 INFO CodeGenerator: Code generated in 4.562917 ms\n",
      "25/05/05 11:25:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:45 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3321 untilOffset=3322, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=90 taskId=270 partitionId=0\n",
      "25/05/05 11:25:45 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3321 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:45 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:45 INFO DAGScheduler: Got job 272 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:25:45 INFO DAGScheduler: Final stage: ResultStage 271 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:25:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:25:45 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:25:45 INFO DAGScheduler: Submitting ResultStage 271 (MapPartitionsRDD[1456] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:25:45 INFO MemoryStore: Block broadcast_361 stored as values in memory (estimated size 47.1 KiB, free 366.0 MiB)\n",
      "25/05/05 11:25:45 INFO MemoryStore: Block broadcast_361_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:25:45 INFO BlockManagerInfo: Added broadcast_361_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:45 INFO SparkContext: Created broadcast 361 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:25:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 271 (MapPartitionsRDD[1456] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:25:45 INFO TaskSchedulerImpl: Adding task set 271.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:25:45 INFO TaskSetManager: Starting task 0.0 in stage 271.0 (TID 271) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:25:45 INFO Executor: Running task 0.0 in stage 271.0 (TID 271)\n",
      "25/05/05 11:25:45 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3321 untilOffset=3322, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=90 taskId=271 partitionId=0\n",
      "25/05/05 11:25:45 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3321 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3322, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:46 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:25:46 INFO DataWritingSparkTask: Committed partition 0 (task 269, attempt 0, stage 269.0)\n",
      "25/05/05 11:25:46 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 502770458 nanos, during time span of 503117125 nanos.\n",
      "25/05/05 11:25:46 INFO Executor: Finished task 0.0 in stage 269.0 (TID 269). 2145 bytes result sent to driver\n",
      "25/05/05 11:25:46 INFO TaskSetManager: Finished task 0.0 in stage 269.0 (TID 269) in 512 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:25:46 INFO TaskSchedulerImpl: Removed TaskSet 269.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:25:46 INFO DAGScheduler: ResultStage 269 (start at NativeMethodAccessorImpl.java:0) finished in 0.513 s\n",
      "25/05/05 11:25:46 INFO DAGScheduler: Job 270 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:25:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 269: Stage finished\n",
      "25/05/05 11:25:46 INFO DAGScheduler: Job 270 finished: start at NativeMethodAccessorImpl.java:0, took 0.513987 s\n",
      "25/05/05 11:25:46 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 90, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:25:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3322, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:46 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:25:46 INFO DataWritingSparkTask: Committed partition 0 (task 270, attempt 0, stage 270.0)\n",
      "25/05/05 11:25:46 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 501708834 nanos, during time span of 503150708 nanos.\n",
      "25/05/05 11:25:46 INFO Executor: Finished task 0.0 in stage 270.0 (TID 270). 3515 bytes result sent to driver\n",
      "25/05/05 11:25:46 INFO TaskSetManager: Finished task 0.0 in stage 270.0 (TID 270) in 513 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:25:46 INFO TaskSchedulerImpl: Removed TaskSet 270.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:25:46 INFO DAGScheduler: ResultStage 270 (start at NativeMethodAccessorImpl.java:0) finished in 0.514 s\n",
      "25/05/05 11:25:46 INFO DAGScheduler: Job 271 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:25:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 270: Stage finished\n",
      "25/05/05 11:25:46 INFO DAGScheduler: Job 271 finished: start at NativeMethodAccessorImpl.java:0, took 0.514945 s\n",
      "25/05/05 11:25:46 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 90, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@54efeb45] is committing.\n",
      "25/05/05 11:25:46 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 90, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@54efeb45] committed.\n",
      "25/05/05 11:25:46 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 90, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:25:46 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/90 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.90.4cb50d20-1581-49c8-880e-0b27c22ae85c.tmp\n",
      "25/05/05 11:25:46 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/90 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.90.3babd505-b85b-47a4-8f61-c165440cf799.tmp\n",
      "25/05/05 11:25:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3322, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:46 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:25:46 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:25:46 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:25:46 INFO connection: Opened connection [connectionId{localValue:179, serverValue:4549}] to localhost:27017\n",
      "25/05/05 11:25:46 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=243125}\n",
      "25/05/05 11:25:46 INFO connection: Opened connection [connectionId{localValue:180, serverValue:4550}] to localhost:27017\n",
      "25/05/05 11:25:46 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:25:46 INFO connection: Closed connection [connectionId{localValue:180, serverValue:4550}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:25:46 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 508211709 nanos, during time span of 515601709 nanos.\n",
      "25/05/05 11:25:46 INFO Executor: Finished task 0.0 in stage 271.0 (TID 271). 1645 bytes result sent to driver\n",
      "25/05/05 11:25:46 INFO TaskSetManager: Finished task 0.0 in stage 271.0 (TID 271) in 521 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:25:46 INFO TaskSchedulerImpl: Removed TaskSet 271.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:25:46 INFO DAGScheduler: ResultStage 271 (start at NativeMethodAccessorImpl.java:0) finished in 0.527 s\n",
      "25/05/05 11:25:46 INFO DAGScheduler: Job 272 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:25:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 271: Stage finished\n",
      "25/05/05 11:25:46 INFO DAGScheduler: Job 272 finished: start at NativeMethodAccessorImpl.java:0, took 0.527081 s\n",
      "25/05/05 11:25:46 INFO MemoryStore: Block broadcast_362 stored as values in memory (estimated size 248.0 B, free 366.0 MiB)\n",
      "25/05/05 11:25:46 INFO MemoryStore: Block broadcast_362_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.0 MiB)\n",
      "25/05/05 11:25:46 INFO BlockManagerInfo: Added broadcast_362_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:25:46 INFO SparkContext: Created broadcast 362 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:46 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.90.4cb50d20-1581-49c8-880e-0b27c22ae85c.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/90\n",
      "25/05/05 11:25:46 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:25:45.453Z\",\n",
      "  \"batchId\" : 90,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 62.5,\n",
      "  \"processedRowsPerSecond\" : 1.5772870662460567,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 527,\n",
      "    \"commitOffsets\" : 31,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 5,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 634,\n",
      "    \"walCommit\" : 66\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3321\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3322\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3322\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 62.5,\n",
      "    \"processedRowsPerSecond\" : 1.5772870662460567,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:25:46 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/90 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.90.a40fd86e-cbc7-4e86-aa14-995f438656da.tmp\n",
      "25/05/05 11:25:46 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.90.3babd505-b85b-47a4-8f61-c165440cf799.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/90\n",
      "25/05/05 11:25:46 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:25:45.456Z\",\n",
      "  \"batchId\" : 90,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 55.55555555555556,\n",
      "  \"processedRowsPerSecond\" : 1.574803149606299,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 530,\n",
      "    \"commitOffsets\" : 32,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 635,\n",
      "    \"walCommit\" : 67\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3321\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3322\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3322\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 55.55555555555556,\n",
      "    \"processedRowsPerSecond\" : 1.574803149606299,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:25:46 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.90.a40fd86e-cbc7-4e86-aa14-995f438656da.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/90\n",
      "25/05/05 11:25:46 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:25:45.456Z\",\n",
      "  \"batchId\" : 90,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 55.55555555555556,\n",
      "  \"processedRowsPerSecond\" : 1.529051987767584,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 556,\n",
      "    \"commitOffsets\" : 25,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 654,\n",
      "    \"walCommit\" : 65\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3321\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3322\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3322\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 55.55555555555556,\n",
      "    \"processedRowsPerSecond\" : 1.529051987767584,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 90\n",
      "-------------------------------------------\n",
      "+----+----+--------+--------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION       |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+--------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A002|R051|02-00-00|34 ST-PENN STA|05/05/2025|11:25:44|REGULAR|9          |6         |2025-05-05 11:25:45.459|\n",
      "+----+----+--------+--------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:25:50 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/91 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.91.c4bb53ea-a359-4d4f-b57b-8444256b5b59.tmp\n",
      "25/05/05 11:25:50 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/91 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.91.54a11258-0342-4238-9bdb-cacd6999ce6b.tmp\n",
      "25/05/05 11:25:50 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/91 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.91.49427434-dc01-496a-b7c1-c8cb706b747d.tmp\n",
      "25/05/05 11:25:50 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.91.c4bb53ea-a359-4d4f-b57b-8444256b5b59.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/91\n",
      "25/05/05 11:25:50 INFO MicroBatchExecution: Committed offsets for batch 91. Metadata OffsetSeqMetadata(0,1746458750944,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:25:50 INFO IncrementalExecution: Current batch timestamp = 1746458750944\n",
      "25/05/05 11:25:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:50 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.91.54a11258-0342-4238-9bdb-cacd6999ce6b.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/91\n",
      "25/05/05 11:25:50 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.91.49427434-dc01-496a-b7c1-c8cb706b747d.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/91\n",
      "25/05/05 11:25:50 INFO MicroBatchExecution: Committed offsets for batch 91. Metadata OffsetSeqMetadata(0,1746458750954,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:25:50 INFO MicroBatchExecution: Committed offsets for batch 91. Metadata OffsetSeqMetadata(0,1746458750955,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:25:50 INFO IncrementalExecution: Current batch timestamp = 1746458750944\n",
      "25/05/05 11:25:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:50 INFO IncrementalExecution: Current batch timestamp = 1746458750955\n",
      "25/05/05 11:25:50 INFO IncrementalExecution: Current batch timestamp = 1746458750954\n",
      "25/05/05 11:25:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:50 INFO IncrementalExecution: Current batch timestamp = 1746458750944\n",
      "25/05/05 11:25:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:50 INFO IncrementalExecution: Current batch timestamp = 1746458750955\n",
      "25/05/05 11:25:50 INFO IncrementalExecution: Current batch timestamp = 1746458750954\n",
      "25/05/05 11:25:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:50 INFO IncrementalExecution: Current batch timestamp = 1746458750955\n",
      "25/05/05 11:25:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:50 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:50 INFO CodeGenerator: Code generated in 5.087041 ms\n",
      "25/05/05 11:25:50 INFO CodeGenerator: Code generated in 5.770209 ms\n",
      "25/05/05 11:25:50 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 91, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:25:50 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:51 INFO DAGScheduler: Got job 273 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:25:51 INFO DAGScheduler: Final stage: ResultStage 272 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:25:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:25:51 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:25:51 INFO DAGScheduler: Submitting ResultStage 272 (MapPartitionsRDD[1461] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:25:51 INFO MemoryStore: Block broadcast_363 stored as values in memory (estimated size 15.2 KiB, free 366.0 MiB)\n",
      "25/05/05 11:25:51 INFO MemoryStore: Block broadcast_363_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:25:51 INFO BlockManagerInfo: Added broadcast_363_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:51 INFO CodeGenerator: Code generated in 11.409542 ms\n",
      "25/05/05 11:25:51 INFO SparkContext: Created broadcast 363 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:25:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 272 (MapPartitionsRDD[1461] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:25:51 INFO TaskSchedulerImpl: Adding task set 272.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:25:51 INFO BlockManagerInfo: Removed broadcast_357_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:51 INFO TaskSetManager: Starting task 0.0 in stage 272.0 (TID 272) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:25:51 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 91, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6c4da3bb]. The input RDD has 1 partitions.\n",
      "25/05/05 11:25:51 INFO Executor: Running task 0.0 in stage 272.0 (TID 272)\n",
      "25/05/05 11:25:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:51 INFO DAGScheduler: Got job 274 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:25:51 INFO DAGScheduler: Final stage: ResultStage 273 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:25:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:25:51 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:25:51 INFO DAGScheduler: Submitting ResultStage 273 (MapPartitionsRDD[1467] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:25:51 INFO MemoryStore: Block broadcast_364 stored as values in memory (estimated size 16.2 KiB, free 366.0 MiB)\n",
      "25/05/05 11:25:51 INFO MemoryStore: Block broadcast_364_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)\n",
      "25/05/05 11:25:51 INFO BlockManagerInfo: Added broadcast_364_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:51 INFO BlockManagerInfo: Removed broadcast_360_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:51 INFO SparkContext: Created broadcast 364 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:25:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 273 (MapPartitionsRDD[1467] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:25:51 INFO TaskSchedulerImpl: Adding task set 273.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:25:51 INFO TaskSetManager: Starting task 0.0 in stage 273.0 (TID 273) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:25:51 INFO Executor: Running task 0.0 in stage 273.0 (TID 273)\n",
      "25/05/05 11:25:51 INFO CodeGenerator: Code generated in 3.882792 ms\n",
      "25/05/05 11:25:51 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3322 untilOffset=3323, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=91 taskId=272 partitionId=0\n",
      "25/05/05 11:25:51 INFO BlockManagerInfo: Removed broadcast_356_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:51 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3322 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:51 INFO CodeGenerator: Code generated in 4.944083 ms\n",
      "25/05/05 11:25:51 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3322 untilOffset=3323, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=91 taskId=273 partitionId=0\n",
      "25/05/05 11:25:51 INFO BlockManagerInfo: Removed broadcast_361_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:51 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3322 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:51 INFO BlockManagerInfo: Removed broadcast_362_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:25:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:51 INFO BlockManagerInfo: Removed broadcast_358_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:25:51 INFO BlockManagerInfo: Removed broadcast_359_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:25:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:51 INFO DAGScheduler: Got job 275 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:25:51 INFO DAGScheduler: Final stage: ResultStage 274 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:25:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:25:51 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:25:51 INFO DAGScheduler: Submitting ResultStage 274 (MapPartitionsRDD[1472] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:25:51 INFO BlockManagerInfo: Removed broadcast_355_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:25:51 INFO MemoryStore: Block broadcast_365 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:25:51 INFO MemoryStore: Block broadcast_365_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.1 MiB)\n",
      "25/05/05 11:25:51 INFO BlockManagerInfo: Added broadcast_365_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:51 INFO SparkContext: Created broadcast 365 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:25:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 274 (MapPartitionsRDD[1472] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:25:51 INFO TaskSchedulerImpl: Adding task set 274.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:25:51 INFO TaskSetManager: Starting task 0.0 in stage 274.0 (TID 274) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:25:51 INFO Executor: Running task 0.0 in stage 274.0 (TID 274)\n",
      "25/05/05 11:25:51 INFO CodeGenerator: Code generated in 6.492625 ms\n",
      "25/05/05 11:25:51 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3322 untilOffset=3323, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=91 taskId=274 partitionId=0\n",
      "25/05/05 11:25:51 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3322 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3323, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:51 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:25:51 INFO DataWritingSparkTask: Committed partition 0 (task 272, attempt 0, stage 272.0)\n",
      "25/05/05 11:25:51 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 506588625 nanos, during time span of 506982208 nanos.\n",
      "25/05/05 11:25:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3323, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:51 INFO Executor: Finished task 0.0 in stage 272.0 (TID 272). 2145 bytes result sent to driver\n",
      "25/05/05 11:25:51 INFO TaskSetManager: Finished task 0.0 in stage 272.0 (TID 272) in 517 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:25:51 INFO TaskSchedulerImpl: Removed TaskSet 272.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:25:51 INFO DAGScheduler: ResultStage 272 (start at NativeMethodAccessorImpl.java:0) finished in 0.521 s\n",
      "25/05/05 11:25:51 INFO DAGScheduler: Job 273 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:25:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 272: Stage finished\n",
      "25/05/05 11:25:51 INFO DAGScheduler: Job 273 finished: start at NativeMethodAccessorImpl.java:0, took 0.528293 s\n",
      "25/05/05 11:25:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 91, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:25:51 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:25:51 INFO DataWritingSparkTask: Committed partition 0 (task 273, attempt 0, stage 273.0)\n",
      "25/05/05 11:25:51 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 503508167 nanos, during time span of 505206458 nanos.\n",
      "25/05/05 11:25:51 INFO Executor: Finished task 0.0 in stage 273.0 (TID 273). 3510 bytes result sent to driver\n",
      "25/05/05 11:25:51 INFO TaskSetManager: Finished task 0.0 in stage 273.0 (TID 273) in 517 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:25:51 INFO TaskSchedulerImpl: Removed TaskSet 273.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:25:51 INFO DAGScheduler: ResultStage 273 (start at NativeMethodAccessorImpl.java:0) finished in 0.518 s\n",
      "25/05/05 11:25:51 INFO DAGScheduler: Job 274 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:25:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 273: Stage finished\n",
      "25/05/05 11:25:51 INFO DAGScheduler: Job 274 finished: start at NativeMethodAccessorImpl.java:0, took 0.519213 s\n",
      "25/05/05 11:25:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 91, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6c4da3bb] is committing.\n",
      "25/05/05 11:25:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 91, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6c4da3bb] committed.\n",
      "25/05/05 11:25:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 91, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:25:51 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/91 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.91.d215b389-7420-4c5d-b832-4b6f416bfffb.tmp\n",
      "25/05/05 11:25:51 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/91 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.91.37a5db5c-44bf-4103-9294-777bc5a88c07.tmp\n",
      "25/05/05 11:25:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3323, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:51 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:25:51 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:25:51 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:25:51 INFO connection: Opened connection [connectionId{localValue:181, serverValue:4551}] to localhost:27017\n",
      "25/05/05 11:25:51 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=291083}\n",
      "25/05/05 11:25:51 INFO connection: Opened connection [connectionId{localValue:182, serverValue:4552}] to localhost:27017\n",
      "25/05/05 11:25:51 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.91.d215b389-7420-4c5d-b832-4b6f416bfffb.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/91\n",
      "25/05/05 11:25:51 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:25:50.954Z\",\n",
      "  \"batchId\" : 91,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "  \"processedRowsPerSecond\" : 1.663893510815308,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 540,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 601,\n",
      "    \"walCommit\" : 28\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3322\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3323\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3323\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "    \"processedRowsPerSecond\" : 1.663893510815308,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:25:51 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:25:51 INFO connection: Closed connection [connectionId{localValue:182, serverValue:4552}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:25:51 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 503165875 nanos, during time span of 509786083 nanos.\n",
      "25/05/05 11:25:51 INFO Executor: Finished task 0.0 in stage 274.0 (TID 274). 1645 bytes result sent to driver\n",
      "25/05/05 11:25:51 INFO TaskSetManager: Finished task 0.0 in stage 274.0 (TID 274) in 525 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:25:51 INFO TaskSchedulerImpl: Removed TaskSet 274.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:25:51 INFO DAGScheduler: ResultStage 274 (start at NativeMethodAccessorImpl.java:0) finished in 0.531 s\n",
      "25/05/05 11:25:51 INFO DAGScheduler: Job 275 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:25:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 274: Stage finished\n",
      "25/05/05 11:25:51 INFO DAGScheduler: Job 275 finished: start at NativeMethodAccessorImpl.java:0, took 0.531669 s\n",
      "25/05/05 11:25:51 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.91.37a5db5c-44bf-4103-9294-777bc5a88c07.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/91\n",
      "25/05/05 11:25:51 INFO MemoryStore: Block broadcast_366 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:25:51 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:25:50.943Z\",\n",
      "  \"batchId\" : 91,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.6260162601626016,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 551,\n",
      "    \"commitOffsets\" : 25,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 615,\n",
      "    \"walCommit\" : 34\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3322\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3323\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3323\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.6260162601626016,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:25:51 INFO MemoryStore: Block broadcast_366_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:25:51 INFO BlockManagerInfo: Added broadcast_366_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:25:51 INFO SparkContext: Created broadcast 366 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:51 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/91 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.91.ed3714d7-a468-4fa1-9d12-dd4da0735bf7.tmp\n",
      "25/05/05 11:25:51 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.91.ed3714d7-a468-4fa1-9d12-dd4da0735bf7.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/91\n",
      "25/05/05 11:25:51 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:25:50.953Z\",\n",
      "  \"batchId\" : 91,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 62.5,\n",
      "  \"processedRowsPerSecond\" : 1.5797788309636651,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 573,\n",
      "    \"commitOffsets\" : 26,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 633,\n",
      "    \"walCommit\" : 29\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3322\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3323\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3323\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 62.5,\n",
      "    \"processedRowsPerSecond\" : 1.5797788309636651,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 91\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION  |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A002|R170|00-00-00|FULTON ST|05/05/2025|11:25:49|REGULAR|10         |14        |2025-05-05 11:25:50.944|\n",
      "+----+----+--------+---------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:25:56 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/92 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.92.c8dfe089-6f9c-40cb-83bf-b8873bc70915.tmp\n",
      "25/05/05 11:25:56 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/92 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.92.45e1afe2-5b90-45b3-bedc-a283e8dee558.tmp\n",
      "25/05/05 11:25:56 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/92 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.92.be02569c-e160-4115-97b6-b4459627d869.tmp\n",
      "25/05/05 11:25:56 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.92.be02569c-e160-4115-97b6-b4459627d869.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/92\n",
      "25/05/05 11:25:56 INFO MicroBatchExecution: Committed offsets for batch 92. Metadata OffsetSeqMetadata(0,1746458756441,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:25:56 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.92.45e1afe2-5b90-45b3-bedc-a283e8dee558.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/92\n",
      "25/05/05 11:25:56 INFO MicroBatchExecution: Committed offsets for batch 92. Metadata OffsetSeqMetadata(0,1746458756442,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:25:56 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.92.c8dfe089-6f9c-40cb-83bf-b8873bc70915.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/92\n",
      "25/05/05 11:25:56 INFO MicroBatchExecution: Committed offsets for batch 92. Metadata OffsetSeqMetadata(0,1746458756442,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:25:56 INFO IncrementalExecution: Current batch timestamp = 1746458756442\n",
      "25/05/05 11:25:56 INFO IncrementalExecution: Current batch timestamp = 1746458756442\n",
      "25/05/05 11:25:56 INFO IncrementalExecution: Current batch timestamp = 1746458756441\n",
      "25/05/05 11:25:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:56 INFO IncrementalExecution: Current batch timestamp = 1746458756441\n",
      "25/05/05 11:25:56 INFO IncrementalExecution: Current batch timestamp = 1746458756442\n",
      "25/05/05 11:25:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:56 INFO IncrementalExecution: Current batch timestamp = 1746458756442\n",
      "25/05/05 11:25:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:56 INFO IncrementalExecution: Current batch timestamp = 1746458756441\n",
      "25/05/05 11:25:56 INFO IncrementalExecution: Current batch timestamp = 1746458756442\n",
      "25/05/05 11:25:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:25:56 INFO CodeGenerator: Code generated in 3.955708 ms\n",
      "25/05/05 11:25:56 INFO CodeGenerator: Code generated in 3.334917 ms\n",
      "25/05/05 11:25:56 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 92, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@d4990e6]. The input RDD has 1 partitions.\n",
      "25/05/05 11:25:56 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 92, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:25:56 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:56 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:56 INFO DAGScheduler: Got job 276 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:25:56 INFO DAGScheduler: Final stage: ResultStage 275 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:25:56 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:25:56 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:25:56 INFO DAGScheduler: Submitting ResultStage 275 (MapPartitionsRDD[1480] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:25:56 INFO MemoryStore: Block broadcast_367 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:25:56 INFO MemoryStore: Block broadcast_367_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.1 MiB)\n",
      "25/05/05 11:25:56 INFO BlockManagerInfo: Added broadcast_367_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:56 INFO SparkContext: Created broadcast 367 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:25:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 275 (MapPartitionsRDD[1480] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:25:56 INFO TaskSchedulerImpl: Adding task set 275.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:25:56 INFO DAGScheduler: Got job 277 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:25:56 INFO DAGScheduler: Final stage: ResultStage 276 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:25:56 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:25:56 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:25:56 INFO DAGScheduler: Submitting ResultStage 276 (MapPartitionsRDD[1483] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:25:56 INFO TaskSetManager: Starting task 0.0 in stage 275.0 (TID 275) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:25:56 INFO Executor: Running task 0.0 in stage 275.0 (TID 275)\n",
      "25/05/05 11:25:56 INFO MemoryStore: Block broadcast_368 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:25:56 INFO MemoryStore: Block broadcast_368_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:25:56 INFO BlockManagerInfo: Added broadcast_368_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:56 INFO SparkContext: Created broadcast 368 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:25:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 276 (MapPartitionsRDD[1483] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:25:56 INFO TaskSchedulerImpl: Adding task set 276.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:25:56 INFO TaskSetManager: Starting task 0.0 in stage 276.0 (TID 276) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:25:56 INFO Executor: Running task 0.0 in stage 276.0 (TID 276)\n",
      "25/05/05 11:25:56 INFO CodeGenerator: Code generated in 4.135375 ms\n",
      "25/05/05 11:25:56 INFO CodeGenerator: Code generated in 3.689084 ms\n",
      "25/05/05 11:25:56 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3323 untilOffset=3324, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=92 taskId=276 partitionId=0\n",
      "25/05/05 11:25:56 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3323 untilOffset=3324, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=92 taskId=275 partitionId=0\n",
      "25/05/05 11:25:56 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3323 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:56 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3323 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:56 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:56 INFO DAGScheduler: Got job 278 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:25:56 INFO DAGScheduler: Final stage: ResultStage 277 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:25:56 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:25:56 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:25:56 INFO DAGScheduler: Submitting ResultStage 277 (MapPartitionsRDD[1488] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:25:56 INFO MemoryStore: Block broadcast_369 stored as values in memory (estimated size 47.1 KiB, free 366.0 MiB)\n",
      "25/05/05 11:25:56 INFO MemoryStore: Block broadcast_369_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.0 MiB)\n",
      "25/05/05 11:25:56 INFO BlockManagerInfo: Added broadcast_369_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:25:56 INFO SparkContext: Created broadcast 369 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:25:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 277 (MapPartitionsRDD[1488] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:25:56 INFO TaskSchedulerImpl: Adding task set 277.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:25:56 INFO TaskSetManager: Starting task 0.0 in stage 277.0 (TID 277) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:25:56 INFO Executor: Running task 0.0 in stage 277.0 (TID 277)\n",
      "25/05/05 11:25:56 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3323 untilOffset=3324, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=92 taskId=277 partitionId=0\n",
      "25/05/05 11:25:56 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3323 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3324, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3324, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:57 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:25:57 INFO DataWritingSparkTask: Committed partition 0 (task 276, attempt 0, stage 276.0)\n",
      "25/05/05 11:25:57 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 503639084 nanos, during time span of 503981708 nanos.\n",
      "25/05/05 11:25:57 INFO Executor: Finished task 0.0 in stage 276.0 (TID 276). 2145 bytes result sent to driver\n",
      "25/05/05 11:25:57 INFO TaskSetManager: Finished task 0.0 in stage 276.0 (TID 276) in 512 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:25:57 INFO TaskSchedulerImpl: Removed TaskSet 276.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:25:57 INFO DAGScheduler: ResultStage 276 (start at NativeMethodAccessorImpl.java:0) finished in 0.513 s\n",
      "25/05/05 11:25:57 INFO DAGScheduler: Job 277 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:25:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 276: Stage finished\n",
      "25/05/05 11:25:57 INFO DAGScheduler: Job 277 finished: start at NativeMethodAccessorImpl.java:0, took 0.514971 s\n",
      "25/05/05 11:25:57 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 92, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:25:57 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:25:57 INFO DataWritingSparkTask: Committed partition 0 (task 275, attempt 0, stage 275.0)\n",
      "25/05/05 11:25:57 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 502494333 nanos, during time span of 504140708 nanos.\n",
      "25/05/05 11:25:57 INFO Executor: Finished task 0.0 in stage 275.0 (TID 275). 3516 bytes result sent to driver\n",
      "25/05/05 11:25:57 INFO TaskSetManager: Finished task 0.0 in stage 275.0 (TID 275) in 514 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:25:57 INFO TaskSchedulerImpl: Removed TaskSet 275.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:25:57 INFO DAGScheduler: ResultStage 275 (start at NativeMethodAccessorImpl.java:0) finished in 0.516 s\n",
      "25/05/05 11:25:57 INFO DAGScheduler: Job 276 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:25:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 275: Stage finished\n",
      "25/05/05 11:25:57 INFO DAGScheduler: Job 276 finished: start at NativeMethodAccessorImpl.java:0, took 0.516089 s\n",
      "25/05/05 11:25:57 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 92, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@d4990e6] is committing.\n",
      "25/05/05 11:25:57 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 92, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@d4990e6] committed.\n",
      "25/05/05 11:25:57 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 92, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:25:57 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/92 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.92.75478fd5-2fd8-4f81-a69b-1efe68b455cf.tmp\n",
      "25/05/05 11:25:57 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/92 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.92.a5bc0a65-51cc-4cac-b15d-716bc7bf46d1.tmp\n",
      "25/05/05 11:25:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:25:57 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3324, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:25:57 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:25:57 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:25:57 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:25:57 INFO connection: Opened connection [connectionId{localValue:183, serverValue:4553}] to localhost:27017\n",
      "25/05/05 11:25:57 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=212958}\n",
      "25/05/05 11:25:57 INFO connection: Opened connection [connectionId{localValue:184, serverValue:4554}] to localhost:27017\n",
      "25/05/05 11:25:57 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:25:57 INFO connection: Closed connection [connectionId{localValue:184, serverValue:4554}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:25:57 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 502899375 nanos, during time span of 509001625 nanos.\n",
      "25/05/05 11:25:57 INFO Executor: Finished task 0.0 in stage 277.0 (TID 277). 1645 bytes result sent to driver\n",
      "25/05/05 11:25:57 INFO TaskSetManager: Finished task 0.0 in stage 277.0 (TID 277) in 515 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:25:57 INFO TaskSchedulerImpl: Removed TaskSet 277.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:25:57 INFO DAGScheduler: ResultStage 277 (start at NativeMethodAccessorImpl.java:0) finished in 0.520 s\n",
      "25/05/05 11:25:57 INFO DAGScheduler: Job 278 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:25:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 277: Stage finished\n",
      "25/05/05 11:25:57 INFO DAGScheduler: Job 278 finished: start at NativeMethodAccessorImpl.java:0, took 0.520888 s\n",
      "25/05/05 11:25:57 INFO MemoryStore: Block broadcast_370 stored as values in memory (estimated size 248.0 B, free 366.0 MiB)\n",
      "25/05/05 11:25:57 INFO MemoryStore: Block broadcast_370_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.0 MiB)\n",
      "25/05/05 11:25:57 INFO BlockManagerInfo: Added broadcast_370_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:25:57 INFO SparkContext: Created broadcast 370 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:25:57 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.92.75478fd5-2fd8-4f81-a69b-1efe68b455cf.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/92\n",
      "25/05/05 11:25:57 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:25:56.440Z\",\n",
      "  \"batchId\" : 92,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.6,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 529,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 625,\n",
      "    \"walCommit\" : 61\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3323\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3324\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3324\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.6,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:25:57 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/92 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.92.cc21581b-9a3e-4b8a-a073-d8dddfc99986.tmp\n",
      "25/05/05 11:25:57 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.92.a5bc0a65-51cc-4cac-b15d-716bc7bf46d1.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/92\n",
      "25/05/05 11:25:57 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:25:56.439Z\",\n",
      "  \"batchId\" : 92,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.5873015873015872,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 533,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 630,\n",
      "    \"walCommit\" : 62\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3323\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3324\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3324\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.5873015873015872,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:25:57 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.92.cc21581b-9a3e-4b8a-a073-d8dddfc99986.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/92\n",
      "25/05/05 11:25:57 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:25:56.440Z\",\n",
      "  \"batchId\" : 92,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.547987616099071,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 552,\n",
      "    \"commitOffsets\" : 26,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 646,\n",
      "    \"walCommit\" : 61\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3323\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3324\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3324\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.547987616099071,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 92\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A001|R079|01-00-00|WORLD TRADE CTR|05/05/2025|11:25:55|REGULAR|4          |14        |2025-05-05 11:25:56.441|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:26:01 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/93 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.93.f7418948-8640-435a-8f58-2029590e3675.tmp\n",
      "25/05/05 11:26:01 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/93 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.93.4e31cbc0-9a65-4b9b-b3ed-f1cb0e019355.tmp\n",
      "25/05/05 11:26:01 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/93 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.93.09893849-ceab-49ad-b15d-c2b0acda8bdc.tmp\n",
      "25/05/05 11:26:01 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.93.f7418948-8640-435a-8f58-2029590e3675.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/93\n",
      "25/05/05 11:26:01 INFO MicroBatchExecution: Committed offsets for batch 93. Metadata OffsetSeqMetadata(0,1746458761901,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:26:01 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.93.4e31cbc0-9a65-4b9b-b3ed-f1cb0e019355.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/93\n",
      "25/05/05 11:26:01 INFO MicroBatchExecution: Committed offsets for batch 93. Metadata OffsetSeqMetadata(0,1746458761903,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:26:01 INFO IncrementalExecution: Current batch timestamp = 1746458761901\n",
      "25/05/05 11:26:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:01 INFO IncrementalExecution: Current batch timestamp = 1746458761903\n",
      "25/05/05 11:26:01 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.93.09893849-ceab-49ad-b15d-c2b0acda8bdc.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/93\n",
      "25/05/05 11:26:01 INFO MicroBatchExecution: Committed offsets for batch 93. Metadata OffsetSeqMetadata(0,1746458761906,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:26:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:01 INFO IncrementalExecution: Current batch timestamp = 1746458761903\n",
      "25/05/05 11:26:01 INFO IncrementalExecution: Current batch timestamp = 1746458761901\n",
      "25/05/05 11:26:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:01 INFO IncrementalExecution: Current batch timestamp = 1746458761906\n",
      "25/05/05 11:26:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:01 INFO IncrementalExecution: Current batch timestamp = 1746458761906\n",
      "25/05/05 11:26:01 INFO IncrementalExecution: Current batch timestamp = 1746458761903\n",
      "25/05/05 11:26:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:01 INFO IncrementalExecution: Current batch timestamp = 1746458761906\n",
      "25/05/05 11:26:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:01 INFO CodeGenerator: Code generated in 3.916417 ms\n",
      "25/05/05 11:26:01 INFO CodeGenerator: Code generated in 3.297166 ms\n",
      "25/05/05 11:26:01 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 93, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4ccd9ecd]. The input RDD has 1 partitions.\n",
      "25/05/05 11:26:01 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:01 INFO DAGScheduler: Got job 279 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:26:01 INFO DAGScheduler: Final stage: ResultStage 278 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:26:01 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:26:01 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:26:01 INFO DAGScheduler: Submitting ResultStage 278 (MapPartitionsRDD[1496] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:26:01 INFO MemoryStore: Block broadcast_371 stored as values in memory (estimated size 16.2 KiB, free 366.0 MiB)\n",
      "25/05/05 11:26:01 INFO CodeGenerator: Code generated in 3.561708 ms\n",
      "25/05/05 11:26:01 INFO MemoryStore: Block broadcast_371_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)\n",
      "25/05/05 11:26:01 INFO BlockManagerInfo: Added broadcast_371_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:01 INFO SparkContext: Created broadcast 371 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:26:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 278 (MapPartitionsRDD[1496] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:26:01 INFO TaskSchedulerImpl: Adding task set 278.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:26:01 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 93, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:26:01 INFO TaskSetManager: Starting task 0.0 in stage 278.0 (TID 278) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:26:01 INFO Executor: Running task 0.0 in stage 278.0 (TID 278)\n",
      "25/05/05 11:26:01 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:01 INFO DAGScheduler: Got job 280 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:26:01 INFO DAGScheduler: Final stage: ResultStage 279 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:26:01 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:26:01 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:26:01 INFO DAGScheduler: Submitting ResultStage 279 (MapPartitionsRDD[1499] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:26:01 INFO MemoryStore: Block broadcast_372 stored as values in memory (estimated size 15.2 KiB, free 365.9 MiB)\n",
      "25/05/05 11:26:01 INFO MemoryStore: Block broadcast_372_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 365.9 MiB)\n",
      "25/05/05 11:26:01 INFO BlockManagerInfo: Added broadcast_372_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:01 INFO SparkContext: Created broadcast 372 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:26:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 279 (MapPartitionsRDD[1499] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:26:01 INFO TaskSchedulerImpl: Adding task set 279.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:26:01 INFO TaskSetManager: Starting task 0.0 in stage 279.0 (TID 279) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:26:01 INFO Executor: Running task 0.0 in stage 279.0 (TID 279)\n",
      "25/05/05 11:26:01 INFO CodeGenerator: Code generated in 9.731083 ms\n",
      "25/05/05 11:26:01 INFO BlockManagerInfo: Removed broadcast_363_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:01 INFO BlockManagerInfo: Removed broadcast_364_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:01 INFO CodeGenerator: Code generated in 9.517 ms\n",
      "25/05/05 11:26:01 INFO BlockManagerInfo: Removed broadcast_368_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:01 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3324 untilOffset=3325, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=93 taskId=279 partitionId=0\n",
      "25/05/05 11:26:01 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3324 untilOffset=3325, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=93 taskId=278 partitionId=0\n",
      "25/05/05 11:26:01 INFO BlockManagerInfo: Removed broadcast_367_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:01 INFO BlockManagerInfo: Removed broadcast_365_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:01 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3324 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:01 INFO BlockManagerInfo: Removed broadcast_369_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:26:01 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3324 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:01 INFO BlockManagerInfo: Removed broadcast_370_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:26:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:01 INFO BlockManagerInfo: Removed broadcast_366_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:26:01 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:01 INFO DAGScheduler: Got job 281 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:26:01 INFO DAGScheduler: Final stage: ResultStage 280 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:26:01 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:26:01 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:26:01 INFO DAGScheduler: Submitting ResultStage 280 (MapPartitionsRDD[1504] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:26:01 INFO MemoryStore: Block broadcast_373 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:26:01 INFO MemoryStore: Block broadcast_373_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.1 MiB)\n",
      "25/05/05 11:26:01 INFO BlockManagerInfo: Added broadcast_373_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:01 INFO SparkContext: Created broadcast 373 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:26:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 280 (MapPartitionsRDD[1504] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:26:01 INFO TaskSchedulerImpl: Adding task set 280.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:26:01 INFO TaskSetManager: Starting task 0.0 in stage 280.0 (TID 280) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:26:01 INFO Executor: Running task 0.0 in stage 280.0 (TID 280)\n",
      "25/05/05 11:26:01 INFO CodeGenerator: Code generated in 3.6695 ms\n",
      "25/05/05 11:26:01 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3324 untilOffset=3325, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=93 taskId=280 partitionId=0\n",
      "25/05/05 11:26:01 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3324 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3325, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3325, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:02 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:26:02 INFO DataWritingSparkTask: Committed partition 0 (task 279, attempt 0, stage 279.0)\n",
      "25/05/05 11:26:02 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 502288666 nanos, during time span of 502642667 nanos.\n",
      "25/05/05 11:26:02 INFO Executor: Finished task 0.0 in stage 279.0 (TID 279). 2188 bytes result sent to driver\n",
      "25/05/05 11:26:02 INFO TaskSetManager: Finished task 0.0 in stage 279.0 (TID 279) in 517 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:26:02 INFO TaskSchedulerImpl: Removed TaskSet 279.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:26:02 INFO DAGScheduler: ResultStage 279 (start at NativeMethodAccessorImpl.java:0) finished in 0.518 s\n",
      "25/05/05 11:26:02 INFO DAGScheduler: Job 280 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:26:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 279: Stage finished\n",
      "25/05/05 11:26:02 INFO DAGScheduler: Job 280 finished: start at NativeMethodAccessorImpl.java:0, took 0.517977 s\n",
      "25/05/05 11:26:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 93, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:26:02 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:26:02 INFO DataWritingSparkTask: Committed partition 0 (task 278, attempt 0, stage 278.0)\n",
      "25/05/05 11:26:02 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 502141292 nanos, during time span of 503758166 nanos.\n",
      "25/05/05 11:26:02 INFO Executor: Finished task 0.0 in stage 278.0 (TID 278). 3559 bytes result sent to driver\n",
      "25/05/05 11:26:02 INFO TaskSetManager: Finished task 0.0 in stage 278.0 (TID 278) in 519 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:26:02 INFO TaskSchedulerImpl: Removed TaskSet 278.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:26:02 INFO DAGScheduler: ResultStage 278 (start at NativeMethodAccessorImpl.java:0) finished in 0.521 s\n",
      "25/05/05 11:26:02 INFO DAGScheduler: Job 279 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:26:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 278: Stage finished\n",
      "25/05/05 11:26:02 INFO DAGScheduler: Job 279 finished: start at NativeMethodAccessorImpl.java:0, took 0.522101 s\n",
      "25/05/05 11:26:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 93, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4ccd9ecd] is committing.\n",
      "25/05/05 11:26:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 93, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4ccd9ecd] committed.\n",
      "25/05/05 11:26:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 93, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:26:02 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/93 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.93.0a209edb-94c4-45f5-8599-51e5ab2cd76f.tmp\n",
      "25/05/05 11:26:02 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/93 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.93.c79498a8-9fb3-4044-bef8-cc15ac520987.tmp\n",
      "25/05/05 11:26:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3325, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:02 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:26:02 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:26:02 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:26:02 INFO connection: Opened connection [connectionId{localValue:185, serverValue:4555}] to localhost:27017\n",
      "25/05/05 11:26:02 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=607333}\n",
      "25/05/05 11:26:02 INFO connection: Opened connection [connectionId{localValue:186, serverValue:4556}] to localhost:27017\n",
      "25/05/05 11:26:02 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:26:02 INFO connection: Closed connection [connectionId{localValue:186, serverValue:4556}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:26:02 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 502722333 nanos, during time span of 512231584 nanos.\n",
      "25/05/05 11:26:02 INFO Executor: Finished task 0.0 in stage 280.0 (TID 280). 1645 bytes result sent to driver\n",
      "25/05/05 11:26:02 INFO TaskSetManager: Finished task 0.0 in stage 280.0 (TID 280) in 522 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:26:02 INFO TaskSchedulerImpl: Removed TaskSet 280.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:26:02 INFO DAGScheduler: ResultStage 280 (start at NativeMethodAccessorImpl.java:0) finished in 0.526 s\n",
      "25/05/05 11:26:02 INFO DAGScheduler: Job 281 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:26:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 280: Stage finished\n",
      "25/05/05 11:26:02 INFO DAGScheduler: Job 281 finished: start at NativeMethodAccessorImpl.java:0, took 0.526414 s\n",
      "25/05/05 11:26:02 INFO MemoryStore: Block broadcast_374 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:26:02 INFO MemoryStore: Block broadcast_374_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:26:02 INFO BlockManagerInfo: Added broadcast_374_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:26:02 INFO SparkContext: Created broadcast 374 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:02 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.93.0a209edb-94c4-45f5-8599-51e5ab2cd76f.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/93\n",
      "25/05/05 11:26:02 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:26:01.902Z\",\n",
      "  \"batchId\" : 93,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.669449081803005,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 534,\n",
      "    \"commitOffsets\" : 31,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 599,\n",
      "    \"walCommit\" : 30\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3324\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3325\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3325\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.669449081803005,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:26:02 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/93 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.93.3fe37d1d-0276-4f82-9eec-2f12c7071bc3.tmp\n",
      "25/05/05 11:26:02 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.93.c79498a8-9fb3-4044-bef8-cc15ac520987.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/93\n",
      "25/05/05 11:26:02 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:26:01.905Z\",\n",
      "  \"batchId\" : 93,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.6666666666666667,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 537,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 1,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 2,\n",
      "    \"triggerExecution\" : 600,\n",
      "    \"walCommit\" : 29\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3324\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3325\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3325\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.6666666666666667,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:26:02 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.93.3fe37d1d-0276-4f82-9eec-2f12c7071bc3.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/93\n",
      "25/05/05 11:26:02 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:26:01.900Z\",\n",
      "  \"batchId\" : 93,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "  \"processedRowsPerSecond\" : 1.6051364365971108,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 561,\n",
      "    \"commitOffsets\" : 26,\n",
      "    \"getBatch\" : 1,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 623,\n",
      "    \"walCommit\" : 30\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3324\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3325\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3325\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "    \"processedRowsPerSecond\" : 1.6051364365971108,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 93\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R022|R033|01-00-00|42 ST-PORT AUTH|05/05/2025|11:26:00|REGULAR|6          |13        |2025-05-05 11:26:01.906|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:26:07 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/94 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.94.11ac6998-4f97-4839-8600-53bdaafee685.tmp\n",
      "25/05/05 11:26:07 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/94 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.94.0ddb49e5-fa74-44ca-9e61-5e7f45d5b46c.tmp\n",
      "25/05/05 11:26:07 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/94 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.94.c5d8c454-2c16-4e79-ac32-0a9a6277e9c0.tmp\n",
      "25/05/05 11:26:07 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.94.0ddb49e5-fa74-44ca-9e61-5e7f45d5b46c.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/94\n",
      "25/05/05 11:26:07 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.94.11ac6998-4f97-4839-8600-53bdaafee685.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/94\n",
      "25/05/05 11:26:07 INFO MicroBatchExecution: Committed offsets for batch 94. Metadata OffsetSeqMetadata(0,1746458767389,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:26:07 INFO MicroBatchExecution: Committed offsets for batch 94. Metadata OffsetSeqMetadata(0,1746458767393,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:26:07 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.94.c5d8c454-2c16-4e79-ac32-0a9a6277e9c0.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/94\n",
      "25/05/05 11:26:07 INFO MicroBatchExecution: Committed offsets for batch 94. Metadata OffsetSeqMetadata(0,1746458767397,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:26:07 INFO IncrementalExecution: Current batch timestamp = 1746458767393\n",
      "25/05/05 11:26:07 INFO IncrementalExecution: Current batch timestamp = 1746458767389\n",
      "25/05/05 11:26:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:07 INFO IncrementalExecution: Current batch timestamp = 1746458767393\n",
      "25/05/05 11:26:07 INFO IncrementalExecution: Current batch timestamp = 1746458767397\n",
      "25/05/05 11:26:07 INFO IncrementalExecution: Current batch timestamp = 1746458767389\n",
      "25/05/05 11:26:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:07 INFO IncrementalExecution: Current batch timestamp = 1746458767389\n",
      "25/05/05 11:26:07 INFO IncrementalExecution: Current batch timestamp = 1746458767393\n",
      "25/05/05 11:26:07 INFO IncrementalExecution: Current batch timestamp = 1746458767397\n",
      "25/05/05 11:26:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:07 INFO CodeGenerator: Code generated in 4.07575 ms\n",
      "25/05/05 11:26:07 INFO CodeGenerator: Code generated in 3.737333 ms\n",
      "25/05/05 11:26:07 INFO CodeGenerator: Code generated in 3.607167 ms\n",
      "25/05/05 11:26:07 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 94, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:26:07 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 94, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4ed0c6cb]. The input RDD has 1 partitions.\n",
      "25/05/05 11:26:07 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:07 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:07 INFO DAGScheduler: Got job 282 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:26:07 INFO DAGScheduler: Final stage: ResultStage 281 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:26:07 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:26:07 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:26:07 INFO DAGScheduler: Submitting ResultStage 281 (MapPartitionsRDD[1514] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:26:07 INFO MemoryStore: Block broadcast_375 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:26:07 INFO MemoryStore: Block broadcast_375_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:26:07 INFO BlockManagerInfo: Added broadcast_375_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:07 INFO SparkContext: Created broadcast 375 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:26:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 281 (MapPartitionsRDD[1514] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:26:07 INFO TaskSchedulerImpl: Adding task set 281.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:26:07 INFO DAGScheduler: Got job 283 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:26:07 INFO DAGScheduler: Final stage: ResultStage 282 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:26:07 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:26:07 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:26:07 INFO DAGScheduler: Submitting ResultStage 282 (MapPartitionsRDD[1515] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:26:07 INFO TaskSetManager: Starting task 0.0 in stage 281.0 (TID 281) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:26:07 INFO Executor: Running task 0.0 in stage 281.0 (TID 281)\n",
      "25/05/05 11:26:07 INFO MemoryStore: Block broadcast_376 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:26:07 INFO MemoryStore: Block broadcast_376_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)\n",
      "25/05/05 11:26:07 INFO BlockManagerInfo: Added broadcast_376_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:07 INFO SparkContext: Created broadcast 376 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:26:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 282 (MapPartitionsRDD[1515] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:26:07 INFO TaskSchedulerImpl: Adding task set 282.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:26:07 INFO TaskSetManager: Starting task 0.0 in stage 282.0 (TID 282) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:26:07 INFO Executor: Running task 0.0 in stage 282.0 (TID 282)\n",
      "25/05/05 11:26:07 INFO CodeGenerator: Code generated in 4.259834 ms\n",
      "25/05/05 11:26:07 INFO CodeGenerator: Code generated in 3.737458 ms\n",
      "25/05/05 11:26:07 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3325 untilOffset=3326, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=94 taskId=281 partitionId=0\n",
      "25/05/05 11:26:07 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3325 untilOffset=3326, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=94 taskId=282 partitionId=0\n",
      "25/05/05 11:26:07 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3325 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:07 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3325 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:07 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:07 INFO DAGScheduler: Got job 284 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:26:07 INFO DAGScheduler: Final stage: ResultStage 283 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:26:07 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:26:07 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:26:07 INFO DAGScheduler: Submitting ResultStage 283 (MapPartitionsRDD[1520] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:26:07 INFO MemoryStore: Block broadcast_377 stored as values in memory (estimated size 47.1 KiB, free 366.0 MiB)\n",
      "25/05/05 11:26:07 INFO MemoryStore: Block broadcast_377_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:26:07 INFO BlockManagerInfo: Added broadcast_377_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:07 INFO SparkContext: Created broadcast 377 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:26:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 283 (MapPartitionsRDD[1520] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:26:07 INFO TaskSchedulerImpl: Adding task set 283.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:26:07 INFO TaskSetManager: Starting task 0.0 in stage 283.0 (TID 283) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:26:07 INFO Executor: Running task 0.0 in stage 283.0 (TID 283)\n",
      "25/05/05 11:26:07 INFO CodeGenerator: Code generated in 3.6825 ms\n",
      "25/05/05 11:26:07 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3325 untilOffset=3326, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=94 taskId=283 partitionId=0\n",
      "25/05/05 11:26:07 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3325 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3326, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3326, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:07 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:26:07 INFO DataWritingSparkTask: Committed partition 0 (task 281, attempt 0, stage 281.0)\n",
      "25/05/05 11:26:07 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 507426958 nanos, during time span of 507735167 nanos.\n",
      "25/05/05 11:26:07 INFO Executor: Finished task 0.0 in stage 281.0 (TID 281). 2145 bytes result sent to driver\n",
      "25/05/05 11:26:07 INFO TaskSetManager: Finished task 0.0 in stage 281.0 (TID 281) in 516 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:26:07 INFO TaskSchedulerImpl: Removed TaskSet 281.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:26:07 INFO DAGScheduler: ResultStage 281 (start at NativeMethodAccessorImpl.java:0) finished in 0.518 s\n",
      "25/05/05 11:26:07 INFO DAGScheduler: Job 282 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:26:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 281: Stage finished\n",
      "25/05/05 11:26:07 INFO DAGScheduler: Job 282 finished: start at NativeMethodAccessorImpl.java:0, took 0.518560 s\n",
      "25/05/05 11:26:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 94, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:26:07 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:26:07 INFO DataWritingSparkTask: Committed partition 0 (task 282, attempt 0, stage 282.0)\n",
      "25/05/05 11:26:07 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 506521709 nanos, during time span of 508144875 nanos.\n",
      "25/05/05 11:26:07 INFO Executor: Finished task 0.0 in stage 282.0 (TID 282). 3516 bytes result sent to driver\n",
      "25/05/05 11:26:07 INFO TaskSetManager: Finished task 0.0 in stage 282.0 (TID 282) in 516 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:26:07 INFO TaskSchedulerImpl: Removed TaskSet 282.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:26:07 INFO DAGScheduler: ResultStage 282 (start at NativeMethodAccessorImpl.java:0) finished in 0.517 s\n",
      "25/05/05 11:26:07 INFO DAGScheduler: Job 283 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:26:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 282: Stage finished\n",
      "25/05/05 11:26:07 INFO DAGScheduler: Job 283 finished: start at NativeMethodAccessorImpl.java:0, took 0.519635 s\n",
      "25/05/05 11:26:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 94, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4ed0c6cb] is committing.\n",
      "25/05/05 11:26:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 94, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4ed0c6cb] committed.\n",
      "25/05/05 11:26:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 94, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:26:07 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/94 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.94.31a27963-887f-4ed4-a2e2-70ec4c96ba3c.tmp\n",
      "25/05/05 11:26:07 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/94 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.94.1f5c88e1-0e11-4c3e-967c-67ae69d3b637.tmp\n",
      "25/05/05 11:26:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3326, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:08 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:26:08 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:26:08 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:26:08 INFO connection: Opened connection [connectionId{localValue:187, serverValue:4557}] to localhost:27017\n",
      "25/05/05 11:26:08 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=570333}\n",
      "25/05/05 11:26:08 INFO connection: Opened connection [connectionId{localValue:188, serverValue:4558}] to localhost:27017\n",
      "25/05/05 11:26:08 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:26:08 INFO connection: Closed connection [connectionId{localValue:188, serverValue:4558}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:26:08 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 505241750 nanos, during time span of 514791625 nanos.\n",
      "25/05/05 11:26:08 INFO Executor: Finished task 0.0 in stage 283.0 (TID 283). 1645 bytes result sent to driver\n",
      "25/05/05 11:26:08 INFO TaskSetManager: Finished task 0.0 in stage 283.0 (TID 283) in 524 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:26:08 INFO TaskSchedulerImpl: Removed TaskSet 283.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:26:08 INFO DAGScheduler: ResultStage 283 (start at NativeMethodAccessorImpl.java:0) finished in 0.529 s\n",
      "25/05/05 11:26:08 INFO DAGScheduler: Job 284 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:26:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 283: Stage finished\n",
      "25/05/05 11:26:08 INFO DAGScheduler: Job 284 finished: start at NativeMethodAccessorImpl.java:0, took 0.529534 s\n",
      "25/05/05 11:26:08 INFO MemoryStore: Block broadcast_378 stored as values in memory (estimated size 248.0 B, free 366.0 MiB)\n",
      "25/05/05 11:26:08 INFO MemoryStore: Block broadcast_378_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.0 MiB)\n",
      "25/05/05 11:26:08 INFO BlockManagerInfo: Added broadcast_378_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:26:08 INFO SparkContext: Created broadcast 378 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:08 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.94.31a27963-887f-4ed4-a2e2-70ec4c96ba3c.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/94\n",
      "25/05/05 11:26:08 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:26:07.392Z\",\n",
      "  \"batchId\" : 94,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.6025641025641026,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 533,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 624,\n",
      "    \"walCommit\" : 56\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3325\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3326\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3326\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.6025641025641026,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:26:08 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.94.1f5c88e1-0e11-4c3e-967c-67ae69d3b637.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/94\n",
      "25/05/05 11:26:08 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:26:07.387Z\",\n",
      "  \"batchId\" : 94,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.5797788309636651,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 538,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 633,\n",
      "    \"walCommit\" : 60\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3325\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3326\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3326\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.5797788309636651,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:26:08 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/94 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.94.ea232e25-583f-4ebb-a66b-4f3cc228f4ea.tmp\n",
      "25/05/05 11:26:08 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.94.ea232e25-583f-4ebb-a66b-4f3cc228f4ea.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/94\n",
      "25/05/05 11:26:08 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:26:07.395Z\",\n",
      "  \"batchId\" : 94,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.5503875968992247,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 559,\n",
      "    \"commitOffsets\" : 25,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 645,\n",
      "    \"walCommit\" : 55\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3325\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3326\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3326\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.5503875968992247,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 94\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|B013|R081|02-00-00|161/YANKEE STAD|05/05/2025|11:26:06|REGULAR|12         |7         |2025-05-05 11:26:07.389|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:26:13 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/95 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.95.312f33e0-fceb-4eb6-9f30-7608f2904bd2.tmp\n",
      "25/05/05 11:26:13 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/95 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.95.fbf53553-a442-4f2b-9510-846a4d35201e.tmp\n",
      "25/05/05 11:26:13 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/95 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.95.2e324e8f-b730-44d3-8212-5f0b86499fb6.tmp\n",
      "25/05/05 11:26:13 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.95.312f33e0-fceb-4eb6-9f30-7608f2904bd2.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/95\n",
      "25/05/05 11:26:13 INFO MicroBatchExecution: Committed offsets for batch 95. Metadata OffsetSeqMetadata(0,1746458773858,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:26:13 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.95.fbf53553-a442-4f2b-9510-846a4d35201e.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/95\n",
      "25/05/05 11:26:13 INFO MicroBatchExecution: Committed offsets for batch 95. Metadata OffsetSeqMetadata(0,1746458773859,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:26:13 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.95.2e324e8f-b730-44d3-8212-5f0b86499fb6.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/95\n",
      "25/05/05 11:26:13 INFO MicroBatchExecution: Committed offsets for batch 95. Metadata OffsetSeqMetadata(0,1746458773876,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:26:13 INFO IncrementalExecution: Current batch timestamp = 1746458773858\n",
      "25/05/05 11:26:13 INFO IncrementalExecution: Current batch timestamp = 1746458773859\n",
      "25/05/05 11:26:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:13 INFO IncrementalExecution: Current batch timestamp = 1746458773859\n",
      "25/05/05 11:26:13 INFO IncrementalExecution: Current batch timestamp = 1746458773876\n",
      "25/05/05 11:26:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:13 INFO IncrementalExecution: Current batch timestamp = 1746458773858\n",
      "25/05/05 11:26:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:13 INFO IncrementalExecution: Current batch timestamp = 1746458773859\n",
      "25/05/05 11:26:13 INFO IncrementalExecution: Current batch timestamp = 1746458773876\n",
      "25/05/05 11:26:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:13 INFO IncrementalExecution: Current batch timestamp = 1746458773858\n",
      "25/05/05 11:26:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:13 INFO CodeGenerator: Code generated in 3.857084 ms\n",
      "25/05/05 11:26:13 INFO CodeGenerator: Code generated in 3.849583 ms\n",
      "25/05/05 11:26:13 INFO CodeGenerator: Code generated in 4.533541 ms\n",
      "25/05/05 11:26:13 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 95, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:26:13 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:13 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 95, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@76a75ed4]. The input RDD has 1 partitions.\n",
      "25/05/05 11:26:13 INFO DAGScheduler: Got job 285 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:26:13 INFO DAGScheduler: Final stage: ResultStage 284 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:26:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:26:13 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:26:13 INFO DAGScheduler: Submitting ResultStage 284 (MapPartitionsRDD[1528] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:26:13 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:13 INFO MemoryStore: Block broadcast_379 stored as values in memory (estimated size 15.2 KiB, free 366.0 MiB)\n",
      "25/05/05 11:26:13 INFO MemoryStore: Block broadcast_379_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:26:13 INFO BlockManagerInfo: Added broadcast_379_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:13 INFO SparkContext: Created broadcast 379 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:26:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 284 (MapPartitionsRDD[1528] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:26:13 INFO TaskSchedulerImpl: Adding task set 284.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:26:13 INFO DAGScheduler: Got job 286 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:26:13 INFO DAGScheduler: Final stage: ResultStage 285 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:26:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:26:13 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:26:13 INFO DAGScheduler: Submitting ResultStage 285 (MapPartitionsRDD[1531] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:26:13 INFO TaskSetManager: Starting task 0.0 in stage 284.0 (TID 284) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:26:13 INFO Executor: Running task 0.0 in stage 284.0 (TID 284)\n",
      "25/05/05 11:26:13 INFO MemoryStore: Block broadcast_380 stored as values in memory (estimated size 16.2 KiB, free 365.9 MiB)\n",
      "25/05/05 11:26:13 INFO MemoryStore: Block broadcast_380_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 365.9 MiB)\n",
      "25/05/05 11:26:13 INFO BlockManagerInfo: Added broadcast_380_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:13 INFO SparkContext: Created broadcast 380 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:26:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 285 (MapPartitionsRDD[1531] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:26:13 INFO TaskSchedulerImpl: Adding task set 285.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:26:13 INFO TaskSetManager: Starting task 0.0 in stage 285.0 (TID 285) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:26:13 INFO Executor: Running task 0.0 in stage 285.0 (TID 285)\n",
      "25/05/05 11:26:13 INFO CodeGenerator: Code generated in 4.28375 ms\n",
      "25/05/05 11:26:13 INFO CodeGenerator: Code generated in 3.520875 ms\n",
      "25/05/05 11:26:13 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3326 untilOffset=3327, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=95 taskId=284 partitionId=0\n",
      "25/05/05 11:26:13 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3326 untilOffset=3327, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=95 taskId=285 partitionId=0\n",
      "25/05/05 11:26:13 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3326 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:13 INFO BlockManagerInfo: Removed broadcast_375_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:13 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3326 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:13 INFO BlockManagerInfo: Removed broadcast_374_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:26:13 INFO BlockManagerInfo: Removed broadcast_377_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:13 INFO BlockManagerInfo: Removed broadcast_372_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:13 INFO BlockManagerInfo: Removed broadcast_373_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:13 INFO BlockManagerInfo: Removed broadcast_378_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:26:13 INFO BlockManagerInfo: Removed broadcast_371_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:26:13 INFO BlockManagerInfo: Removed broadcast_376_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:26:13 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:13 INFO DAGScheduler: Got job 287 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:26:13 INFO DAGScheduler: Final stage: ResultStage 286 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:26:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:26:13 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:26:13 INFO DAGScheduler: Submitting ResultStage 286 (MapPartitionsRDD[1536] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:26:13 INFO MemoryStore: Block broadcast_381 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:26:13 INFO MemoryStore: Block broadcast_381_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:26:13 INFO BlockManagerInfo: Added broadcast_381_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:13 INFO SparkContext: Created broadcast 381 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:26:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 286 (MapPartitionsRDD[1536] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:26:13 INFO TaskSchedulerImpl: Adding task set 286.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:26:13 INFO TaskSetManager: Starting task 0.0 in stage 286.0 (TID 286) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:26:13 INFO Executor: Running task 0.0 in stage 286.0 (TID 286)\n",
      "25/05/05 11:26:13 INFO CodeGenerator: Code generated in 3.649208 ms\n",
      "25/05/05 11:26:13 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3326 untilOffset=3327, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=95 taskId=286 partitionId=0\n",
      "25/05/05 11:26:13 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3326 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3327, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3327, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:14 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:26:14 INFO DataWritingSparkTask: Committed partition 0 (task 284, attempt 0, stage 284.0)\n",
      "25/05/05 11:26:14 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 506295209 nanos, during time span of 506644416 nanos.\n",
      "25/05/05 11:26:14 INFO Executor: Finished task 0.0 in stage 284.0 (TID 284). 2188 bytes result sent to driver\n",
      "25/05/05 11:26:14 INFO TaskSetManager: Finished task 0.0 in stage 284.0 (TID 284) in 523 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:26:14 INFO TaskSchedulerImpl: Removed TaskSet 284.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:26:14 INFO DAGScheduler: ResultStage 284 (start at NativeMethodAccessorImpl.java:0) finished in 0.525 s\n",
      "25/05/05 11:26:14 INFO DAGScheduler: Job 285 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:26:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 284: Stage finished\n",
      "25/05/05 11:26:14 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:26:14 INFO DataWritingSparkTask: Committed partition 0 (task 285, attempt 0, stage 285.0)\n",
      "25/05/05 11:26:14 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 504685042 nanos, during time span of 506498750 nanos.\n",
      "25/05/05 11:26:14 INFO DAGScheduler: Job 285 finished: start at NativeMethodAccessorImpl.java:0, took 0.525594 s\n",
      "25/05/05 11:26:14 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 95, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:26:14 INFO Executor: Finished task 0.0 in stage 285.0 (TID 285). 3559 bytes result sent to driver\n",
      "25/05/05 11:26:14 INFO TaskSetManager: Finished task 0.0 in stage 285.0 (TID 285) in 523 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:26:14 INFO TaskSchedulerImpl: Removed TaskSet 285.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:26:14 INFO DAGScheduler: ResultStage 285 (start at NativeMethodAccessorImpl.java:0) finished in 0.526 s\n",
      "25/05/05 11:26:14 INFO DAGScheduler: Job 286 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:26:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 285: Stage finished\n",
      "25/05/05 11:26:14 INFO DAGScheduler: Job 286 finished: start at NativeMethodAccessorImpl.java:0, took 0.527052 s\n",
      "25/05/05 11:26:14 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 95, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@76a75ed4] is committing.\n",
      "25/05/05 11:26:14 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 95, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@76a75ed4] committed.\n",
      "25/05/05 11:26:14 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 95, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:26:14 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/95 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.95.3d1a9b81-0676-4e61-b815-82669b9bc2bc.tmp\n",
      "25/05/05 11:26:14 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/95 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.95.758a349d-bd02-42ba-83e2-bb2a162adae3.tmp\n",
      "25/05/05 11:26:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3327, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:14 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:26:14 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:26:14 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:26:14 INFO connection: Opened connection [connectionId{localValue:189, serverValue:4559}] to localhost:27017\n",
      "25/05/05 11:26:14 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=553875}\n",
      "25/05/05 11:26:14 INFO connection: Opened connection [connectionId{localValue:190, serverValue:4560}] to localhost:27017\n",
      "25/05/05 11:26:14 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:26:14 INFO connection: Closed connection [connectionId{localValue:190, serverValue:4560}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:26:14 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 503943875 nanos, during time span of 519069584 nanos.\n",
      "25/05/05 11:26:14 INFO Executor: Finished task 0.0 in stage 286.0 (TID 286). 1645 bytes result sent to driver\n",
      "25/05/05 11:26:14 INFO TaskSetManager: Finished task 0.0 in stage 286.0 (TID 286) in 529 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:26:14 INFO TaskSchedulerImpl: Removed TaskSet 286.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:26:14 INFO DAGScheduler: ResultStage 286 (start at NativeMethodAccessorImpl.java:0) finished in 0.533 s\n",
      "25/05/05 11:26:14 INFO DAGScheduler: Job 287 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:26:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 286: Stage finished\n",
      "25/05/05 11:26:14 INFO DAGScheduler: Job 287 finished: start at NativeMethodAccessorImpl.java:0, took 0.533775 s\n",
      "25/05/05 11:26:14 INFO MemoryStore: Block broadcast_382 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:26:14 INFO MemoryStore: Block broadcast_382_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:26:14 INFO BlockManagerInfo: Added broadcast_382_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:26:14 INFO SparkContext: Created broadcast 382 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:14 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.95.3d1a9b81-0676-4e61-b815-82669b9bc2bc.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/95\n",
      "25/05/05 11:26:14 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:26:13.855Z\",\n",
      "  \"batchId\" : 95,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 62.5,\n",
      "  \"processedRowsPerSecond\" : 1.5600624024960998,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 540,\n",
      "    \"commitOffsets\" : 43,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 641,\n",
      "    \"walCommit\" : 51\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3326\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3327\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3327\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 62.5,\n",
      "    \"processedRowsPerSecond\" : 1.5600624024960998,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:26:14 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/95 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.95.c082071d-0d54-4977-b844-c20dac5328a2.tmp\n",
      "25/05/05 11:26:14 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.95.758a349d-bd02-42ba-83e2-bb2a162adae3.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/95\n",
      "25/05/05 11:26:14 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:26:13.855Z\",\n",
      "  \"batchId\" : 95,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 62.5,\n",
      "  \"processedRowsPerSecond\" : 1.5455950540958268,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 546,\n",
      "    \"commitOffsets\" : 43,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 4,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 647,\n",
      "    \"walCommit\" : 50\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3326\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3327\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3327\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 62.5,\n",
      "    \"processedRowsPerSecond\" : 1.5455950540958268,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:26:14 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.95.c082071d-0d54-4977-b844-c20dac5328a2.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/95\n",
      "25/05/05 11:26:14 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:26:13.869Z\",\n",
      "  \"batchId\" : 95,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "  \"processedRowsPerSecond\" : 1.5432098765432098,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 573,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 6,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 648,\n",
      "    \"walCommit\" : 36\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3326\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3327\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3327\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "    \"processedRowsPerSecond\" : 1.5432098765432098,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 95\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R023|R045|00-00-00|34 ST-HERALD SQ|05/05/2025|11:26:12|REGULAR|7          |8         |2025-05-05 11:26:13.859|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:26:20 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/96 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.96.fb730b55-7a5a-48c6-8496-93adf135b5b6.tmp\n",
      "25/05/05 11:26:20 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/96 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.96.bc2e7232-9d94-4f8a-a57f-3b11b9045659.tmp\n",
      "25/05/05 11:26:20 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/96 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.96.33373f9a-1cde-4348-a7ac-ccc4cc878c31.tmp\n",
      "25/05/05 11:26:20 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.96.bc2e7232-9d94-4f8a-a57f-3b11b9045659.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/96\n",
      "25/05/05 11:26:20 INFO MicroBatchExecution: Committed offsets for batch 96. Metadata OffsetSeqMetadata(0,1746458780342,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:26:20 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.96.fb730b55-7a5a-48c6-8496-93adf135b5b6.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/96\n",
      "25/05/05 11:26:20 INFO MicroBatchExecution: Committed offsets for batch 96. Metadata OffsetSeqMetadata(0,1746458780341,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:26:20 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.96.33373f9a-1cde-4348-a7ac-ccc4cc878c31.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/96\n",
      "25/05/05 11:26:20 INFO MicroBatchExecution: Committed offsets for batch 96. Metadata OffsetSeqMetadata(0,1746458780341,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:26:20 INFO IncrementalExecution: Current batch timestamp = 1746458780341\n",
      "25/05/05 11:26:20 INFO IncrementalExecution: Current batch timestamp = 1746458780342\n",
      "25/05/05 11:26:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:20 INFO IncrementalExecution: Current batch timestamp = 1746458780341\n",
      "25/05/05 11:26:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:20 INFO IncrementalExecution: Current batch timestamp = 1746458780342\n",
      "25/05/05 11:26:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:20 INFO IncrementalExecution: Current batch timestamp = 1746458780341\n",
      "25/05/05 11:26:20 INFO IncrementalExecution: Current batch timestamp = 1746458780341\n",
      "25/05/05 11:26:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:20 INFO IncrementalExecution: Current batch timestamp = 1746458780342\n",
      "25/05/05 11:26:20 INFO IncrementalExecution: Current batch timestamp = 1746458780341\n",
      "25/05/05 11:26:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:20 INFO CodeGenerator: Code generated in 4.992583 ms\n",
      "25/05/05 11:26:20 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 96, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@478edf56]. The input RDD has 1 partitions.\n",
      "25/05/05 11:26:20 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:20 INFO DAGScheduler: Got job 288 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:26:20 INFO DAGScheduler: Final stage: ResultStage 287 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:26:20 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:26:20 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:26:20 INFO DAGScheduler: Submitting ResultStage 287 (MapPartitionsRDD[1544] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:26:20 INFO MemoryStore: Block broadcast_383 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:26:20 INFO MemoryStore: Block broadcast_383_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.1 MiB)\n",
      "25/05/05 11:26:20 INFO BlockManagerInfo: Added broadcast_383_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:20 INFO SparkContext: Created broadcast 383 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:26:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 287 (MapPartitionsRDD[1544] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:26:20 INFO TaskSchedulerImpl: Adding task set 287.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:26:20 INFO CodeGenerator: Code generated in 4.517292 ms\n",
      "25/05/05 11:26:20 INFO TaskSetManager: Starting task 0.0 in stage 287.0 (TID 287) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:26:20 INFO Executor: Running task 0.0 in stage 287.0 (TID 287)\n",
      "25/05/05 11:26:20 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 96, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:26:20 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:20 INFO DAGScheduler: Got job 289 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:26:20 INFO DAGScheduler: Final stage: ResultStage 288 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:26:20 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:26:20 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:26:20 INFO DAGScheduler: Submitting ResultStage 288 (MapPartitionsRDD[1547] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:26:20 INFO MemoryStore: Block broadcast_384 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:26:20 INFO MemoryStore: Block broadcast_384_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:26:20 INFO BlockManagerInfo: Added broadcast_384_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:20 INFO SparkContext: Created broadcast 384 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:26:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 288 (MapPartitionsRDD[1547] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:26:20 INFO TaskSchedulerImpl: Adding task set 288.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:26:20 INFO TaskSetManager: Starting task 0.0 in stage 288.0 (TID 288) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:26:20 INFO Executor: Running task 0.0 in stage 288.0 (TID 288)\n",
      "25/05/05 11:26:20 INFO CodeGenerator: Code generated in 4.090041 ms\n",
      "25/05/05 11:26:20 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3327 untilOffset=3328, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=96 taskId=287 partitionId=0\n",
      "25/05/05 11:26:20 INFO CodeGenerator: Code generated in 3.825083 ms\n",
      "25/05/05 11:26:20 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3327 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:20 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3327 untilOffset=3328, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=96 taskId=288 partitionId=0\n",
      "25/05/05 11:26:20 INFO NetworkClient: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Node -1 disconnected.\n",
      "25/05/05 11:26:20 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3327 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:20 INFO NetworkClient: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Node -1 disconnected.\n",
      "25/05/05 11:26:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:20 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:20 INFO DAGScheduler: Got job 290 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:26:20 INFO DAGScheduler: Final stage: ResultStage 289 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:26:20 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:26:20 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:26:20 INFO DAGScheduler: Submitting ResultStage 289 (MapPartitionsRDD[1552] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:26:20 INFO MemoryStore: Block broadcast_385 stored as values in memory (estimated size 47.1 KiB, free 366.0 MiB)\n",
      "25/05/05 11:26:20 INFO MemoryStore: Block broadcast_385_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.0 MiB)\n",
      "25/05/05 11:26:20 INFO BlockManagerInfo: Added broadcast_385_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:20 INFO SparkContext: Created broadcast 385 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:26:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 289 (MapPartitionsRDD[1552] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:26:20 INFO TaskSchedulerImpl: Adding task set 289.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:26:20 INFO TaskSetManager: Starting task 0.0 in stage 289.0 (TID 289) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:26:20 INFO Executor: Running task 0.0 in stage 289.0 (TID 289)\n",
      "25/05/05 11:26:20 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3327 untilOffset=3328, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=96 taskId=289 partitionId=0\n",
      "25/05/05 11:26:20 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3327 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:20 INFO NetworkClient: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Node -1 disconnected.\n",
      "25/05/05 11:26:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3328, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3328, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:20 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:26:20 INFO DataWritingSparkTask: Committed partition 0 (task 288, attempt 0, stage 288.0)\n",
      "25/05/05 11:26:20 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 508581583 nanos, during time span of 508942583 nanos.\n",
      "25/05/05 11:26:20 INFO Executor: Finished task 0.0 in stage 288.0 (TID 288). 2145 bytes result sent to driver\n",
      "25/05/05 11:26:20 INFO TaskSetManager: Finished task 0.0 in stage 288.0 (TID 288) in 517 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:26:20 INFO TaskSchedulerImpl: Removed TaskSet 288.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:26:20 INFO DAGScheduler: ResultStage 288 (start at NativeMethodAccessorImpl.java:0) finished in 0.519 s\n",
      "25/05/05 11:26:20 INFO DAGScheduler: Job 289 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:26:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 288: Stage finished\n",
      "25/05/05 11:26:20 INFO DAGScheduler: Job 289 finished: start at NativeMethodAccessorImpl.java:0, took 0.519506 s\n",
      "25/05/05 11:26:20 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:26:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 96, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:26:20 INFO DataWritingSparkTask: Committed partition 0 (task 287, attempt 0, stage 287.0)\n",
      "25/05/05 11:26:20 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 509872958 nanos, during time span of 511800500 nanos.\n",
      "25/05/05 11:26:20 INFO Executor: Finished task 0.0 in stage 287.0 (TID 287). 3510 bytes result sent to driver\n",
      "25/05/05 11:26:20 INFO TaskSetManager: Finished task 0.0 in stage 287.0 (TID 287) in 522 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:26:20 INFO TaskSchedulerImpl: Removed TaskSet 287.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:26:20 INFO DAGScheduler: ResultStage 287 (start at NativeMethodAccessorImpl.java:0) finished in 0.525 s\n",
      "25/05/05 11:26:20 INFO DAGScheduler: Job 288 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:26:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 287: Stage finished\n",
      "25/05/05 11:26:20 INFO DAGScheduler: Job 288 finished: start at NativeMethodAccessorImpl.java:0, took 0.524992 s\n",
      "25/05/05 11:26:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 96, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@478edf56] is committing.\n",
      "25/05/05 11:26:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 96, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@478edf56] committed.\n",
      "25/05/05 11:26:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 96, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:26:20 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/96 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.96.5dd35d34-9443-40da-b2c3-f327ffb44b6b.tmp\n",
      "25/05/05 11:26:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3328, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:20 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:26:20 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:26:20 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:26:20 INFO connection: Opened connection [connectionId{localValue:191, serverValue:4561}] to localhost:27017\n",
      "25/05/05 11:26:20 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=239334}\n",
      "25/05/05 11:26:20 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/96 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.96.ce63be1e-20e0-48e0-a8d8-c515dd8885be.tmp\n",
      "25/05/05 11:26:20 INFO connection: Opened connection [connectionId{localValue:192, serverValue:4562}] to localhost:27017\n",
      "25/05/05 11:26:20 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:26:20 INFO connection: Closed connection [connectionId{localValue:192, serverValue:4562}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:26:20 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 505094166 nanos, during time span of 511028500 nanos.\n",
      "25/05/05 11:26:20 INFO Executor: Finished task 0.0 in stage 289.0 (TID 289). 1645 bytes result sent to driver\n",
      "25/05/05 11:26:20 INFO TaskSetManager: Finished task 0.0 in stage 289.0 (TID 289) in 517 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:26:20 INFO TaskSchedulerImpl: Removed TaskSet 289.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:26:20 INFO DAGScheduler: ResultStage 289 (start at NativeMethodAccessorImpl.java:0) finished in 0.521 s\n",
      "25/05/05 11:26:20 INFO DAGScheduler: Job 290 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:26:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 289: Stage finished\n",
      "25/05/05 11:26:20 INFO DAGScheduler: Job 290 finished: start at NativeMethodAccessorImpl.java:0, took 0.522225 s\n",
      "25/05/05 11:26:20 INFO MemoryStore: Block broadcast_386 stored as values in memory (estimated size 248.0 B, free 366.0 MiB)\n",
      "25/05/05 11:26:20 INFO MemoryStore: Block broadcast_386_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.0 MiB)\n",
      "25/05/05 11:26:20 INFO BlockManagerInfo: Added broadcast_386_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:26:20 INFO SparkContext: Created broadcast 386 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:20 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/96 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.96.94f92697-c251-4ea5-bb13-efba06858300.tmp\n",
      "25/05/05 11:26:20 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.96.5dd35d34-9443-40da-b2c3-f327ffb44b6b.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/96\n",
      "25/05/05 11:26:20 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:26:20.336Z\",\n",
      "  \"batchId\" : 96,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 62.5,\n",
      "  \"processedRowsPerSecond\" : 1.5923566878980893,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 536,\n",
      "    \"commitOffsets\" : 32,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 5,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 628,\n",
      "    \"walCommit\" : 52\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3327\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3328\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3328\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 62.5,\n",
      "    \"processedRowsPerSecond\" : 1.5923566878980893,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:26:20 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.96.ce63be1e-20e0-48e0-a8d8-c515dd8885be.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/96\n",
      "25/05/05 11:26:20 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:26:20.335Z\",\n",
      "  \"batchId\" : 96,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.5797788309636651,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 542,\n",
      "    \"commitOffsets\" : 31,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 7,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 633,\n",
      "    \"walCommit\" : 49\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3327\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3328\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3328\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.5797788309636651,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:26:20 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.96.94f92697-c251-4ea5-bb13-efba06858300.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/96\n",
      "25/05/05 11:26:20 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:26:20.335Z\",\n",
      "  \"batchId\" : 96,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 62.5,\n",
      "  \"processedRowsPerSecond\" : 1.557632398753894,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 553,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 6,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 642,\n",
      "    \"walCommit\" : 50\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3327\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3328\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3328\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 62.5,\n",
      "    \"processedRowsPerSecond\" : 1.557632398753894,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 96\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION  |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A002|R170|00-00-00|FULTON ST|05/05/2025|11:26:19|REGULAR|9          |7         |2025-05-05 11:26:20.342|\n",
      "+----+----+--------+---------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:26:25 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/97 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.97.c61edbfc-a8f7-4360-8059-a676ecfee654.tmp\n",
      "25/05/05 11:26:25 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/97 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.97.ce3f5b97-cb74-4597-b672-4c390c3a3adf.tmp\n",
      "25/05/05 11:26:25 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/97 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.97.f3366de2-ae54-4541-9db3-2c737809ec4a.tmp\n",
      "25/05/05 11:26:25 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.97.c61edbfc-a8f7-4360-8059-a676ecfee654.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/97\n",
      "25/05/05 11:26:25 INFO MicroBatchExecution: Committed offsets for batch 97. Metadata OffsetSeqMetadata(0,1746458785887,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:26:25 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.97.f3366de2-ae54-4541-9db3-2c737809ec4a.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/97\n",
      "25/05/05 11:26:25 INFO MicroBatchExecution: Committed offsets for batch 97. Metadata OffsetSeqMetadata(0,1746458785888,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:26:25 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.97.ce3f5b97-cb74-4597-b672-4c390c3a3adf.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/97\n",
      "25/05/05 11:26:25 INFO MicroBatchExecution: Committed offsets for batch 97. Metadata OffsetSeqMetadata(0,1746458785888,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:26:25 INFO IncrementalExecution: Current batch timestamp = 1746458785888\n",
      "25/05/05 11:26:25 INFO IncrementalExecution: Current batch timestamp = 1746458785887\n",
      "25/05/05 11:26:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:25 INFO IncrementalExecution: Current batch timestamp = 1746458785888\n",
      "25/05/05 11:26:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:25 INFO IncrementalExecution: Current batch timestamp = 1746458785888\n",
      "25/05/05 11:26:25 INFO IncrementalExecution: Current batch timestamp = 1746458785887\n",
      "25/05/05 11:26:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:25 INFO IncrementalExecution: Current batch timestamp = 1746458785888\n",
      "25/05/05 11:26:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:25 INFO IncrementalExecution: Current batch timestamp = 1746458785888\n",
      "25/05/05 11:26:25 INFO IncrementalExecution: Current batch timestamp = 1746458785887\n",
      "25/05/05 11:26:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:25 INFO CodeGenerator: Code generated in 14.314167 ms\n",
      "25/05/05 11:26:25 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 97, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:26:25 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:25 INFO DAGScheduler: Got job 291 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:26:25 INFO DAGScheduler: Final stage: ResultStage 290 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:26:25 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:26:25 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:26:25 INFO DAGScheduler: Submitting ResultStage 290 (MapPartitionsRDD[1560] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:26:25 INFO CodeGenerator: Code generated in 17.963791 ms\n",
      "25/05/05 11:26:25 INFO MemoryStore: Block broadcast_387 stored as values in memory (estimated size 15.2 KiB, free 366.0 MiB)\n",
      "25/05/05 11:26:25 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 97, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@80fec12]. The input RDD has 1 partitions.\n",
      "25/05/05 11:26:25 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:25 INFO MemoryStore: Block broadcast_387_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:26:25 INFO BlockManagerInfo: Added broadcast_387_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:25 INFO SparkContext: Created broadcast 387 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:26:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 290 (MapPartitionsRDD[1560] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:26:25 INFO TaskSchedulerImpl: Adding task set 290.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:26:25 INFO DAGScheduler: Got job 292 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:26:25 INFO DAGScheduler: Final stage: ResultStage 291 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:26:25 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:26:25 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:26:25 INFO DAGScheduler: Submitting ResultStage 291 (MapPartitionsRDD[1563] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:26:25 INFO TaskSetManager: Starting task 0.0 in stage 290.0 (TID 290) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:26:25 INFO Executor: Running task 0.0 in stage 290.0 (TID 290)\n",
      "25/05/05 11:26:25 INFO MemoryStore: Block broadcast_388 stored as values in memory (estimated size 16.2 KiB, free 365.9 MiB)\n",
      "25/05/05 11:26:25 INFO MemoryStore: Block broadcast_388_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 365.9 MiB)\n",
      "25/05/05 11:26:25 INFO BlockManagerInfo: Added broadcast_388_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:25 INFO SparkContext: Created broadcast 388 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:26:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 291 (MapPartitionsRDD[1563] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:26:25 INFO TaskSchedulerImpl: Adding task set 291.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:26:25 INFO TaskSetManager: Starting task 0.0 in stage 291.0 (TID 291) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:26:25 INFO Executor: Running task 0.0 in stage 291.0 (TID 291)\n",
      "25/05/05 11:26:25 INFO CodeGenerator: Code generated in 4.505833 ms\n",
      "25/05/05 11:26:25 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3328 untilOffset=3329, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=97 taskId=290 partitionId=0\n",
      "25/05/05 11:26:25 INFO CodeGenerator: Code generated in 4.443792 ms\n",
      "25/05/05 11:26:25 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3328 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:26 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3328 untilOffset=3329, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=97 taskId=291 partitionId=0\n",
      "25/05/05 11:26:26 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3328 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:26 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:26 INFO DAGScheduler: Got job 293 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:26:26 INFO DAGScheduler: Final stage: ResultStage 292 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:26:26 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:26:26 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:26:26 INFO DAGScheduler: Submitting ResultStage 292 (MapPartitionsRDD[1568] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:26:26 INFO MemoryStore: Block broadcast_389 stored as values in memory (estimated size 47.1 KiB, free 365.9 MiB)\n",
      "25/05/05 11:26:26 INFO MemoryStore: Block broadcast_389_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 365.9 MiB)\n",
      "25/05/05 11:26:26 INFO BlockManagerInfo: Added broadcast_389_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:26 INFO SparkContext: Created broadcast 389 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:26:26 INFO BlockManagerInfo: Removed broadcast_379_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 292 (MapPartitionsRDD[1568] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:26:26 INFO TaskSchedulerImpl: Adding task set 292.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:26:26 INFO TaskSetManager: Starting task 0.0 in stage 292.0 (TID 292) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:26:26 INFO Executor: Running task 0.0 in stage 292.0 (TID 292)\n",
      "25/05/05 11:26:26 INFO BlockManagerInfo: Removed broadcast_386_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:26:26 INFO BlockManagerInfo: Removed broadcast_381_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:26 INFO BlockManagerInfo: Removed broadcast_383_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:26 INFO BlockManagerInfo: Removed broadcast_385_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:26 INFO BlockManagerInfo: Removed broadcast_384_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:26 INFO BlockManagerInfo: Removed broadcast_382_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:26:26 INFO BlockManagerInfo: Removed broadcast_380_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:26 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3328 untilOffset=3329, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=97 taskId=292 partitionId=0\n",
      "25/05/05 11:26:26 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3328 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3329, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3329, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:26 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:26:26 INFO DataWritingSparkTask: Committed partition 0 (task 290, attempt 0, stage 290.0)\n",
      "25/05/05 11:26:26 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 507919334 nanos, during time span of 508149667 nanos.\n",
      "25/05/05 11:26:26 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:26:26 INFO DataWritingSparkTask: Committed partition 0 (task 291, attempt 0, stage 291.0)\n",
      "25/05/05 11:26:26 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 503678166 nanos, during time span of 505265666 nanos.\n",
      "25/05/05 11:26:26 INFO Executor: Finished task 0.0 in stage 290.0 (TID 290). 2231 bytes result sent to driver\n",
      "25/05/05 11:26:26 INFO Executor: Finished task 0.0 in stage 291.0 (TID 291). 3601 bytes result sent to driver\n",
      "25/05/05 11:26:26 INFO TaskSetManager: Finished task 0.0 in stage 290.0 (TID 290) in 518 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:26:26 INFO TaskSchedulerImpl: Removed TaskSet 290.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:26:26 INFO TaskSetManager: Finished task 0.0 in stage 291.0 (TID 291) in 516 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:26:26 INFO TaskSchedulerImpl: Removed TaskSet 291.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:26:26 INFO DAGScheduler: ResultStage 290 (start at NativeMethodAccessorImpl.java:0) finished in 0.522 s\n",
      "25/05/05 11:26:26 INFO DAGScheduler: Job 291 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:26:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 290: Stage finished\n",
      "25/05/05 11:26:26 INFO DAGScheduler: Job 291 finished: start at NativeMethodAccessorImpl.java:0, took 0.523352 s\n",
      "25/05/05 11:26:26 INFO DAGScheduler: ResultStage 291 (start at NativeMethodAccessorImpl.java:0) finished in 0.518 s\n",
      "25/05/05 11:26:26 INFO DAGScheduler: Job 292 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:26:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 291: Stage finished\n",
      "25/05/05 11:26:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 97, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:26:26 INFO DAGScheduler: Job 292 finished: start at NativeMethodAccessorImpl.java:0, took 0.519649 s\n",
      "25/05/05 11:26:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 97, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@80fec12] is committing.\n",
      "25/05/05 11:26:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 97, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@80fec12] committed.\n",
      "25/05/05 11:26:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 97, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:26:26 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/97 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.97.219c23b0-de7a-449d-a156-0377e1c57650.tmp\n",
      "25/05/05 11:26:26 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/97 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.97.77905d5a-4124-4cad-8230-023dba6d13d1.tmp\n",
      "25/05/05 11:26:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3329, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:26 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:26:26 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:26:26 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:26:26 INFO connection: Opened connection [connectionId{localValue:193, serverValue:4563}] to localhost:27017\n",
      "25/05/05 11:26:26 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=190458}\n",
      "25/05/05 11:26:26 INFO connection: Opened connection [connectionId{localValue:194, serverValue:4564}] to localhost:27017\n",
      "25/05/05 11:26:26 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:26:26 INFO connection: Closed connection [connectionId{localValue:194, serverValue:4564}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:26:26 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 503355542 nanos, during time span of 507450125 nanos.\n",
      "25/05/05 11:26:26 INFO Executor: Finished task 0.0 in stage 292.0 (TID 292). 1645 bytes result sent to driver\n",
      "25/05/05 11:26:26 INFO TaskSetManager: Finished task 0.0 in stage 292.0 (TID 292) in 514 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:26:26 INFO TaskSchedulerImpl: Removed TaskSet 292.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:26:26 INFO DAGScheduler: ResultStage 292 (start at NativeMethodAccessorImpl.java:0) finished in 0.523 s\n",
      "25/05/05 11:26:26 INFO DAGScheduler: Job 293 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:26:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 292: Stage finished\n",
      "25/05/05 11:26:26 INFO DAGScheduler: Job 293 finished: start at NativeMethodAccessorImpl.java:0, took 0.523375 s\n",
      "25/05/05 11:26:26 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.97.219c23b0-de7a-449d-a156-0377e1c57650.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/97\n",
      "25/05/05 11:26:26 INFO MemoryStore: Block broadcast_390 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:26:26 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:26:25.886Z\",\n",
      "  \"batchId\" : 97,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.5384615384615383,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 558,\n",
      "    \"commitOffsets\" : 26,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 649,\n",
      "    \"walCommit\" : 59\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3328\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3329\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3329\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.5384615384615383,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:26:26 INFO MemoryStore: Block broadcast_390_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:26:26 INFO BlockManagerInfo: Added broadcast_390_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:26:26 INFO SparkContext: Created broadcast 390 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:26 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.97.77905d5a-4124-4cad-8230-023dba6d13d1.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/97\n",
      "25/05/05 11:26:26 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:26:25.886Z\",\n",
      "  \"batchId\" : 97,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.5267175572519083,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 564,\n",
      "    \"commitOffsets\" : 26,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 655,\n",
      "    \"walCommit\" : 59\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3328\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3329\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3329\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.5267175572519083,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:26:26 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/97 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.97.e166a81d-32af-4d9d-bb61-1794fe20566e.tmp\n",
      "25/05/05 11:26:26 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.97.e166a81d-32af-4d9d-bb61-1794fe20566e.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/97\n",
      "25/05/05 11:26:26 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:26:25.886Z\",\n",
      "  \"batchId\" : 97,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.4814814814814814,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 584,\n",
      "    \"commitOffsets\" : 25,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 675,\n",
      "    \"walCommit\" : 59\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3328\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3329\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3329\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.4814814814814814,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 97\n",
      "-------------------------------------------\n",
      "+----+----+--------+--------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION       |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+--------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A002|R051|02-00-00|34 ST-PENN STA|05/05/2025|11:26:24|REGULAR|12         |3         |2025-05-05 11:26:25.888|\n",
      "+----+----+--------+--------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:26:30 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/98 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.98.30e0f20f-169c-4e5f-9030-301d7925ac44.tmp\n",
      "25/05/05 11:26:30 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/98 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.98.8bed9c2c-f740-4e6b-8b1b-d1ce54693d7d.tmp\n",
      "25/05/05 11:26:30 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/98 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.98.6caed8aa-0321-4e23-ac90-2b282bc8aa44.tmp\n",
      "25/05/05 11:26:30 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.98.30e0f20f-169c-4e5f-9030-301d7925ac44.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/98\n",
      "25/05/05 11:26:30 INFO MicroBatchExecution: Committed offsets for batch 98. Metadata OffsetSeqMetadata(0,1746458790315,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:26:30 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.98.8bed9c2c-f740-4e6b-8b1b-d1ce54693d7d.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/98\n",
      "25/05/05 11:26:30 INFO MicroBatchExecution: Committed offsets for batch 98. Metadata OffsetSeqMetadata(0,1746458790324,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:26:30 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.98.6caed8aa-0321-4e23-ac90-2b282bc8aa44.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/98\n",
      "25/05/05 11:26:30 INFO MicroBatchExecution: Committed offsets for batch 98. Metadata OffsetSeqMetadata(0,1746458790324,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:26:30 INFO IncrementalExecution: Current batch timestamp = 1746458790315\n",
      "25/05/05 11:26:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:30 INFO IncrementalExecution: Current batch timestamp = 1746458790324\n",
      "25/05/05 11:26:30 INFO IncrementalExecution: Current batch timestamp = 1746458790324\n",
      "25/05/05 11:26:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:30 INFO IncrementalExecution: Current batch timestamp = 1746458790315\n",
      "25/05/05 11:26:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:30 INFO IncrementalExecution: Current batch timestamp = 1746458790324\n",
      "25/05/05 11:26:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:30 INFO IncrementalExecution: Current batch timestamp = 1746458790324\n",
      "25/05/05 11:26:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:30 INFO IncrementalExecution: Current batch timestamp = 1746458790324\n",
      "25/05/05 11:26:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:30 INFO IncrementalExecution: Current batch timestamp = 1746458790324\n",
      "25/05/05 11:26:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:30 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:30 INFO CodeGenerator: Code generated in 4.031667 ms\n",
      "25/05/05 11:26:30 INFO CodeGenerator: Code generated in 3.273625 ms\n",
      "25/05/05 11:26:30 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 98, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:26:30 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 98, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@3a9a0cdc]. The input RDD has 1 partitions.\n",
      "25/05/05 11:26:30 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:30 INFO DAGScheduler: Got job 294 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:26:30 INFO DAGScheduler: Final stage: ResultStage 293 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:26:30 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:26:30 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:30 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:26:30 INFO DAGScheduler: Submitting ResultStage 293 (MapPartitionsRDD[1578] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:26:30 INFO MemoryStore: Block broadcast_391 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:26:30 INFO MemoryStore: Block broadcast_391_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:26:30 INFO BlockManagerInfo: Added broadcast_391_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:30 INFO SparkContext: Created broadcast 391 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:26:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 293 (MapPartitionsRDD[1578] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:26:30 INFO TaskSchedulerImpl: Adding task set 293.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:26:30 INFO DAGScheduler: Got job 295 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:26:30 INFO DAGScheduler: Final stage: ResultStage 294 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:26:30 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:26:30 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:26:30 INFO DAGScheduler: Submitting ResultStage 294 (MapPartitionsRDD[1579] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:26:30 INFO TaskSetManager: Starting task 0.0 in stage 293.0 (TID 293) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:26:30 INFO MemoryStore: Block broadcast_392 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:26:30 INFO Executor: Running task 0.0 in stage 293.0 (TID 293)\n",
      "25/05/05 11:26:30 INFO MemoryStore: Block broadcast_392_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)\n",
      "25/05/05 11:26:30 INFO BlockManagerInfo: Added broadcast_392_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:30 INFO SparkContext: Created broadcast 392 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:26:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 294 (MapPartitionsRDD[1579] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:26:30 INFO TaskSchedulerImpl: Adding task set 294.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:26:30 INFO TaskSetManager: Starting task 0.0 in stage 294.0 (TID 294) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:26:30 INFO Executor: Running task 0.0 in stage 294.0 (TID 294)\n",
      "25/05/05 11:26:30 INFO CodeGenerator: Code generated in 3.955375 ms\n",
      "25/05/05 11:26:30 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3329 untilOffset=3330, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=98 taskId=293 partitionId=0\n",
      "25/05/05 11:26:30 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3329 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:30 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3329 untilOffset=3330, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=98 taskId=294 partitionId=0\n",
      "25/05/05 11:26:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:30 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3329 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:30 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:30 INFO DAGScheduler: Got job 296 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:26:30 INFO DAGScheduler: Final stage: ResultStage 295 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:26:30 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:26:30 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:26:30 INFO DAGScheduler: Submitting ResultStage 295 (MapPartitionsRDD[1584] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:26:30 INFO MemoryStore: Block broadcast_393 stored as values in memory (estimated size 47.1 KiB, free 366.0 MiB)\n",
      "25/05/05 11:26:30 INFO MemoryStore: Block broadcast_393_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:26:30 INFO BlockManagerInfo: Added broadcast_393_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:30 INFO SparkContext: Created broadcast 393 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:26:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 295 (MapPartitionsRDD[1584] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:26:30 INFO TaskSchedulerImpl: Adding task set 295.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:26:30 INFO TaskSetManager: Starting task 0.0 in stage 295.0 (TID 295) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:26:30 INFO Executor: Running task 0.0 in stage 295.0 (TID 295)\n",
      "25/05/05 11:26:30 INFO CodeGenerator: Code generated in 3.802708 ms\n",
      "25/05/05 11:26:30 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3329 untilOffset=3330, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=98 taskId=295 partitionId=0\n",
      "25/05/05 11:26:30 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3329 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3330, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3330, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:30 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:26:30 INFO DataWritingSparkTask: Committed partition 0 (task 293, attempt 0, stage 293.0)\n",
      "25/05/05 11:26:30 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 504889542 nanos, during time span of 505253250 nanos.\n",
      "25/05/05 11:26:30 INFO Executor: Finished task 0.0 in stage 293.0 (TID 293). 2145 bytes result sent to driver\n",
      "25/05/05 11:26:30 INFO TaskSetManager: Finished task 0.0 in stage 293.0 (TID 293) in 514 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:26:30 INFO TaskSchedulerImpl: Removed TaskSet 293.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:26:30 INFO DAGScheduler: ResultStage 293 (start at NativeMethodAccessorImpl.java:0) finished in 0.515 s\n",
      "25/05/05 11:26:30 INFO DAGScheduler: Job 294 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:26:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 293: Stage finished\n",
      "25/05/05 11:26:30 INFO DAGScheduler: Job 294 finished: start at NativeMethodAccessorImpl.java:0, took 0.516032 s\n",
      "25/05/05 11:26:30 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 98, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:26:30 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:26:30 INFO DataWritingSparkTask: Committed partition 0 (task 294, attempt 0, stage 294.0)\n",
      "25/05/05 11:26:30 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 502738458 nanos, during time span of 504365917 nanos.\n",
      "25/05/05 11:26:30 INFO Executor: Finished task 0.0 in stage 294.0 (TID 294). 3516 bytes result sent to driver\n",
      "25/05/05 11:26:30 INFO TaskSetManager: Finished task 0.0 in stage 294.0 (TID 294) in 513 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:26:30 INFO TaskSchedulerImpl: Removed TaskSet 294.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:26:30 INFO DAGScheduler: ResultStage 294 (start at NativeMethodAccessorImpl.java:0) finished in 0.515 s\n",
      "25/05/05 11:26:30 INFO DAGScheduler: Job 295 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:26:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 294: Stage finished\n",
      "25/05/05 11:26:30 INFO DAGScheduler: Job 295 finished: start at NativeMethodAccessorImpl.java:0, took 0.516784 s\n",
      "25/05/05 11:26:30 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 98, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@3a9a0cdc] is committing.\n",
      "25/05/05 11:26:30 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 98, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@3a9a0cdc] committed.\n",
      "25/05/05 11:26:30 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 98, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:26:30 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/98 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.98.0a53ecb2-23da-4c8e-8e04-ae6552bdf129.tmp\n",
      "25/05/05 11:26:30 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/98 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.98.26aa49f1-cad4-4767-b22c-33b7d79d6f09.tmp\n",
      "25/05/05 11:26:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3330, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:30 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:26:30 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:26:30 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:26:30 INFO connection: Opened connection [connectionId{localValue:195, serverValue:4565}] to localhost:27017\n",
      "25/05/05 11:26:30 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=291625}\n",
      "25/05/05 11:26:30 INFO connection: Opened connection [connectionId{localValue:196, serverValue:4566}] to localhost:27017\n",
      "25/05/05 11:26:30 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:26:30 INFO connection: Closed connection [connectionId{localValue:196, serverValue:4566}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:26:30 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 505034459 nanos, during time span of 509409541 nanos.\n",
      "25/05/05 11:26:30 INFO Executor: Finished task 0.0 in stage 295.0 (TID 295). 1645 bytes result sent to driver\n",
      "25/05/05 11:26:30 INFO TaskSetManager: Finished task 0.0 in stage 295.0 (TID 295) in 520 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:26:30 INFO TaskSchedulerImpl: Removed TaskSet 295.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:26:30 INFO DAGScheduler: ResultStage 295 (start at NativeMethodAccessorImpl.java:0) finished in 0.523 s\n",
      "25/05/05 11:26:30 INFO DAGScheduler: Job 296 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:26:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 295: Stage finished\n",
      "25/05/05 11:26:30 INFO DAGScheduler: Job 296 finished: start at NativeMethodAccessorImpl.java:0, took 0.523631 s\n",
      "25/05/05 11:26:30 INFO MemoryStore: Block broadcast_394 stored as values in memory (estimated size 248.0 B, free 366.0 MiB)\n",
      "25/05/05 11:26:30 INFO MemoryStore: Block broadcast_394_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.0 MiB)\n",
      "25/05/05 11:26:30 INFO BlockManagerInfo: Added broadcast_394_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:26:30 INFO SparkContext: Created broadcast 394 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:30 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.98.0a53ecb2-23da-4c8e-8e04-ae6552bdf129.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/98\n",
      "25/05/05 11:26:30 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:26:30.321Z\",\n",
      "  \"batchId\" : 98,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.6260162601626016,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 528,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 615,\n",
      "    \"walCommit\" : 53\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3329\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3330\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3330\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.6260162601626016,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:26:30 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/98 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.98.0c586f38-18ec-4434-b438-dd8595aae5ea.tmp\n",
      "25/05/05 11:26:30 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.98.26aa49f1-cad4-4767-b22c-33b7d79d6f09.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/98\n",
      "25/05/05 11:26:30 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:26:30.318Z\",\n",
      "  \"batchId\" : 98,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.6025641025641026,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 534,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 6,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 624,\n",
      "    \"walCommit\" : 53\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3329\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3330\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3330\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.6025641025641026,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:26:30 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.98.0c586f38-18ec-4434-b438-dd8595aae5ea.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/98\n",
      "25/05/05 11:26:30 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:26:30.315Z\",\n",
      "  \"batchId\" : 98,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.5552099533437014,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 551,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 0,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 643,\n",
      "    \"walCommit\" : 60\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3329\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3330\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3330\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.5552099533437014,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 98\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R023|R045|00-00-00|34 ST-HERALD SQ|05/05/2025|11:26:29|REGULAR|3          |14        |2025-05-05 11:26:30.324|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:26:35 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/99 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.99.c0a9e848-e869-4037-8634-b0348e80cfaa.tmp\n",
      "25/05/05 11:26:35 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/99 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.99.6aba2c59-3f2d-4b3e-a5e3-43b9e58415fd.tmp\n",
      "25/05/05 11:26:35 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/99 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.99.913f8b2d-c633-4550-a3f9-d0cd98bb45bd.tmp\n",
      "25/05/05 11:26:35 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.99.c0a9e848-e869-4037-8634-b0348e80cfaa.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/99\n",
      "25/05/05 11:26:35 INFO MicroBatchExecution: Committed offsets for batch 99. Metadata OffsetSeqMetadata(0,1746458795723,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:26:35 INFO IncrementalExecution: Current batch timestamp = 1746458795723\n",
      "25/05/05 11:26:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:35 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.99.6aba2c59-3f2d-4b3e-a5e3-43b9e58415fd.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/99\n",
      "25/05/05 11:26:35 INFO MicroBatchExecution: Committed offsets for batch 99. Metadata OffsetSeqMetadata(0,1746458795737,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:26:35 INFO IncrementalExecution: Current batch timestamp = 1746458795723\n",
      "25/05/05 11:26:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:35 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.99.913f8b2d-c633-4550-a3f9-d0cd98bb45bd.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/99\n",
      "25/05/05 11:26:35 INFO MicroBatchExecution: Committed offsets for batch 99. Metadata OffsetSeqMetadata(0,1746458795737,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:26:35 INFO IncrementalExecution: Current batch timestamp = 1746458795737\n",
      "25/05/05 11:26:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:35 INFO IncrementalExecution: Current batch timestamp = 1746458795723\n",
      "25/05/05 11:26:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:35 INFO IncrementalExecution: Current batch timestamp = 1746458795737\n",
      "25/05/05 11:26:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:35 INFO IncrementalExecution: Current batch timestamp = 1746458795737\n",
      "25/05/05 11:26:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:35 INFO IncrementalExecution: Current batch timestamp = 1746458795737\n",
      "25/05/05 11:26:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:35 INFO IncrementalExecution: Current batch timestamp = 1746458795737\n",
      "25/05/05 11:26:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:35 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:35 INFO CodeGenerator: Code generated in 4.42 ms\n",
      "25/05/05 11:26:35 INFO CodeGenerator: Code generated in 4.300375 ms\n",
      "25/05/05 11:26:35 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 99, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@57b1cbc3]. The input RDD has 1 partitions.\n",
      "25/05/05 11:26:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:35 INFO DAGScheduler: Got job 297 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:26:35 INFO DAGScheduler: Final stage: ResultStage 296 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:26:35 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:26:35 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:26:35 INFO DAGScheduler: Submitting ResultStage 296 (MapPartitionsRDD[1590] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:26:35 INFO MemoryStore: Block broadcast_395 stored as values in memory (estimated size 16.2 KiB, free 366.0 MiB)\n",
      "25/05/05 11:26:35 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 99, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:26:35 INFO MemoryStore: Block broadcast_395_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)\n",
      "25/05/05 11:26:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:35 INFO BlockManagerInfo: Added broadcast_395_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:35 INFO SparkContext: Created broadcast 395 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:26:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 296 (MapPartitionsRDD[1590] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:26:35 INFO TaskSchedulerImpl: Adding task set 296.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:26:35 INFO DAGScheduler: Got job 298 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:26:35 INFO DAGScheduler: Final stage: ResultStage 297 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:26:35 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:26:35 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:26:35 INFO DAGScheduler: Submitting ResultStage 297 (MapPartitionsRDD[1595] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:26:35 INFO TaskSetManager: Starting task 0.0 in stage 296.0 (TID 296) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:26:35 INFO Executor: Running task 0.0 in stage 296.0 (TID 296)\n",
      "25/05/05 11:26:35 INFO MemoryStore: Block broadcast_396 stored as values in memory (estimated size 15.2 KiB, free 365.9 MiB)\n",
      "25/05/05 11:26:35 INFO MemoryStore: Block broadcast_396_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 365.9 MiB)\n",
      "25/05/05 11:26:35 INFO BlockManagerInfo: Added broadcast_396_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:35 INFO SparkContext: Created broadcast 396 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:26:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 297 (MapPartitionsRDD[1595] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:26:35 INFO TaskSchedulerImpl: Adding task set 297.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:26:35 INFO TaskSetManager: Starting task 0.0 in stage 297.0 (TID 297) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:26:35 INFO Executor: Running task 0.0 in stage 297.0 (TID 297)\n",
      "25/05/05 11:26:35 INFO CodeGenerator: Code generated in 4.056625 ms\n",
      "25/05/05 11:26:35 INFO CodeGenerator: Code generated in 3.784833 ms\n",
      "25/05/05 11:26:35 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3330 untilOffset=3331, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=99 taskId=297 partitionId=0\n",
      "25/05/05 11:26:35 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3330 untilOffset=3331, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=99 taskId=296 partitionId=0\n",
      "25/05/05 11:26:35 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3330 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:35 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3330 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:35 INFO DAGScheduler: Got job 299 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:26:35 INFO DAGScheduler: Final stage: ResultStage 298 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:26:35 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:26:35 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:26:35 INFO DAGScheduler: Submitting ResultStage 298 (MapPartitionsRDD[1600] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:26:35 INFO MemoryStore: Block broadcast_397 stored as values in memory (estimated size 47.1 KiB, free 365.9 MiB)\n",
      "25/05/05 11:26:35 INFO MemoryStore: Block broadcast_397_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 365.9 MiB)\n",
      "25/05/05 11:26:35 INFO BlockManagerInfo: Added broadcast_397_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:35 INFO SparkContext: Created broadcast 397 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:26:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 298 (MapPartitionsRDD[1600] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:26:35 INFO TaskSchedulerImpl: Adding task set 298.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:26:35 INFO TaskSetManager: Starting task 0.0 in stage 298.0 (TID 298) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:26:35 INFO Executor: Running task 0.0 in stage 298.0 (TID 298)\n",
      "25/05/05 11:26:35 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3330 untilOffset=3331, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=99 taskId=298 partitionId=0\n",
      "25/05/05 11:26:35 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3330 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:35 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:36 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:36 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:36 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:36 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:36 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3331, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:36 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3331, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:36 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:26:36 INFO DataWritingSparkTask: Committed partition 0 (task 297, attempt 0, stage 297.0)\n",
      "25/05/05 11:26:36 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 503650583 nanos, during time span of 503877167 nanos.\n",
      "25/05/05 11:26:36 INFO Executor: Finished task 0.0 in stage 297.0 (TID 297). 2145 bytes result sent to driver\n",
      "25/05/05 11:26:36 INFO TaskSetManager: Finished task 0.0 in stage 297.0 (TID 297) in 512 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:26:36 INFO TaskSchedulerImpl: Removed TaskSet 297.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:26:36 INFO DAGScheduler: ResultStage 297 (start at NativeMethodAccessorImpl.java:0) finished in 0.513 s\n",
      "25/05/05 11:26:36 INFO DAGScheduler: Job 298 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:26:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 297: Stage finished\n",
      "25/05/05 11:26:36 INFO DAGScheduler: Job 298 finished: start at NativeMethodAccessorImpl.java:0, took 0.514395 s\n",
      "25/05/05 11:26:36 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 99, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:26:36 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:26:36 INFO DataWritingSparkTask: Committed partition 0 (task 296, attempt 0, stage 296.0)\n",
      "25/05/05 11:26:36 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 503290042 nanos, during time span of 504868000 nanos.\n",
      "25/05/05 11:26:36 INFO Executor: Finished task 0.0 in stage 296.0 (TID 296). 3514 bytes result sent to driver\n",
      "25/05/05 11:26:36 INFO TaskSetManager: Finished task 0.0 in stage 296.0 (TID 296) in 514 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:26:36 INFO TaskSchedulerImpl: Removed TaskSet 296.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:26:36 INFO DAGScheduler: ResultStage 296 (start at NativeMethodAccessorImpl.java:0) finished in 0.516 s\n",
      "25/05/05 11:26:36 INFO DAGScheduler: Job 297 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:26:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 296: Stage finished\n",
      "25/05/05 11:26:36 INFO DAGScheduler: Job 297 finished: start at NativeMethodAccessorImpl.java:0, took 0.516973 s\n",
      "25/05/05 11:26:36 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 99, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@57b1cbc3] is committing.\n",
      "25/05/05 11:26:36 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 99, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@57b1cbc3] committed.\n",
      "25/05/05 11:26:36 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 99, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:26:36 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/99 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.99.92d2b99b-67c6-40b7-89b1-fa442df94943.tmp\n",
      "25/05/05 11:26:36 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/99 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.99.c34e6685-33cf-4cef-8de8-3cf1eec05e5d.tmp\n",
      "25/05/05 11:26:36 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:36 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:36 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.99.92d2b99b-67c6-40b7-89b1-fa442df94943.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/99\n",
      "25/05/05 11:26:36 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.99.c34e6685-33cf-4cef-8de8-3cf1eec05e5d.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/99\n",
      "25/05/05 11:26:36 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:26:35.734Z\",\n",
      "  \"batchId\" : 99,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.6447368421052633,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 529,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 608,\n",
      "    \"walCommit\" : 46\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3330\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3331\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3331\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.6447368421052633,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:26:36 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:26:35.722Z\",\n",
      "  \"batchId\" : 99,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.6129032258064517,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 532,\n",
      "    \"commitOffsets\" : 31,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 620,\n",
      "    \"walCommit\" : 51\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3330\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3331\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3331\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.6129032258064517,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:26:36 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3331, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:36 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:26:36 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:26:36 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:26:36 INFO connection: Opened connection [connectionId{localValue:197, serverValue:4567}] to localhost:27017\n",
      "25/05/05 11:26:36 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=271458}\n",
      "25/05/05 11:26:36 INFO connection: Opened connection [connectionId{localValue:198, serverValue:4568}] to localhost:27017\n",
      "25/05/05 11:26:36 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:26:36 INFO connection: Closed connection [connectionId{localValue:198, serverValue:4568}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:26:36 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 536820375 nanos, during time span of 540940750 nanos.\n",
      "25/05/05 11:26:36 INFO Executor: Finished task 0.0 in stage 298.0 (TID 298). 1645 bytes result sent to driver\n",
      "25/05/05 11:26:36 INFO TaskSetManager: Finished task 0.0 in stage 298.0 (TID 298) in 547 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:26:36 INFO TaskSchedulerImpl: Removed TaskSet 298.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:26:36 INFO DAGScheduler: ResultStage 298 (start at NativeMethodAccessorImpl.java:0) finished in 0.551 s\n",
      "25/05/05 11:26:36 INFO DAGScheduler: Job 299 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:26:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 298: Stage finished\n",
      "25/05/05 11:26:36 INFO DAGScheduler: Job 299 finished: start at NativeMethodAccessorImpl.java:0, took 0.551926 s\n",
      "25/05/05 11:26:36 INFO MemoryStore: Block broadcast_398 stored as values in memory (estimated size 248.0 B, free 365.9 MiB)\n",
      "25/05/05 11:26:36 INFO MemoryStore: Block broadcast_398_piece0 stored as bytes in memory (estimated size 413.0 B, free 365.9 MiB)\n",
      "25/05/05 11:26:36 INFO BlockManagerInfo: Added broadcast_398_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:26:36 INFO SparkContext: Created broadcast 398 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:36 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/99 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.99.08b7f2a1-c196-401a-bc2f-3185b1067dfb.tmp\n",
      "25/05/05 11:26:36 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.99.08b7f2a1-c196-401a-bc2f-3185b1067dfb.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/99\n",
      "25/05/05 11:26:36 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:26:35.735Z\",\n",
      "  \"batchId\" : 99,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.5313935681470137,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 580,\n",
      "    \"commitOffsets\" : 24,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 653,\n",
      "    \"walCommit\" : 43\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3330\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3331\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3331\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.5313935681470137,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 99\n",
      "-------------------------------------------\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION      |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R016|R032|00-00-01|TIME SQ-42 ST|05/05/2025|11:26:34|REGULAR|14         |8         |2025-05-05 11:26:35.737|\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:26:37 INFO BlockManagerInfo: Removed broadcast_396_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:37 INFO BlockManagerInfo: Removed broadcast_389_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:37 INFO BlockManagerInfo: Removed broadcast_398_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:26:37 INFO BlockManagerInfo: Removed broadcast_390_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:26:37 INFO BlockManagerInfo: Removed broadcast_392_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:37 INFO BlockManagerInfo: Removed broadcast_394_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:26:37 INFO BlockManagerInfo: Removed broadcast_387_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:37 INFO BlockManagerInfo: Removed broadcast_397_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:37 INFO BlockManagerInfo: Removed broadcast_388_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:37 INFO BlockManagerInfo: Removed broadcast_395_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:37 INFO BlockManagerInfo: Removed broadcast_391_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:26:37 INFO BlockManagerInfo: Removed broadcast_393_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:26:40 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/100 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.100.70720f59-76c4-43fd-be50-2507952e4e55.tmp\n",
      "25/05/05 11:26:40 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/100 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.100.3606a128-2880-4a71-8d84-538f7199fbeb.tmp\n",
      "25/05/05 11:26:40 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/100 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.100.17a6481b-eb63-4b61-9304-05c551f46dcf.tmp\n",
      "25/05/05 11:26:40 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.100.70720f59-76c4-43fd-be50-2507952e4e55.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/100\n",
      "25/05/05 11:26:40 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.100.3606a128-2880-4a71-8d84-538f7199fbeb.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/100\n",
      "25/05/05 11:26:40 INFO MicroBatchExecution: Committed offsets for batch 100. Metadata OffsetSeqMetadata(0,1746458800116,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:26:40 INFO MicroBatchExecution: Committed offsets for batch 100. Metadata OffsetSeqMetadata(0,1746458800117,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:26:40 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.100.17a6481b-eb63-4b61-9304-05c551f46dcf.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/100\n",
      "25/05/05 11:26:40 INFO MicroBatchExecution: Committed offsets for batch 100. Metadata OffsetSeqMetadata(0,1746458800118,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:26:40 INFO IncrementalExecution: Current batch timestamp = 1746458800117\n",
      "25/05/05 11:26:40 INFO IncrementalExecution: Current batch timestamp = 1746458800116\n",
      "25/05/05 11:26:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:40 INFO IncrementalExecution: Current batch timestamp = 1746458800118\n",
      "25/05/05 11:26:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:40 INFO IncrementalExecution: Current batch timestamp = 1746458800116\n",
      "25/05/05 11:26:40 INFO IncrementalExecution: Current batch timestamp = 1746458800117\n",
      "25/05/05 11:26:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:40 INFO IncrementalExecution: Current batch timestamp = 1746458800118\n",
      "25/05/05 11:26:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:40 INFO IncrementalExecution: Current batch timestamp = 1746458800117\n",
      "25/05/05 11:26:40 INFO IncrementalExecution: Current batch timestamp = 1746458800116\n",
      "25/05/05 11:26:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:40 INFO CodeGenerator: Code generated in 3.606833 ms\n",
      "25/05/05 11:26:40 INFO CodeGenerator: Code generated in 3.120291 ms\n",
      "25/05/05 11:26:40 INFO CodeGenerator: Code generated in 3.475125 ms\n",
      "25/05/05 11:26:40 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 100, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:26:40 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:40 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 100, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@5628ec79]. The input RDD has 1 partitions.\n",
      "25/05/05 11:26:40 INFO DAGScheduler: Got job 300 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:26:40 INFO DAGScheduler: Final stage: ResultStage 299 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:26:40 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:26:40 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:26:40 INFO DAGScheduler: Submitting ResultStage 299 (MapPartitionsRDD[1610] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:26:40 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:40 INFO MemoryStore: Block broadcast_399 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:26:40 INFO MemoryStore: Block broadcast_399_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:26:40 INFO BlockManagerInfo: Added broadcast_399_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:26:40 INFO SparkContext: Created broadcast 399 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:26:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 299 (MapPartitionsRDD[1610] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:26:40 INFO TaskSchedulerImpl: Adding task set 299.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:26:40 INFO DAGScheduler: Got job 301 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:26:40 INFO DAGScheduler: Final stage: ResultStage 300 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:26:40 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:26:40 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:26:40 INFO DAGScheduler: Submitting ResultStage 300 (MapPartitionsRDD[1611] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:26:40 INFO MemoryStore: Block broadcast_400 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:26:40 INFO MemoryStore: Block broadcast_400_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:26:40 INFO TaskSetManager: Starting task 0.0 in stage 299.0 (TID 299) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:26:40 INFO BlockManagerInfo: Added broadcast_400_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:26:40 INFO SparkContext: Created broadcast 400 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:26:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 300 (MapPartitionsRDD[1611] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:26:40 INFO TaskSchedulerImpl: Adding task set 300.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:26:40 INFO Executor: Running task 0.0 in stage 299.0 (TID 299)\n",
      "25/05/05 11:26:40 INFO TaskSetManager: Starting task 0.0 in stage 300.0 (TID 300) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:26:40 INFO Executor: Running task 0.0 in stage 300.0 (TID 300)\n",
      "25/05/05 11:26:40 INFO CodeGenerator: Code generated in 3.575584 ms\n",
      "25/05/05 11:26:40 INFO CodeGenerator: Code generated in 5.866042 ms\n",
      "25/05/05 11:26:40 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3331 untilOffset=3332, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=100 taskId=300 partitionId=0\n",
      "25/05/05 11:26:40 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3331 untilOffset=3332, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=100 taskId=299 partitionId=0\n",
      "25/05/05 11:26:40 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3331 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:40 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3331 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:40 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:40 INFO DAGScheduler: Got job 302 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:26:40 INFO DAGScheduler: Final stage: ResultStage 301 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:26:40 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:26:40 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:26:40 INFO DAGScheduler: Submitting ResultStage 301 (MapPartitionsRDD[1616] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:26:40 INFO MemoryStore: Block broadcast_401 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:26:40 INFO MemoryStore: Block broadcast_401_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.1 MiB)\n",
      "25/05/05 11:26:40 INFO BlockManagerInfo: Added broadcast_401_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:40 INFO SparkContext: Created broadcast 401 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:26:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 301 (MapPartitionsRDD[1616] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:26:40 INFO TaskSchedulerImpl: Adding task set 301.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:26:40 INFO TaskSetManager: Starting task 0.0 in stage 301.0 (TID 301) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:26:40 INFO Executor: Running task 0.0 in stage 301.0 (TID 301)\n",
      "25/05/05 11:26:40 INFO CodeGenerator: Code generated in 3.61975 ms\n",
      "25/05/05 11:26:40 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3331 untilOffset=3332, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=100 taskId=301 partitionId=0\n",
      "25/05/05 11:26:40 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3331 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3332, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3332, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:40 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:26:40 INFO DataWritingSparkTask: Committed partition 0 (task 299, attempt 0, stage 299.0)\n",
      "25/05/05 11:26:40 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 503165708 nanos, during time span of 503376375 nanos.\n",
      "25/05/05 11:26:40 INFO Executor: Finished task 0.0 in stage 299.0 (TID 299). 2137 bytes result sent to driver\n",
      "25/05/05 11:26:40 INFO TaskSetManager: Finished task 0.0 in stage 299.0 (TID 299) in 516 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:26:40 INFO TaskSchedulerImpl: Removed TaskSet 299.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:26:40 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:26:40 INFO DataWritingSparkTask: Committed partition 0 (task 300, attempt 0, stage 300.0)\n",
      "25/05/05 11:26:40 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 504324000 nanos, during time span of 505924708 nanos.\n",
      "25/05/05 11:26:40 INFO DAGScheduler: ResultStage 299 (start at NativeMethodAccessorImpl.java:0) finished in 0.519 s\n",
      "25/05/05 11:26:40 INFO Executor: Finished task 0.0 in stage 300.0 (TID 300). 3509 bytes result sent to driver\n",
      "25/05/05 11:26:40 INFO DAGScheduler: Job 300 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:26:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 299: Stage finished\n",
      "25/05/05 11:26:40 INFO TaskSetManager: Finished task 0.0 in stage 300.0 (TID 300) in 515 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:26:40 INFO TaskSchedulerImpl: Removed TaskSet 300.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:26:40 INFO DAGScheduler: Job 300 finished: start at NativeMethodAccessorImpl.java:0, took 0.519693 s\n",
      "25/05/05 11:26:40 INFO DAGScheduler: ResultStage 300 (start at NativeMethodAccessorImpl.java:0) finished in 0.517 s\n",
      "25/05/05 11:26:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 100, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:26:40 INFO DAGScheduler: Job 301 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:26:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 300: Stage finished\n",
      "25/05/05 11:26:40 INFO DAGScheduler: Job 301 finished: start at NativeMethodAccessorImpl.java:0, took 0.519456 s\n",
      "25/05/05 11:26:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 100, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@5628ec79] is committing.\n",
      "25/05/05 11:26:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 100, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@5628ec79] committed.\n",
      "25/05/05 11:26:40 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 100, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:26:40 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/100 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.100.c2ec7810-ce78-4f5e-adba-43f49a1ad861.tmp\n",
      "25/05/05 11:26:40 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/100 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.100.aecb27ad-c1be-4c03-afa6-d03f18779e8c.tmp\n",
      "25/05/05 11:26:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:40 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3332, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:40 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:26:40 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:26:40 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:26:40 INFO connection: Opened connection [connectionId{localValue:199, serverValue:4569}] to localhost:27017\n",
      "25/05/05 11:26:40 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=345917}\n",
      "25/05/05 11:26:40 INFO connection: Opened connection [connectionId{localValue:200, serverValue:4570}] to localhost:27017\n",
      "25/05/05 11:26:40 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:26:40 INFO connection: Closed connection [connectionId{localValue:200, serverValue:4570}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:26:40 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 506415166 nanos, during time span of 511530167 nanos.\n",
      "25/05/05 11:26:40 INFO Executor: Finished task 0.0 in stage 301.0 (TID 301). 1645 bytes result sent to driver\n",
      "25/05/05 11:26:40 INFO TaskSetManager: Finished task 0.0 in stage 301.0 (TID 301) in 521 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:26:40 INFO TaskSchedulerImpl: Removed TaskSet 301.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:26:40 INFO DAGScheduler: ResultStage 301 (start at NativeMethodAccessorImpl.java:0) finished in 0.525 s\n",
      "25/05/05 11:26:40 INFO DAGScheduler: Job 302 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:26:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 301: Stage finished\n",
      "25/05/05 11:26:40 INFO DAGScheduler: Job 302 finished: start at NativeMethodAccessorImpl.java:0, took 0.526514 s\n",
      "25/05/05 11:26:40 INFO MemoryStore: Block broadcast_402 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:26:40 INFO MemoryStore: Block broadcast_402_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:26:40 INFO BlockManagerInfo: Added broadcast_402_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:26:40 INFO SparkContext: Created broadcast 402 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:40 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.100.c2ec7810-ce78-4f5e-adba-43f49a1ad861.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/100\n",
      "25/05/05 11:26:40 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:26:40.115Z\",\n",
      "  \"batchId\" : 100,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.6025641025641026,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 533,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 624,\n",
      "    \"walCommit\" : 56\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3331\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3332\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3332\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.6025641025641026,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:26:40 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/100 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.100.6609dd50-0e1b-4bad-85c6-5fd5168f5e58.tmp\n",
      "25/05/05 11:26:40 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.100.aecb27ad-c1be-4c03-afa6-d03f18779e8c.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/100\n",
      "25/05/05 11:26:40 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:26:40.115Z\",\n",
      "  \"batchId\" : 100,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.589825119236884,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 538,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 629,\n",
      "    \"walCommit\" : 57\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3331\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3332\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3332\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.589825119236884,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:26:40 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.100.6609dd50-0e1b-4bad-85c6-5fd5168f5e58.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/100\n",
      "25/05/05 11:26:40 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:26:40.115Z\",\n",
      "  \"batchId\" : 100,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.5527950310559007,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 557,\n",
      "    \"commitOffsets\" : 23,\n",
      "    \"getBatch\" : 1,\n",
      "    \"latestOffset\" : 3,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 644,\n",
      "    \"walCommit\" : 56\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3331\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3332\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3332\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.5527950310559007,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 100\n",
      "-------------------------------------------\n",
      "+----+----+--------+--------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+--------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R028|R088|00-00-00|CANAL ST|05/05/2025|11:26:39|REGULAR|11         |5         |2025-05-05 11:26:40.116|\n",
      "+----+----+--------+--------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:26:46 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/101 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.101.e9a17eea-5215-43d8-bdc8-18bc14a3dfe6.tmp\n",
      "25/05/05 11:26:46 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/101 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.101.0c7346f0-1b5b-43c3-a07a-fd160cab14e9.tmp\n",
      "25/05/05 11:26:46 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/101 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.101.3600d499-acf1-4746-8c8d-f9b4343b6a91.tmp\n",
      "25/05/05 11:26:46 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.101.0c7346f0-1b5b-43c3-a07a-fd160cab14e9.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/101\n",
      "25/05/05 11:26:46 INFO MicroBatchExecution: Committed offsets for batch 101. Metadata OffsetSeqMetadata(0,1746458806535,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:26:46 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.101.e9a17eea-5215-43d8-bdc8-18bc14a3dfe6.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/101\n",
      "25/05/05 11:26:46 INFO MicroBatchExecution: Committed offsets for batch 101. Metadata OffsetSeqMetadata(0,1746458806535,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:26:46 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.101.3600d499-acf1-4746-8c8d-f9b4343b6a91.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/101\n",
      "25/05/05 11:26:46 INFO MicroBatchExecution: Committed offsets for batch 101. Metadata OffsetSeqMetadata(0,1746458806536,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:26:46 INFO IncrementalExecution: Current batch timestamp = 1746458806535\n",
      "25/05/05 11:26:46 INFO IncrementalExecution: Current batch timestamp = 1746458806535\n",
      "25/05/05 11:26:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:46 INFO IncrementalExecution: Current batch timestamp = 1746458806536\n",
      "25/05/05 11:26:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:46 INFO IncrementalExecution: Current batch timestamp = 1746458806535\n",
      "25/05/05 11:26:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:46 INFO IncrementalExecution: Current batch timestamp = 1746458806535\n",
      "25/05/05 11:26:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:46 INFO IncrementalExecution: Current batch timestamp = 1746458806536\n",
      "25/05/05 11:26:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:46 INFO IncrementalExecution: Current batch timestamp = 1746458806535\n",
      "25/05/05 11:26:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:46 INFO IncrementalExecution: Current batch timestamp = 1746458806536\n",
      "25/05/05 11:26:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:46 INFO CodeGenerator: Code generated in 3.903166 ms\n",
      "25/05/05 11:26:46 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 101, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@29f5b62b]. The input RDD has 1 partitions.\n",
      "25/05/05 11:26:46 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:46 INFO DAGScheduler: Got job 303 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:26:46 INFO DAGScheduler: Final stage: ResultStage 302 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:26:46 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:26:46 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:26:46 INFO DAGScheduler: Submitting ResultStage 302 (MapPartitionsRDD[1624] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:26:46 INFO MemoryStore: Block broadcast_403 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:26:46 INFO MemoryStore: Block broadcast_403_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.1 MiB)\n",
      "25/05/05 11:26:46 INFO BlockManagerInfo: Added broadcast_403_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:46 INFO SparkContext: Created broadcast 403 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:26:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 302 (MapPartitionsRDD[1624] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:26:46 INFO TaskSchedulerImpl: Adding task set 302.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:26:46 INFO TaskSetManager: Starting task 0.0 in stage 302.0 (TID 302) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:26:46 INFO Executor: Running task 0.0 in stage 302.0 (TID 302)\n",
      "25/05/05 11:26:46 INFO CodeGenerator: Code generated in 3.820666 ms\n",
      "25/05/05 11:26:46 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 101, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:26:46 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:46 INFO DAGScheduler: Got job 304 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:26:46 INFO DAGScheduler: Final stage: ResultStage 303 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:26:46 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:26:46 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:26:46 INFO DAGScheduler: Submitting ResultStage 303 (MapPartitionsRDD[1627] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:26:46 INFO MemoryStore: Block broadcast_404 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:26:46 INFO CodeGenerator: Code generated in 3.621583 ms\n",
      "25/05/05 11:26:46 INFO MemoryStore: Block broadcast_404_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:26:46 INFO BlockManagerInfo: Added broadcast_404_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:46 INFO SparkContext: Created broadcast 404 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:26:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 303 (MapPartitionsRDD[1627] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:26:46 INFO TaskSchedulerImpl: Adding task set 303.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:26:46 INFO TaskSetManager: Starting task 0.0 in stage 303.0 (TID 303) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:26:46 INFO Executor: Running task 0.0 in stage 303.0 (TID 303)\n",
      "25/05/05 11:26:46 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3332 untilOffset=3333, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=101 taskId=302 partitionId=0\n",
      "25/05/05 11:26:46 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3332 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:46 INFO CodeGenerator: Code generated in 3.701959 ms\n",
      "25/05/05 11:26:46 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3332 untilOffset=3333, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=101 taskId=303 partitionId=0\n",
      "25/05/05 11:26:46 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3332 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:46 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:46 INFO DAGScheduler: Got job 305 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:26:46 INFO DAGScheduler: Final stage: ResultStage 304 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:26:46 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:26:46 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:26:46 INFO DAGScheduler: Submitting ResultStage 304 (MapPartitionsRDD[1632] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:26:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:46 INFO MemoryStore: Block broadcast_405 stored as values in memory (estimated size 47.1 KiB, free 366.0 MiB)\n",
      "25/05/05 11:26:46 INFO MemoryStore: Block broadcast_405_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:26:46 INFO BlockManagerInfo: Added broadcast_405_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:46 INFO SparkContext: Created broadcast 405 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:26:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 304 (MapPartitionsRDD[1632] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:26:46 INFO TaskSchedulerImpl: Adding task set 304.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:26:46 INFO TaskSetManager: Starting task 0.0 in stage 304.0 (TID 304) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:26:46 INFO Executor: Running task 0.0 in stage 304.0 (TID 304)\n",
      "25/05/05 11:26:46 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3332 untilOffset=3333, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=101 taskId=304 partitionId=0\n",
      "25/05/05 11:26:46 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3332 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:47 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:47 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:47 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3333, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:47 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:26:47 INFO DataWritingSparkTask: Committed partition 0 (task 302, attempt 0, stage 302.0)\n",
      "25/05/05 11:26:47 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 503666958 nanos, during time span of 505346291 nanos.\n",
      "25/05/05 11:26:47 INFO Executor: Finished task 0.0 in stage 302.0 (TID 302). 3511 bytes result sent to driver\n",
      "25/05/05 11:26:47 INFO TaskSetManager: Finished task 0.0 in stage 302.0 (TID 302) in 514 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:26:47 INFO TaskSchedulerImpl: Removed TaskSet 302.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:26:47 INFO DAGScheduler: ResultStage 302 (start at NativeMethodAccessorImpl.java:0) finished in 0.515 s\n",
      "25/05/05 11:26:47 INFO DAGScheduler: Job 303 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:26:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 302: Stage finished\n",
      "25/05/05 11:26:47 INFO DAGScheduler: Job 303 finished: start at NativeMethodAccessorImpl.java:0, took 0.516835 s\n",
      "25/05/05 11:26:47 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 101, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@29f5b62b] is committing.\n",
      "25/05/05 11:26:47 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 101, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@29f5b62b] committed.\n",
      "25/05/05 11:26:47 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:47 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:47 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3333, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:47 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:26:47 INFO DataWritingSparkTask: Committed partition 0 (task 303, attempt 0, stage 303.0)\n",
      "25/05/05 11:26:47 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 501711708 nanos, during time span of 501976625 nanos.\n",
      "25/05/05 11:26:47 INFO Executor: Finished task 0.0 in stage 303.0 (TID 303). 2145 bytes result sent to driver\n",
      "25/05/05 11:26:47 INFO TaskSetManager: Finished task 0.0 in stage 303.0 (TID 303) in 510 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:26:47 INFO TaskSchedulerImpl: Removed TaskSet 303.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:26:47 INFO DAGScheduler: ResultStage 303 (start at NativeMethodAccessorImpl.java:0) finished in 0.512 s\n",
      "25/05/05 11:26:47 INFO DAGScheduler: Job 304 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:26:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 303: Stage finished\n",
      "25/05/05 11:26:47 INFO DAGScheduler: Job 304 finished: start at NativeMethodAccessorImpl.java:0, took 0.513637 s\n",
      "25/05/05 11:26:47 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 101, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:26:47 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/101 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.101.16bee1af-3acd-49fa-b0d0-05c4121657f2.tmp\n",
      "25/05/05 11:26:47 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 101, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:26:47 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:47 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:47 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3333, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:47 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:26:47 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:26:47 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:26:47 INFO connection: Opened connection [connectionId{localValue:201, serverValue:4571}] to localhost:27017\n",
      "25/05/05 11:26:47 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=204167}\n",
      "25/05/05 11:26:47 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/101 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.101.21e7f2a8-3ba9-4caa-a0ea-5e0f64911d53.tmp\n",
      "25/05/05 11:26:47 INFO connection: Opened connection [connectionId{localValue:202, serverValue:4572}] to localhost:27017\n",
      "25/05/05 11:26:47 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:26:47 INFO connection: Closed connection [connectionId{localValue:202, serverValue:4572}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:26:47 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 503427000 nanos, during time span of 507168875 nanos.\n",
      "25/05/05 11:26:47 INFO Executor: Finished task 0.0 in stage 304.0 (TID 304). 1645 bytes result sent to driver\n",
      "25/05/05 11:26:47 INFO TaskSetManager: Finished task 0.0 in stage 304.0 (TID 304) in 512 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:26:47 INFO TaskSchedulerImpl: Removed TaskSet 304.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:26:47 INFO DAGScheduler: ResultStage 304 (start at NativeMethodAccessorImpl.java:0) finished in 0.517 s\n",
      "25/05/05 11:26:47 INFO DAGScheduler: Job 305 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:26:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 304: Stage finished\n",
      "25/05/05 11:26:47 INFO DAGScheduler: Job 305 finished: start at NativeMethodAccessorImpl.java:0, took 0.517152 s\n",
      "25/05/05 11:26:47 INFO MemoryStore: Block broadcast_406 stored as values in memory (estimated size 248.0 B, free 366.0 MiB)\n",
      "25/05/05 11:26:47 INFO MemoryStore: Block broadcast_406_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.0 MiB)\n",
      "25/05/05 11:26:47 INFO BlockManagerInfo: Added broadcast_406_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:26:47 INFO SparkContext: Created broadcast 406 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:47 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/101 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.101.c47cb8d0-742b-4f0a-8881-e6d6105e48a4.tmp\n",
      "25/05/05 11:26:47 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.101.16bee1af-3acd-49fa-b0d0-05c4121657f2.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/101\n",
      "25/05/05 11:26:47 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:26:46.534Z\",\n",
      "  \"batchId\" : 101,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.6207455429497568,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 531,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 617,\n",
      "    \"walCommit\" : 53\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3332\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3333\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3333\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.6207455429497568,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:26:47 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.101.21e7f2a8-3ba9-4caa-a0ea-5e0f64911d53.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/101\n",
      "25/05/05 11:26:47 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:26:46.534Z\",\n",
      "  \"batchId\" : 101,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "  \"processedRowsPerSecond\" : 1.6051364365971108,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 537,\n",
      "    \"commitOffsets\" : 26,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 623,\n",
      "    \"walCommit\" : 54\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3332\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3333\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3333\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "    \"processedRowsPerSecond\" : 1.6051364365971108,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:26:47 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.101.c47cb8d0-742b-4f0a-8881-e6d6105e48a4.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/101\n",
      "25/05/05 11:26:47 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:26:46.534Z\",\n",
      "  \"batchId\" : 101,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.5847860538827259,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 549,\n",
      "    \"commitOffsets\" : 23,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 631,\n",
      "    \"walCommit\" : 53\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3332\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3333\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3333\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.5847860538827259,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 101\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R023|R045|00-00-00|34 ST-HERALD SQ|05/05/2025|11:26:45|REGULAR|10         |10        |2025-05-05 11:26:46.536|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:26:48 INFO BlockManagerInfo: Removed broadcast_403_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:48 INFO BlockManagerInfo: Removed broadcast_404_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:48 INFO BlockManagerInfo: Removed broadcast_406_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:26:48 INFO BlockManagerInfo: Removed broadcast_405_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:48 INFO BlockManagerInfo: Removed broadcast_399_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:48 INFO BlockManagerInfo: Removed broadcast_401_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:26:48 INFO BlockManagerInfo: Removed broadcast_400_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:26:48 INFO BlockManagerInfo: Removed broadcast_402_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:26:52 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/102 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.102.2fbf1aa3-8998-48f6-8832-09d73224ddb6.tmp\n",
      "25/05/05 11:26:52 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/102 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.102.937c27bb-55a6-40ee-ad08-09c3eea670f1.tmp\n",
      "25/05/05 11:26:52 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/102 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.102.06ffbfa5-aa2f-4703-a766-1c78d6e8d0ab.tmp\n",
      "25/05/05 11:26:52 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.102.2fbf1aa3-8998-48f6-8832-09d73224ddb6.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/102\n",
      "25/05/05 11:26:52 INFO MicroBatchExecution: Committed offsets for batch 102. Metadata OffsetSeqMetadata(0,1746458812930,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:26:52 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.102.937c27bb-55a6-40ee-ad08-09c3eea670f1.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/102\n",
      "25/05/05 11:26:52 INFO MicroBatchExecution: Committed offsets for batch 102. Metadata OffsetSeqMetadata(0,1746458812929,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:26:52 INFO IncrementalExecution: Current batch timestamp = 1746458812930\n",
      "25/05/05 11:26:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:52 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.102.06ffbfa5-aa2f-4703-a766-1c78d6e8d0ab.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/102\n",
      "25/05/05 11:26:52 INFO MicroBatchExecution: Committed offsets for batch 102. Metadata OffsetSeqMetadata(0,1746458812935,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:26:52 INFO IncrementalExecution: Current batch timestamp = 1746458812929\n",
      "25/05/05 11:26:52 INFO IncrementalExecution: Current batch timestamp = 1746458812930\n",
      "25/05/05 11:26:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:52 INFO IncrementalExecution: Current batch timestamp = 1746458812935\n",
      "25/05/05 11:26:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:52 INFO IncrementalExecution: Current batch timestamp = 1746458812929\n",
      "25/05/05 11:26:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:52 INFO IncrementalExecution: Current batch timestamp = 1746458812930\n",
      "25/05/05 11:26:52 INFO IncrementalExecution: Current batch timestamp = 1746458812935\n",
      "25/05/05 11:26:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:52 INFO IncrementalExecution: Current batch timestamp = 1746458812929\n",
      "25/05/05 11:26:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:52 INFO CodeGenerator: Code generated in 4.905458 ms\n",
      "25/05/05 11:26:52 INFO CodeGenerator: Code generated in 4.84175 ms\n",
      "25/05/05 11:26:52 INFO CodeGenerator: Code generated in 3.449875 ms\n",
      "25/05/05 11:26:52 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 102, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:26:52 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 102, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@3b506dc1]. The input RDD has 1 partitions.\n",
      "25/05/05 11:26:52 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:52 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:52 INFO DAGScheduler: Got job 306 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:26:52 INFO DAGScheduler: Final stage: ResultStage 305 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:26:52 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:26:52 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:26:52 INFO DAGScheduler: Submitting ResultStage 305 (MapPartitionsRDD[1642] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:26:52 INFO MemoryStore: Block broadcast_407 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:26:53 INFO MemoryStore: Block broadcast_407_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:26:53 INFO BlockManagerInfo: Added broadcast_407_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:26:53 INFO SparkContext: Created broadcast 407 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:26:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 305 (MapPartitionsRDD[1642] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:26:53 INFO TaskSchedulerImpl: Adding task set 305.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:26:53 INFO DAGScheduler: Got job 307 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:26:53 INFO DAGScheduler: Final stage: ResultStage 306 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:26:53 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:26:53 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:26:53 INFO DAGScheduler: Submitting ResultStage 306 (MapPartitionsRDD[1643] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:26:53 INFO TaskSetManager: Starting task 0.0 in stage 305.0 (TID 305) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:26:53 INFO Executor: Running task 0.0 in stage 305.0 (TID 305)\n",
      "25/05/05 11:26:53 INFO MemoryStore: Block broadcast_408 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:26:53 INFO MemoryStore: Block broadcast_408_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:26:53 INFO BlockManagerInfo: Added broadcast_408_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:26:53 INFO SparkContext: Created broadcast 408 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:26:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 306 (MapPartitionsRDD[1643] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:26:53 INFO TaskSchedulerImpl: Adding task set 306.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:26:53 INFO TaskSetManager: Starting task 0.0 in stage 306.0 (TID 306) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:26:53 INFO Executor: Running task 0.0 in stage 306.0 (TID 306)\n",
      "25/05/05 11:26:53 INFO CodeGenerator: Code generated in 4.170125 ms\n",
      "25/05/05 11:26:53 INFO CodeGenerator: Code generated in 3.520916 ms\n",
      "25/05/05 11:26:53 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3333 untilOffset=3334, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=102 taskId=305 partitionId=0\n",
      "25/05/05 11:26:53 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3333 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:53 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3333 untilOffset=3334, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=102 taskId=306 partitionId=0\n",
      "25/05/05 11:26:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:53 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3333 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:53 INFO DAGScheduler: Got job 308 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:26:53 INFO DAGScheduler: Final stage: ResultStage 307 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:26:53 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:26:53 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:26:53 INFO DAGScheduler: Submitting ResultStage 307 (MapPartitionsRDD[1648] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:26:53 INFO MemoryStore: Block broadcast_409 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:26:53 INFO MemoryStore: Block broadcast_409_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.1 MiB)\n",
      "25/05/05 11:26:53 INFO BlockManagerInfo: Added broadcast_409_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:53 INFO SparkContext: Created broadcast 409 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:26:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 307 (MapPartitionsRDD[1648] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:26:53 INFO TaskSchedulerImpl: Adding task set 307.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:26:53 INFO TaskSetManager: Starting task 0.0 in stage 307.0 (TID 307) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:26:53 INFO Executor: Running task 0.0 in stage 307.0 (TID 307)\n",
      "25/05/05 11:26:53 INFO CodeGenerator: Code generated in 3.709625 ms\n",
      "25/05/05 11:26:53 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3333 untilOffset=3334, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=102 taskId=307 partitionId=0\n",
      "25/05/05 11:26:53 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3333 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3334, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3334, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:53 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:26:53 INFO DataWritingSparkTask: Committed partition 0 (task 305, attempt 0, stage 305.0)\n",
      "25/05/05 11:26:53 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 504752875 nanos, during time span of 505145958 nanos.\n",
      "25/05/05 11:26:53 INFO Executor: Finished task 0.0 in stage 305.0 (TID 305). 2145 bytes result sent to driver\n",
      "25/05/05 11:26:53 INFO TaskSetManager: Finished task 0.0 in stage 305.0 (TID 305) in 513 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:26:53 INFO TaskSchedulerImpl: Removed TaskSet 305.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:26:53 INFO DAGScheduler: ResultStage 305 (start at NativeMethodAccessorImpl.java:0) finished in 0.516 s\n",
      "25/05/05 11:26:53 INFO DAGScheduler: Job 306 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:26:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 305: Stage finished\n",
      "25/05/05 11:26:53 INFO DAGScheduler: Job 306 finished: start at NativeMethodAccessorImpl.java:0, took 0.516123 s\n",
      "25/05/05 11:26:53 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 102, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:26:53 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:26:53 INFO DataWritingSparkTask: Committed partition 0 (task 306, attempt 0, stage 306.0)\n",
      "25/05/05 11:26:53 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 502195542 nanos, during time span of 503790125 nanos.\n",
      "25/05/05 11:26:53 INFO Executor: Finished task 0.0 in stage 306.0 (TID 306). 3516 bytes result sent to driver\n",
      "25/05/05 11:26:53 INFO TaskSetManager: Finished task 0.0 in stage 306.0 (TID 306) in 514 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:26:53 INFO TaskSchedulerImpl: Removed TaskSet 306.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:26:53 INFO DAGScheduler: ResultStage 306 (start at NativeMethodAccessorImpl.java:0) finished in 0.515 s\n",
      "25/05/05 11:26:53 INFO DAGScheduler: Job 307 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:26:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 306: Stage finished\n",
      "25/05/05 11:26:53 INFO DAGScheduler: Job 307 finished: start at NativeMethodAccessorImpl.java:0, took 0.517060 s\n",
      "25/05/05 11:26:53 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 102, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@3b506dc1] is committing.\n",
      "25/05/05 11:26:53 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 102, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@3b506dc1] committed.\n",
      "25/05/05 11:26:53 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 102, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:26:53 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/102 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.102.1ebf4843-d2bf-4548-ac38-d89e7cae5e2d.tmp\n",
      "25/05/05 11:26:53 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/102 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.102.af5d258c-6447-45c2-ada2-26a2a07be91c.tmp\n",
      "25/05/05 11:26:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3334, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:53 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:26:53 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:26:53 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:26:53 INFO connection: Opened connection [connectionId{localValue:203, serverValue:4573}] to localhost:27017\n",
      "25/05/05 11:26:53 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=245333}\n",
      "25/05/05 11:26:53 INFO connection: Opened connection [connectionId{localValue:204, serverValue:4574}] to localhost:27017\n",
      "25/05/05 11:26:53 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:26:53 INFO connection: Closed connection [connectionId{localValue:204, serverValue:4574}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:26:53 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 503971875 nanos, during time span of 508100709 nanos.\n",
      "25/05/05 11:26:53 INFO Executor: Finished task 0.0 in stage 307.0 (TID 307). 1645 bytes result sent to driver\n",
      "25/05/05 11:26:53 INFO TaskSetManager: Finished task 0.0 in stage 307.0 (TID 307) in 518 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:26:53 INFO TaskSchedulerImpl: Removed TaskSet 307.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:26:53 INFO DAGScheduler: ResultStage 307 (start at NativeMethodAccessorImpl.java:0) finished in 0.522 s\n",
      "25/05/05 11:26:53 INFO DAGScheduler: Job 308 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:26:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 307: Stage finished\n",
      "25/05/05 11:26:53 INFO DAGScheduler: Job 308 finished: start at NativeMethodAccessorImpl.java:0, took 0.522331 s\n",
      "25/05/05 11:26:53 INFO MemoryStore: Block broadcast_410 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:26:53 INFO MemoryStore: Block broadcast_410_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:26:53 INFO BlockManagerInfo: Added broadcast_410_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:26:53 INFO SparkContext: Created broadcast 410 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:53 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.102.1ebf4843-d2bf-4548-ac38-d89e7cae5e2d.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/102\n",
      "25/05/05 11:26:53 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:26:52.929Z\",\n",
      "  \"batchId\" : 102,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.6260162601626016,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 529,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 0,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 615,\n",
      "    \"walCommit\" : 55\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3333\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3334\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3334\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.6260162601626016,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:26:53 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/102 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.102.bec8c11f-4e6b-47e6-a078-9c609e76298c.tmp\n",
      "25/05/05 11:26:53 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.102.af5d258c-6447-45c2-ada2-26a2a07be91c.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/102\n",
      "25/05/05 11:26:53 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:26:52.929Z\",\n",
      "  \"batchId\" : 102,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.6155088852988693,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 536,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 619,\n",
      "    \"walCommit\" : 51\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3333\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3334\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3334\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.6155088852988693,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:26:53 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.102.bec8c11f-4e6b-47e6-a078-9c609e76298c.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/102\n",
      "25/05/05 11:26:53 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:26:52.934Z\",\n",
      "  \"batchId\" : 102,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.5847860538827259,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 551,\n",
      "    \"commitOffsets\" : 26,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 631,\n",
      "    \"walCommit\" : 50\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3333\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3334\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3334\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.5847860538827259,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 102\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time           |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+----------------------+\n",
      "|B013|R081|02-00-00|161/YANKEE STAD|05/05/2025|11:26:51|REGULAR|6          |10        |2025-05-05 11:26:52.93|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:26:58 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/103 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.103.a1425012-9a84-45ca-a614-12c1b5bb66ad.tmp\n",
      "25/05/05 11:26:58 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/103 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.103.fd0d047a-573e-4c9d-9933-138947b3f417.tmp\n",
      "25/05/05 11:26:58 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/103 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.103.75ded2b6-48a3-4155-8f56-2c0eeaa30a9b.tmp\n",
      "25/05/05 11:26:58 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.103.fd0d047a-573e-4c9d-9933-138947b3f417.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/103\n",
      "25/05/05 11:26:58 INFO MicroBatchExecution: Committed offsets for batch 103. Metadata OffsetSeqMetadata(0,1746458818335,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:26:58 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.103.a1425012-9a84-45ca-a614-12c1b5bb66ad.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/103\n",
      "25/05/05 11:26:58 INFO MicroBatchExecution: Committed offsets for batch 103. Metadata OffsetSeqMetadata(0,1746458818330,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:26:58 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.103.75ded2b6-48a3-4155-8f56-2c0eeaa30a9b.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/103\n",
      "25/05/05 11:26:58 INFO MicroBatchExecution: Committed offsets for batch 103. Metadata OffsetSeqMetadata(0,1746458818335,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:26:58 INFO IncrementalExecution: Current batch timestamp = 1746458818335\n",
      "25/05/05 11:26:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:58 INFO IncrementalExecution: Current batch timestamp = 1746458818330\n",
      "25/05/05 11:26:58 INFO IncrementalExecution: Current batch timestamp = 1746458818335\n",
      "25/05/05 11:26:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:58 INFO IncrementalExecution: Current batch timestamp = 1746458818335\n",
      "25/05/05 11:26:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:58 INFO IncrementalExecution: Current batch timestamp = 1746458818330\n",
      "25/05/05 11:26:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:58 INFO IncrementalExecution: Current batch timestamp = 1746458818335\n",
      "25/05/05 11:26:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:58 INFO IncrementalExecution: Current batch timestamp = 1746458818335\n",
      "25/05/05 11:26:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:58 INFO IncrementalExecution: Current batch timestamp = 1746458818330\n",
      "25/05/05 11:26:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:58 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:26:58 INFO CodeGenerator: Code generated in 3.821041 ms\n",
      "25/05/05 11:26:58 INFO CodeGenerator: Code generated in 3.343209 ms\n",
      "25/05/05 11:26:58 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 103, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@2745a6ef]. The input RDD has 1 partitions.\n",
      "25/05/05 11:26:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:58 INFO DAGScheduler: Got job 309 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:26:58 INFO DAGScheduler: Final stage: ResultStage 308 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:26:58 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:26:58 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:26:58 INFO DAGScheduler: Submitting ResultStage 308 (MapPartitionsRDD[1657] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:26:58 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 103, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:26:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:58 INFO MemoryStore: Block broadcast_411 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:26:58 INFO MemoryStore: Block broadcast_411_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.1 MiB)\n",
      "25/05/05 11:26:58 INFO BlockManagerInfo: Added broadcast_411_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:58 INFO SparkContext: Created broadcast 411 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:26:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 308 (MapPartitionsRDD[1657] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:26:58 INFO TaskSchedulerImpl: Adding task set 308.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:26:58 INFO DAGScheduler: Got job 310 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:26:58 INFO DAGScheduler: Final stage: ResultStage 309 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:26:58 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:26:58 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:26:58 INFO DAGScheduler: Submitting ResultStage 309 (MapPartitionsRDD[1659] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:26:58 INFO TaskSetManager: Starting task 0.0 in stage 308.0 (TID 308) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:26:58 INFO Executor: Running task 0.0 in stage 308.0 (TID 308)\n",
      "25/05/05 11:26:58 INFO MemoryStore: Block broadcast_412 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:26:58 INFO MemoryStore: Block broadcast_412_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:26:58 INFO BlockManagerInfo: Added broadcast_412_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:58 INFO SparkContext: Created broadcast 412 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:26:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 309 (MapPartitionsRDD[1659] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:26:58 INFO TaskSchedulerImpl: Adding task set 309.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:26:58 INFO TaskSetManager: Starting task 0.0 in stage 309.0 (TID 309) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:26:58 INFO Executor: Running task 0.0 in stage 309.0 (TID 309)\n",
      "25/05/05 11:26:58 INFO CodeGenerator: Code generated in 3.530667 ms\n",
      "25/05/05 11:26:58 INFO CodeGenerator: Code generated in 3.578125 ms\n",
      "25/05/05 11:26:58 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3334 untilOffset=3335, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=103 taskId=309 partitionId=0\n",
      "25/05/05 11:26:58 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3334 untilOffset=3335, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=103 taskId=308 partitionId=0\n",
      "25/05/05 11:26:58 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3334 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:58 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3334 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:58 INFO DAGScheduler: Got job 311 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:26:58 INFO DAGScheduler: Final stage: ResultStage 310 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:26:58 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:26:58 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:26:58 INFO DAGScheduler: Submitting ResultStage 310 (MapPartitionsRDD[1664] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:26:58 INFO MemoryStore: Block broadcast_413 stored as values in memory (estimated size 47.1 KiB, free 366.0 MiB)\n",
      "25/05/05 11:26:58 INFO MemoryStore: Block broadcast_413_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.0 MiB)\n",
      "25/05/05 11:26:58 INFO BlockManagerInfo: Added broadcast_413_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:58 INFO SparkContext: Created broadcast 413 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:26:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 310 (MapPartitionsRDD[1664] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:26:58 INFO TaskSchedulerImpl: Adding task set 310.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:26:58 INFO TaskSetManager: Starting task 0.0 in stage 310.0 (TID 310) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:26:58 INFO Executor: Running task 0.0 in stage 310.0 (TID 310)\n",
      "25/05/05 11:26:58 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3334 untilOffset=3335, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=103 taskId=310 partitionId=0\n",
      "25/05/05 11:26:58 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3334 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3335, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3335, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:58 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:26:58 INFO DataWritingSparkTask: Committed partition 0 (task 309, attempt 0, stage 309.0)\n",
      "25/05/05 11:26:58 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 502866916 nanos, during time span of 503417250 nanos.\n",
      "25/05/05 11:26:58 INFO Executor: Finished task 0.0 in stage 309.0 (TID 309). 2145 bytes result sent to driver\n",
      "25/05/05 11:26:58 INFO TaskSetManager: Finished task 0.0 in stage 309.0 (TID 309) in 511 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:26:58 INFO TaskSchedulerImpl: Removed TaskSet 309.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:26:58 INFO DAGScheduler: ResultStage 309 (start at NativeMethodAccessorImpl.java:0) finished in 0.512 s\n",
      "25/05/05 11:26:58 INFO DAGScheduler: Job 310 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:26:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 309: Stage finished\n",
      "25/05/05 11:26:58 INFO DAGScheduler: Job 310 finished: start at NativeMethodAccessorImpl.java:0, took 0.514022 s\n",
      "25/05/05 11:26:58 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 103, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:26:58 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:26:58 INFO DataWritingSparkTask: Committed partition 0 (task 308, attempt 0, stage 308.0)\n",
      "25/05/05 11:26:58 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 502600208 nanos, during time span of 504718875 nanos.\n",
      "25/05/05 11:26:58 INFO Executor: Finished task 0.0 in stage 308.0 (TID 308). 3514 bytes result sent to driver\n",
      "25/05/05 11:26:58 INFO TaskSetManager: Finished task 0.0 in stage 308.0 (TID 308) in 513 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:26:58 INFO TaskSchedulerImpl: Removed TaskSet 308.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:26:58 INFO DAGScheduler: ResultStage 308 (start at NativeMethodAccessorImpl.java:0) finished in 0.515 s\n",
      "25/05/05 11:26:58 INFO DAGScheduler: Job 309 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:26:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 308: Stage finished\n",
      "25/05/05 11:26:58 INFO DAGScheduler: Job 309 finished: start at NativeMethodAccessorImpl.java:0, took 0.515857 s\n",
      "25/05/05 11:26:58 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 103, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@2745a6ef] is committing.\n",
      "25/05/05 11:26:58 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 103, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@2745a6ef] committed.\n",
      "25/05/05 11:26:58 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 103, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:26:58 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/103 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.103.d7d96e32-043b-45e5-b863-36d1d7fc3fde.tmp\n",
      "25/05/05 11:26:58 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/103 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.103.3326d4b3-3279-4376-a750-6303ed8f2b0e.tmp\n",
      "25/05/05 11:26:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:26:58 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3335, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:26:58 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:26:58 INFO connection: Opened connection [connectionId{localValue:205, serverValue:4575}] to localhost:27017\n",
      "25/05/05 11:26:58 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:26:58 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=6088833}\n",
      "25/05/05 11:26:58 INFO connection: Opened connection [connectionId{localValue:206, serverValue:4576}] to localhost:27017\n",
      "25/05/05 11:26:58 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:26:58 INFO connection: Closed connection [connectionId{localValue:206, serverValue:4576}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:26:58 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 502647875 nanos, during time span of 512483541 nanos.\n",
      "25/05/05 11:26:58 INFO Executor: Finished task 0.0 in stage 310.0 (TID 310). 1645 bytes result sent to driver\n",
      "25/05/05 11:26:58 INFO TaskSetManager: Finished task 0.0 in stage 310.0 (TID 310) in 518 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:26:58 INFO TaskSchedulerImpl: Removed TaskSet 310.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:26:58 INFO DAGScheduler: ResultStage 310 (start at NativeMethodAccessorImpl.java:0) finished in 0.522 s\n",
      "25/05/05 11:26:58 INFO DAGScheduler: Job 311 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:26:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 310: Stage finished\n",
      "25/05/05 11:26:58 INFO DAGScheduler: Job 311 finished: start at NativeMethodAccessorImpl.java:0, took 0.522704 s\n",
      "25/05/05 11:26:58 INFO MemoryStore: Block broadcast_414 stored as values in memory (estimated size 248.0 B, free 366.0 MiB)\n",
      "25/05/05 11:26:58 INFO MemoryStore: Block broadcast_414_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.0 MiB)\n",
      "25/05/05 11:26:58 INFO BlockManagerInfo: Added broadcast_414_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:26:58 INFO SparkContext: Created broadcast 414 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:26:58 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.103.d7d96e32-043b-45e5-b863-36d1d7fc3fde.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/103\n",
      "25/05/05 11:26:58 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:26:58.334Z\",\n",
      "  \"batchId\" : 103,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.639344262295082,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 527,\n",
      "    \"commitOffsets\" : 26,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 0,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 610,\n",
      "    \"walCommit\" : 51\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3334\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3335\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3335\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.639344262295082,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:26:58 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/103 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.103.38838199-7bcd-4ce4-9871-959b3774822b.tmp\n",
      "25/05/05 11:26:58 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.103.3326d4b3-3279-4376-a750-6303ed8f2b0e.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/103\n",
      "25/05/05 11:26:58 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:26:58.329Z\",\n",
      "  \"batchId\" : 103,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.6129032258064517,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 532,\n",
      "    \"commitOffsets\" : 26,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 620,\n",
      "    \"walCommit\" : 58\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3334\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3335\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3335\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.6129032258064517,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:26:58 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.103.38838199-7bcd-4ce4-9871-959b3774822b.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/103\n",
      "25/05/05 11:26:58 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:26:58.334Z\",\n",
      "  \"batchId\" : 103,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.5797788309636651,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 550,\n",
      "    \"commitOffsets\" : 25,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 633,\n",
      "    \"walCommit\" : 53\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3334\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3335\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3335\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.5797788309636651,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 103\n",
      "-------------------------------------------\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+----------------------+\n",
      "|C/A |UNIT|SCP     |STATION      |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time           |\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+----------------------+\n",
      "|R016|R032|00-00-01|TIME SQ-42 ST|05/05/2025|11:26:57|REGULAR|12         |8         |2025-05-05 11:26:58.33|\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:26:59 INFO BlockManagerInfo: Removed broadcast_410_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:26:59 INFO BlockManagerInfo: Removed broadcast_408_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:59 INFO BlockManagerInfo: Removed broadcast_411_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:59 INFO BlockManagerInfo: Removed broadcast_409_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:59 INFO BlockManagerInfo: Removed broadcast_414_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:26:59 INFO BlockManagerInfo: Removed broadcast_407_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:26:59 INFO BlockManagerInfo: Removed broadcast_412_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:26:59 INFO BlockManagerInfo: Removed broadcast_413_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:27:04 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/104 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.104.f3026928-ba02-41f6-ae46-e013c4643583.tmp\n",
      "25/05/05 11:27:04 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/104 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.104.d8b00e46-2914-4547-8920-2e18893b33bf.tmp\n",
      "25/05/05 11:27:04 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/104 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.104.63317f7b-974c-4534-8464-ab1db2b8c76e.tmp\n",
      "25/05/05 11:27:04 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.104.f3026928-ba02-41f6-ae46-e013c4643583.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/104\n",
      "25/05/05 11:27:04 INFO MicroBatchExecution: Committed offsets for batch 104. Metadata OffsetSeqMetadata(0,1746458824731,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:27:04 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.104.63317f7b-974c-4534-8464-ab1db2b8c76e.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/104\n",
      "25/05/05 11:27:04 INFO MicroBatchExecution: Committed offsets for batch 104. Metadata OffsetSeqMetadata(0,1746458824731,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:27:04 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.104.d8b00e46-2914-4547-8920-2e18893b33bf.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/104\n",
      "25/05/05 11:27:04 INFO MicroBatchExecution: Committed offsets for batch 104. Metadata OffsetSeqMetadata(0,1746458824732,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:27:04 INFO IncrementalExecution: Current batch timestamp = 1746458824731\n",
      "25/05/05 11:27:04 INFO IncrementalExecution: Current batch timestamp = 1746458824731\n",
      "25/05/05 11:27:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:04 INFO IncrementalExecution: Current batch timestamp = 1746458824732\n",
      "25/05/05 11:27:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:04 INFO IncrementalExecution: Current batch timestamp = 1746458824731\n",
      "25/05/05 11:27:04 INFO IncrementalExecution: Current batch timestamp = 1746458824732\n",
      "25/05/05 11:27:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:04 INFO IncrementalExecution: Current batch timestamp = 1746458824731\n",
      "25/05/05 11:27:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:04 INFO IncrementalExecution: Current batch timestamp = 1746458824732\n",
      "25/05/05 11:27:04 INFO IncrementalExecution: Current batch timestamp = 1746458824731\n",
      "25/05/05 11:27:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:04 INFO CodeGenerator: Code generated in 4.025875 ms\n",
      "25/05/05 11:27:04 INFO CodeGenerator: Code generated in 3.583375 ms\n",
      "25/05/05 11:27:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 104, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:27:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 104, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@1aeef96e]. The input RDD has 1 partitions.\n",
      "25/05/05 11:27:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:04 INFO DAGScheduler: Got job 312 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:27:04 INFO DAGScheduler: Final stage: ResultStage 311 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:27:04 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:27:04 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:27:04 INFO DAGScheduler: Submitting ResultStage 311 (MapPartitionsRDD[1674] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:27:04 INFO MemoryStore: Block broadcast_415 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:27:04 INFO MemoryStore: Block broadcast_415_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:27:04 INFO BlockManagerInfo: Added broadcast_415_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:27:04 INFO SparkContext: Created broadcast 415 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:27:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 311 (MapPartitionsRDD[1674] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:27:04 INFO TaskSchedulerImpl: Adding task set 311.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:27:04 INFO DAGScheduler: Got job 313 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:27:04 INFO DAGScheduler: Final stage: ResultStage 312 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:27:04 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:27:04 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:27:04 INFO DAGScheduler: Submitting ResultStage 312 (MapPartitionsRDD[1675] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:27:04 INFO TaskSetManager: Starting task 0.0 in stage 311.0 (TID 311) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:27:04 INFO Executor: Running task 0.0 in stage 311.0 (TID 311)\n",
      "25/05/05 11:27:04 INFO MemoryStore: Block broadcast_416 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:27:04 INFO MemoryStore: Block broadcast_416_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:27:04 INFO BlockManagerInfo: Added broadcast_416_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:27:04 INFO SparkContext: Created broadcast 416 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:27:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 312 (MapPartitionsRDD[1675] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:27:04 INFO TaskSchedulerImpl: Adding task set 312.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:27:04 INFO TaskSetManager: Starting task 0.0 in stage 312.0 (TID 312) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:27:04 INFO Executor: Running task 0.0 in stage 312.0 (TID 312)\n",
      "25/05/05 11:27:04 INFO CodeGenerator: Code generated in 3.897917 ms\n",
      "25/05/05 11:27:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3335 untilOffset=3336, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=104 taskId=311 partitionId=0\n",
      "25/05/05 11:27:04 INFO CodeGenerator: Code generated in 3.546666 ms\n",
      "25/05/05 11:27:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3335 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3335 untilOffset=3336, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=104 taskId=312 partitionId=0\n",
      "25/05/05 11:27:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3335 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:04 INFO DAGScheduler: Got job 314 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:27:04 INFO DAGScheduler: Final stage: ResultStage 313 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:27:04 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:27:04 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:27:04 INFO DAGScheduler: Submitting ResultStage 313 (MapPartitionsRDD[1680] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:27:04 INFO MemoryStore: Block broadcast_417 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:27:04 INFO MemoryStore: Block broadcast_417_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.1 MiB)\n",
      "25/05/05 11:27:04 INFO BlockManagerInfo: Added broadcast_417_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:27:04 INFO SparkContext: Created broadcast 417 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:27:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 313 (MapPartitionsRDD[1680] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:27:04 INFO TaskSchedulerImpl: Adding task set 313.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:27:04 INFO TaskSetManager: Starting task 0.0 in stage 313.0 (TID 313) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:27:04 INFO Executor: Running task 0.0 in stage 313.0 (TID 313)\n",
      "25/05/05 11:27:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3335 untilOffset=3336, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=104 taskId=313 partitionId=0\n",
      "25/05/05 11:27:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3335 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3336, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3336, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:05 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:27:05 INFO DataWritingSparkTask: Committed partition 0 (task 311, attempt 0, stage 311.0)\n",
      "25/05/05 11:27:05 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 503299375 nanos, during time span of 503653458 nanos.\n",
      "25/05/05 11:27:05 INFO Executor: Finished task 0.0 in stage 311.0 (TID 311). 2145 bytes result sent to driver\n",
      "25/05/05 11:27:05 INFO TaskSetManager: Finished task 0.0 in stage 311.0 (TID 311) in 512 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:27:05 INFO TaskSchedulerImpl: Removed TaskSet 311.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:27:05 INFO DAGScheduler: ResultStage 311 (start at NativeMethodAccessorImpl.java:0) finished in 0.513 s\n",
      "25/05/05 11:27:05 INFO DAGScheduler: Job 312 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:27:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 311: Stage finished\n",
      "25/05/05 11:27:05 INFO DAGScheduler: Job 312 finished: start at NativeMethodAccessorImpl.java:0, took 0.514257 s\n",
      "25/05/05 11:27:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 104, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:27:05 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:27:05 INFO DataWritingSparkTask: Committed partition 0 (task 312, attempt 0, stage 312.0)\n",
      "25/05/05 11:27:05 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 501643083 nanos, during time span of 503192458 nanos.\n",
      "25/05/05 11:27:05 INFO Executor: Finished task 0.0 in stage 312.0 (TID 312). 3510 bytes result sent to driver\n",
      "25/05/05 11:27:05 INFO TaskSetManager: Finished task 0.0 in stage 312.0 (TID 312) in 512 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:27:05 INFO TaskSchedulerImpl: Removed TaskSet 312.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:27:05 INFO DAGScheduler: ResultStage 312 (start at NativeMethodAccessorImpl.java:0) finished in 0.513 s\n",
      "25/05/05 11:27:05 INFO DAGScheduler: Job 313 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:27:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 312: Stage finished\n",
      "25/05/05 11:27:05 INFO DAGScheduler: Job 313 finished: start at NativeMethodAccessorImpl.java:0, took 0.514994 s\n",
      "25/05/05 11:27:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 104, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@1aeef96e] is committing.\n",
      "25/05/05 11:27:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 104, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@1aeef96e] committed.\n",
      "25/05/05 11:27:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 104, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:27:05 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/104 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.104.ebe3b8dc-7f6b-4b55-bd6c-26bdb88e34e3.tmp\n",
      "25/05/05 11:27:05 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/104 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.104.bde39955-607d-4e3a-8215-1e31f1e2132e.tmp\n",
      "25/05/05 11:27:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3336, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:05 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:27:05 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:27:05 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:27:05 INFO connection: Opened connection [connectionId{localValue:207, serverValue:4577}] to localhost:27017\n",
      "25/05/05 11:27:05 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=238375}\n",
      "25/05/05 11:27:05 INFO connection: Opened connection [connectionId{localValue:208, serverValue:4578}] to localhost:27017\n",
      "25/05/05 11:27:05 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:27:05 INFO connection: Closed connection [connectionId{localValue:208, serverValue:4578}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:27:05 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 502553292 nanos, during time span of 506623167 nanos.\n",
      "25/05/05 11:27:05 INFO Executor: Finished task 0.0 in stage 313.0 (TID 313). 1645 bytes result sent to driver\n",
      "25/05/05 11:27:05 INFO TaskSetManager: Finished task 0.0 in stage 313.0 (TID 313) in 512 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:27:05 INFO TaskSchedulerImpl: Removed TaskSet 313.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:27:05 INFO DAGScheduler: ResultStage 313 (start at NativeMethodAccessorImpl.java:0) finished in 0.516 s\n",
      "25/05/05 11:27:05 INFO DAGScheduler: Job 314 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:27:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 313: Stage finished\n",
      "25/05/05 11:27:05 INFO DAGScheduler: Job 314 finished: start at NativeMethodAccessorImpl.java:0, took 0.516394 s\n",
      "25/05/05 11:27:05 INFO MemoryStore: Block broadcast_418 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:27:05 INFO MemoryStore: Block broadcast_418_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:27:05 INFO BlockManagerInfo: Added broadcast_418_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:27:05 INFO SparkContext: Created broadcast 418 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:05 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/104 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.104.7f007a25-55f2-469b-88f6-67cc2e9b8457.tmp\n",
      "25/05/05 11:27:05 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.104.ebe3b8dc-7f6b-4b55-bd6c-26bdb88e34e3.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/104\n",
      "25/05/05 11:27:05 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:27:04.730Z\",\n",
      "  \"batchId\" : 104,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "  \"processedRowsPerSecond\" : 1.6260162601626016,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 526,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 615,\n",
      "    \"walCommit\" : 55\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3335\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3336\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3336\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "    \"processedRowsPerSecond\" : 1.6260162601626016,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:27:05 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.104.bde39955-607d-4e3a-8215-1e31f1e2132e.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/104\n",
      "25/05/05 11:27:05 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:27:04.730Z\",\n",
      "  \"batchId\" : 104,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "  \"processedRowsPerSecond\" : 1.6129032258064517,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 532,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 620,\n",
      "    \"walCommit\" : 55\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3335\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3336\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3336\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "    \"processedRowsPerSecond\" : 1.6129032258064517,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:27:05 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.104.7f007a25-55f2-469b-88f6-67cc2e9b8457.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/104\n",
      "25/05/05 11:27:05 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:27:04.730Z\",\n",
      "  \"batchId\" : 104,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "  \"processedRowsPerSecond\" : 1.5847860538827259,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 546,\n",
      "    \"commitOffsets\" : 25,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 631,\n",
      "    \"walCommit\" : 54\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3335\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3336\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3336\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 58.8235294117647,\n",
      "    \"processedRowsPerSecond\" : 1.5847860538827259,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 104\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION  |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|A002|R170|00-00-00|FULTON ST|05/05/2025|11:27:03|REGULAR|3          |8         |2025-05-05 11:27:04.731|\n",
      "+----+----+--------+---------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:27:09 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/105 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.105.9e5ce1d3-0663-4ede-94f0-d837d7a6cb78.tmp\n",
      "25/05/05 11:27:09 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/105 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.105.9d7a69b6-2ba4-49e3-ab93-77d6d43a3f70.tmp\n",
      "25/05/05 11:27:09 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/105 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.105.87f78ef8-e9ab-4806-a2a2-f6a0b17102e8.tmp\n",
      "25/05/05 11:27:09 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.105.9e5ce1d3-0663-4ede-94f0-d837d7a6cb78.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/105\n",
      "25/05/05 11:27:09 INFO MicroBatchExecution: Committed offsets for batch 105. Metadata OffsetSeqMetadata(0,1746458829217,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:27:09 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.105.87f78ef8-e9ab-4806-a2a2-f6a0b17102e8.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/105\n",
      "25/05/05 11:27:09 INFO MicroBatchExecution: Committed offsets for batch 105. Metadata OffsetSeqMetadata(0,1746458829217,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:27:09 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.105.9d7a69b6-2ba4-49e3-ab93-77d6d43a3f70.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/105\n",
      "25/05/05 11:27:09 INFO MicroBatchExecution: Committed offsets for batch 105. Metadata OffsetSeqMetadata(0,1746458829216,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:27:09 INFO IncrementalExecution: Current batch timestamp = 1746458829217\n",
      "25/05/05 11:27:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:09 INFO IncrementalExecution: Current batch timestamp = 1746458829217\n",
      "25/05/05 11:27:09 INFO IncrementalExecution: Current batch timestamp = 1746458829216\n",
      "25/05/05 11:27:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:09 INFO IncrementalExecution: Current batch timestamp = 1746458829217\n",
      "25/05/05 11:27:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:09 INFO IncrementalExecution: Current batch timestamp = 1746458829217\n",
      "25/05/05 11:27:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:09 INFO IncrementalExecution: Current batch timestamp = 1746458829216\n",
      "25/05/05 11:27:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:09 INFO IncrementalExecution: Current batch timestamp = 1746458829217\n",
      "25/05/05 11:27:09 INFO IncrementalExecution: Current batch timestamp = 1746458829217\n",
      "25/05/05 11:27:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:09 INFO CodeGenerator: Code generated in 3.946583 ms\n",
      "25/05/05 11:27:09 INFO CodeGenerator: Code generated in 3.5245 ms\n",
      "25/05/05 11:27:09 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 105, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4cc4a14d]. The input RDD has 1 partitions.\n",
      "25/05/05 11:27:09 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 105, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:27:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:09 INFO DAGScheduler: Got job 315 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:27:09 INFO DAGScheduler: Final stage: ResultStage 314 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:27:09 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:27:09 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:27:09 INFO DAGScheduler: Submitting ResultStage 314 (MapPartitionsRDD[1690] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:27:09 INFO MemoryStore: Block broadcast_419 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:27:09 INFO MemoryStore: Block broadcast_419_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.1 MiB)\n",
      "25/05/05 11:27:09 INFO BlockManagerInfo: Added broadcast_419_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:27:09 INFO SparkContext: Created broadcast 419 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:27:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 314 (MapPartitionsRDD[1690] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:27:09 INFO TaskSchedulerImpl: Adding task set 314.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:27:09 INFO DAGScheduler: Got job 316 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:27:09 INFO DAGScheduler: Final stage: ResultStage 315 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:27:09 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:27:09 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:27:09 INFO DAGScheduler: Submitting ResultStage 315 (MapPartitionsRDD[1691] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:27:09 INFO TaskSetManager: Starting task 0.0 in stage 314.0 (TID 314) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:27:09 INFO Executor: Running task 0.0 in stage 314.0 (TID 314)\n",
      "25/05/05 11:27:09 INFO MemoryStore: Block broadcast_420 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:27:09 INFO MemoryStore: Block broadcast_420_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:27:09 INFO BlockManagerInfo: Added broadcast_420_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:27:09 INFO SparkContext: Created broadcast 420 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:27:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 315 (MapPartitionsRDD[1691] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:27:09 INFO TaskSchedulerImpl: Adding task set 315.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:27:09 INFO TaskSetManager: Starting task 0.0 in stage 315.0 (TID 315) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:27:09 INFO Executor: Running task 0.0 in stage 315.0 (TID 315)\n",
      "25/05/05 11:27:09 INFO CodeGenerator: Code generated in 3.816625 ms\n",
      "25/05/05 11:27:09 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3336 untilOffset=3337, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=105 taskId=315 partitionId=0\n",
      "25/05/05 11:27:09 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3336 untilOffset=3337, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=105 taskId=314 partitionId=0\n",
      "25/05/05 11:27:09 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3336 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:09 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3336 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:09 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:09 INFO DAGScheduler: Got job 317 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:27:09 INFO DAGScheduler: Final stage: ResultStage 316 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:27:09 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:27:09 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:27:09 INFO DAGScheduler: Submitting ResultStage 316 (MapPartitionsRDD[1696] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:27:09 INFO MemoryStore: Block broadcast_421 stored as values in memory (estimated size 47.1 KiB, free 366.0 MiB)\n",
      "25/05/05 11:27:09 INFO MemoryStore: Block broadcast_421_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:27:09 INFO BlockManagerInfo: Added broadcast_421_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:27:09 INFO SparkContext: Created broadcast 421 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:27:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 316 (MapPartitionsRDD[1696] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:27:09 INFO TaskSchedulerImpl: Adding task set 316.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:27:09 INFO TaskSetManager: Starting task 0.0 in stage 316.0 (TID 316) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:27:09 INFO Executor: Running task 0.0 in stage 316.0 (TID 316)\n",
      "25/05/05 11:27:09 INFO CodeGenerator: Code generated in 3.52575 ms\n",
      "25/05/05 11:27:09 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3336 untilOffset=3337, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=105 taskId=316 partitionId=0\n",
      "25/05/05 11:27:09 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3336 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3337, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3337, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:09 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:27:09 INFO DataWritingSparkTask: Committed partition 0 (task 315, attempt 0, stage 315.0)\n",
      "25/05/05 11:27:09 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 504895292 nanos, during time span of 505281541 nanos.\n",
      "25/05/05 11:27:09 INFO Executor: Finished task 0.0 in stage 315.0 (TID 315). 2145 bytes result sent to driver\n",
      "25/05/05 11:27:09 INFO TaskSetManager: Finished task 0.0 in stage 315.0 (TID 315) in 512 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:27:09 INFO TaskSchedulerImpl: Removed TaskSet 315.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:27:09 INFO DAGScheduler: ResultStage 315 (start at NativeMethodAccessorImpl.java:0) finished in 0.513 s\n",
      "25/05/05 11:27:09 INFO DAGScheduler: Job 316 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:27:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 315: Stage finished\n",
      "25/05/05 11:27:09 INFO DAGScheduler: Job 316 finished: start at NativeMethodAccessorImpl.java:0, took 0.515450 s\n",
      "25/05/05 11:27:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 105, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:27:09 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:27:09 INFO DataWritingSparkTask: Committed partition 0 (task 314, attempt 0, stage 314.0)\n",
      "25/05/05 11:27:09 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 504238375 nanos, during time span of 505891291 nanos.\n",
      "25/05/05 11:27:09 INFO Executor: Finished task 0.0 in stage 314.0 (TID 314). 3516 bytes result sent to driver\n",
      "25/05/05 11:27:09 INFO TaskSetManager: Finished task 0.0 in stage 314.0 (TID 314) in 515 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:27:09 INFO TaskSchedulerImpl: Removed TaskSet 314.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:27:09 INFO DAGScheduler: ResultStage 314 (start at NativeMethodAccessorImpl.java:0) finished in 0.517 s\n",
      "25/05/05 11:27:09 INFO DAGScheduler: Job 315 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:27:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 314: Stage finished\n",
      "25/05/05 11:27:09 INFO DAGScheduler: Job 315 finished: start at NativeMethodAccessorImpl.java:0, took 0.516984 s\n",
      "25/05/05 11:27:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 105, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4cc4a14d] is committing.\n",
      "25/05/05 11:27:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 105, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4cc4a14d] committed.\n",
      "25/05/05 11:27:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 105, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:27:09 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/105 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.105.ebdee3d9-673c-47cc-9b38-ec82d18a70c7.tmp\n",
      "25/05/05 11:27:09 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/105 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.105.662e15e8-d242-473d-9733-f2cf1ed334b9.tmp\n",
      "25/05/05 11:27:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3337, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:09 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:27:09 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:27:09 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:27:09 INFO connection: Opened connection [connectionId{localValue:209, serverValue:4579}] to localhost:27017\n",
      "25/05/05 11:27:09 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=197084}\n",
      "25/05/05 11:27:09 INFO connection: Opened connection [connectionId{localValue:210, serverValue:4580}] to localhost:27017\n",
      "25/05/05 11:27:09 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:27:09 INFO connection: Closed connection [connectionId{localValue:210, serverValue:4580}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:27:09 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 506217667 nanos, during time span of 510556334 nanos.\n",
      "25/05/05 11:27:09 INFO Executor: Finished task 0.0 in stage 316.0 (TID 316). 1645 bytes result sent to driver\n",
      "25/05/05 11:27:09 INFO TaskSetManager: Finished task 0.0 in stage 316.0 (TID 316) in 520 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:27:09 INFO TaskSchedulerImpl: Removed TaskSet 316.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:27:09 INFO DAGScheduler: ResultStage 316 (start at NativeMethodAccessorImpl.java:0) finished in 0.523 s\n",
      "25/05/05 11:27:09 INFO DAGScheduler: Job 317 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:27:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 316: Stage finished\n",
      "25/05/05 11:27:09 INFO DAGScheduler: Job 317 finished: start at NativeMethodAccessorImpl.java:0, took 0.524010 s\n",
      "25/05/05 11:27:09 INFO MemoryStore: Block broadcast_422 stored as values in memory (estimated size 248.0 B, free 366.0 MiB)\n",
      "25/05/05 11:27:09 INFO MemoryStore: Block broadcast_422_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.0 MiB)\n",
      "25/05/05 11:27:09 INFO BlockManagerInfo: Added broadcast_422_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:27:09 INFO SparkContext: Created broadcast 422 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:09 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.105.ebdee3d9-673c-47cc-9b38-ec82d18a70c7.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/105\n",
      "25/05/05 11:27:09 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:27:09.215Z\",\n",
      "  \"batchId\" : 105,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.6181229773462784,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 528,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 618,\n",
      "    \"walCommit\" : 58\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3336\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3337\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3337\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.6181229773462784,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:27:09 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/105 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.105.f5d5be93-b61e-43c4-a3be-a90ef4fd200d.tmp\n",
      "25/05/05 11:27:09 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.105.662e15e8-d242-473d-9733-f2cf1ed334b9.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/105\n",
      "25/05/05 11:27:09 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:27:09.215Z\",\n",
      "  \"batchId\" : 105,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.607717041800643,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 533,\n",
      "    \"commitOffsets\" : 26,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 622,\n",
      "    \"walCommit\" : 56\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3336\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3337\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3337\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.607717041800643,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:27:09 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.105.f5d5be93-b61e-43c4-a3be-a90ef4fd200d.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/105\n",
      "25/05/05 11:27:09 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:27:09.215Z\",\n",
      "  \"batchId\" : 105,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.5625,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 552,\n",
      "    \"commitOffsets\" : 25,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 640,\n",
      "    \"walCommit\" : 59\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3336\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3337\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3337\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.5625,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 105\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R023|R045|00-00-00|34 ST-HERALD SQ|05/05/2025|11:27:08|REGULAR|12         |5         |2025-05-05 11:27:09.217|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:27:10 INFO BlockManagerInfo: Removed broadcast_421_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:27:10 INFO BlockManagerInfo: Removed broadcast_420_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:27:10 INFO BlockManagerInfo: Removed broadcast_417_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:27:10 INFO BlockManagerInfo: Removed broadcast_418_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:27:10 INFO BlockManagerInfo: Removed broadcast_419_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:27:10 INFO BlockManagerInfo: Removed broadcast_415_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:27:10 INFO BlockManagerInfo: Removed broadcast_422_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:27:10 INFO BlockManagerInfo: Removed broadcast_416_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:27:15 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/106 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.106.21d916dd-9f18-4414-ac91-efdc6273c080.tmp\n",
      "25/05/05 11:27:15 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/106 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.106.b4d7f624-4c71-47a9-972c-ee502da9aad6.tmp\n",
      "25/05/05 11:27:15 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/106 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.106.eb80d681-2eda-4432-9ff9-0f0e3b5b3384.tmp\n",
      "25/05/05 11:27:15 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.106.21d916dd-9f18-4414-ac91-efdc6273c080.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/106\n",
      "25/05/05 11:27:15 INFO MicroBatchExecution: Committed offsets for batch 106. Metadata OffsetSeqMetadata(0,1746458835620,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:27:15 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.106.b4d7f624-4c71-47a9-972c-ee502da9aad6.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/106\n",
      "25/05/05 11:27:15 INFO MicroBatchExecution: Committed offsets for batch 106. Metadata OffsetSeqMetadata(0,1746458835619,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:27:15 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.106.eb80d681-2eda-4432-9ff9-0f0e3b5b3384.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/106\n",
      "25/05/05 11:27:15 INFO MicroBatchExecution: Committed offsets for batch 106. Metadata OffsetSeqMetadata(0,1746458835625,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:27:15 INFO IncrementalExecution: Current batch timestamp = 1746458835619\n",
      "25/05/05 11:27:15 INFO IncrementalExecution: Current batch timestamp = 1746458835620\n",
      "25/05/05 11:27:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:15 INFO IncrementalExecution: Current batch timestamp = 1746458835625\n",
      "25/05/05 11:27:15 INFO IncrementalExecution: Current batch timestamp = 1746458835619\n",
      "25/05/05 11:27:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:15 INFO IncrementalExecution: Current batch timestamp = 1746458835620\n",
      "25/05/05 11:27:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:15 INFO IncrementalExecution: Current batch timestamp = 1746458835625\n",
      "25/05/05 11:27:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:15 INFO IncrementalExecution: Current batch timestamp = 1746458835619\n",
      "25/05/05 11:27:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:15 INFO IncrementalExecution: Current batch timestamp = 1746458835625\n",
      "25/05/05 11:27:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:15 INFO CodeGenerator: Code generated in 3.917917 ms\n",
      "25/05/05 11:27:15 INFO CodeGenerator: Code generated in 4.311333 ms\n",
      "25/05/05 11:27:15 INFO CodeGenerator: Code generated in 3.723917 ms\n",
      "25/05/05 11:27:15 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 106, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:27:15 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:15 INFO DAGScheduler: Got job 318 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:27:15 INFO DAGScheduler: Final stage: ResultStage 317 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:27:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:27:15 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:27:15 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 106, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@14981f4a]. The input RDD has 1 partitions.\n",
      "25/05/05 11:27:15 INFO DAGScheduler: Submitting ResultStage 317 (MapPartitionsRDD[1705] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:27:15 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:15 INFO MemoryStore: Block broadcast_423 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:27:15 INFO MemoryStore: Block broadcast_423_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:27:15 INFO BlockManagerInfo: Added broadcast_423_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:27:15 INFO SparkContext: Created broadcast 423 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:27:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 317 (MapPartitionsRDD[1705] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:27:15 INFO TaskSchedulerImpl: Adding task set 317.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:27:15 INFO DAGScheduler: Got job 319 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:27:15 INFO DAGScheduler: Final stage: ResultStage 318 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:27:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:27:15 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:27:15 INFO DAGScheduler: Submitting ResultStage 318 (MapPartitionsRDD[1707] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:27:15 INFO TaskSetManager: Starting task 0.0 in stage 317.0 (TID 317) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:27:15 INFO Executor: Running task 0.0 in stage 317.0 (TID 317)\n",
      "25/05/05 11:27:15 INFO MemoryStore: Block broadcast_424 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:27:15 INFO MemoryStore: Block broadcast_424_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:27:15 INFO BlockManagerInfo: Added broadcast_424_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:27:15 INFO SparkContext: Created broadcast 424 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:27:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 318 (MapPartitionsRDD[1707] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:27:15 INFO TaskSchedulerImpl: Adding task set 318.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:27:15 INFO TaskSetManager: Starting task 0.0 in stage 318.0 (TID 318) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:27:15 INFO Executor: Running task 0.0 in stage 318.0 (TID 318)\n",
      "25/05/05 11:27:15 INFO CodeGenerator: Code generated in 3.944167 ms\n",
      "25/05/05 11:27:15 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3337 untilOffset=3338, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=106 taskId=317 partitionId=0\n",
      "25/05/05 11:27:15 INFO CodeGenerator: Code generated in 3.517375 ms\n",
      "25/05/05 11:27:15 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3337 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:15 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3337 untilOffset=3338, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=106 taskId=318 partitionId=0\n",
      "25/05/05 11:27:15 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:15 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3337 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:15 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:15 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:15 INFO DAGScheduler: Got job 320 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:27:15 INFO DAGScheduler: Final stage: ResultStage 319 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:27:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:27:15 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:27:15 INFO DAGScheduler: Submitting ResultStage 319 (MapPartitionsRDD[1712] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:27:15 INFO MemoryStore: Block broadcast_425 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:27:15 INFO MemoryStore: Block broadcast_425_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:27:15 INFO BlockManagerInfo: Added broadcast_425_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:27:15 INFO SparkContext: Created broadcast 425 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:27:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 319 (MapPartitionsRDD[1712] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:27:15 INFO TaskSchedulerImpl: Adding task set 319.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:27:15 INFO TaskSetManager: Starting task 0.0 in stage 319.0 (TID 319) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:27:15 INFO Executor: Running task 0.0 in stage 319.0 (TID 319)\n",
      "25/05/05 11:27:15 INFO CodeGenerator: Code generated in 3.581833 ms\n",
      "25/05/05 11:27:15 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3337 untilOffset=3338, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=106 taskId=319 partitionId=0\n",
      "25/05/05 11:27:15 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3337 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:15 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:16 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:16 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:16 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:16 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:16 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3338, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:16 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3338, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:16 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:27:16 INFO DataWritingSparkTask: Committed partition 0 (task 317, attempt 0, stage 317.0)\n",
      "25/05/05 11:27:16 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 504836875 nanos, during time span of 505212125 nanos.\n",
      "25/05/05 11:27:16 INFO Executor: Finished task 0.0 in stage 317.0 (TID 317). 2145 bytes result sent to driver\n",
      "25/05/05 11:27:16 INFO TaskSetManager: Finished task 0.0 in stage 317.0 (TID 317) in 513 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:27:16 INFO TaskSchedulerImpl: Removed TaskSet 317.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:27:16 INFO DAGScheduler: ResultStage 317 (start at NativeMethodAccessorImpl.java:0) finished in 0.515 s\n",
      "25/05/05 11:27:16 INFO DAGScheduler: Job 318 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:27:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 317: Stage finished\n",
      "25/05/05 11:27:16 INFO DAGScheduler: Job 318 finished: start at NativeMethodAccessorImpl.java:0, took 0.515741 s\n",
      "25/05/05 11:27:16 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:27:16 INFO DataWritingSparkTask: Committed partition 0 (task 318, attempt 0, stage 318.0)\n",
      "25/05/05 11:27:16 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 502643958 nanos, during time span of 504259959 nanos.\n",
      "25/05/05 11:27:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 106, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:27:16 INFO Executor: Finished task 0.0 in stage 318.0 (TID 318). 3514 bytes result sent to driver\n",
      "25/05/05 11:27:16 INFO TaskSetManager: Finished task 0.0 in stage 318.0 (TID 318) in 514 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:27:16 INFO TaskSchedulerImpl: Removed TaskSet 318.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:27:16 INFO DAGScheduler: ResultStage 318 (start at NativeMethodAccessorImpl.java:0) finished in 0.516 s\n",
      "25/05/05 11:27:16 INFO DAGScheduler: Job 319 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:27:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 318: Stage finished\n",
      "25/05/05 11:27:16 INFO DAGScheduler: Job 319 finished: start at NativeMethodAccessorImpl.java:0, took 0.517257 s\n",
      "25/05/05 11:27:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 106, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@14981f4a] is committing.\n",
      "25/05/05 11:27:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 106, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@14981f4a] committed.\n",
      "25/05/05 11:27:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 106, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:27:16 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/106 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.106.43570772-be1d-4473-b786-f9784ed651b8.tmp\n",
      "25/05/05 11:27:16 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/106 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.106.86e58579-149e-48e6-8011-8eed422059df.tmp\n",
      "25/05/05 11:27:16 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:16 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:16 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3338, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:16 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:27:16 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:27:16 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:27:16 INFO connection: Opened connection [connectionId{localValue:211, serverValue:4581}] to localhost:27017\n",
      "25/05/05 11:27:16 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=183416}\n",
      "25/05/05 11:27:16 INFO connection: Opened connection [connectionId{localValue:212, serverValue:4582}] to localhost:27017\n",
      "25/05/05 11:27:16 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:27:16 INFO connection: Closed connection [connectionId{localValue:212, serverValue:4582}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:27:16 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 504699208 nanos, during time span of 508930125 nanos.\n",
      "25/05/05 11:27:16 INFO Executor: Finished task 0.0 in stage 319.0 (TID 319). 1645 bytes result sent to driver\n",
      "25/05/05 11:27:16 INFO TaskSetManager: Finished task 0.0 in stage 319.0 (TID 319) in 519 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:27:16 INFO TaskSchedulerImpl: Removed TaskSet 319.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:27:16 INFO DAGScheduler: ResultStage 319 (start at NativeMethodAccessorImpl.java:0) finished in 0.523 s\n",
      "25/05/05 11:27:16 INFO DAGScheduler: Job 320 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:27:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 319: Stage finished\n",
      "25/05/05 11:27:16 INFO DAGScheduler: Job 320 finished: start at NativeMethodAccessorImpl.java:0, took 0.523369 s\n",
      "25/05/05 11:27:16 INFO MemoryStore: Block broadcast_426 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:27:16 INFO MemoryStore: Block broadcast_426_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:27:16 INFO BlockManagerInfo: Added broadcast_426_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:27:16 INFO SparkContext: Created broadcast 426 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:16 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/106 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.106.54eeff93-9abe-4a75-a8ca-e92df65ede9e.tmp\n",
      "25/05/05 11:27:16 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.106.43570772-be1d-4473-b786-f9784ed651b8.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/106\n",
      "25/05/05 11:27:16 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:27:15.624Z\",\n",
      "  \"batchId\" : 106,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.6447368421052633,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 532,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 608,\n",
      "    \"walCommit\" : 41\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3337\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3338\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3338\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.6447368421052633,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:27:16 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.106.86e58579-149e-48e6-8011-8eed422059df.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/106\n",
      "25/05/05 11:27:16 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:27:15.618Z\",\n",
      "  \"batchId\" : 106,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.6207455429497568,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 539,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 617,\n",
      "    \"walCommit\" : 44\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3337\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3338\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3338\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.6207455429497568,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:27:16 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.106.54eeff93-9abe-4a75-a8ca-e92df65ede9e.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/106\n",
      "25/05/05 11:27:16 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:27:15.618Z\",\n",
      "  \"batchId\" : 106,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.589825119236884,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 554,\n",
      "    \"commitOffsets\" : 25,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 629,\n",
      "    \"walCommit\" : 43\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3337\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3338\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3338\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.589825119236884,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 106\n",
      "-------------------------------------------\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION      |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R016|R032|00-00-01|TIME SQ-42 ST|05/05/2025|11:27:14|REGULAR|14         |6         |2025-05-05 11:27:15.619|\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:27:20 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/107 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.107.fb3242d0-30d2-40c0-9e9a-1e6d8b24547c.tmp\n",
      "25/05/05 11:27:20 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/107 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.107.46640418-5fbe-46ce-96df-9ffedd21a7a6.tmp\n",
      "25/05/05 11:27:20 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/107 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.107.c9aca10c-f5a1-4c32-af3b-21a5c76aa903.tmp\n",
      "25/05/05 11:27:20 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.107.c9aca10c-f5a1-4c32-af3b-21a5c76aa903.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/107\n",
      "25/05/05 11:27:20 INFO MicroBatchExecution: Committed offsets for batch 107. Metadata OffsetSeqMetadata(0,1746458840037,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:27:20 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.107.fb3242d0-30d2-40c0-9e9a-1e6d8b24547c.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/107\n",
      "25/05/05 11:27:20 INFO MicroBatchExecution: Committed offsets for batch 107. Metadata OffsetSeqMetadata(0,1746458840034,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:27:20 INFO IncrementalExecution: Current batch timestamp = 1746458840037\n",
      "25/05/05 11:27:20 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.107.46640418-5fbe-46ce-96df-9ffedd21a7a6.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/107\n",
      "25/05/05 11:27:20 INFO MicroBatchExecution: Committed offsets for batch 107. Metadata OffsetSeqMetadata(0,1746458840039,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:27:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:20 INFO IncrementalExecution: Current batch timestamp = 1746458840034\n",
      "25/05/05 11:27:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:20 INFO IncrementalExecution: Current batch timestamp = 1746458840037\n",
      "25/05/05 11:27:20 INFO IncrementalExecution: Current batch timestamp = 1746458840039\n",
      "25/05/05 11:27:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:20 INFO IncrementalExecution: Current batch timestamp = 1746458840034\n",
      "25/05/05 11:27:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:20 INFO IncrementalExecution: Current batch timestamp = 1746458840039\n",
      "25/05/05 11:27:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:20 INFO IncrementalExecution: Current batch timestamp = 1746458840034\n",
      "25/05/05 11:27:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:20 INFO IncrementalExecution: Current batch timestamp = 1746458840039\n",
      "25/05/05 11:27:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:20 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:20 INFO CodeGenerator: Code generated in 4.451833 ms\n",
      "25/05/05 11:27:20 INFO CodeGenerator: Code generated in 3.501333 ms\n",
      "25/05/05 11:27:20 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 107, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:27:20 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:20 INFO CodeGenerator: Code generated in 3.4485 ms\n",
      "25/05/05 11:27:20 INFO DAGScheduler: Got job 321 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:27:20 INFO DAGScheduler: Final stage: ResultStage 320 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:27:20 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:27:20 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:27:20 INFO DAGScheduler: Submitting ResultStage 320 (MapPartitionsRDD[1720] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:27:20 INFO MemoryStore: Block broadcast_427 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:27:20 INFO MemoryStore: Block broadcast_427_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:27:20 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 107, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@50383209]. The input RDD has 1 partitions.\n",
      "25/05/05 11:27:20 INFO BlockManagerInfo: Added broadcast_427_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:27:20 INFO SparkContext: Created broadcast 427 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:27:20 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 320 (MapPartitionsRDD[1720] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:27:20 INFO TaskSchedulerImpl: Adding task set 320.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:27:20 INFO DAGScheduler: Got job 322 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:27:20 INFO DAGScheduler: Final stage: ResultStage 321 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:27:20 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:27:20 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:27:20 INFO DAGScheduler: Submitting ResultStage 321 (MapPartitionsRDD[1723] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:27:20 INFO TaskSetManager: Starting task 0.0 in stage 320.0 (TID 320) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:27:20 INFO Executor: Running task 0.0 in stage 320.0 (TID 320)\n",
      "25/05/05 11:27:20 INFO MemoryStore: Block broadcast_428 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:27:20 INFO MemoryStore: Block broadcast_428_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)\n",
      "25/05/05 11:27:20 INFO BlockManagerInfo: Added broadcast_428_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:27:20 INFO SparkContext: Created broadcast 428 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:27:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 321 (MapPartitionsRDD[1723] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:27:20 INFO TaskSchedulerImpl: Adding task set 321.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:27:20 INFO TaskSetManager: Starting task 0.0 in stage 321.0 (TID 321) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:27:20 INFO Executor: Running task 0.0 in stage 321.0 (TID 321)\n",
      "25/05/05 11:27:20 INFO CodeGenerator: Code generated in 3.932083 ms\n",
      "25/05/05 11:27:20 INFO CodeGenerator: Code generated in 3.659792 ms\n",
      "25/05/05 11:27:20 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3338 untilOffset=3339, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=107 taskId=320 partitionId=0\n",
      "25/05/05 11:27:20 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3338 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:20 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3338 untilOffset=3339, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=107 taskId=321 partitionId=0\n",
      "25/05/05 11:27:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:20 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3338 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:20 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:20 INFO DAGScheduler: Got job 323 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:27:20 INFO DAGScheduler: Final stage: ResultStage 322 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:27:20 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:27:20 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:27:20 INFO DAGScheduler: Submitting ResultStage 322 (MapPartitionsRDD[1728] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:27:20 INFO MemoryStore: Block broadcast_429 stored as values in memory (estimated size 47.1 KiB, free 366.0 MiB)\n",
      "25/05/05 11:27:20 INFO MemoryStore: Block broadcast_429_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:27:20 INFO BlockManagerInfo: Added broadcast_429_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:27:20 INFO SparkContext: Created broadcast 429 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:27:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 322 (MapPartitionsRDD[1728] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:27:20 INFO TaskSchedulerImpl: Adding task set 322.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:27:20 INFO TaskSetManager: Starting task 0.0 in stage 322.0 (TID 322) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:27:20 INFO Executor: Running task 0.0 in stage 322.0 (TID 322)\n",
      "25/05/05 11:27:20 INFO CodeGenerator: Code generated in 3.5385 ms\n",
      "25/05/05 11:27:20 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3338 untilOffset=3339, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=107 taskId=322 partitionId=0\n",
      "25/05/05 11:27:20 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3338 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3339, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3339, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:20 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:27:20 INFO DataWritingSparkTask: Committed partition 0 (task 320, attempt 0, stage 320.0)\n",
      "25/05/05 11:27:20 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 505576834 nanos, during time span of 506018750 nanos.\n",
      "25/05/05 11:27:20 INFO Executor: Finished task 0.0 in stage 320.0 (TID 320). 2188 bytes result sent to driver\n",
      "25/05/05 11:27:20 INFO TaskSetManager: Finished task 0.0 in stage 320.0 (TID 320) in 515 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:27:20 INFO TaskSchedulerImpl: Removed TaskSet 320.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:27:20 INFO DAGScheduler: ResultStage 320 (start at NativeMethodAccessorImpl.java:0) finished in 0.516 s\n",
      "25/05/05 11:27:20 INFO DAGScheduler: Job 321 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:27:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 320: Stage finished\n",
      "25/05/05 11:27:20 INFO DAGScheduler: Job 321 finished: start at NativeMethodAccessorImpl.java:0, took 0.517017 s\n",
      "25/05/05 11:27:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 107, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:27:20 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:27:20 INFO DataWritingSparkTask: Committed partition 0 (task 321, attempt 0, stage 321.0)\n",
      "25/05/05 11:27:20 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 502933334 nanos, during time span of 504706916 nanos.\n",
      "25/05/05 11:27:20 INFO Executor: Finished task 0.0 in stage 321.0 (TID 321). 3516 bytes result sent to driver\n",
      "25/05/05 11:27:20 INFO TaskSetManager: Finished task 0.0 in stage 321.0 (TID 321) in 514 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:27:20 INFO TaskSchedulerImpl: Removed TaskSet 321.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:27:20 INFO DAGScheduler: ResultStage 321 (start at NativeMethodAccessorImpl.java:0) finished in 0.516 s\n",
      "25/05/05 11:27:20 INFO DAGScheduler: Job 322 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:27:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 321: Stage finished\n",
      "25/05/05 11:27:20 INFO DAGScheduler: Job 322 finished: start at NativeMethodAccessorImpl.java:0, took 0.516799 s\n",
      "25/05/05 11:27:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 107, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@50383209] is committing.\n",
      "25/05/05 11:27:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 107, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@50383209] committed.\n",
      "25/05/05 11:27:20 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/107 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.107.2a349368-dd27-4d8e-bcbb-77656615fdda.tmp\n",
      "25/05/05 11:27:20 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 107, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:27:20 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/107 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.107.3fd0b767-ec05-4ce4-87b8-d0a9413af686.tmp\n",
      "25/05/05 11:27:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:20 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3339, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:20 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:27:20 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:27:20 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:27:20 INFO connection: Opened connection [connectionId{localValue:213, serverValue:4583}] to localhost:27017\n",
      "25/05/05 11:27:20 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=166417}\n",
      "25/05/05 11:27:20 INFO connection: Opened connection [connectionId{localValue:214, serverValue:4584}] to localhost:27017\n",
      "25/05/05 11:27:20 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:27:20 INFO connection: Closed connection [connectionId{localValue:214, serverValue:4584}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:27:20 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 504314792 nanos, during time span of 508333459 nanos.\n",
      "25/05/05 11:27:20 INFO Executor: Finished task 0.0 in stage 322.0 (TID 322). 1645 bytes result sent to driver\n",
      "25/05/05 11:27:20 INFO TaskSetManager: Finished task 0.0 in stage 322.0 (TID 322) in 518 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:27:20 INFO TaskSchedulerImpl: Removed TaskSet 322.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:27:20 INFO DAGScheduler: ResultStage 322 (start at NativeMethodAccessorImpl.java:0) finished in 0.524 s\n",
      "25/05/05 11:27:20 INFO DAGScheduler: Job 323 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:27:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 322: Stage finished\n",
      "25/05/05 11:27:20 INFO DAGScheduler: Job 323 finished: start at NativeMethodAccessorImpl.java:0, took 0.524948 s\n",
      "25/05/05 11:27:20 INFO MemoryStore: Block broadcast_430 stored as values in memory (estimated size 248.0 B, free 366.0 MiB)\n",
      "25/05/05 11:27:20 INFO MemoryStore: Block broadcast_430_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.0 MiB)\n",
      "25/05/05 11:27:20 INFO BlockManagerInfo: Added broadcast_430_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:27:20 INFO SparkContext: Created broadcast 430 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:20 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.107.2a349368-dd27-4d8e-bcbb-77656615fdda.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/107\n",
      "25/05/05 11:27:20 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:27:20.037Z\",\n",
      "  \"batchId\" : 107,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.6339869281045751,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 529,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 612,\n",
      "    \"walCommit\" : 50\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3338\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3339\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3339\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.6339869281045751,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:27:20 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/107 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.107.54d4ba26-65cf-4a8c-9897-23cf016f2919.tmp\n",
      "25/05/05 11:27:20 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.107.3fd0b767-ec05-4ce4-87b8-d0a9413af686.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/107\n",
      "25/05/05 11:27:20 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:27:20.033Z\",\n",
      "  \"batchId\" : 107,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.607717041800643,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 537,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 622,\n",
      "    \"walCommit\" : 53\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3338\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3339\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3339\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.607717041800643,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:27:20 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.107.54d4ba26-65cf-4a8c-9897-23cf016f2919.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/107\n",
      "25/05/05 11:27:20 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:27:20.036Z\",\n",
      "  \"batchId\" : 107,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.5822784810126582,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 554,\n",
      "    \"commitOffsets\" : 24,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 632,\n",
      "    \"walCommit\" : 48\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3338\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3339\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3339\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.5822784810126582,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 107\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|B013|R081|02-00-00|161/YANKEE STAD|05/05/2025|11:27:19|REGULAR|7          |12        |2025-05-05 11:27:20.034|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:27:22 INFO BlockManagerInfo: Removed broadcast_424_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:27:22 INFO BlockManagerInfo: Removed broadcast_425_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:27:22 INFO BlockManagerInfo: Removed broadcast_427_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:27:22 INFO BlockManagerInfo: Removed broadcast_430_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:27:22 INFO BlockManagerInfo: Removed broadcast_423_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:27:22 INFO BlockManagerInfo: Removed broadcast_426_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:27:22 INFO BlockManagerInfo: Removed broadcast_428_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:27:22 INFO BlockManagerInfo: Removed broadcast_429_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:27:25 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/108 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.108.e330a0a5-ffe4-4a56-b8d0-443c18115177.tmp\n",
      "25/05/05 11:27:25 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/108 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.108.9f27a345-2edb-474e-981b-304f07573f65.tmp\n",
      "25/05/05 11:27:25 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/108 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.108.536ff9d8-c162-4cf1-874e-f43f70a83370.tmp\n",
      "25/05/05 11:27:25 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.108.9f27a345-2edb-474e-981b-304f07573f65.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/108\n",
      "25/05/05 11:27:25 INFO MicroBatchExecution: Committed offsets for batch 108. Metadata OffsetSeqMetadata(0,1746458845466,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:27:25 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.108.536ff9d8-c162-4cf1-874e-f43f70a83370.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/108\n",
      "25/05/05 11:27:25 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.108.e330a0a5-ffe4-4a56-b8d0-443c18115177.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/108\n",
      "25/05/05 11:27:25 INFO MicroBatchExecution: Committed offsets for batch 108. Metadata OffsetSeqMetadata(0,1746458845466,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:27:25 INFO MicroBatchExecution: Committed offsets for batch 108. Metadata OffsetSeqMetadata(0,1746458845466,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:27:25 INFO IncrementalExecution: Current batch timestamp = 1746458845466\n",
      "25/05/05 11:27:25 INFO IncrementalExecution: Current batch timestamp = 1746458845466\n",
      "25/05/05 11:27:25 INFO IncrementalExecution: Current batch timestamp = 1746458845466\n",
      "25/05/05 11:27:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:25 INFO IncrementalExecution: Current batch timestamp = 1746458845466\n",
      "25/05/05 11:27:25 INFO IncrementalExecution: Current batch timestamp = 1746458845466\n",
      "25/05/05 11:27:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:25 INFO IncrementalExecution: Current batch timestamp = 1746458845466\n",
      "25/05/05 11:27:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:25 INFO IncrementalExecution: Current batch timestamp = 1746458845466\n",
      "25/05/05 11:27:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:25 INFO IncrementalExecution: Current batch timestamp = 1746458845466\n",
      "25/05/05 11:27:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:25 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:25 INFO CodeGenerator: Code generated in 4.123 ms\n",
      "25/05/05 11:27:25 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 108, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:27:25 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 108, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@200441b1]. The input RDD has 1 partitions.\n",
      "25/05/05 11:27:25 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:25 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:25 INFO DAGScheduler: Got job 324 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:27:25 INFO DAGScheduler: Final stage: ResultStage 323 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:27:25 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:27:25 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:27:25 INFO DAGScheduler: Submitting ResultStage 323 (MapPartitionsRDD[1738] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:27:25 INFO MemoryStore: Block broadcast_431 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:27:25 INFO MemoryStore: Block broadcast_431_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:27:25 INFO BlockManagerInfo: Added broadcast_431_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:27:25 INFO SparkContext: Created broadcast 431 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:27:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 323 (MapPartitionsRDD[1738] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:27:25 INFO TaskSchedulerImpl: Adding task set 323.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:27:25 INFO DAGScheduler: Got job 325 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:27:25 INFO DAGScheduler: Final stage: ResultStage 324 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:27:25 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:27:25 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:27:25 INFO DAGScheduler: Submitting ResultStage 324 (MapPartitionsRDD[1739] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:27:25 INFO TaskSetManager: Starting task 0.0 in stage 323.0 (TID 323) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:27:25 INFO Executor: Running task 0.0 in stage 323.0 (TID 323)\n",
      "25/05/05 11:27:25 INFO MemoryStore: Block broadcast_432 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:27:25 INFO MemoryStore: Block broadcast_432_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:27:25 INFO BlockManagerInfo: Added broadcast_432_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:27:25 INFO SparkContext: Created broadcast 432 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:27:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 324 (MapPartitionsRDD[1739] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:27:25 INFO TaskSchedulerImpl: Adding task set 324.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:27:25 INFO TaskSetManager: Starting task 0.0 in stage 324.0 (TID 324) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:27:25 INFO Executor: Running task 0.0 in stage 324.0 (TID 324)\n",
      "25/05/05 11:27:25 INFO CodeGenerator: Code generated in 3.771667 ms\n",
      "25/05/05 11:27:25 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3339 untilOffset=3340, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=108 taskId=323 partitionId=0\n",
      "25/05/05 11:27:25 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3339 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:25 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3339 untilOffset=3340, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=108 taskId=324 partitionId=0\n",
      "25/05/05 11:27:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:25 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3339 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:25 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:25 INFO DAGScheduler: Got job 326 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:27:25 INFO DAGScheduler: Final stage: ResultStage 325 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:27:25 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:27:25 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:27:25 INFO DAGScheduler: Submitting ResultStage 325 (MapPartitionsRDD[1744] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:27:25 INFO MemoryStore: Block broadcast_433 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:27:25 INFO MemoryStore: Block broadcast_433_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:27:25 INFO BlockManagerInfo: Added broadcast_433_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:27:25 INFO SparkContext: Created broadcast 433 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:27:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 325 (MapPartitionsRDD[1744] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:27:25 INFO TaskSchedulerImpl: Adding task set 325.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:27:25 INFO TaskSetManager: Starting task 0.0 in stage 325.0 (TID 325) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:27:25 INFO Executor: Running task 0.0 in stage 325.0 (TID 325)\n",
      "25/05/05 11:27:25 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3339 untilOffset=3340, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=108 taskId=325 partitionId=0\n",
      "25/05/05 11:27:25 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3339 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:25 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3340, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:26 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:27:26 INFO DataWritingSparkTask: Committed partition 0 (task 323, attempt 0, stage 323.0)\n",
      "25/05/05 11:27:26 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 502799750 nanos, during time span of 503260625 nanos.\n",
      "25/05/05 11:27:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:26 INFO Executor: Finished task 0.0 in stage 323.0 (TID 323). 2145 bytes result sent to driver\n",
      "25/05/05 11:27:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3340, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:26 INFO TaskSetManager: Finished task 0.0 in stage 323.0 (TID 323) in 512 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:27:26 INFO TaskSchedulerImpl: Removed TaskSet 323.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:27:26 INFO DAGScheduler: ResultStage 323 (start at NativeMethodAccessorImpl.java:0) finished in 0.513 s\n",
      "25/05/05 11:27:26 INFO DAGScheduler: Job 324 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:27:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 323: Stage finished\n",
      "25/05/05 11:27:26 INFO DAGScheduler: Job 324 finished: start at NativeMethodAccessorImpl.java:0, took 0.513924 s\n",
      "25/05/05 11:27:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 108, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:27:26 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:27:26 INFO DataWritingSparkTask: Committed partition 0 (task 324, attempt 0, stage 324.0)\n",
      "25/05/05 11:27:26 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 500952958 nanos, during time span of 502349916 nanos.\n",
      "25/05/05 11:27:26 INFO Executor: Finished task 0.0 in stage 324.0 (TID 324). 3516 bytes result sent to driver\n",
      "25/05/05 11:27:26 INFO TaskSetManager: Finished task 0.0 in stage 324.0 (TID 324) in 512 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:27:26 INFO TaskSchedulerImpl: Removed TaskSet 324.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:27:26 INFO DAGScheduler: ResultStage 324 (start at NativeMethodAccessorImpl.java:0) finished in 0.513 s\n",
      "25/05/05 11:27:26 INFO DAGScheduler: Job 325 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:27:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 324: Stage finished\n",
      "25/05/05 11:27:26 INFO DAGScheduler: Job 325 finished: start at NativeMethodAccessorImpl.java:0, took 0.515569 s\n",
      "25/05/05 11:27:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 108, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@200441b1] is committing.\n",
      "25/05/05 11:27:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 108, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@200441b1] committed.\n",
      "25/05/05 11:27:26 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 108, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:27:26 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/108 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.108.d5041ae3-7ce0-42f9-a6d7-afb00fd29389.tmp\n",
      "25/05/05 11:27:26 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/108 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.108.ae7063e5-c35b-47ca-aaeb-cff27f1a9dde.tmp\n",
      "25/05/05 11:27:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:26 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3340, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:26 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:27:26 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:27:26 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:27:26 INFO connection: Opened connection [connectionId{localValue:215, serverValue:4585}] to localhost:27017\n",
      "25/05/05 11:27:26 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=176333}\n",
      "25/05/05 11:27:26 INFO connection: Opened connection [connectionId{localValue:216, serverValue:4586}] to localhost:27017\n",
      "25/05/05 11:27:26 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:27:26 INFO connection: Closed connection [connectionId{localValue:216, serverValue:4586}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:27:26 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 502564583 nanos, during time span of 506464250 nanos.\n",
      "25/05/05 11:27:26 INFO Executor: Finished task 0.0 in stage 325.0 (TID 325). 1645 bytes result sent to driver\n",
      "25/05/05 11:27:26 INFO TaskSetManager: Finished task 0.0 in stage 325.0 (TID 325) in 512 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:27:26 INFO TaskSchedulerImpl: Removed TaskSet 325.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:27:26 INFO DAGScheduler: ResultStage 325 (start at NativeMethodAccessorImpl.java:0) finished in 0.515 s\n",
      "25/05/05 11:27:26 INFO DAGScheduler: Job 326 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:27:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 325: Stage finished\n",
      "25/05/05 11:27:26 INFO DAGScheduler: Job 326 finished: start at NativeMethodAccessorImpl.java:0, took 0.516248 s\n",
      "25/05/05 11:27:26 INFO MemoryStore: Block broadcast_434 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:27:26 INFO MemoryStore: Block broadcast_434_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:27:26 INFO BlockManagerInfo: Added broadcast_434_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:27:26 INFO SparkContext: Created broadcast 434 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:26 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/108 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.108.af57bc01-af9d-412f-8c00-80d5e35df677.tmp\n",
      "25/05/05 11:27:26 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.108.d5041ae3-7ce0-42f9-a6d7-afb00fd29389.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/108\n",
      "25/05/05 11:27:26 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:27:25.465Z\",\n",
      "  \"batchId\" : 108,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.6891891891891893,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 526,\n",
      "    \"commitOffsets\" : 29,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 592,\n",
      "    \"walCommit\" : 32\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3339\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3340\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3340\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.6891891891891893,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:27:26 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.108.ae7063e5-c35b-47ca-aaeb-cff27f1a9dde.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/108\n",
      "25/05/05 11:27:26 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:27:25.465Z\",\n",
      "  \"batchId\" : 108,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.680672268907563,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 531,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 595,\n",
      "    \"walCommit\" : 32\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3339\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3340\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3340\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.680672268907563,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:27:26 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.108.af57bc01-af9d-412f-8c00-80d5e35df677.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/108\n",
      "25/05/05 11:27:26 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:27:25.465Z\",\n",
      "  \"batchId\" : 108,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.6447368421052633,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 546,\n",
      "    \"commitOffsets\" : 25,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 608,\n",
      "    \"walCommit\" : 32\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3339\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3340\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3340\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.6447368421052633,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 108\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|B013|R081|02-00-00|161/YANKEE STAD|05/05/2025|11:27:24|REGULAR|14         |3         |2025-05-05 11:27:25.466|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:27:31 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/109 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.109.58806860-aa1c-4af6-8bdd-3f253983324b.tmp\n",
      "25/05/05 11:27:31 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/109 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.109.80e28d72-f288-4be8-bb93-90d1699aac95.tmp\n",
      "25/05/05 11:27:31 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/109 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.109.a4761c31-718d-459b-9779-d18e00d67a1b.tmp\n",
      "25/05/05 11:27:31 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.109.58806860-aa1c-4af6-8bdd-3f253983324b.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/109\n",
      "25/05/05 11:27:31 INFO MicroBatchExecution: Committed offsets for batch 109. Metadata OffsetSeqMetadata(0,1746458851902,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:27:31 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.109.80e28d72-f288-4be8-bb93-90d1699aac95.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/109\n",
      "25/05/05 11:27:31 INFO MicroBatchExecution: Committed offsets for batch 109. Metadata OffsetSeqMetadata(0,1746458851902,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:27:31 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.109.a4761c31-718d-459b-9779-d18e00d67a1b.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/109\n",
      "25/05/05 11:27:31 INFO MicroBatchExecution: Committed offsets for batch 109. Metadata OffsetSeqMetadata(0,1746458851903,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:27:31 INFO IncrementalExecution: Current batch timestamp = 1746458851902\n",
      "25/05/05 11:27:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:31 INFO IncrementalExecution: Current batch timestamp = 1746458851902\n",
      "25/05/05 11:27:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:31 INFO IncrementalExecution: Current batch timestamp = 1746458851903\n",
      "25/05/05 11:27:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:31 INFO IncrementalExecution: Current batch timestamp = 1746458851902\n",
      "25/05/05 11:27:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:31 INFO IncrementalExecution: Current batch timestamp = 1746458851902\n",
      "25/05/05 11:27:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:31 INFO IncrementalExecution: Current batch timestamp = 1746458851903\n",
      "25/05/05 11:27:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:31 INFO IncrementalExecution: Current batch timestamp = 1746458851902\n",
      "25/05/05 11:27:31 INFO IncrementalExecution: Current batch timestamp = 1746458851902\n",
      "25/05/05 11:27:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:31 INFO CodeGenerator: Code generated in 3.680042 ms\n",
      "25/05/05 11:27:31 INFO CodeGenerator: Code generated in 3.376042 ms\n",
      "25/05/05 11:27:31 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 109, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:27:31 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 109, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@2e40f206]. The input RDD has 1 partitions.\n",
      "25/05/05 11:27:31 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:31 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:31 INFO DAGScheduler: Got job 327 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:27:31 INFO DAGScheduler: Final stage: ResultStage 326 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:27:31 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:27:31 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:27:31 INFO DAGScheduler: Submitting ResultStage 326 (MapPartitionsRDD[1754] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:27:31 INFO MemoryStore: Block broadcast_435 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:27:31 INFO MemoryStore: Block broadcast_435_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:27:31 INFO BlockManagerInfo: Added broadcast_435_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:27:31 INFO SparkContext: Created broadcast 435 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:27:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 326 (MapPartitionsRDD[1754] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:27:31 INFO TaskSchedulerImpl: Adding task set 326.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:27:31 INFO DAGScheduler: Got job 328 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:27:31 INFO DAGScheduler: Final stage: ResultStage 327 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:27:31 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:27:31 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:27:31 INFO DAGScheduler: Submitting ResultStage 327 (MapPartitionsRDD[1755] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:27:31 INFO TaskSetManager: Starting task 0.0 in stage 326.0 (TID 326) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:27:31 INFO Executor: Running task 0.0 in stage 326.0 (TID 326)\n",
      "25/05/05 11:27:31 INFO MemoryStore: Block broadcast_436 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:27:31 INFO MemoryStore: Block broadcast_436_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)\n",
      "25/05/05 11:27:31 INFO BlockManagerInfo: Added broadcast_436_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:27:31 INFO SparkContext: Created broadcast 436 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:27:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 327 (MapPartitionsRDD[1755] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:27:31 INFO TaskSchedulerImpl: Adding task set 327.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:27:31 INFO TaskSetManager: Starting task 0.0 in stage 327.0 (TID 327) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:27:31 INFO Executor: Running task 0.0 in stage 327.0 (TID 327)\n",
      "25/05/05 11:27:31 INFO CodeGenerator: Code generated in 3.546958 ms\n",
      "25/05/05 11:27:31 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3340 untilOffset=3341, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=109 taskId=326 partitionId=0\n",
      "25/05/05 11:27:31 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3340 untilOffset=3341, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=109 taskId=327 partitionId=0\n",
      "25/05/05 11:27:31 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3340 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:31 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3340 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:31 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:31 INFO DAGScheduler: Got job 329 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:27:31 INFO DAGScheduler: Final stage: ResultStage 328 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:27:31 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:27:31 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:27:31 INFO DAGScheduler: Submitting ResultStage 328 (MapPartitionsRDD[1760] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:27:31 INFO MemoryStore: Block broadcast_437 stored as values in memory (estimated size 47.1 KiB, free 366.0 MiB)\n",
      "25/05/05 11:27:31 INFO MemoryStore: Block broadcast_437_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:27:31 INFO BlockManagerInfo: Added broadcast_437_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:27:31 INFO SparkContext: Created broadcast 437 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:27:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 328 (MapPartitionsRDD[1760] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:27:31 INFO TaskSchedulerImpl: Adding task set 328.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:27:31 INFO TaskSetManager: Starting task 0.0 in stage 328.0 (TID 328) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:27:31 INFO Executor: Running task 0.0 in stage 328.0 (TID 328)\n",
      "25/05/05 11:27:32 INFO CodeGenerator: Code generated in 3.444459 ms\n",
      "25/05/05 11:27:32 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3340 untilOffset=3341, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=109 taskId=328 partitionId=0\n",
      "25/05/05 11:27:32 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3340 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3341, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3341, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:32 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:27:32 INFO DataWritingSparkTask: Committed partition 0 (task 326, attempt 0, stage 326.0)\n",
      "25/05/05 11:27:32 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 503485458 nanos, during time span of 503830917 nanos.\n",
      "25/05/05 11:27:32 INFO Executor: Finished task 0.0 in stage 326.0 (TID 326). 2145 bytes result sent to driver\n",
      "25/05/05 11:27:32 INFO TaskSetManager: Finished task 0.0 in stage 326.0 (TID 326) in 512 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:27:32 INFO TaskSchedulerImpl: Removed TaskSet 326.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:27:32 INFO DAGScheduler: ResultStage 326 (start at NativeMethodAccessorImpl.java:0) finished in 0.513 s\n",
      "25/05/05 11:27:32 INFO DAGScheduler: Job 327 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:27:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 326: Stage finished\n",
      "25/05/05 11:27:32 INFO DAGScheduler: Job 327 finished: start at NativeMethodAccessorImpl.java:0, took 0.513593 s\n",
      "25/05/05 11:27:32 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:27:32 INFO DataWritingSparkTask: Committed partition 0 (task 327, attempt 0, stage 327.0)\n",
      "25/05/05 11:27:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 109, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:27:32 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 502536875 nanos, during time span of 504464334 nanos.\n",
      "25/05/05 11:27:32 INFO Executor: Finished task 0.0 in stage 327.0 (TID 327). 3514 bytes result sent to driver\n",
      "25/05/05 11:27:32 INFO TaskSetManager: Finished task 0.0 in stage 327.0 (TID 327) in 512 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:27:32 INFO TaskSchedulerImpl: Removed TaskSet 327.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:27:32 INFO DAGScheduler: ResultStage 327 (start at NativeMethodAccessorImpl.java:0) finished in 0.513 s\n",
      "25/05/05 11:27:32 INFO DAGScheduler: Job 328 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:27:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 327: Stage finished\n",
      "25/05/05 11:27:32 INFO DAGScheduler: Job 328 finished: start at NativeMethodAccessorImpl.java:0, took 0.514297 s\n",
      "25/05/05 11:27:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 109, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@2e40f206] is committing.\n",
      "25/05/05 11:27:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 109, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@2e40f206] committed.\n",
      "25/05/05 11:27:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 109, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:27:32 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/109 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.109.a153b410-9d09-4148-ac5d-856fedc45328.tmp\n",
      "25/05/05 11:27:32 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/109 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.109.aeb49b1f-b3cc-4864-9816-e70f3f853ed8.tmp\n",
      "25/05/05 11:27:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3341, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:32 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:27:32 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:27:32 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:27:32 INFO connection: Opened connection [connectionId{localValue:217, serverValue:4587}] to localhost:27017\n",
      "25/05/05 11:27:32 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=352500}\n",
      "25/05/05 11:27:32 INFO connection: Opened connection [connectionId{localValue:218, serverValue:4588}] to localhost:27017\n",
      "25/05/05 11:27:32 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:27:32 INFO connection: Closed connection [connectionId{localValue:218, serverValue:4588}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:27:32 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 502438208 nanos, during time span of 507312084 nanos.\n",
      "25/05/05 11:27:32 INFO Executor: Finished task 0.0 in stage 328.0 (TID 328). 1645 bytes result sent to driver\n",
      "25/05/05 11:27:32 INFO TaskSetManager: Finished task 0.0 in stage 328.0 (TID 328) in 517 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:27:32 INFO TaskSchedulerImpl: Removed TaskSet 328.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:27:32 INFO DAGScheduler: ResultStage 328 (start at NativeMethodAccessorImpl.java:0) finished in 0.520 s\n",
      "25/05/05 11:27:32 INFO DAGScheduler: Job 329 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:27:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 328: Stage finished\n",
      "25/05/05 11:27:32 INFO DAGScheduler: Job 329 finished: start at NativeMethodAccessorImpl.java:0, took 0.521033 s\n",
      "25/05/05 11:27:32 INFO MemoryStore: Block broadcast_438 stored as values in memory (estimated size 248.0 B, free 366.0 MiB)\n",
      "25/05/05 11:27:32 INFO MemoryStore: Block broadcast_438_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.0 MiB)\n",
      "25/05/05 11:27:32 INFO BlockManagerInfo: Added broadcast_438_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:27:32 INFO SparkContext: Created broadcast 438 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:32 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.109.a153b410-9d09-4148-ac5d-856fedc45328.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/109\n",
      "25/05/05 11:27:32 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:27:31.901Z\",\n",
      "  \"batchId\" : 109,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.6260162601626016,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 527,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 615,\n",
      "    \"walCommit\" : 56\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3340\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3341\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3341\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.6260162601626016,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:27:32 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/109 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.109.3f8b3e39-72dd-4129-b948-ec3932828f67.tmp\n",
      "25/05/05 11:27:32 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.109.aeb49b1f-b3cc-4864-9816-e70f3f853ed8.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/109\n",
      "25/05/05 11:27:32 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:27:31.901Z\",\n",
      "  \"batchId\" : 109,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.6129032258064517,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 532,\n",
      "    \"commitOffsets\" : 26,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 620,\n",
      "    \"walCommit\" : 57\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3340\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3341\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3341\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.6129032258064517,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:27:32 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.109.3f8b3e39-72dd-4129-b948-ec3932828f67.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/109\n",
      "25/05/05 11:27:32 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:27:31.901Z\",\n",
      "  \"batchId\" : 109,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.572327044025157,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 549,\n",
      "    \"commitOffsets\" : 24,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 636,\n",
      "    \"walCommit\" : 57\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3340\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3341\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3341\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.572327044025157,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 109\n",
      "-------------------------------------------\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION      |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R016|R032|00-00-01|TIME SQ-42 ST|05/05/2025|11:27:30|REGULAR|9          |7         |2025-05-05 11:27:31.902|\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:27:34 INFO BlockManagerInfo: Removed broadcast_435_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:27:34 INFO BlockManagerInfo: Removed broadcast_433_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:27:34 INFO BlockManagerInfo: Removed broadcast_434_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:27:34 INFO BlockManagerInfo: Removed broadcast_438_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:27:34 INFO BlockManagerInfo: Removed broadcast_437_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:27:34 INFO BlockManagerInfo: Removed broadcast_436_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:27:34 INFO BlockManagerInfo: Removed broadcast_431_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:27:34 INFO BlockManagerInfo: Removed broadcast_432_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:27:38 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/110 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.110.6b74c0e1-45c0-4493-8193-7e9a68e390c9.tmp\n",
      "25/05/05 11:27:38 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/110 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.110.7985c220-8d95-40cf-b7dd-2e16ab378551.tmp\n",
      "25/05/05 11:27:38 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/110 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.110.be3111c2-e631-4f99-a2df-894219ebaa4f.tmp\n",
      "25/05/05 11:27:38 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.110.6b74c0e1-45c0-4493-8193-7e9a68e390c9.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/110\n",
      "25/05/05 11:27:38 INFO MicroBatchExecution: Committed offsets for batch 110. Metadata OffsetSeqMetadata(0,1746458858316,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:27:38 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.110.7985c220-8d95-40cf-b7dd-2e16ab378551.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/110\n",
      "25/05/05 11:27:38 INFO MicroBatchExecution: Committed offsets for batch 110. Metadata OffsetSeqMetadata(0,1746458858317,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:27:38 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.110.be3111c2-e631-4f99-a2df-894219ebaa4f.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/110\n",
      "25/05/05 11:27:38 INFO MicroBatchExecution: Committed offsets for batch 110. Metadata OffsetSeqMetadata(0,1746458858316,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:27:38 INFO IncrementalExecution: Current batch timestamp = 1746458858317\n",
      "25/05/05 11:27:38 INFO IncrementalExecution: Current batch timestamp = 1746458858316\n",
      "25/05/05 11:27:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:38 INFO IncrementalExecution: Current batch timestamp = 1746458858316\n",
      "25/05/05 11:27:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:38 INFO IncrementalExecution: Current batch timestamp = 1746458858316\n",
      "25/05/05 11:27:38 INFO IncrementalExecution: Current batch timestamp = 1746458858317\n",
      "25/05/05 11:27:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:38 INFO IncrementalExecution: Current batch timestamp = 1746458858316\n",
      "25/05/05 11:27:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:38 INFO IncrementalExecution: Current batch timestamp = 1746458858316\n",
      "25/05/05 11:27:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:38 INFO IncrementalExecution: Current batch timestamp = 1746458858316\n",
      "25/05/05 11:27:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:38 INFO CodeGenerator: Code generated in 4.172709 ms\n",
      "25/05/05 11:27:38 INFO CodeGenerator: Code generated in 3.187416 ms\n",
      "25/05/05 11:27:38 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 110, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:27:38 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 110, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@2a7d66d7]. The input RDD has 1 partitions.\n",
      "25/05/05 11:27:38 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:38 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:38 INFO DAGScheduler: Got job 330 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:27:38 INFO DAGScheduler: Final stage: ResultStage 329 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:27:38 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:27:38 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:27:38 INFO DAGScheduler: Submitting ResultStage 329 (MapPartitionsRDD[1770] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:27:38 INFO MemoryStore: Block broadcast_439 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:27:38 INFO MemoryStore: Block broadcast_439_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:27:38 INFO BlockManagerInfo: Added broadcast_439_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:27:38 INFO SparkContext: Created broadcast 439 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:27:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 329 (MapPartitionsRDD[1770] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:27:38 INFO TaskSchedulerImpl: Adding task set 329.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:27:38 INFO DAGScheduler: Got job 331 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:27:38 INFO DAGScheduler: Final stage: ResultStage 330 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:27:38 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:27:38 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:27:38 INFO DAGScheduler: Submitting ResultStage 330 (MapPartitionsRDD[1771] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:27:38 INFO TaskSetManager: Starting task 0.0 in stage 329.0 (TID 329) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:27:38 INFO Executor: Running task 0.0 in stage 329.0 (TID 329)\n",
      "25/05/05 11:27:38 INFO MemoryStore: Block broadcast_440 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:27:38 INFO MemoryStore: Block broadcast_440_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:27:38 INFO BlockManagerInfo: Added broadcast_440_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:27:38 INFO SparkContext: Created broadcast 440 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:27:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 330 (MapPartitionsRDD[1771] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:27:38 INFO TaskSchedulerImpl: Adding task set 330.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:27:38 INFO TaskSetManager: Starting task 0.0 in stage 330.0 (TID 330) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:27:38 INFO Executor: Running task 0.0 in stage 330.0 (TID 330)\n",
      "25/05/05 11:27:38 INFO CodeGenerator: Code generated in 3.718167 ms\n",
      "25/05/05 11:27:38 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3341 untilOffset=3342, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=110 taskId=329 partitionId=0\n",
      "25/05/05 11:27:38 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3341 untilOffset=3342, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=110 taskId=330 partitionId=0\n",
      "25/05/05 11:27:38 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3341 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:38 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3341 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:38 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:38 INFO DAGScheduler: Got job 332 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:27:38 INFO DAGScheduler: Final stage: ResultStage 331 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:27:38 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:27:38 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:27:38 INFO DAGScheduler: Submitting ResultStage 331 (MapPartitionsRDD[1776] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:27:38 INFO MemoryStore: Block broadcast_441 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:27:38 INFO MemoryStore: Block broadcast_441_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:27:38 INFO BlockManagerInfo: Added broadcast_441_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:27:38 INFO SparkContext: Created broadcast 441 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:27:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 331 (MapPartitionsRDD[1776] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:27:38 INFO TaskSchedulerImpl: Adding task set 331.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:27:38 INFO TaskSetManager: Starting task 0.0 in stage 331.0 (TID 331) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:27:38 INFO Executor: Running task 0.0 in stage 331.0 (TID 331)\n",
      "25/05/05 11:27:38 INFO CodeGenerator: Code generated in 3.655375 ms\n",
      "25/05/05 11:27:38 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3341 untilOffset=3342, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=110 taskId=331 partitionId=0\n",
      "25/05/05 11:27:38 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3341 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3342, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3342, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:38 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:27:38 INFO DataWritingSparkTask: Committed partition 0 (task 329, attempt 0, stage 329.0)\n",
      "25/05/05 11:27:38 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 503502333 nanos, during time span of 503861083 nanos.\n",
      "25/05/05 11:27:38 INFO Executor: Finished task 0.0 in stage 329.0 (TID 329). 2145 bytes result sent to driver\n",
      "25/05/05 11:27:38 INFO TaskSetManager: Finished task 0.0 in stage 329.0 (TID 329) in 513 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:27:38 INFO TaskSchedulerImpl: Removed TaskSet 329.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:27:38 INFO DAGScheduler: ResultStage 329 (start at NativeMethodAccessorImpl.java:0) finished in 0.515 s\n",
      "25/05/05 11:27:38 INFO DAGScheduler: Job 330 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:27:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 329: Stage finished\n",
      "25/05/05 11:27:38 INFO DAGScheduler: Job 330 finished: start at NativeMethodAccessorImpl.java:0, took 0.515343 s\n",
      "25/05/05 11:27:38 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 110, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:27:38 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:27:38 INFO DataWritingSparkTask: Committed partition 0 (task 330, attempt 0, stage 330.0)\n",
      "25/05/05 11:27:38 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 502420833 nanos, during time span of 504626792 nanos.\n",
      "25/05/05 11:27:38 INFO Executor: Finished task 0.0 in stage 330.0 (TID 330). 3514 bytes result sent to driver\n",
      "25/05/05 11:27:38 INFO TaskSetManager: Finished task 0.0 in stage 330.0 (TID 330) in 513 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:27:38 INFO TaskSchedulerImpl: Removed TaskSet 330.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:27:38 INFO DAGScheduler: ResultStage 330 (start at NativeMethodAccessorImpl.java:0) finished in 0.515 s\n",
      "25/05/05 11:27:38 INFO DAGScheduler: Job 331 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:27:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 330: Stage finished\n",
      "25/05/05 11:27:38 INFO DAGScheduler: Job 331 finished: start at NativeMethodAccessorImpl.java:0, took 0.517206 s\n",
      "25/05/05 11:27:38 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 110, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@2a7d66d7] is committing.\n",
      "25/05/05 11:27:38 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 110, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@2a7d66d7] committed.\n",
      "25/05/05 11:27:38 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 110, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:27:38 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/110 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.110.7ab5aa01-4255-4d48-9185-53d70d301236.tmp\n",
      "25/05/05 11:27:38 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/110 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.110.c609d49c-515d-4b9e-b1a5-470cb505adf2.tmp\n",
      "25/05/05 11:27:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:38 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3342, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:38 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:27:38 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:27:38 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:27:38 INFO connection: Opened connection [connectionId{localValue:219, serverValue:4589}] to localhost:27017\n",
      "25/05/05 11:27:38 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=196292}\n",
      "25/05/05 11:27:38 INFO connection: Opened connection [connectionId{localValue:220, serverValue:4590}] to localhost:27017\n",
      "25/05/05 11:27:38 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:27:38 INFO connection: Closed connection [connectionId{localValue:220, serverValue:4590}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:27:38 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 504228791 nanos, during time span of 508482375 nanos.\n",
      "25/05/05 11:27:38 INFO Executor: Finished task 0.0 in stage 331.0 (TID 331). 1645 bytes result sent to driver\n",
      "25/05/05 11:27:38 INFO TaskSetManager: Finished task 0.0 in stage 331.0 (TID 331) in 519 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:27:38 INFO TaskSchedulerImpl: Removed TaskSet 331.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:27:38 INFO DAGScheduler: ResultStage 331 (start at NativeMethodAccessorImpl.java:0) finished in 0.523 s\n",
      "25/05/05 11:27:38 INFO DAGScheduler: Job 332 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:27:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 331: Stage finished\n",
      "25/05/05 11:27:38 INFO DAGScheduler: Job 332 finished: start at NativeMethodAccessorImpl.java:0, took 0.523584 s\n",
      "25/05/05 11:27:38 INFO MemoryStore: Block broadcast_442 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:27:38 INFO MemoryStore: Block broadcast_442_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:27:38 INFO BlockManagerInfo: Added broadcast_442_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:27:38 INFO SparkContext: Created broadcast 442 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:38 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.110.7ab5aa01-4255-4d48-9185-53d70d301236.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/110\n",
      "25/05/05 11:27:38 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:27:38.315Z\",\n",
      "  \"batchId\" : 110,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.6181229773462784,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 530,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 0,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 618,\n",
      "    \"walCommit\" : 55\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3341\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3342\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3342\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.6181229773462784,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:27:38 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/110 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.110.ff4e2f53-d4cd-4b79-a165-dfab88ae1d13.tmp\n",
      "25/05/05 11:27:38 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.110.c609d49c-515d-4b9e-b1a5-470cb505adf2.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/110\n",
      "25/05/05 11:27:38 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:27:38.315Z\",\n",
      "  \"batchId\" : 110,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.6051364365971108,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 536,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 623,\n",
      "    \"walCommit\" : 54\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3341\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3342\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3342\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.6051364365971108,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:27:38 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.110.ff4e2f53-d4cd-4b79-a165-dfab88ae1d13.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/110\n",
      "25/05/05 11:27:38 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:27:38.315Z\",\n",
      "  \"batchId\" : 110,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.5698587127158556,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 554,\n",
      "    \"commitOffsets\" : 24,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 637,\n",
      "    \"walCommit\" : 53\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3341\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3342\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3342\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.5698587127158556,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 110\n",
      "-------------------------------------------\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION      |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R016|R032|00-00-01|TIME SQ-42 ST|05/05/2025|11:27:37|REGULAR|5          |12        |2025-05-05 11:27:38.316|\n",
      "+----+----+--------+-------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:27:44 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/111 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.111.d96326a7-8e0a-47ae-b88a-dddb3243b889.tmp\n",
      "25/05/05 11:27:44 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/111 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.111.996da557-868e-40b8-88db-eeece7c726fb.tmp\n",
      "25/05/05 11:27:44 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/111 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.111.ded1165a-676d-4f1f-adf0-e7befdf76d7a.tmp\n",
      "25/05/05 11:27:44 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.111.d96326a7-8e0a-47ae-b88a-dddb3243b889.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/111\n",
      "25/05/05 11:27:44 INFO MicroBatchExecution: Committed offsets for batch 111. Metadata OffsetSeqMetadata(0,1746458864704,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:27:44 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.111.996da557-868e-40b8-88db-eeece7c726fb.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/111\n",
      "25/05/05 11:27:44 INFO MicroBatchExecution: Committed offsets for batch 111. Metadata OffsetSeqMetadata(0,1746458864708,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:27:44 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.111.ded1165a-676d-4f1f-adf0-e7befdf76d7a.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/111\n",
      "25/05/05 11:27:44 INFO IncrementalExecution: Current batch timestamp = 1746458864704\n",
      "25/05/05 11:27:44 INFO MicroBatchExecution: Committed offsets for batch 111. Metadata OffsetSeqMetadata(0,1746458864709,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:27:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:44 INFO IncrementalExecution: Current batch timestamp = 1746458864708\n",
      "25/05/05 11:27:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:44 INFO IncrementalExecution: Current batch timestamp = 1746458864709\n",
      "25/05/05 11:27:44 INFO IncrementalExecution: Current batch timestamp = 1746458864704\n",
      "25/05/05 11:27:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:44 INFO IncrementalExecution: Current batch timestamp = 1746458864708\n",
      "25/05/05 11:27:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:44 INFO IncrementalExecution: Current batch timestamp = 1746458864709\n",
      "25/05/05 11:27:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:44 INFO IncrementalExecution: Current batch timestamp = 1746458864708\n",
      "25/05/05 11:27:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:44 INFO IncrementalExecution: Current batch timestamp = 1746458864709\n",
      "25/05/05 11:27:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:44 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:44 INFO CodeGenerator: Code generated in 4.383458 ms\n",
      "25/05/05 11:27:44 INFO CodeGenerator: Code generated in 4.023708 ms\n",
      "25/05/05 11:27:44 INFO CodeGenerator: Code generated in 3.998917 ms\n",
      "25/05/05 11:27:44 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 111, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@2c72ab58]. The input RDD has 1 partitions.\n",
      "25/05/05 11:27:44 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:44 INFO DAGScheduler: Got job 333 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:27:44 INFO DAGScheduler: Final stage: ResultStage 332 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:27:44 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:27:44 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:27:44 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 111, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:27:44 INFO DAGScheduler: Submitting ResultStage 332 (MapPartitionsRDD[1784] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:27:44 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:44 INFO MemoryStore: Block broadcast_443 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:27:44 INFO MemoryStore: Block broadcast_443_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.1 MiB)\n",
      "25/05/05 11:27:44 INFO BlockManagerInfo: Added broadcast_443_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:27:44 INFO SparkContext: Created broadcast 443 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:27:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 332 (MapPartitionsRDD[1784] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:27:44 INFO TaskSchedulerImpl: Adding task set 332.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:27:44 INFO DAGScheduler: Got job 334 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:27:44 INFO DAGScheduler: Final stage: ResultStage 333 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:27:44 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:27:44 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:27:44 INFO DAGScheduler: Submitting ResultStage 333 (MapPartitionsRDD[1787] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:27:44 INFO TaskSetManager: Starting task 0.0 in stage 332.0 (TID 332) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:27:44 INFO MemoryStore: Block broadcast_444 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:27:44 INFO MemoryStore: Block broadcast_444_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:27:44 INFO Executor: Running task 0.0 in stage 332.0 (TID 332)\n",
      "25/05/05 11:27:44 INFO BlockManagerInfo: Added broadcast_444_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:27:44 INFO SparkContext: Created broadcast 444 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:27:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 333 (MapPartitionsRDD[1787] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:27:44 INFO TaskSchedulerImpl: Adding task set 333.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:27:44 INFO TaskSetManager: Starting task 0.0 in stage 333.0 (TID 333) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:27:44 INFO Executor: Running task 0.0 in stage 333.0 (TID 333)\n",
      "25/05/05 11:27:44 INFO CodeGenerator: Code generated in 3.808375 ms\n",
      "25/05/05 11:27:44 INFO CodeGenerator: Code generated in 3.783334 ms\n",
      "25/05/05 11:27:44 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3342 untilOffset=3343, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=111 taskId=333 partitionId=0\n",
      "25/05/05 11:27:44 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3342 untilOffset=3343, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=111 taskId=332 partitionId=0\n",
      "25/05/05 11:27:44 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3342 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:44 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3342 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:44 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:44 INFO DAGScheduler: Got job 335 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:27:44 INFO DAGScheduler: Final stage: ResultStage 334 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:27:44 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:27:44 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:27:44 INFO DAGScheduler: Submitting ResultStage 334 (MapPartitionsRDD[1792] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:27:44 INFO MemoryStore: Block broadcast_445 stored as values in memory (estimated size 47.1 KiB, free 366.0 MiB)\n",
      "25/05/05 11:27:44 INFO MemoryStore: Block broadcast_445_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.0 MiB)\n",
      "25/05/05 11:27:44 INFO BlockManagerInfo: Added broadcast_445_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:27:44 INFO SparkContext: Created broadcast 445 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:27:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 334 (MapPartitionsRDD[1792] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:27:44 INFO TaskSchedulerImpl: Adding task set 334.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:27:44 INFO TaskSetManager: Starting task 0.0 in stage 334.0 (TID 334) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:27:44 INFO Executor: Running task 0.0 in stage 334.0 (TID 334)\n",
      "25/05/05 11:27:44 INFO CodeGenerator: Code generated in 3.447334 ms\n",
      "25/05/05 11:27:44 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3342 untilOffset=3343, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=111 taskId=334 partitionId=0\n",
      "25/05/05 11:27:44 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3342 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:44 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3343, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3343, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:45 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:27:45 INFO DataWritingSparkTask: Committed partition 0 (task 333, attempt 0, stage 333.0)\n",
      "25/05/05 11:27:45 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 503806041 nanos, during time span of 504102042 nanos.\n",
      "25/05/05 11:27:45 INFO Executor: Finished task 0.0 in stage 333.0 (TID 333). 2188 bytes result sent to driver\n",
      "25/05/05 11:27:45 INFO TaskSetManager: Finished task 0.0 in stage 333.0 (TID 333) in 513 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:27:45 INFO TaskSchedulerImpl: Removed TaskSet 333.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:27:45 INFO DAGScheduler: ResultStage 333 (start at NativeMethodAccessorImpl.java:0) finished in 0.514 s\n",
      "25/05/05 11:27:45 INFO DAGScheduler: Job 334 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:27:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 333: Stage finished\n",
      "25/05/05 11:27:45 INFO DAGScheduler: Job 334 finished: start at NativeMethodAccessorImpl.java:0, took 0.516581 s\n",
      "25/05/05 11:27:45 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:27:45 INFO DataWritingSparkTask: Committed partition 0 (task 332, attempt 0, stage 332.0)\n",
      "25/05/05 11:27:45 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 504080166 nanos, during time span of 505748167 nanos.\n",
      "25/05/05 11:27:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 111, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:27:45 INFO Executor: Finished task 0.0 in stage 332.0 (TID 332). 3511 bytes result sent to driver\n",
      "25/05/05 11:27:45 INFO TaskSetManager: Finished task 0.0 in stage 332.0 (TID 332) in 516 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:27:45 INFO TaskSchedulerImpl: Removed TaskSet 332.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:27:45 INFO DAGScheduler: ResultStage 332 (start at NativeMethodAccessorImpl.java:0) finished in 0.517 s\n",
      "25/05/05 11:27:45 INFO DAGScheduler: Job 333 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:27:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 332: Stage finished\n",
      "25/05/05 11:27:45 INFO DAGScheduler: Job 333 finished: start at NativeMethodAccessorImpl.java:0, took 0.518281 s\n",
      "25/05/05 11:27:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 111, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@2c72ab58] is committing.\n",
      "25/05/05 11:27:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 111, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@2c72ab58] committed.\n",
      "25/05/05 11:27:45 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 111, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:27:45 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/111 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.111.a578453f-97f4-4dc0-952f-b8f485c41198.tmp\n",
      "25/05/05 11:27:45 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/111 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.111.7314fee9-a367-4784-b4e5-fab5fa3598da.tmp\n",
      "25/05/05 11:27:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3343, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:45 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:27:45 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:27:45 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:27:45 INFO connection: Opened connection [connectionId{localValue:221, serverValue:4591}] to localhost:27017\n",
      "25/05/05 11:27:45 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=206208}\n",
      "25/05/05 11:27:45 INFO connection: Opened connection [connectionId{localValue:222, serverValue:4592}] to localhost:27017\n",
      "25/05/05 11:27:45 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:27:45 INFO connection: Closed connection [connectionId{localValue:222, serverValue:4592}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:27:45 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 503259209 nanos, during time span of 507457750 nanos.\n",
      "25/05/05 11:27:45 INFO Executor: Finished task 0.0 in stage 334.0 (TID 334). 1645 bytes result sent to driver\n",
      "25/05/05 11:27:45 INFO TaskSetManager: Finished task 0.0 in stage 334.0 (TID 334) in 516 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:27:45 INFO TaskSchedulerImpl: Removed TaskSet 334.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:27:45 INFO DAGScheduler: ResultStage 334 (start at NativeMethodAccessorImpl.java:0) finished in 0.520 s\n",
      "25/05/05 11:27:45 INFO DAGScheduler: Job 335 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:27:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 334: Stage finished\n",
      "25/05/05 11:27:45 INFO DAGScheduler: Job 335 finished: start at NativeMethodAccessorImpl.java:0, took 0.521722 s\n",
      "25/05/05 11:27:45 INFO MemoryStore: Block broadcast_446 stored as values in memory (estimated size 248.0 B, free 366.0 MiB)\n",
      "25/05/05 11:27:45 INFO MemoryStore: Block broadcast_446_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.0 MiB)\n",
      "25/05/05 11:27:45 INFO BlockManagerInfo: Added broadcast_446_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:27:45 INFO SparkContext: Created broadcast 446 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:45 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/111 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.111.2fe136cd-fed4-4f25-9dff-8b5545bc52b4.tmp\n",
      "25/05/05 11:27:45 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.111.a578453f-97f4-4dc0-952f-b8f485c41198.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/111\n",
      "25/05/05 11:27:45 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:27:44.706Z\",\n",
      "  \"batchId\" : 111,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "  \"processedRowsPerSecond\" : 1.6051364365971108,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 533,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 623,\n",
      "    \"walCommit\" : 56\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3342\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3343\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3343\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "    \"processedRowsPerSecond\" : 1.6051364365971108,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:27:45 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.111.7314fee9-a367-4784-b4e5-fab5fa3598da.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/111\n",
      "25/05/05 11:27:45 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:27:44.708Z\",\n",
      "  \"batchId\" : 111,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.6,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 537,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 625,\n",
      "    \"walCommit\" : 57\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3342\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3343\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3343\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.6,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:27:45 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.111.2fe136cd-fed4-4f25-9dff-8b5545bc52b4.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/111\n",
      "25/05/05 11:27:45 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:27:44.704Z\",\n",
      "  \"batchId\" : 111,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.557632398753894,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 553,\n",
      "    \"commitOffsets\" : 26,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 0,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 642,\n",
      "    \"walCommit\" : 58\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3342\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3343\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3343\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.557632398753894,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 111\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R023|R045|00-00-00|34 ST-HERALD SQ|05/05/2025|11:27:43|REGULAR|10         |10        |2025-05-05 11:27:44.709|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:27:45 INFO BlockManagerInfo: Removed broadcast_444_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:27:45 INFO BlockManagerInfo: Removed broadcast_443_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:27:45 INFO BlockManagerInfo: Removed broadcast_440_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:27:45 INFO BlockManagerInfo: Removed broadcast_445_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:27:45 INFO BlockManagerInfo: Removed broadcast_439_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:27:45 INFO BlockManagerInfo: Removed broadcast_442_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:27:45 INFO BlockManagerInfo: Removed broadcast_441_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:27:45 INFO BlockManagerInfo: Removed broadcast_446_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.3 MiB)\n",
      "25/05/05 11:27:51 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/112 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.112.d4d3070e-e3ed-4044-9983-9254b7452719.tmp\n",
      "25/05/05 11:27:51 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/112 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.112.ab3636aa-349d-4788-bae6-703a95fe5bd7.tmp\n",
      "25/05/05 11:27:51 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/112 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.112.da773790-b4aa-4cb4-a477-58d1748b04b3.tmp\n",
      "25/05/05 11:27:51 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.112.d4d3070e-e3ed-4044-9983-9254b7452719.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/112\n",
      "25/05/05 11:27:51 INFO MicroBatchExecution: Committed offsets for batch 112. Metadata OffsetSeqMetadata(0,1746458871103,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:27:51 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.112.ab3636aa-349d-4788-bae6-703a95fe5bd7.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/112\n",
      "25/05/05 11:27:51 INFO MicroBatchExecution: Committed offsets for batch 112. Metadata OffsetSeqMetadata(0,1746458871103,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:27:51 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.112.da773790-b4aa-4cb4-a477-58d1748b04b3.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/112\n",
      "25/05/05 11:27:51 INFO MicroBatchExecution: Committed offsets for batch 112. Metadata OffsetSeqMetadata(0,1746458871110,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:27:51 INFO IncrementalExecution: Current batch timestamp = 1746458871103\n",
      "25/05/05 11:27:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:51 INFO IncrementalExecution: Current batch timestamp = 1746458871103\n",
      "25/05/05 11:27:51 INFO IncrementalExecution: Current batch timestamp = 1746458871110\n",
      "25/05/05 11:27:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:51 INFO IncrementalExecution: Current batch timestamp = 1746458871103\n",
      "25/05/05 11:27:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:51 INFO IncrementalExecution: Current batch timestamp = 1746458871110\n",
      "25/05/05 11:27:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:51 INFO IncrementalExecution: Current batch timestamp = 1746458871103\n",
      "25/05/05 11:27:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:51 INFO IncrementalExecution: Current batch timestamp = 1746458871103\n",
      "25/05/05 11:27:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:51 INFO IncrementalExecution: Current batch timestamp = 1746458871103\n",
      "25/05/05 11:27:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:51 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:51 INFO CodeGenerator: Code generated in 3.813708 ms\n",
      "25/05/05 11:27:51 INFO CodeGenerator: Code generated in 2.796292 ms\n",
      "25/05/05 11:27:51 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 112, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6e758523]. The input RDD has 1 partitions.\n",
      "25/05/05 11:27:51 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 112, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:27:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:51 INFO DAGScheduler: Got job 336 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:27:51 INFO DAGScheduler: Final stage: ResultStage 335 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:27:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:27:51 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:27:51 INFO DAGScheduler: Submitting ResultStage 335 (MapPartitionsRDD[1803] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:27:51 INFO MemoryStore: Block broadcast_447 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:27:51 INFO MemoryStore: Block broadcast_447_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:27:51 INFO BlockManagerInfo: Added broadcast_447_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:27:51 INFO SparkContext: Created broadcast 447 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:27:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 335 (MapPartitionsRDD[1803] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:27:51 INFO TaskSchedulerImpl: Adding task set 335.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:27:51 INFO DAGScheduler: Got job 337 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:27:51 INFO DAGScheduler: Final stage: ResultStage 336 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:27:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:27:51 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:27:51 INFO DAGScheduler: Submitting ResultStage 336 (MapPartitionsRDD[1802] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:27:51 INFO TaskSetManager: Starting task 0.0 in stage 335.0 (TID 335) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:27:51 INFO Executor: Running task 0.0 in stage 335.0 (TID 335)\n",
      "25/05/05 11:27:51 INFO MemoryStore: Block broadcast_448 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:27:51 INFO MemoryStore: Block broadcast_448_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:27:51 INFO BlockManagerInfo: Added broadcast_448_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:27:51 INFO SparkContext: Created broadcast 448 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:27:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 336 (MapPartitionsRDD[1802] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:27:51 INFO TaskSchedulerImpl: Adding task set 336.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:27:51 INFO TaskSetManager: Starting task 0.0 in stage 336.0 (TID 336) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:27:51 INFO Executor: Running task 0.0 in stage 336.0 (TID 336)\n",
      "25/05/05 11:27:51 INFO CodeGenerator: Code generated in 3.510208 ms\n",
      "25/05/05 11:27:51 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3343 untilOffset=3344, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=112 taskId=335 partitionId=0\n",
      "25/05/05 11:27:51 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3343 untilOffset=3344, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=112 taskId=336 partitionId=0\n",
      "25/05/05 11:27:51 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3343 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:51 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3343 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:51 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:51 INFO DAGScheduler: Got job 338 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:27:51 INFO DAGScheduler: Final stage: ResultStage 337 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:27:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:27:51 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:27:51 INFO DAGScheduler: Submitting ResultStage 337 (MapPartitionsRDD[1808] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:27:51 INFO MemoryStore: Block broadcast_449 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:27:51 INFO MemoryStore: Block broadcast_449_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.1 MiB)\n",
      "25/05/05 11:27:51 INFO BlockManagerInfo: Added broadcast_449_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:27:51 INFO SparkContext: Created broadcast 449 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:27:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 337 (MapPartitionsRDD[1808] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:27:51 INFO TaskSchedulerImpl: Adding task set 337.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:27:51 INFO TaskSetManager: Starting task 0.0 in stage 337.0 (TID 337) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:27:51 INFO Executor: Running task 0.0 in stage 337.0 (TID 337)\n",
      "25/05/05 11:27:51 INFO CodeGenerator: Code generated in 3.6345 ms\n",
      "25/05/05 11:27:51 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3343 untilOffset=3344, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=112 taskId=337 partitionId=0\n",
      "25/05/05 11:27:51 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3343 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3344, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3344, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:51 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:27:51 INFO DataWritingSparkTask: Committed partition 0 (task 335, attempt 0, stage 335.0)\n",
      "25/05/05 11:27:51 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 503200250 nanos, during time span of 504093584 nanos.\n",
      "25/05/05 11:27:51 INFO Executor: Finished task 0.0 in stage 335.0 (TID 335). 2145 bytes result sent to driver\n",
      "25/05/05 11:27:51 INFO TaskSetManager: Finished task 0.0 in stage 335.0 (TID 335) in 511 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:27:51 INFO TaskSchedulerImpl: Removed TaskSet 335.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:27:51 INFO DAGScheduler: ResultStage 335 (start at NativeMethodAccessorImpl.java:0) finished in 0.514 s\n",
      "25/05/05 11:27:51 INFO DAGScheduler: Job 336 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:27:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 335: Stage finished\n",
      "25/05/05 11:27:51 INFO DAGScheduler: Job 336 finished: start at NativeMethodAccessorImpl.java:0, took 0.513742 s\n",
      "25/05/05 11:27:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 112, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:27:51 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:27:51 INFO DataWritingSparkTask: Committed partition 0 (task 336, attempt 0, stage 336.0)\n",
      "25/05/05 11:27:51 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 502394042 nanos, during time span of 504493083 nanos.\n",
      "25/05/05 11:27:51 INFO Executor: Finished task 0.0 in stage 336.0 (TID 336). 3516 bytes result sent to driver\n",
      "25/05/05 11:27:51 INFO TaskSetManager: Finished task 0.0 in stage 336.0 (TID 336) in 512 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:27:51 INFO TaskSchedulerImpl: Removed TaskSet 336.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:27:51 INFO DAGScheduler: ResultStage 336 (start at NativeMethodAccessorImpl.java:0) finished in 0.513 s\n",
      "25/05/05 11:27:51 INFO DAGScheduler: Job 337 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:27:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 336: Stage finished\n",
      "25/05/05 11:27:51 INFO DAGScheduler: Job 337 finished: start at NativeMethodAccessorImpl.java:0, took 0.514833 s\n",
      "25/05/05 11:27:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 112, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6e758523] is committing.\n",
      "25/05/05 11:27:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 112, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6e758523] committed.\n",
      "25/05/05 11:27:51 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 112, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:27:51 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/112 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.112.bad16fb8-4f50-4494-8ac4-b1d9419e1d02.tmp\n",
      "25/05/05 11:27:51 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/112 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.112.0824be1c-0217-4535-a674-3c0d819c0ec3.tmp\n",
      "25/05/05 11:27:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:51 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3344, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:51 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:27:51 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:27:51 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:27:51 INFO connection: Opened connection [connectionId{localValue:223, serverValue:4593}] to localhost:27017\n",
      "25/05/05 11:27:51 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=320958}\n",
      "25/05/05 11:27:51 INFO connection: Opened connection [connectionId{localValue:224, serverValue:4594}] to localhost:27017\n",
      "25/05/05 11:27:51 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:27:51 INFO connection: Closed connection [connectionId{localValue:224, serverValue:4594}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:27:51 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 504305916 nanos, during time span of 508521375 nanos.\n",
      "25/05/05 11:27:51 INFO Executor: Finished task 0.0 in stage 337.0 (TID 337). 1645 bytes result sent to driver\n",
      "25/05/05 11:27:51 INFO TaskSetManager: Finished task 0.0 in stage 337.0 (TID 337) in 518 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:27:51 INFO TaskSchedulerImpl: Removed TaskSet 337.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:27:51 INFO DAGScheduler: ResultStage 337 (start at NativeMethodAccessorImpl.java:0) finished in 0.521 s\n",
      "25/05/05 11:27:51 INFO DAGScheduler: Job 338 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:27:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 337: Stage finished\n",
      "25/05/05 11:27:51 INFO DAGScheduler: Job 338 finished: start at NativeMethodAccessorImpl.java:0, took 0.522418 s\n",
      "25/05/05 11:27:51 INFO MemoryStore: Block broadcast_450 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:27:51 INFO MemoryStore: Block broadcast_450_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:27:51 INFO BlockManagerInfo: Added broadcast_450_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:27:51 INFO SparkContext: Created broadcast 450 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:51 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.112.bad16fb8-4f50-4494-8ac4-b1d9419e1d02.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/112\n",
      "25/05/05 11:27:51 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:27:51.102Z\",\n",
      "  \"batchId\" : 112,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.6339869281045751,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 526,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 612,\n",
      "    \"walCommit\" : 52\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3343\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3344\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3344\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.6339869281045751,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:27:51 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/112 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.112.57fd7d88-e1b9-4f61-afd5-7981b8658bd1.tmp\n",
      "25/05/05 11:27:51 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.112.0824be1c-0217-4535-a674-3c0d819c0ec3.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/112\n",
      "25/05/05 11:27:51 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:27:51.102Z\",\n",
      "  \"batchId\" : 112,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "  \"processedRowsPerSecond\" : 1.6233766233766234,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 529,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 616,\n",
      "    \"walCommit\" : 54\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3343\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3344\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3344\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 71.42857142857143,\n",
      "    \"processedRowsPerSecond\" : 1.6233766233766234,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:27:51 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.112.57fd7d88-e1b9-4f61-afd5-7981b8658bd1.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/112\n",
      "25/05/05 11:27:51 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:27:51.107Z\",\n",
      "  \"batchId\" : 112,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.594896331738437,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 549,\n",
      "    \"commitOffsets\" : 24,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 627,\n",
      "    \"walCommit\" : 47\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3343\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3344\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3344\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.594896331738437,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 112\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|B013|R081|02-00-00|161/YANKEE STAD|05/05/2025|11:27:50|REGULAR|8          |12        |2025-05-05 11:27:51.103|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:27:55 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/113 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.113.b4826e17-b0fb-484c-a29f-48ad76c1fa5f.tmp\n",
      "25/05/05 11:27:55 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/113 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.113.d4b89be7-8169-4ae0-b8a5-0a8a59346e57.tmp\n",
      "25/05/05 11:27:55 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/113 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.113.85ecf0dc-bd63-4d10-949f-c47baa08c563.tmp\n",
      "25/05/05 11:27:55 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.113.d4b89be7-8169-4ae0-b8a5-0a8a59346e57.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/113\n",
      "25/05/05 11:27:55 INFO MicroBatchExecution: Committed offsets for batch 113. Metadata OffsetSeqMetadata(0,1746458875593,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:27:55 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.113.b4826e17-b0fb-484c-a29f-48ad76c1fa5f.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/113\n",
      "25/05/05 11:27:55 INFO MicroBatchExecution: Committed offsets for batch 113. Metadata OffsetSeqMetadata(0,1746458875594,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:27:55 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.113.85ecf0dc-bd63-4d10-949f-c47baa08c563.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/113\n",
      "25/05/05 11:27:55 INFO MicroBatchExecution: Committed offsets for batch 113. Metadata OffsetSeqMetadata(0,1746458875593,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:27:55 INFO IncrementalExecution: Current batch timestamp = 1746458875593\n",
      "25/05/05 11:27:55 INFO IncrementalExecution: Current batch timestamp = 1746458875594\n",
      "25/05/05 11:27:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:55 INFO IncrementalExecution: Current batch timestamp = 1746458875593\n",
      "25/05/05 11:27:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:55 INFO IncrementalExecution: Current batch timestamp = 1746458875594\n",
      "25/05/05 11:27:55 INFO IncrementalExecution: Current batch timestamp = 1746458875593\n",
      "25/05/05 11:27:55 INFO IncrementalExecution: Current batch timestamp = 1746458875593\n",
      "25/05/05 11:27:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:55 INFO IncrementalExecution: Current batch timestamp = 1746458875594\n",
      "25/05/05 11:27:55 INFO IncrementalExecution: Current batch timestamp = 1746458875593\n",
      "25/05/05 11:27:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:55 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:27:55 INFO CodeGenerator: Code generated in 3.675417 ms\n",
      "25/05/05 11:27:55 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 113, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:27:55 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:55 INFO CodeGenerator: Code generated in 2.865792 ms\n",
      "25/05/05 11:27:55 INFO DAGScheduler: Got job 339 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:27:55 INFO DAGScheduler: Final stage: ResultStage 338 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:27:55 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:27:55 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:27:55 INFO DAGScheduler: Submitting ResultStage 338 (MapPartitionsRDD[1814] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:27:55 INFO MemoryStore: Block broadcast_451 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:27:55 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 113, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@309e1b34]. The input RDD has 1 partitions.\n",
      "25/05/05 11:27:55 INFO MemoryStore: Block broadcast_451_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:27:55 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:55 INFO BlockManagerInfo: Added broadcast_451_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:27:55 INFO SparkContext: Created broadcast 451 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:27:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 338 (MapPartitionsRDD[1814] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:27:55 INFO TaskSchedulerImpl: Adding task set 338.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:27:55 INFO DAGScheduler: Got job 340 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:27:55 INFO DAGScheduler: Final stage: ResultStage 339 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:27:55 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:27:55 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:27:55 INFO DAGScheduler: Submitting ResultStage 339 (MapPartitionsRDD[1819] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:27:55 INFO TaskSetManager: Starting task 0.0 in stage 338.0 (TID 338) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:27:55 INFO Executor: Running task 0.0 in stage 338.0 (TID 338)\n",
      "25/05/05 11:27:55 INFO MemoryStore: Block broadcast_452 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:27:55 INFO MemoryStore: Block broadcast_452_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.0 MiB)\n",
      "25/05/05 11:27:55 INFO BlockManagerInfo: Added broadcast_452_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:27:55 INFO SparkContext: Created broadcast 452 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:27:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 339 (MapPartitionsRDD[1819] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:27:55 INFO TaskSchedulerImpl: Adding task set 339.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:27:55 INFO TaskSetManager: Starting task 0.0 in stage 339.0 (TID 339) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:27:55 INFO Executor: Running task 0.0 in stage 339.0 (TID 339)\n",
      "25/05/05 11:27:55 INFO CodeGenerator: Code generated in 3.762416 ms\n",
      "25/05/05 11:27:55 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3344 untilOffset=3345, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=113 taskId=338 partitionId=0\n",
      "25/05/05 11:27:55 INFO CodeGenerator: Code generated in 3.528958 ms\n",
      "25/05/05 11:27:55 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3344 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:55 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3344 untilOffset=3345, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=113 taskId=339 partitionId=0\n",
      "25/05/05 11:27:55 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:55 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3344 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:55 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:55 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:55 INFO DAGScheduler: Got job 341 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:27:55 INFO DAGScheduler: Final stage: ResultStage 340 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:27:55 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:27:55 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:27:55 INFO DAGScheduler: Submitting ResultStage 340 (MapPartitionsRDD[1824] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:27:55 INFO MemoryStore: Block broadcast_453 stored as values in memory (estimated size 47.1 KiB, free 366.0 MiB)\n",
      "25/05/05 11:27:55 INFO MemoryStore: Block broadcast_453_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:27:55 INFO BlockManagerInfo: Added broadcast_453_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:27:55 INFO SparkContext: Created broadcast 453 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:27:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 340 (MapPartitionsRDD[1824] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:27:55 INFO TaskSchedulerImpl: Adding task set 340.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:27:55 INFO TaskSetManager: Starting task 0.0 in stage 340.0 (TID 340) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:27:55 INFO Executor: Running task 0.0 in stage 340.0 (TID 340)\n",
      "25/05/05 11:27:55 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3344 untilOffset=3345, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=113 taskId=340 partitionId=0\n",
      "25/05/05 11:27:55 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3344 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:55 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3345, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3345, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:56 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:27:56 INFO DataWritingSparkTask: Committed partition 0 (task 338, attempt 0, stage 338.0)\n",
      "25/05/05 11:27:56 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 505391084 nanos, during time span of 505775833 nanos.\n",
      "25/05/05 11:27:56 INFO Executor: Finished task 0.0 in stage 338.0 (TID 338). 2145 bytes result sent to driver\n",
      "25/05/05 11:27:56 INFO TaskSetManager: Finished task 0.0 in stage 338.0 (TID 338) in 513 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:27:56 INFO TaskSchedulerImpl: Removed TaskSet 338.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:27:56 INFO DAGScheduler: ResultStage 338 (start at NativeMethodAccessorImpl.java:0) finished in 0.515 s\n",
      "25/05/05 11:27:56 INFO DAGScheduler: Job 339 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:27:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 338: Stage finished\n",
      "25/05/05 11:27:56 INFO DAGScheduler: Job 339 finished: start at NativeMethodAccessorImpl.java:0, took 0.516164 s\n",
      "25/05/05 11:27:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 113, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:27:56 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:27:56 INFO DataWritingSparkTask: Committed partition 0 (task 339, attempt 0, stage 339.0)\n",
      "25/05/05 11:27:56 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 503674208 nanos, during time span of 505331458 nanos.\n",
      "25/05/05 11:27:56 INFO Executor: Finished task 0.0 in stage 339.0 (TID 339). 3516 bytes result sent to driver\n",
      "25/05/05 11:27:56 INFO TaskSetManager: Finished task 0.0 in stage 339.0 (TID 339) in 513 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:27:56 INFO TaskSchedulerImpl: Removed TaskSet 339.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:27:56 INFO DAGScheduler: ResultStage 339 (start at NativeMethodAccessorImpl.java:0) finished in 0.514 s\n",
      "25/05/05 11:27:56 INFO DAGScheduler: Job 340 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:27:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 339: Stage finished\n",
      "25/05/05 11:27:56 INFO DAGScheduler: Job 340 finished: start at NativeMethodAccessorImpl.java:0, took 0.515705 s\n",
      "25/05/05 11:27:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 113, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@309e1b34] is committing.\n",
      "25/05/05 11:27:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 113, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@309e1b34] committed.\n",
      "25/05/05 11:27:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 113, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:27:56 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/113 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.113.7307c1bf-a660-4469-b0ed-2e716227b468.tmp\n",
      "25/05/05 11:27:56 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/113 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.113.c08fca0d-0d80-4b49-aae5-92303e1986ad.tmp\n",
      "25/05/05 11:27:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:27:56 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3345, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:27:56 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:27:56 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:27:56 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:27:56 INFO connection: Opened connection [connectionId{localValue:225, serverValue:4595}] to localhost:27017\n",
      "25/05/05 11:27:56 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=259625}\n",
      "25/05/05 11:27:56 INFO connection: Opened connection [connectionId{localValue:226, serverValue:4596}] to localhost:27017\n",
      "25/05/05 11:27:56 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:27:56 INFO connection: Closed connection [connectionId{localValue:226, serverValue:4596}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:27:56 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 502909333 nanos, during time span of 506561417 nanos.\n",
      "25/05/05 11:27:56 INFO Executor: Finished task 0.0 in stage 340.0 (TID 340). 1645 bytes result sent to driver\n",
      "25/05/05 11:27:56 INFO TaskSetManager: Finished task 0.0 in stage 340.0 (TID 340) in 512 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:27:56 INFO TaskSchedulerImpl: Removed TaskSet 340.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:27:56 INFO DAGScheduler: ResultStage 340 (start at NativeMethodAccessorImpl.java:0) finished in 0.516 s\n",
      "25/05/05 11:27:56 INFO DAGScheduler: Job 341 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:27:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 340: Stage finished\n",
      "25/05/05 11:27:56 INFO DAGScheduler: Job 341 finished: start at NativeMethodAccessorImpl.java:0, took 0.517430 s\n",
      "25/05/05 11:27:56 INFO MemoryStore: Block broadcast_454 stored as values in memory (estimated size 248.0 B, free 366.0 MiB)\n",
      "25/05/05 11:27:56 INFO MemoryStore: Block broadcast_454_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.0 MiB)\n",
      "25/05/05 11:27:56 INFO BlockManagerInfo: Added broadcast_454_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:27:56 INFO SparkContext: Created broadcast 454 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:27:56 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/113 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.113.648e3942-11f6-4d5a-81c6-396f59da9a1a.tmp\n",
      "25/05/05 11:27:56 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.113.7307c1bf-a660-4469-b0ed-2e716227b468.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/113\n",
      "25/05/05 11:27:56 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:27:55.592Z\",\n",
      "  \"batchId\" : 113,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "  \"processedRowsPerSecond\" : 1.6339869281045751,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 527,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 612,\n",
      "    \"walCommit\" : 50\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3344\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3345\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3345\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "    \"processedRowsPerSecond\" : 1.6339869281045751,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:27:56 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.113.c08fca0d-0d80-4b49-aae5-92303e1986ad.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/113\n",
      "25/05/05 11:27:56 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:27:55.592Z\",\n",
      "  \"batchId\" : 113,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "  \"processedRowsPerSecond\" : 1.6233766233766234,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 532,\n",
      "    \"commitOffsets\" : 27,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 616,\n",
      "    \"walCommit\" : 51\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3344\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3345\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3345\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 90.90909090909092,\n",
      "    \"processedRowsPerSecond\" : 1.6233766233766234,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:27:56 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.113.648e3942-11f6-4d5a-81c6-396f59da9a1a.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/113\n",
      "25/05/05 11:27:56 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:27:55.592Z\",\n",
      "  \"batchId\" : 113,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.5923566878980893,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 545,\n",
      "    \"commitOffsets\" : 26,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 628,\n",
      "    \"walCommit\" : 52\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3344\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3345\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3345\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.5923566878980893,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 113\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R023|R045|00-00-00|34 ST-HERALD SQ|05/05/2025|11:27:54|REGULAR|11         |12        |2025-05-05 11:27:55.593|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:27:57 INFO BlockManagerInfo: Removed broadcast_449_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:27:57 INFO BlockManagerInfo: Removed broadcast_450_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:27:57 INFO BlockManagerInfo: Removed broadcast_454_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:27:57 INFO BlockManagerInfo: Removed broadcast_451_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:27:57 INFO BlockManagerInfo: Removed broadcast_453_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:27:57 INFO BlockManagerInfo: Removed broadcast_448_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:27:57 INFO BlockManagerInfo: Removed broadcast_447_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:27:57 INFO BlockManagerInfo: Removed broadcast_452_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:28:01 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/114 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.114.765aa0b3-5fff-44d4-9e3b-77cbdd4be380.tmp\n",
      "25/05/05 11:28:01 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/114 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.114.cd5dbec3-3cbc-4cbc-b3b0-7162ee27e06f.tmp\n",
      "25/05/05 11:28:01 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/114 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.114.00a270fa-1603-4933-8ce6-21bdfd862fef.tmp\n",
      "25/05/05 11:28:01 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.114.00a270fa-1603-4933-8ce6-21bdfd862fef.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/114\n",
      "25/05/05 11:28:01 INFO MicroBatchExecution: Committed offsets for batch 114. Metadata OffsetSeqMetadata(0,1746458881012,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:28:01 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.114.765aa0b3-5fff-44d4-9e3b-77cbdd4be380.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/114\n",
      "25/05/05 11:28:01 INFO MicroBatchExecution: Committed offsets for batch 114. Metadata OffsetSeqMetadata(0,1746458881006,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:28:01 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.114.cd5dbec3-3cbc-4cbc-b3b0-7162ee27e06f.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/114\n",
      "25/05/05 11:28:01 INFO MicroBatchExecution: Committed offsets for batch 114. Metadata OffsetSeqMetadata(0,1746458881006,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:28:01 INFO IncrementalExecution: Current batch timestamp = 1746458881012\n",
      "25/05/05 11:28:01 INFO IncrementalExecution: Current batch timestamp = 1746458881006\n",
      "25/05/05 11:28:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:28:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:28:01 INFO IncrementalExecution: Current batch timestamp = 1746458881006\n",
      "25/05/05 11:28:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:28:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:28:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:28:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:28:01 INFO IncrementalExecution: Current batch timestamp = 1746458881012\n",
      "25/05/05 11:28:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:28:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:28:01 INFO IncrementalExecution: Current batch timestamp = 1746458881006\n",
      "25/05/05 11:28:01 INFO IncrementalExecution: Current batch timestamp = 1746458881006\n",
      "25/05/05 11:28:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:28:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:28:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:28:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:28:01 INFO IncrementalExecution: Current batch timestamp = 1746458881012\n",
      "25/05/05 11:28:01 INFO IncrementalExecution: Current batch timestamp = 1746458881006\n",
      "25/05/05 11:28:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:28:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:28:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:28:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:28:01 INFO CodeGenerator: Code generated in 3.854583 ms\n",
      "25/05/05 11:28:01 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 114, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:28:01 INFO CodeGenerator: Code generated in 3.286625 ms\n",
      "25/05/05 11:28:01 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:28:01 INFO DAGScheduler: Got job 342 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:28:01 INFO DAGScheduler: Final stage: ResultStage 341 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:28:01 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:28:01 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:28:01 INFO DAGScheduler: Submitting ResultStage 341 (MapPartitionsRDD[1832] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:28:01 INFO MemoryStore: Block broadcast_455 stored as values in memory (estimated size 15.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:28:01 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 114, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@7ce4f7f0]. The input RDD has 1 partitions.\n",
      "25/05/05 11:28:01 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:28:01 INFO MemoryStore: Block broadcast_455_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)\n",
      "25/05/05 11:28:01 INFO BlockManagerInfo: Added broadcast_455_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:28:01 INFO SparkContext: Created broadcast 455 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:28:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 341 (MapPartitionsRDD[1832] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:28:01 INFO TaskSchedulerImpl: Adding task set 341.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:28:01 INFO DAGScheduler: Got job 343 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:28:01 INFO DAGScheduler: Final stage: ResultStage 342 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:28:01 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:28:01 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:28:01 INFO DAGScheduler: Submitting ResultStage 342 (MapPartitionsRDD[1835] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:28:01 INFO TaskSetManager: Starting task 0.0 in stage 341.0 (TID 341) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:28:01 INFO Executor: Running task 0.0 in stage 341.0 (TID 341)\n",
      "25/05/05 11:28:01 INFO MemoryStore: Block broadcast_456 stored as values in memory (estimated size 16.2 KiB, free 366.2 MiB)\n",
      "25/05/05 11:28:01 INFO MemoryStore: Block broadcast_456_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.2 MiB)\n",
      "25/05/05 11:28:01 INFO BlockManagerInfo: Added broadcast_456_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:28:01 INFO SparkContext: Created broadcast 456 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:28:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 342 (MapPartitionsRDD[1835] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:28:01 INFO TaskSchedulerImpl: Adding task set 342.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:28:01 INFO TaskSetManager: Starting task 0.0 in stage 342.0 (TID 342) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:28:01 INFO Executor: Running task 0.0 in stage 342.0 (TID 342)\n",
      "25/05/05 11:28:01 INFO CodeGenerator: Code generated in 4.957791 ms\n",
      "25/05/05 11:28:01 INFO CodeGenerator: Code generated in 4.497541 ms\n",
      "25/05/05 11:28:01 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3345 untilOffset=3346, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=114 taskId=341 partitionId=0\n",
      "25/05/05 11:28:01 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3345 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:28:01 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3345 untilOffset=3346, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=114 taskId=342 partitionId=0\n",
      "25/05/05 11:28:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:28:01 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3345 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:28:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:28:01 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:28:01 INFO DAGScheduler: Got job 344 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:28:01 INFO DAGScheduler: Final stage: ResultStage 343 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:28:01 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:28:01 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:28:01 INFO DAGScheduler: Submitting ResultStage 343 (MapPartitionsRDD[1840] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:28:01 INFO MemoryStore: Block broadcast_457 stored as values in memory (estimated size 47.1 KiB, free 366.1 MiB)\n",
      "25/05/05 11:28:01 INFO MemoryStore: Block broadcast_457_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 366.1 MiB)\n",
      "25/05/05 11:28:01 INFO BlockManagerInfo: Added broadcast_457_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:28:01 INFO SparkContext: Created broadcast 457 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:28:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 343 (MapPartitionsRDD[1840] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:28:01 INFO TaskSchedulerImpl: Adding task set 343.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:28:01 INFO TaskSetManager: Starting task 0.0 in stage 343.0 (TID 343) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:28:01 INFO Executor: Running task 0.0 in stage 343.0 (TID 343)\n",
      "25/05/05 11:28:01 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3345 untilOffset=3346, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=114 taskId=343 partitionId=0\n",
      "25/05/05 11:28:01 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3345 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:28:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:28:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:28:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:28:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:28:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:28:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3346, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:28:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3346, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:28:01 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:28:01 INFO DataWritingSparkTask: Committed partition 0 (task 341, attempt 0, stage 341.0)\n",
      "25/05/05 11:28:01 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 503114375 nanos, during time span of 503516459 nanos.\n",
      "25/05/05 11:28:01 INFO Executor: Finished task 0.0 in stage 341.0 (TID 341). 2145 bytes result sent to driver\n",
      "25/05/05 11:28:01 INFO TaskSetManager: Finished task 0.0 in stage 341.0 (TID 341) in 513 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:28:01 INFO TaskSchedulerImpl: Removed TaskSet 341.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:28:01 INFO DAGScheduler: ResultStage 341 (start at NativeMethodAccessorImpl.java:0) finished in 0.515 s\n",
      "25/05/05 11:28:01 INFO DAGScheduler: Job 342 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:28:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 341: Stage finished\n",
      "25/05/05 11:28:01 INFO DAGScheduler: Job 342 finished: start at NativeMethodAccessorImpl.java:0, took 0.515459 s\n",
      "25/05/05 11:28:01 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 114, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:28:01 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:28:01 INFO DataWritingSparkTask: Committed partition 0 (task 342, attempt 0, stage 342.0)\n",
      "25/05/05 11:28:01 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 501753084 nanos, during time span of 503394833 nanos.\n",
      "25/05/05 11:28:01 INFO Executor: Finished task 0.0 in stage 342.0 (TID 342). 3516 bytes result sent to driver\n",
      "25/05/05 11:28:01 INFO TaskSetManager: Finished task 0.0 in stage 342.0 (TID 342) in 512 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:28:01 INFO TaskSchedulerImpl: Removed TaskSet 342.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:28:01 INFO DAGScheduler: ResultStage 342 (start at NativeMethodAccessorImpl.java:0) finished in 0.514 s\n",
      "25/05/05 11:28:01 INFO DAGScheduler: Job 343 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:28:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 342: Stage finished\n",
      "25/05/05 11:28:01 INFO DAGScheduler: Job 343 finished: start at NativeMethodAccessorImpl.java:0, took 0.515087 s\n",
      "25/05/05 11:28:01 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 114, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@7ce4f7f0] is committing.\n",
      "25/05/05 11:28:01 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 114, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@7ce4f7f0] committed.\n",
      "25/05/05 11:28:01 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 114, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:28:01 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/114 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.114.5fc69043-098e-40b7-85ef-fdcdf27f3f77.tmp\n",
      "25/05/05 11:28:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:28:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:28:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3346, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:28:01 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/114 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.114.ae01027e-9db2-4ef7-afc7-a8150f1af9cf.tmp\n",
      "25/05/05 11:28:01 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:28:01 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:28:01 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:28:01 INFO connection: Opened connection [connectionId{localValue:227, serverValue:4597}] to localhost:27017\n",
      "25/05/05 11:28:01 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=234916}\n",
      "25/05/05 11:28:01 INFO connection: Opened connection [connectionId{localValue:228, serverValue:4598}] to localhost:27017\n",
      "25/05/05 11:28:01 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:28:01 INFO connection: Closed connection [connectionId{localValue:228, serverValue:4598}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:28:01 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 501870125 nanos, during time span of 505711542 nanos.\n",
      "25/05/05 11:28:01 INFO Executor: Finished task 0.0 in stage 343.0 (TID 343). 1645 bytes result sent to driver\n",
      "25/05/05 11:28:01 INFO TaskSetManager: Finished task 0.0 in stage 343.0 (TID 343) in 511 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:28:01 INFO TaskSchedulerImpl: Removed TaskSet 343.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:28:01 INFO DAGScheduler: ResultStage 343 (start at NativeMethodAccessorImpl.java:0) finished in 0.514 s\n",
      "25/05/05 11:28:01 INFO DAGScheduler: Job 344 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:28:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 343: Stage finished\n",
      "25/05/05 11:28:01 INFO DAGScheduler: Job 344 finished: start at NativeMethodAccessorImpl.java:0, took 0.514728 s\n",
      "25/05/05 11:28:01 INFO MemoryStore: Block broadcast_458 stored as values in memory (estimated size 248.0 B, free 366.1 MiB)\n",
      "25/05/05 11:28:01 INFO MemoryStore: Block broadcast_458_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.1 MiB)\n",
      "25/05/05 11:28:01 INFO BlockManagerInfo: Added broadcast_458_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:28:01 INFO SparkContext: Created broadcast 458 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:28:01 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/114 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.114.cec43120-3acb-4e47-8f28-65f0b5e081b3.tmp\n",
      "25/05/05 11:28:01 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.114.5fc69043-098e-40b7-85ef-fdcdf27f3f77.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/114\n",
      "25/05/05 11:28:01 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:28:01.010Z\",\n",
      "  \"batchId\" : 114,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "  \"processedRowsPerSecond\" : 1.6339869281045751,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 526,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 612,\n",
      "    \"walCommit\" : 52\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3345\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3346\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3346\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 66.66666666666667,\n",
      "    \"processedRowsPerSecond\" : 1.6339869281045751,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:28:01 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.114.ae01027e-9db2-4ef7-afc7-a8150f1af9cf.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/114\n",
      "25/05/05 11:28:01 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:28:01.005Z\",\n",
      "  \"batchId\" : 114,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.6129032258064517,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 531,\n",
      "    \"commitOffsets\" : 26,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 620,\n",
      "    \"walCommit\" : 59\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3345\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3346\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3346\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.6129032258064517,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:28:01 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.114.cec43120-3acb-4e47-8f28-65f0b5e081b3.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/114\n",
      "25/05/05 11:28:01 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:28:01.005Z\",\n",
      "  \"batchId\" : 114,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.5873015873015872,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 543,\n",
      "    \"commitOffsets\" : 24,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 630,\n",
      "    \"walCommit\" : 58\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3345\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3346\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3346\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.5873015873015872,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 114\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R023|R045|00-00-00|34 ST-HERALD SQ|05/05/2025|11:27:59|REGULAR|8          |9         |2025-05-05 11:28:01.006|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:28:06 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/115 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.115.540aa400-9212-463d-ad1e-8a5eaa62a9d6.tmp\n",
      "25/05/05 11:28:06 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/115 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.115.fcf11886-c75e-4b40-b033-d36badfe1528.tmp\n",
      "25/05/05 11:28:06 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/115 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.115.bd528425-50e0-4ace-83e3-0355a41c0b48.tmp\n",
      "25/05/05 11:28:06 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/.115.540aa400-9212-463d-ad1e-8a5eaa62a9d6.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/offsets/115\n",
      "25/05/05 11:28:06 INFO MicroBatchExecution: Committed offsets for batch 115. Metadata OffsetSeqMetadata(0,1746458886384,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:28:06 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/.115.bd528425-50e0-4ace-83e3-0355a41c0b48.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/offsets/115\n",
      "25/05/05 11:28:06 INFO MicroBatchExecution: Committed offsets for batch 115. Metadata OffsetSeqMetadata(0,1746458886383,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:28:06 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/.115.fcf11886-c75e-4b40-b033-d36badfe1528.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/offsets/115\n",
      "25/05/05 11:28:06 INFO MicroBatchExecution: Committed offsets for batch 115. Metadata OffsetSeqMetadata(0,1746458886383,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 2))\n",
      "25/05/05 11:28:06 INFO IncrementalExecution: Current batch timestamp = 1746458886383\n",
      "25/05/05 11:28:06 INFO IncrementalExecution: Current batch timestamp = 1746458886384\n",
      "25/05/05 11:28:06 INFO IncrementalExecution: Current batch timestamp = 1746458886383\n",
      "25/05/05 11:28:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:28:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:28:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:28:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:28:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:28:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:28:06 INFO IncrementalExecution: Current batch timestamp = 1746458886384\n",
      "25/05/05 11:28:06 INFO IncrementalExecution: Current batch timestamp = 1746458886383\n",
      "25/05/05 11:28:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:28:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:28:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:28:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:28:06 INFO IncrementalExecution: Current batch timestamp = 1746458886383\n",
      "25/05/05 11:28:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:28:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:28:06 INFO IncrementalExecution: Current batch timestamp = 1746458886384\n",
      "25/05/05 11:28:06 INFO IncrementalExecution: Current batch timestamp = 1746458886383\n",
      "25/05/05 11:28:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:28:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:28:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:28:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
      "25/05/05 11:28:06 INFO CodeGenerator: Code generated in 3.901 ms\n",
      "25/05/05 11:28:06 INFO CodeGenerator: Code generated in 3.397833 ms\n",
      "25/05/05 11:28:06 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 115, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@10ea7fd2]. The input RDD has 1 partitions.\n",
      "25/05/05 11:28:06 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:28:06 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 115, writer: ConsoleWriter[numRows=20, truncate=false]]. The input RDD has 1 partitions.\n",
      "25/05/05 11:28:06 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:28:06 INFO DAGScheduler: Got job 345 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:28:06 INFO DAGScheduler: Final stage: ResultStage 344 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:28:06 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:28:06 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:28:06 INFO DAGScheduler: Submitting ResultStage 344 (MapPartitionsRDD[1850] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:28:06 INFO MemoryStore: Block broadcast_459 stored as values in memory (estimated size 16.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:28:06 INFO MemoryStore: Block broadcast_459_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 366.1 MiB)\n",
      "25/05/05 11:28:06 INFO BlockManagerInfo: Added broadcast_459_piece0 in memory on gopalas-laptop.lan:59335 (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:28:06 INFO SparkContext: Created broadcast 459 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:28:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 344 (MapPartitionsRDD[1850] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:28:06 INFO TaskSchedulerImpl: Adding task set 344.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:28:06 INFO DAGScheduler: Got job 346 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:28:06 INFO DAGScheduler: Final stage: ResultStage 345 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:28:06 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:28:06 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:28:06 INFO DAGScheduler: Submitting ResultStage 345 (MapPartitionsRDD[1851] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:28:06 INFO TaskSetManager: Starting task 0.0 in stage 344.0 (TID 344) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:28:06 INFO MemoryStore: Block broadcast_460 stored as values in memory (estimated size 15.2 KiB, free 366.1 MiB)\n",
      "25/05/05 11:28:06 INFO Executor: Running task 0.0 in stage 344.0 (TID 344)\n",
      "25/05/05 11:28:06 INFO MemoryStore: Block broadcast_460_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.0 MiB)\n",
      "25/05/05 11:28:06 INFO BlockManagerInfo: Added broadcast_460_piece0 in memory on gopalas-laptop.lan:59335 (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:28:06 INFO SparkContext: Created broadcast 460 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:28:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 345 (MapPartitionsRDD[1851] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:28:06 INFO TaskSchedulerImpl: Adding task set 345.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:28:06 INFO TaskSetManager: Starting task 0.0 in stage 345.0 (TID 345) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13058 bytes) \n",
      "25/05/05 11:28:06 INFO Executor: Running task 0.0 in stage 345.0 (TID 345)\n",
      "25/05/05 11:28:06 INFO CodeGenerator: Code generated in 3.621834 ms\n",
      "25/05/05 11:28:06 INFO CodeGenerator: Code generated in 3.396417 ms\n",
      "25/05/05 11:28:06 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3346 untilOffset=3347, for query queryId=15579493-3c33-4783-b8e7-e927b8c92a74 batchId=115 taskId=345 partitionId=0\n",
      "25/05/05 11:28:06 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3346 untilOffset=3347, for query queryId=5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa batchId=115 taskId=344 partitionId=0\n",
      "25/05/05 11:28:06 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to offset 3346 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:28:06 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to offset 3346 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:28:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:28:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:28:06 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:28:06 INFO DAGScheduler: Got job 347 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/05/05 11:28:06 INFO DAGScheduler: Final stage: ResultStage 346 (start at NativeMethodAccessorImpl.java:0)\n",
      "25/05/05 11:28:06 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/05/05 11:28:06 INFO DAGScheduler: Missing parents: List()\n",
      "25/05/05 11:28:06 INFO DAGScheduler: Submitting ResultStage 346 (MapPartitionsRDD[1856] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/05/05 11:28:06 INFO MemoryStore: Block broadcast_461 stored as values in memory (estimated size 47.1 KiB, free 366.0 MiB)\n",
      "25/05/05 11:28:06 INFO MemoryStore: Block broadcast_461_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 366.0 MiB)\n",
      "25/05/05 11:28:06 INFO BlockManagerInfo: Added broadcast_461_piece0 in memory on gopalas-laptop.lan:59335 (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:28:06 INFO SparkContext: Created broadcast 461 from broadcast at DAGScheduler.scala:1585\n",
      "25/05/05 11:28:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 346 (MapPartitionsRDD[1856] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/05/05 11:28:06 INFO TaskSchedulerImpl: Adding task set 346.0 with 1 tasks resource profile 0\n",
      "25/05/05 11:28:06 INFO TaskSetManager: Starting task 0.0 in stage 346.0 (TID 346) (gopalas-laptop.lan, executor driver, partition 0, PROCESS_LOCAL, 13056 bytes) \n",
      "25/05/05 11:28:06 INFO Executor: Running task 0.0 in stage 346.0 (TID 346)\n",
      "25/05/05 11:28:06 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=mta_turnstile_topic-0 fromOffset=3346 untilOffset=3347, for query queryId=158f6c2b-9de3-4554-a13e-4b3f3a8337e6 batchId=115 taskId=346 partitionId=0\n",
      "25/05/05 11:28:06 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to offset 3346 for partition mta_turnstile_topic-0\n",
      "25/05/05 11:28:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to earliest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:28:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:28:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:28:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:28:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:28:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3347, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:28:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3347, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:28:06 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:28:06 INFO DataWritingSparkTask: Committed partition 0 (task 345, attempt 0, stage 345.0)\n",
      "25/05/05 11:28:06 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor read 1 records through 1 polls (polled  out 1 records), taking 505717167 nanos, during time span of 506148250 nanos.\n",
      "25/05/05 11:28:06 INFO Executor: Finished task 0.0 in stage 345.0 (TID 345). 2145 bytes result sent to driver\n",
      "25/05/05 11:28:06 INFO TaskSetManager: Finished task 0.0 in stage 345.0 (TID 345) in 513 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:28:06 INFO TaskSchedulerImpl: Removed TaskSet 345.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:28:06 INFO DAGScheduler: ResultStage 345 (start at NativeMethodAccessorImpl.java:0) finished in 0.515 s\n",
      "25/05/05 11:28:06 INFO DAGScheduler: Job 346 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:28:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 345: Stage finished\n",
      "25/05/05 11:28:06 INFO DAGScheduler: Job 346 finished: start at NativeMethodAccessorImpl.java:0, took 0.516579 s\n",
      "25/05/05 11:28:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 115, writer: ConsoleWriter[numRows=20, truncate=false]] is committing.\n",
      "25/05/05 11:28:06 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "25/05/05 11:28:06 INFO DataWritingSparkTask: Committed partition 0 (task 344, attempt 0, stage 344.0)\n",
      "25/05/05 11:28:06 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor read 1 records through 1 polls (polled  out 1 records), taking 505112959 nanos, during time span of 506742959 nanos.\n",
      "25/05/05 11:28:06 INFO Executor: Finished task 0.0 in stage 344.0 (TID 344). 3516 bytes result sent to driver\n",
      "25/05/05 11:28:06 INFO TaskSetManager: Finished task 0.0 in stage 344.0 (TID 344) in 516 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:28:06 INFO TaskSchedulerImpl: Removed TaskSet 344.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:28:06 INFO DAGScheduler: ResultStage 344 (start at NativeMethodAccessorImpl.java:0) finished in 0.517 s\n",
      "25/05/05 11:28:06 INFO DAGScheduler: Job 345 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:28:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 344: Stage finished\n",
      "25/05/05 11:28:06 INFO DAGScheduler: Job 345 finished: start at NativeMethodAccessorImpl.java:0, took 0.517959 s\n",
      "25/05/05 11:28:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 115, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@10ea7fd2] is committing.\n",
      "25/05/05 11:28:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 115, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@10ea7fd2] committed.\n",
      "25/05/05 11:28:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 115, writer: ConsoleWriter[numRows=20, truncate=false]] committed.\n",
      "25/05/05 11:28:06 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/115 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.115.afb56ec3-f768-4af4-a6ed-62e8beedc316.tmp\n",
      "25/05/05 11:28:06 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/115 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.115.34eebb52-9858-4a2c-a4b9-9fa2a4ae67cc.tmp\n",
      "25/05/05 11:28:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:28:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Seeking to latest offset of partition mta_turnstile_topic-0\n",
      "25/05/05 11:28:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting offset for partition mta_turnstile_topic-0 to position FetchPosition{offset=3347, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 0 rack: null)], epoch=0}}.\n",
      "25/05/05 11:28:06 INFO cluster: Cluster created with settings {hosts=[localhost:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms'}\n",
      "25/05/05 11:28:06 INFO MongoClientCache: Creating MongoClient: [localhost:27017]\n",
      "25/05/05 11:28:06 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "25/05/05 11:28:06 INFO connection: Opened connection [connectionId{localValue:229, serverValue:4599}] to localhost:27017\n",
      "25/05/05 11:28:06 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=localhost:27017, type=STANDALONE, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=161833}\n",
      "25/05/05 11:28:06 INFO connection: Opened connection [connectionId{localValue:230, serverValue:4600}] to localhost:27017\n",
      "25/05/05 11:28:06 INFO MongoClientCache: Closing MongoClient: [localhost:27017]\n",
      "25/05/05 11:28:06 INFO connection: Closed connection [connectionId{localValue:230, serverValue:4600}] to localhost:27017 because the pool has been closed.\n",
      "25/05/05 11:28:06 INFO KafkaDataConsumer: From Kafka topicPartition=mta_turnstile_topic-0 groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor read 1 records through 1 polls (polled  out 1 records), taking 506286584 nanos, during time span of 509836375 nanos.\n",
      "25/05/05 11:28:06 INFO Executor: Finished task 0.0 in stage 346.0 (TID 346). 1645 bytes result sent to driver\n",
      "25/05/05 11:28:06 INFO TaskSetManager: Finished task 0.0 in stage 346.0 (TID 346) in 515 ms on gopalas-laptop.lan (executor driver) (1/1)\n",
      "25/05/05 11:28:06 INFO TaskSchedulerImpl: Removed TaskSet 346.0, whose tasks have all completed, from pool \n",
      "25/05/05 11:28:06 INFO DAGScheduler: ResultStage 346 (start at NativeMethodAccessorImpl.java:0) finished in 0.522 s\n",
      "25/05/05 11:28:06 INFO DAGScheduler: Job 347 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/05/05 11:28:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 346: Stage finished\n",
      "25/05/05 11:28:06 INFO DAGScheduler: Job 347 finished: start at NativeMethodAccessorImpl.java:0, took 0.522462 s\n",
      "25/05/05 11:28:06 INFO MemoryStore: Block broadcast_462 stored as values in memory (estimated size 248.0 B, free 366.0 MiB)\n",
      "25/05/05 11:28:06 INFO MemoryStore: Block broadcast_462_piece0 stored as bytes in memory (estimated size 413.0 B, free 366.0 MiB)\n",
      "25/05/05 11:28:06 INFO BlockManagerInfo: Added broadcast_462_piece0 in memory on gopalas-laptop.lan:59335 (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:28:06 INFO SparkContext: Created broadcast 462 from start at NativeMethodAccessorImpl.java:0\n",
      "25/05/05 11:28:06 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/.115.afb56ec3-f768-4af4-a6ed-62e8beedc316.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-5ed78dd3-63f7-4e36-9a00-0b514cfa4dc6/commits/115\n",
      "25/05/05 11:28:06 INFO CheckpointFileManager: Writing atomically to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/115 using temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.115.d4cdfff7-fafe-4520-9e1a-dc0731b92543.tmp\n",
      "25/05/05 11:28:06 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"5f5dffda-cafd-4a97-a970-5f0f9bb4b9fa\",\n",
      "  \"runId\" : \"f332de59-d50a-450a-89c9-a1e60a2a0e5a\",\n",
      "  \"name\" : \"live_turnstile\",\n",
      "  \"timestamp\" : \"2025-05-05T15:28:06.382Z\",\n",
      "  \"batchId\" : 115,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.6260162601626016,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 528,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 3,\n",
      "    \"triggerExecution\" : 615,\n",
      "    \"walCommit\" : 55\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3346\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3347\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3347\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.6260162601626016,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"MemorySink\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:28:07 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/.115.34eebb52-9858-4a2c-a4b9-9fa2a4ae67cc.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-bc2e5356-56e4-4957-aa2c-12ef0ace60da/commits/115\n",
      "25/05/05 11:28:07 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"15579493-3c33-4783-b8e7-e927b8c92a74\",\n",
      "  \"runId\" : \"6025a43e-17ac-47e3-b17e-2ebfc0d8e559\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:28:06.382Z\",\n",
      "  \"batchId\" : 115,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "  \"processedRowsPerSecond\" : 1.6155088852988693,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 534,\n",
      "    \"commitOffsets\" : 26,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 2,\n",
      "    \"queryPlanning\" : 5,\n",
      "    \"triggerExecution\" : 619,\n",
      "    \"walCommit\" : 52\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3346\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3347\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3347\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 83.33333333333333,\n",
      "    \"processedRowsPerSecond\" : 1.6155088852988693,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3bbf8199\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "25/05/05 11:28:07 INFO CheckpointFileManager: Renamed temp file file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/.115.d4cdfff7-fafe-4520-9e1a-dc0731b92543.tmp to file:/private/var/folders/48/hk2s0t4x1xv5gpns58fpsx1m0000gn/T/temporary-886a90c6-9a58-417a-a60c-81be3f85b37c/commits/115\n",
      "25/05/05 11:28:07 INFO MicroBatchExecution: Streaming query made progress: {\n",
      "  \"id\" : \"158f6c2b-9de3-4554-a13e-4b3f3a8337e6\",\n",
      "  \"runId\" : \"0a203767-4c11-40ea-b6d3-4932e8d107ad\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-05-05T15:28:06.382Z\",\n",
      "  \"batchId\" : 115,\n",
      "  \"numInputRows\" : 1,\n",
      "  \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "  \"processedRowsPerSecond\" : 1.5822784810126582,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 550,\n",
      "    \"commitOffsets\" : 23,\n",
      "    \"getBatch\" : 0,\n",
      "    \"latestOffset\" : 1,\n",
      "    \"queryPlanning\" : 4,\n",
      "    \"triggerExecution\" : 632,\n",
      "    \"walCommit\" : 54\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"KafkaV2[Subscribe[mta_turnstile_topic]]\",\n",
      "    \"startOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3346\n",
      "      }\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3347\n",
      "      }\n",
      "    },\n",
      "    \"latestOffset\" : {\n",
      "      \"mta_turnstile_topic\" : {\n",
      "        \"0\" : 3347\n",
      "      }\n",
      "    },\n",
      "    \"numInputRows\" : 1,\n",
      "    \"inputRowsPerSecond\" : 76.92307692307692,\n",
      "    \"processedRowsPerSecond\" : 1.5822784810126582,\n",
      "    \"metrics\" : {\n",
      "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
      "      \"maxOffsetsBehindLatest\" : \"0\",\n",
      "      \"minOffsetsBehindLatest\" : \"0\"\n",
      "    }\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"ForeachBatchSink\",\n",
      "    \"numOutputRows\" : -1\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 115\n",
      "-------------------------------------------\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|C/A |UNIT|SCP     |STATION        |DATE      |TIME    |DESC   |ENTRY_COUNT|EXIT_COUNT|ingest_time            |\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "|R023|R045|00-00-00|34 ST-HERALD SQ|05/05/2025|11:28:05|REGULAR|5          |14        |2025-05-05 11:28:06.384|\n",
      "+----+----+--------+---------------+----------+--------+-------+-----------+----------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 11:28:09 INFO BlockManagerInfo: Removed broadcast_458_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:28:09 INFO BlockManagerInfo: Removed broadcast_461_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.9 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:28:09 INFO BlockManagerInfo: Removed broadcast_459_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:28:09 INFO BlockManagerInfo: Removed broadcast_460_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:28:09 INFO BlockManagerInfo: Removed broadcast_456_piece0 on gopalas-laptop.lan:59335 in memory (size: 6.3 KiB, free: 366.2 MiB)\n",
      "25/05/05 11:28:09 INFO BlockManagerInfo: Removed broadcast_462_piece0 on gopalas-laptop.lan:59335 in memory (size: 413.0 B, free: 366.2 MiB)\n",
      "25/05/05 11:28:09 INFO BlockManagerInfo: Removed broadcast_457_piece0 on gopalas-laptop.lan:59335 in memory (size: 16.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:28:09 INFO BlockManagerInfo: Removed broadcast_455_piece0 on gopalas-laptop.lan:59335 in memory (size: 5.8 KiB, free: 366.3 MiB)\n",
      "25/05/05 11:28:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:28:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:28:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:28:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:28:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:28:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:28:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:28:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:28:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:28:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:28:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:28:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:28:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:28:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:28:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:29:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:29:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:29:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:29:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:29:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:29:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:29:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:29:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:29:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:29:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:29:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:29:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:29:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:29:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:29:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:29:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:29:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:29:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:30:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:30:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:30:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:30:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:30:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:30:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:30:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:30:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:30:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:30:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:30:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:30:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:30:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:30:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:30:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:30:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:30:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:30:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:31:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:31:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:31:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:31:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:31:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:31:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:31:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:31:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:31:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:31:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:31:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:31:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:31:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:31:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:31:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:31:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:31:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:31:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:32:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:32:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:32:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:32:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:32:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:32:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:32:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:32:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:32:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:32:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:32:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:32:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:32:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:32:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:32:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:32:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:32:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:32:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:33:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:33:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:33:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:33:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:33:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:33:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:33:17 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Resetting generation and member id due to: consumer pro-actively leaving the group\n",
      "25/05/05 11:33:17 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1, groupId=spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor] Request joining group due to: consumer pro-actively leaving the group\n",
      "25/05/05 11:33:17 INFO Metrics: Metrics scheduler closed\n",
      "25/05/05 11:33:17 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter\n",
      "25/05/05 11:33:17 INFO Metrics: Metrics reporters closed\n",
      "25/05/05 11:33:17 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-94e3471f-822d-45b0-969c-25a6f0ba2c4e--642622332-executor-1 unregistered\n",
      "25/05/05 11:33:17 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Resetting generation and member id due to: consumer pro-actively leaving the group\n",
      "25/05/05 11:33:17 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2, groupId=spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor] Request joining group due to: consumer pro-actively leaving the group\n",
      "25/05/05 11:33:17 INFO Metrics: Metrics scheduler closed\n",
      "25/05/05 11:33:17 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter\n",
      "25/05/05 11:33:17 INFO Metrics: Metrics reporters closed\n",
      "25/05/05 11:33:17 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-6031fef8-68c2-46b3-b822-edab567f2440-1946334870-executor-2 unregistered\n",
      "25/05/05 11:33:17 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Resetting generation and member id due to: consumer pro-actively leaving the group\n",
      "25/05/05 11:33:17 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3, groupId=spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor] Request joining group due to: consumer pro-actively leaving the group\n",
      "25/05/05 11:33:17 INFO Metrics: Metrics scheduler closed\n",
      "25/05/05 11:33:17 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter\n",
      "25/05/05 11:33:17 INFO Metrics: Metrics reporters closed\n",
      "25/05/05 11:33:17 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-6a8d3f1a-ea83-4fb8-b856-c9411cf90ca9-22346129-executor-3 unregistered\n",
      "25/05/05 11:33:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:33:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:33:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:33:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:33:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:33:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:33:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:33:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:33:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:33:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:33:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:33:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:34:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:34:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:34:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:50:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:50:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:50:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:50:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:50:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:50:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:50:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:50:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:50:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:50:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:50:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:50:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:50:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:50:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:50:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:50:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:50:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:50:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:51:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:51:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:51:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:51:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:51:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:51:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:51:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:51:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:51:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:51:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:51:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:51:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:51:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:51:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:51:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:51:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:51:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:51:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:52:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:52:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:52:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:52:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:52:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:52:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:52:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:52:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:52:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:52:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:52:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:52:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:52:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:52:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:52:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:52:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:52:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:52:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:53:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:53:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:53:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:53:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:53:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n",
      "25/05/05 11:53:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Define output sinks\n",
    "\n",
    "# 1. Console Sink \n",
    "query_console = raw_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()\n",
    "\n",
    "# 2. MongoDB Sink\n",
    "\n",
    "def write_to_mongo(batch_df, batch_id):\n",
    "    batch_df.write \\\n",
    "        .format(\"mongo\") \\\n",
    "        .option(\"uri\", \"mongodb://localhost:27017/mta_db.raw_turnstile\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "\n",
    "query_mongo = raw_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(write_to_mongo) \\\n",
    "    .start()\n",
    "\n",
    "# 3. Memory Sink (optional)\n",
    "query_memory = raw_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"live_turnstile\") \\\n",
    "    .start()\n",
    "\n",
    "# Wait for all queries to complete\n",
    "query_console.awaitTermination()\n",
    "query_mongo.awaitTermination()\n",
    "query_memory.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb1b2fb-204d-4f95-8240-660c58795986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
